<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DEFINING custom_loss FUNCTION with Standard Lambda = 0.5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d4be639c637f3db3c684c66cefad7e0c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DEFINING custom_loss FUNCTION with Standard Lambda = 0.5</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="cell-1" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime, timedelta</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, <span class="st">'../'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.datasets <span class="im">import</span> MEPSDataset19</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Fairness metrics</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.metrics <span class="im">import</span> BinaryLabelDatasetMetric</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Explainers</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.explainers <span class="im">import</span> MetricTextExplainer</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalers</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Bias mitigation techniques</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> Reweighing,DisparateImpactRemover</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> LFR</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> OptimPreproc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-2" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-3" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19 <span class="op">=</span> MEPSDataset19()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-4" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train <span class="op">=</span> MEPSDataset19()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train.features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>array([[53.  ,  1.  , 25.93, ...,  0.  ,  1.  ,  0.  ],
       [56.  ,  1.  , 20.42, ...,  0.  ,  1.  ,  0.  ],
       [23.  ,  1.  , 53.12, ...,  0.  ,  1.  ,  0.  ],
       ...,
       [ 2.  ,  1.  , -1.  , ...,  0.  ,  1.  ,  0.  ],
       [54.  ,  0.  , 43.97, ...,  0.  ,  1.  ,  0.  ],
       [73.  ,  0.  , 42.68, ...,  0.  ,  1.  ,  0.  ]])</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sens_ind <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sens_attr <span class="op">=</span> dataset_orig_panel19_train.protected_attribute_names[sens_ind]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>privileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>sens_attr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>'RACE'</code></pre>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>privileged_groups</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>[{'RACE': 1.0}]</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>unprivileged_groups</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>[{'RACE': 0.0}]</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>               instance weights features                                    \
                                         protected attribute                 
                                     AGE                RACE  PCS42  MCS42   
instance names                                                               
0                  21854.981705     53.0                 1.0  25.93  58.47   
1                  18169.604822     56.0                 1.0  20.42  26.57   
3                  17191.832515     23.0                 1.0  53.12  50.33   
4                  20261.485463      3.0                 1.0  -1.00  -1.00   
5                      0.000000     27.0                 0.0  -1.00  -1.00   
...                         ...      ...                 ...    ...    ...   
16573               4111.315754     25.0                 0.0  56.71  62.39   
16574               5415.228173     25.0                 0.0  56.71  62.39   
16575               3896.116219      2.0                 1.0  -1.00  -1.00   
16576               4883.851005     54.0                 0.0  43.97  42.45   
16577               6630.588948     73.0                 0.0  42.68  43.46   

                                                            ...          \
                                                            ...           
               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   
instance names                                              ...           
0                  3.0      0.0      1.0      0.0      0.0  ...     1.0   
1                 17.0      0.0      1.0      0.0      0.0  ...     1.0   
3                  7.0      0.0      1.0      0.0      0.0  ...     0.0   
4                 -1.0      0.0      1.0      0.0      0.0  ...     0.0   
5                 -1.0      0.0      0.0      1.0      0.0  ...     0.0   
...                ...      ...      ...      ...      ...  ...     ...   
16573              0.0      0.0      0.0      1.0      0.0  ...     0.0   
16574              0.0      0.0      0.0      1.0      0.0  ...     1.0   
16575             -1.0      0.0      0.0      1.0      0.0  ...     0.0   
16576             24.0      1.0      0.0      0.0      0.0  ...     0.0   
16577              0.0      1.0      0.0      0.0      0.0  ...     1.0   

                                                                               \
                                                                                
               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   
instance names                                                                  
0                   1.0      0.0      0.0      0.0      0.0      0.0      1.0   
1                   0.0      0.0      1.0      0.0      0.0      0.0      1.0   
3                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   
4                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   
5                   0.0      0.0      1.0      0.0      0.0      1.0      0.0   
...                 ...      ...      ...      ...      ...      ...      ...   
16573               1.0      0.0      0.0      0.0      0.0      1.0      0.0   
16574               1.0      0.0      0.0      0.0      0.0      1.0      0.0   
16575               1.0      0.0      0.0      0.0      0.0      0.0      1.0   
16576               0.0      0.0      1.0      0.0      0.0      0.0      1.0   
16577               0.0      0.0      1.0      0.0      0.0      0.0      1.0   

                        labels  
                                
               INSCOV=3         
instance names                  
0                   0.0    1.0  
1                   0.0    1.0  
3                   0.0    0.0  
4                   0.0    0.0  
5                   0.0    0.0  
...                 ...    ...  
16573               0.0    0.0  
16574               0.0    0.0  
16575               0.0    0.0  
16576               0.0    0.0  
16577               0.0    0.0  

[15830 rows x 140 columns]</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>metric_orig_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>        dataset_orig_panel19_train,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        privileged_groups<span class="op">=</span>privileged_groups)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-13" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>explainer_orig_panel19_train <span class="op">=</span> MetricTextExplainer(metric_orig_panel19_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>test_name<span class="op">=</span>[<span class="st">'Mean Difference'</span>,<span class="st">'Consistency'</span>,<span class="st">'Statistical Parity Difference'</span>,<span class="st">'Disparate Impact'</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>test_definitions<span class="op">=</span>[<span class="st">'difference between mean values of two labels'</span>,<span class="st">'Individual fairness metric that measures how similar the labels are for similar instances.'</span>,<span class="st">'Difference in selection rates.'</span>,<span class="st">'ratio of positive outcomes in the unprivileged group divided by the ratio of positive outcomes in the privileged group.'</span>]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[explainer_orig_panel19_train.mean_difference(),explainer_orig_panel19_train.consistency(),explainer_orig_panel19_train.statistical_parity_difference(),explainer_orig_panel19_train.disparate_impact()]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>test_status<span class="op">=</span>[<span class="st">'Bias Detected'</span>,<span class="st">'Bias Not Detected'</span>,<span class="st">'Bias Detected'</span>,<span class="st">'Bias Detected'</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame({<span class="st">'Test Name'</span>:test_name,<span class="st">'Test Definitions'</span>:test_definitions,<span class="st">'Test Results'</span>:test_results,<span class="st">'Test Status'</span>:test_status})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>test_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',
 'Consistency (Zemel, et al. 2013): [0.83665193]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>RW <span class="op">=</span> Reweighing(unprivileged_groups<span class="op">=</span>unprivileged_groups,privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_rw <span class="op">=</span> RW.fit_transform(dataset_orig_panel19_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-18" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Statistical Parity Difference</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>statistical_parity_difference <span class="op">=</span> metric_orig_panel19_train.statistical_parity_difference()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Statistical Parity Difference (SPD):"</span>, statistical_parity_difference)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Disparate Impact</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>disparate_impact <span class="op">=</span> metric_orig_panel19_train.disparate_impact()</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Disparate Impact (DI):"</span>, disparate_impact)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Statistical Parity Difference (SPD): -0.13507447726478142
Disparate Impact (DI): 0.49826823461176517</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>X_train[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>array([37.  ,  0.  , 48.09, 36.94,  7.  ,  0.  ,  0.  ,  1.  ,  0.  ,
        1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
        0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,
        0.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,
        1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
        1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,
        0.  ,  0.  ,  1.  ])</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dataset_orig_panel19_train.features</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dataset_orig_panel19_train.labels.ravel()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale features</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Train logistic regression model</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_scaled, y_train)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities on the same scaled training data</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>train_probabilities <span class="op">=</span> model.predict_proba(X_train_scaled)[:, <span class="dv">1</span>]</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculation of discrimination index without modifying dataset structure</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>sens_attr_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_discrimination(X, probabilities, sens_attr_index, unprivileged_val, privileged_val):</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter by sensitive attribute for unprivileged and privileged groups</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    unpriv_indices <span class="op">=</span> X[:, sens_attr_index] <span class="op">==</span> unprivileged_val</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    priv_indices <span class="op">=</span> X[:, sens_attr_index] <span class="op">==</span> privileged_val</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean probabilities for both groups</span></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    mean_prob_unpriv <span class="op">=</span> probabilities[unpriv_indices].mean()</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    mean_prob_priv <span class="op">=</span> probabilities[priv_indices].mean()</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discrimination index</span></span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> mean_prob_priv <span class="op">-</span> mean_prob_unpriv</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> discrimination</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Define unprivileged and privileged values</span></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>unprivileged_val <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>privileged_val <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute discrimination</span></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>discrimination_index <span class="op">=</span> calculate_discrimination(X_train, train_probabilities, sens_attr_index, unprivileged_val, privileged_val)</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Discrimination Index: </span><span class="sc">{:.4f}</span><span class="st">"</span>.<span class="bu">format</span>(discrimination_index))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Discrimination Index: 0.1315</code></pre>
</div>
</div>
<div id="cell-22" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>X_train_scaled</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>array([[-0.76152525, -0.7401269 , -1.16660499, ..., -1.06074916,
        -0.744093  ,  2.78531021],
       [ 0.08369436, -0.7401269 ,  0.72413337, ..., -1.06074916,
        -0.744093  ,  2.78531021],
       [-0.45012855, -0.7401269 ,  0.94174738, ...,  0.94272994,
        -0.744093  , -0.35902644],
       ...,
       [ 0.79545824,  1.35111965,  0.09517111, ..., -1.06074916,
         1.34391803, -0.35902644],
       [-0.09424661, -0.7401269 ,  1.09658071, ...,  0.94272994,
        -0.744093  , -0.35902644],
       [-0.62806952, -0.7401269 ,  0.4680036 , ...,  0.94272994,
        -0.744093  , -0.35902644]])</code></pre>
</div>
</div>
<div id="cell-23" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)  <span class="co"># Adjust input size to exclude sensitive attribute</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom loss function</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(output, target, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.1</span>, k<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.<span class="bu">sum</span>(mask_unpriv) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> torch.<span class="bu">sum</span>(mask_priv) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> torch.mean(output[mask_unpriv])</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> torch.mean(output[mask_priv])</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)  <span class="co"># Handle cases where one group might be missing</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming your dataset is loaded correctly</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()  <span class="co"># Make sure this conversion is done correctly</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)  <span class="co"># Ensure targets are correctly shaped</span></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]  <span class="co"># Extract the sensitive features</span></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Exclude the sensitive attribute from the main features</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> custom_loss(outputs, targets.squeeze(), sensitive_features)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Loss: 0.6436237692832947
Epoch 2, Loss: 0.6127090454101562
Epoch 3, Loss: 0.5840549468994141
Epoch 4, Loss: 0.557695209980011
Epoch 5, Loss: 0.5335997343063354
Epoch 6, Loss: 0.5117872953414917
Epoch 7, Loss: 0.49214401841163635
Epoch 8, Loss: 0.47453635931015015
Epoch 9, Loss: 0.45879927277565
Epoch 10, Loss: 0.44474470615386963</code></pre>
</div>
</div>
<div id="cell-24" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom loss function</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(output, target, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.01</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.<span class="bu">sum</span>(mask_unpriv) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> torch.<span class="bu">sum</span>(mask_priv) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> torch.mean(output[mask_unpriv])</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> torch.mean(output[mask_priv])</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)  <span class="co"># Handle cases where one group might be missing</span></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_accuracy(predictions, targets):</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> (predictions <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (predicted_classes <span class="op">==</span> targets).<span class="bu">float</span>().mean()</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation</span></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Test data (assuming it's prepared similarly)</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> torch.tensor(X_test_scaled).<span class="bu">float</span>()</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>test_targets <span class="op">=</span> torch.tensor(y_test).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>test_sensitive_features <span class="op">=</span> test_data[:, <span class="dv">1</span>]</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> torch.cat((test_data[:, :<span class="dv">1</span>], test_data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> custom_loss(outputs, targets.squeeze(), sensitive_features)</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model(test_features)</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">=</span> custom_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, '</span></span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>    model.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.7673807740211487, Train Acc: 24.21%, Test Loss: 0.7409172654151917, Test Acc: 31.46%
Epoch 2, Train Loss: 0.7404283881187439, Train Acc: 32.01%, Test Loss: 0.716571569442749, Test Acc: 41.63%
Epoch 3, Train Loss: 0.7157283425331116, Train Acc: 41.29%, Test Loss: 0.6942327618598938, Test Acc: 53.85%
Epoch 4, Train Loss: 0.693112313747406, Train Acc: 53.62%, Test Loss: 0.6738004684448242, Test Acc: 67.66%
Epoch 5, Train Loss: 0.6722973585128784, Train Acc: 68.04%, Test Loss: 0.6548610329627991, Test Acc: 74.76%
Epoch 6, Train Loss: 0.6529971957206726, Train Acc: 75.09%, Test Loss: 0.637105405330658, Test Acc: 77.51%
Epoch 7, Train Loss: 0.6348973512649536, Train Acc: 78.02%, Test Loss: 0.6203395128250122, Test Acc: 79.41%
Epoch 8, Train Loss: 0.6178111433982849, Train Acc: 79.60%, Test Loss: 0.6044129729270935, Test Acc: 80.51%
Epoch 9, Train Loss: 0.6016006469726562, Train Acc: 80.84%, Test Loss: 0.589247465133667, Test Acc: 81.30%
Epoch 10, Train Loss: 0.5861577391624451, Train Acc: 81.67%, Test Loss: 0.5747091770172119, Test Acc: 81.87%</code></pre>
</div>
</div>
<div id="cell-25" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize binary cross-entropy loss</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_accuracy(predictions, targets):</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> (predictions <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (predicted_classes <span class="op">==</span> targets).<span class="bu">float</span>().mean()</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Test data (assuming it's prepared similarly)</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> torch.tensor(X_test_scaled).<span class="bu">float</span>()</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>test_targets <span class="op">=</span> torch.tensor(y_test).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>test_sensitive_features <span class="op">=</span> test_data[:, <span class="dv">1</span>]</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> torch.cat((test_data[:, :<span class="dv">1</span>], test_data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(outputs, targets.squeeze())</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model(test_features)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">=</span> criterion(test_outputs, test_targets.squeeze())</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, '</span></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>    model.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.7674269676208496, Train Acc: 34.36%, Test Loss: 0.7305501103401184, Test Acc: 39.80%
Epoch 2, Train Loss: 0.7305780053138733, Train Acc: 40.48%, Test Loss: 0.6975762248039246, Test Acc: 49.31%
Epoch 3, Train Loss: 0.6973879337310791, Train Acc: 49.25%, Test Loss: 0.668036162853241, Test Acc: 66.20%
Epoch 4, Train Loss: 0.6675889492034912, Train Acc: 66.80%, Test Loss: 0.6415244936943054, Test Acc: 75.43%
Epoch 5, Train Loss: 0.6408513188362122, Train Acc: 75.71%, Test Loss: 0.61774742603302, Test Acc: 78.68%
Epoch 6, Train Loss: 0.6167588233947754, Train Acc: 78.40%, Test Loss: 0.5962265729904175, Test Acc: 80.39%
Epoch 7, Train Loss: 0.5949429273605347, Train Acc: 80.14%, Test Loss: 0.5766510367393494, Test Acc: 81.40%
Epoch 8, Train Loss: 0.5751211643218994, Train Acc: 81.06%, Test Loss: 0.5587018132209778, Test Acc: 81.84%
Epoch 9, Train Loss: 0.5569527745246887, Train Acc: 81.71%, Test Loss: 0.5421833395957947, Test Acc: 82.25%
Epoch 10, Train Loss: 0.5401964783668518, Train Acc: 82.35%, Test Loss: 0.5268954634666443, Test Acc: 82.72%</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset, random_split</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper function to calculate accuracy</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> binary_accuracy(y_pred, y_true):</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applying threshold to get binary output</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    y_pred_tag <span class="op">=</span> torch.<span class="bu">round</span>(y_pred)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    correct_results_sum <span class="op">=</span> (y_pred_tag <span class="op">==</span> y_true).<span class="bu">sum</span>().<span class="bu">float</span>()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> correct_results_sum <span class="op">/</span> y_true.shape[<span class="dv">0</span>]</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> torch.<span class="bu">round</span>(acc <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom discrimination loss function</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discrimination_loss(outputs, targets, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.5</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    prob_unpriv <span class="op">=</span> torch.mean(outputs[mask_unpriv])</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    prob_priv <span class="op">=</span> torch.mean(outputs[mask_priv])</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple binary crossentropy loss model for comparison</span></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleBinaryClassifier(nn.Module):</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleBinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare datasets</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(features, targets, sensitive_features)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model and optimizer</span></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> train_loader:</span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> custom_loss:</span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a>            train_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> val_loader:</span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs)</span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> custom_loss:</span>
<span id="cb36-91"><a href="#cb36-91" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> loss.item()</span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a>                val_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average loss and accuracy</span></span>
<span id="cb36-98"><a href="#cb36-98" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb36-99"><a href="#cb36-99" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">, Val Acc: </span><span class="sc">{</span>val_acc<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb36-104"><a href="#cb36-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-105"><a href="#cb36-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with discrimination loss</span></span>
<span id="cb36-106"><a href="#cb36-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-107"><a href="#cb36-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Train another model with only binary crossentropy</span></span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span>discrimination_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: nan, Train Acc: 85.77, Val Loss: nan, Val Acc: 85.97
Epoch 2, Train Loss: nan, Train Acc: 86.90, Val Loss: nan, Val Acc: 86.05
Epoch 3, Train Loss: nan, Train Acc: 87.13, Val Loss: nan, Val Acc: 86.30
Epoch 4, Train Loss: nan, Train Acc: 87.40, Val Loss: nan, Val Acc: 86.24
Epoch 5, Train Loss: nan, Train Acc: 87.93, Val Loss: nan, Val Acc: 86.33
Epoch 6, Train Loss: nan, Train Acc: 88.23, Val Loss: nan, Val Acc: 86.01
Epoch 7, Train Loss: nan, Train Acc: 88.21, Val Loss: nan, Val Acc: 86.25
Epoch 8, Train Loss: nan, Train Acc: 88.37, Val Loss: nan, Val Acc: 86.17
Epoch 9, Train Loss: nan, Train Acc: 88.96, Val Loss: nan, Val Acc: 85.75
Epoch 10, Train Loss: nan, Train Acc: 88.86, Val Loss: nan, Val Acc: 85.99</code></pre>
</div>
</div>
<div id="cell-28" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>simple_model <span class="op">=</span> SimpleBinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>simple_optimizer <span class="op">=</span> optim.Adam(simple_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>train_model(simple_model, simple_optimizer, train_loader, val_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.3526, Train Acc: 85.74, Val Loss: 0.3346, Val Acc: 86.34
Epoch 2, Train Loss: 0.3199, Train Acc: 86.70, Val Loss: 0.3338, Val Acc: 85.76
Epoch 3, Train Loss: 0.3129, Train Acc: 87.31, Val Loss: 0.3343, Val Acc: 86.40
Epoch 4, Train Loss: 0.3095, Train Acc: 87.41, Val Loss: 0.3304, Val Acc: 86.11
Epoch 5, Train Loss: 0.3047, Train Acc: 87.72, Val Loss: 0.3305, Val Acc: 86.55
Epoch 6, Train Loss: 0.2998, Train Acc: 88.05, Val Loss: 0.3306, Val Acc: 86.38
Epoch 7, Train Loss: 0.2960, Train Acc: 88.30, Val Loss: 0.3326, Val Acc: 86.10
Epoch 8, Train Loss: 0.2913, Train Acc: 88.62, Val Loss: 0.3351, Val Acc: 85.94
Epoch 9, Train Loss: 0.2864, Train Acc: 88.95, Val Loss: 0.3380, Val Acc: 86.04
Epoch 10, Train Loss: 0.2806, Train Acc: 89.21, Val Loss: 0.3388, Val Acc: 85.90</code></pre>
</div>
</div>
<div id="cell-29" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset, random_split</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper function to calculate accuracy</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> binary_accuracy(y_pred, y_true):</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applying threshold to get binary output</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    y_pred_tag <span class="op">=</span> torch.<span class="bu">round</span>(y_pred)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    correct_results_sum <span class="op">=</span> (y_pred_tag <span class="op">==</span> y_true).<span class="bu">sum</span>().<span class="bu">float</span>()</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> correct_results_sum <span class="op">/</span> y_true.shape[<span class="dv">0</span>]</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> torch.<span class="bu">round</span>(acc <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom discrimination loss function</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discrimination_loss(outputs, targets, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.5</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>    prob_unpriv <span class="op">=</span> torch.mean(outputs[mask_unpriv])</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>    prob_priv <span class="op">=</span> torch.mean(outputs[mask_priv])</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple binary crossentropy loss model for comparison</span></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleBinaryClassifier(nn.Module):</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleBinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]  <span class="co"># Extract the sensitive feature</span></span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Exclude the sensitive attribute</span></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup DataLoader</span></span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(features, targets, sensitive_features)</span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-65"><a href="#cb41-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-66"><a href="#cb41-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model and optimizer</span></span>
<span id="cb41-67"><a href="#cb41-67" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb41-68"><a href="#cb41-68" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb41-69"><a href="#cb41-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-70"><a href="#cb41-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb41-71"><a href="#cb41-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-72"><a href="#cb41-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb41-73"><a href="#cb41-73" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb41-74"><a href="#cb41-74" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-75"><a href="#cb41-75" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-76"><a href="#cb41-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> train_loader:</span>
<span id="cb41-77"><a href="#cb41-77" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb41-78"><a href="#cb41-78" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb41-79"><a href="#cb41-79" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> custom_loss:</span>
<span id="cb41-80"><a href="#cb41-80" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb41-81"><a href="#cb41-81" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb41-82"><a href="#cb41-82" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb41-83"><a href="#cb41-83" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb41-84"><a href="#cb41-84" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb41-85"><a href="#cb41-85" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb41-86"><a href="#cb41-86" aria-hidden="true" tabindex="-1"></a>            train_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb41-87"><a href="#cb41-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-88"><a href="#cb41-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb41-89"><a href="#cb41-89" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb41-90"><a href="#cb41-90" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-91"><a href="#cb41-91" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-92"><a href="#cb41-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-93"><a href="#cb41-93" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> val_loader:</span>
<span id="cb41-94"><a href="#cb41-94" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs)</span>
<span id="cb41-95"><a href="#cb41-95" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> custom_loss:</span>
<span id="cb41-96"><a href="#cb41-96" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb41-97"><a href="#cb41-97" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb41-98"><a href="#cb41-98" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb41-99"><a href="#cb41-99" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> loss.item()</span>
<span id="cb41-100"><a href="#cb41-100" aria-hidden="true" tabindex="-1"></a>                val_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb41-101"><a href="#cb41-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-102"><a href="#cb41-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average loss and accuracy</span></span>
<span id="cb41-103"><a href="#cb41-103" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb41-104"><a href="#cb41-104" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb41-105"><a href="#cb41-105" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb41-106"><a href="#cb41-106" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb41-107"><a href="#cb41-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-108"><a href="#cb41-108" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">, Val Acc: </span><span class="sc">{</span>val_acc<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb41-109"><a href="#cb41-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-110"><a href="#cb41-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with discrimination loss</span></span>
<span id="cb41-111"><a href="#cb41-111" aria-hidden="true" tabindex="-1"></a>train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span>discrimination_loss)</span>
<span id="cb41-112"><a href="#cb41-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-113"><a href="#cb41-113" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train another model with only binary crossentropy</span></span>
<span id="cb41-114"><a href="#cb41-114" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: nan, Train Acc: 86.01, Val Loss: nan, Val Acc: 86.56
Epoch 2, Train Loss: nan, Train Acc: 86.83, Val Loss: nan, Val Acc: 86.97
Epoch 3, Train Loss: nan, Train Acc: 87.25, Val Loss: nan, Val Acc: 87.54
Epoch 4, Train Loss: nan, Train Acc: 87.22, Val Loss: nan, Val Acc: 87.04
Epoch 5, Train Loss: nan, Train Acc: 87.67, Val Loss: nan, Val Acc: 87.53
Epoch 6, Train Loss: nan, Train Acc: 87.95, Val Loss: nan, Val Acc: 87.24
Epoch 7, Train Loss: nan, Train Acc: 88.06, Val Loss: nan, Val Acc: 87.11
Epoch 8, Train Loss: nan, Train Acc: 88.62, Val Loss: nan, Val Acc: 86.70
Epoch 9, Train Loss: nan, Train Acc: 88.73, Val Loss: nan, Val Acc: 86.92
Epoch 10, Train Loss: nan, Train Acc: 89.14, Val Loss: nan, Val Acc: 86.97</code></pre>
</div>
</div>
<div id="cell-30" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>simple_model <span class="op">=</span> SimpleBinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>simple_optimizer <span class="op">=</span> optim.Adam(simple_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>train_model(simple_model, simple_optimizer, train_loader, val_loader)  <span class="co"># This time without custom loss</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-31" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Layer, Input, Dense</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> BinaryCrossentropy</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DiscriminationLayer(Layer):</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sensitive_index, k<span class="op">=</span><span class="dv">2</span>, <span class="op">**</span>kwargs):</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sensitive_index <span class="op">=</span> sensitive_index</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> k</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>        y_pred, features <span class="op">=</span> inputs</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> features[:, <span class="va">self</span>.sensitive_index]</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> tf.cast(tf.equal(sensitive_attr, <span class="dv">0</span>), dtype<span class="op">=</span>tf.float32)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> tf.cast(tf.equal(sensitive_attr, <span class="dv">1</span>), dtype<span class="op">=</span>tf.float32)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> tf.reduce_sum(y_pred <span class="op">*</span> mask_unpriv) <span class="op">/</span> (tf.reduce_sum(mask_unpriv) <span class="op">+</span> epsilon)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> tf.reduce_sum(y_pred <span class="op">*</span> mask_priv) <span class="op">/</span> (tf.reduce_sum(mask_priv) <span class="op">+</span> epsilon)</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> tf.<span class="bu">pow</span>((prob_priv <span class="op">-</span> prob_unpriv), <span class="va">self</span>.k)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [y_pred, discrimination]</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(y_true, y_pred_and_discrimination, lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    y_pred, discrimination <span class="op">=</span> y_pred_and_discrimination</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, y_pred)</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">-</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Model building</span></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>input_features <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Update this to your sensitive attribute index</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(input_features)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>disc_layer <span class="op">=</span> DiscriminationLayer(sensitive_index<span class="op">=</span>sensitive_index)([predictions, input_features])</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>input_features, outputs<span class="op">=</span>disc_layer)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Model compilation and training</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>), loss<span class="op">=</span><span class="kw">lambda</span> y_true, y_pred: custom_loss(y_true, y_pred, <span class="fl">0.5</span>), metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg ansi-bold">OperatorNotAllowedInGraphError</span>            Traceback (most recent call last)
Cell <span class="ansi-green-fg ansi-bold">In[60], line 42</span>
<span class="ansi-green-fg">     40</span> <span style="font-style:italic;color:rgb(95,135,135)"># Model compilation and training</span>
<span class="ansi-green-fg">     41</span> model<span style="color:rgb(98,98,98)">.</span>compile(optimizer<span style="color:rgb(98,98,98)">=</span>Adam(learning_rate<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.001</span>), loss<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">lambda</span> y_true, y_pred: custom_loss(y_true, y_pred, <span style="color:rgb(98,98,98)">0.5</span>), metrics<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">accuracy</span><span style="color:rgb(175,0,0)">'</span>])
<span class="ansi-green-fg ansi-bold">---&gt; 42</span> history <span style="color:rgb(98,98,98)">=</span> model<span style="color:rgb(98,98,98)">.</span>fit(X_train_scaled, y_train, epochs<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">10</span>, batch_size<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">32</span>, validation_split<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.2</span>)

File <span class="ansi-green-fg ansi-bold">c:\Users\srinivas\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122</span>, in <span class="ansi-cyan-fg">filter_traceback.&lt;locals&gt;.error_handler</span><span class="ansi-blue-fg ansi-bold">(*args, **kwargs)</span>
<span class="ansi-green-fg">    119</span>     filtered_tb <span style="color:rgb(98,98,98)">=</span> _process_traceback_frames(e<span style="color:rgb(98,98,98)">.</span>__traceback__)
<span class="ansi-green-fg">    120</span>     <span style="font-style:italic;color:rgb(95,135,135)"># To get the full stack trace, call:</span>
<span class="ansi-green-fg">    121</span>     <span style="font-style:italic;color:rgb(95,135,135)"># `keras.config.disable_traceback_filtering()`</span>
<span class="ansi-green-fg ansi-bold">--&gt; 122</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> e<span style="color:rgb(98,98,98)">.</span>with_traceback(filtered_tb) <span style="font-weight:bold;color:rgb(0,135,0)">from</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg">    123</span> <span style="font-weight:bold;color:rgb(0,135,0)">finally</span>:
<span class="ansi-green-fg">    124</span>     <span style="font-weight:bold;color:rgb(0,135,0)">del</span> filtered_tb

File <span class="ansi-green-fg ansi-bold">c:\Users\srinivas\anaconda3\Lib\site-packages\tensorflow\python\eager\polymorphic_function\autograph_util.py:52</span>, in <span class="ansi-cyan-fg">py_func_from_autograph.&lt;locals&gt;.autograph_handler</span><span class="ansi-blue-fg ansi-bold">(*args, **kwargs)</span>
<span class="ansi-green-fg">     50</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:  <span style="font-style:italic;color:rgb(95,135,135)"># pylint:disable=broad-except</span>
<span class="ansi-green-fg">     51</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">hasattr</span>(e, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">ag_error_metadata</span><span style="color:rgb(175,0,0)">"</span>):
<span class="ansi-green-fg ansi-bold">---&gt; 52</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> e<span style="color:rgb(98,98,98)">.</span>ag_error_metadata<span style="color:rgb(98,98,98)">.</span>to_exception(e)
<span class="ansi-green-fg">     53</span>   <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">     54</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span>

<span class="ansi-red-fg ansi-bold">OperatorNotAllowedInGraphError</span>: in user code:

    File "C:\Users\srinivas\AppData\Local\Temp\ipykernel_19508\3984959856.py", line 26, in custom_loss  *
        y_pred, discrimination = y_pred_and_discrimination

    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.
</pre>
</div>
</div>
</div>
<section id="defining-custom_loss-function-with-custom-lambda-here-0.01" class="level3">
<h3 class="anchored" data-anchor-id="defining-custom_loss-function-with-custom-lambda-here-0.01">DEFINING custom_loss FUNCTION with custom Lambda , here 0.01</h3>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> BinaryCrossentropy</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Dense</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(sensitive_attr, lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Creates a custom loss function that incorporates discrimination penalty.</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co">    sensitive_attr (int): Index of the sensitive attribute in the input features.</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co">    lambda_val (float): Regularization strength for the discrimination penalty.</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co">    loss (function): A loss function that takes (y_true, y_pred).</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, y_pred)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate discrimination index</span></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We assume sensitive attribute is binary and 0 is unprivileged, 1 is privileged</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(K.equal(sensitive_attr, <span class="dv">0</span>), <span class="st">'float32'</span>)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(K.equal(sensitive_attr, <span class="dv">1</span>), <span class="st">'float32'</span>)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Probabilities of positive class</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prob_unpriv = K.mean(y_pred * mask_unpriv) / K.mean(mask_unpriv)</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prob_priv = K.mean(y_pred * mask_priv) / K.mean(mask_priv)</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.mean(y_pred <span class="op">*</span> mask_unpriv) <span class="op">/</span> (K.mean(mask_unpriv) <span class="op">+</span> epsilon)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.mean(y_pred <span class="op">*</span> mask_priv) <span class="op">/</span> (K.mean(mask_priv) <span class="op">+</span> epsilon)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discrimination as the squared difference in probabilities</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(discrimination)</span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Custom loss calculation</span></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">-</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train_scaled.shape[<span class="dv">1</span>]  <span class="co"># Number of features</span></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model architecture</span></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with the custom loss function</span></span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(sensitive_attr<span class="op">=</span>X_train_scaled[:, sensitive_index], lambda_val<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
Tensor("compile_loss/loss/Square:0", shape=(), dtype=float32)
Tensor("compile_loss/loss/Square:0", shape=(), dtype=float32)
299/317  0s 1ms/step - accuracy: 0.8033 - loss: 0.4365Tensor("compile_loss/loss/Square:0", shape=(), dtype=float32)
317/317  1s 2ms/step - accuracy: 0.8058 - loss: 0.4327 - val_accuracy: 0.8547 - val_loss: 0.3441
Epoch 2/100
317/317  0s 1ms/step - accuracy: 0.8763 - loss: 0.3119 - val_accuracy: 0.8543 - val_loss: 0.3379
Epoch 3/100
317/317  0s 1ms/step - accuracy: 0.8794 - loss: 0.2985 - val_accuracy: 0.8587 - val_loss: 0.3372
Epoch 4/100
317/317  0s 1ms/step - accuracy: 0.8796 - loss: 0.3065 - val_accuracy: 0.8579 - val_loss: 0.3346
Epoch 5/100
317/317  1s 2ms/step - accuracy: 0.8846 - loss: 0.2883 - val_accuracy: 0.8591 - val_loss: 0.3314
Epoch 6/100
317/317  0s 1ms/step - accuracy: 0.8845 - loss: 0.2882 - val_accuracy: 0.8563 - val_loss: 0.3369
Epoch 7/100
317/317  0s 1ms/step - accuracy: 0.8925 - loss: 0.2760 - val_accuracy: 0.8579 - val_loss: 0.3360
Epoch 8/100
317/317  0s 1ms/step - accuracy: 0.8917 - loss: 0.2739 - val_accuracy: 0.8579 - val_loss: 0.3495
Epoch 9/100
317/317  0s 1ms/step - accuracy: 0.8951 - loss: 0.2717 - val_accuracy: 0.8512 - val_loss: 0.3459
Epoch 10/100
317/317  0s 1ms/step - accuracy: 0.9040 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3561
Epoch 11/100
317/317  0s 1ms/step - accuracy: 0.9010 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3470
Epoch 12/100
317/317  0s 1ms/step - accuracy: 0.9050 - loss: 0.2490 - val_accuracy: 0.8512 - val_loss: 0.3560
Epoch 13/100
317/317  0s 1ms/step - accuracy: 0.9083 - loss: 0.2481 - val_accuracy: 0.8523 - val_loss: 0.3670
Epoch 14/100
317/317  0s 1ms/step - accuracy: 0.9054 - loss: 0.2492 - val_accuracy: 0.8539 - val_loss: 0.3552
Epoch 15/100
317/317  0s 1ms/step - accuracy: 0.9137 - loss: 0.2301 - val_accuracy: 0.8508 - val_loss: 0.3714
Epoch 16/100
317/317  0s 1ms/step - accuracy: 0.9157 - loss: 0.2330 - val_accuracy: 0.8523 - val_loss: 0.3805
Epoch 17/100
317/317  0s 1ms/step - accuracy: 0.9144 - loss: 0.2339 - val_accuracy: 0.8500 - val_loss: 0.3780
Epoch 18/100
317/317  0s 1ms/step - accuracy: 0.9118 - loss: 0.2302 - val_accuracy: 0.8555 - val_loss: 0.3919
Epoch 19/100
317/317  1s 2ms/step - accuracy: 0.9162 - loss: 0.2299 - val_accuracy: 0.8480 - val_loss: 0.3883
Epoch 20/100
317/317  0s 1ms/step - accuracy: 0.9226 - loss: 0.2144 - val_accuracy: 0.8460 - val_loss: 0.4096
Epoch 21/100
317/317  0s 1ms/step - accuracy: 0.9211 - loss: 0.2217 - val_accuracy: 0.8516 - val_loss: 0.4037
Epoch 22/100
317/317  0s 1ms/step - accuracy: 0.9192 - loss: 0.2170 - val_accuracy: 0.8500 - val_loss: 0.4034
Epoch 23/100
317/317  1s 2ms/step - accuracy: 0.9270 - loss: 0.2019 - val_accuracy: 0.8413 - val_loss: 0.4156
Epoch 24/100
317/317  1s 2ms/step - accuracy: 0.9334 - loss: 0.1965 - val_accuracy: 0.8488 - val_loss: 0.4213
Epoch 25/100
317/317  0s 1ms/step - accuracy: 0.9256 - loss: 0.2058 - val_accuracy: 0.8512 - val_loss: 0.4194
Epoch 26/100
317/317  0s 1ms/step - accuracy: 0.9230 - loss: 0.2079 - val_accuracy: 0.8437 - val_loss: 0.4259
Epoch 27/100
317/317  0s 1ms/step - accuracy: 0.9303 - loss: 0.2013 - val_accuracy: 0.8441 - val_loss: 0.4273
Epoch 28/100
317/317  1s 2ms/step - accuracy: 0.9316 - loss: 0.1936 - val_accuracy: 0.8381 - val_loss: 0.4543
Epoch 29/100
317/317  0s 1ms/step - accuracy: 0.9313 - loss: 0.2016 - val_accuracy: 0.8468 - val_loss: 0.4473
Epoch 30/100
317/317  1s 2ms/step - accuracy: 0.9373 - loss: 0.1827 - val_accuracy: 0.8417 - val_loss: 0.4511
Epoch 31/100
317/317  0s 1ms/step - accuracy: 0.9346 - loss: 0.1876 - val_accuracy: 0.8350 - val_loss: 0.4681
Epoch 32/100
317/317  0s 1ms/step - accuracy: 0.9367 - loss: 0.1823 - val_accuracy: 0.8441 - val_loss: 0.4578
Epoch 33/100
317/317  0s 1ms/step - accuracy: 0.9380 - loss: 0.1823 - val_accuracy: 0.8433 - val_loss: 0.4643
Epoch 34/100
317/317  0s 1ms/step - accuracy: 0.9382 - loss: 0.1730 - val_accuracy: 0.8429 - val_loss: 0.4602
Epoch 35/100
317/317  0s 1ms/step - accuracy: 0.9407 - loss: 0.1748 - val_accuracy: 0.8397 - val_loss: 0.4727
Epoch 36/100
317/317  1s 1ms/step - accuracy: 0.9402 - loss: 0.1778 - val_accuracy: 0.8373 - val_loss: 0.4841
Epoch 37/100
317/317  0s 2ms/step - accuracy: 0.9409 - loss: 0.1663 - val_accuracy: 0.8350 - val_loss: 0.4929
Epoch 38/100
317/317  0s 1ms/step - accuracy: 0.9420 - loss: 0.1681 - val_accuracy: 0.8421 - val_loss: 0.4988
Epoch 39/100
317/317  0s 1ms/step - accuracy: 0.9475 - loss: 0.1611 - val_accuracy: 0.8429 - val_loss: 0.4904
Epoch 40/100
317/317  0s 1ms/step - accuracy: 0.9457 - loss: 0.1633 - val_accuracy: 0.8425 - val_loss: 0.5039
Epoch 41/100
317/317  0s 1ms/step - accuracy: 0.9436 - loss: 0.1630 - val_accuracy: 0.8413 - val_loss: 0.5223
Epoch 42/100
317/317  1s 2ms/step - accuracy: 0.9502 - loss: 0.1493 - val_accuracy: 0.8452 - val_loss: 0.5219
Epoch 43/100
317/317  0s 1ms/step - accuracy: 0.9460 - loss: 0.1563 - val_accuracy: 0.8405 - val_loss: 0.5146
Epoch 44/100
317/317  0s 1ms/step - accuracy: 0.9419 - loss: 0.1671 - val_accuracy: 0.8373 - val_loss: 0.6083
Epoch 45/100
317/317  0s 1ms/step - accuracy: 0.9563 - loss: 0.1423 - val_accuracy: 0.8441 - val_loss: 0.5172
Epoch 46/100
317/317  0s 1ms/step - accuracy: 0.9518 - loss: 0.1563 - val_accuracy: 0.8393 - val_loss: 0.5386
Epoch 47/100
317/317  1s 2ms/step - accuracy: 0.9515 - loss: 0.1459 - val_accuracy: 0.8401 - val_loss: 0.5497
Epoch 48/100
317/317  0s 1ms/step - accuracy: 0.9506 - loss: 0.1465 - val_accuracy: 0.8267 - val_loss: 0.5693
Epoch 49/100
317/317  0s 1ms/step - accuracy: 0.9482 - loss: 0.1613 - val_accuracy: 0.8362 - val_loss: 0.5661
Epoch 50/100
317/317  0s 1ms/step - accuracy: 0.9562 - loss: 0.1408 - val_accuracy: 0.8377 - val_loss: 0.5786
Epoch 51/100
317/317  0s 1ms/step - accuracy: 0.9569 - loss: 0.1348 - val_accuracy: 0.8287 - val_loss: 0.5823
Epoch 52/100
317/317  0s 1ms/step - accuracy: 0.9506 - loss: 0.1394 - val_accuracy: 0.8389 - val_loss: 0.5750
Epoch 53/100
317/317  0s 1ms/step - accuracy: 0.9569 - loss: 0.1372 - val_accuracy: 0.8362 - val_loss: 0.5819
Epoch 54/100
317/317  0s 1ms/step - accuracy: 0.9597 - loss: 0.1356 - val_accuracy: 0.8298 - val_loss: 0.5924
Epoch 55/100
317/317  0s 1ms/step - accuracy: 0.9589 - loss: 0.1328 - val_accuracy: 0.8310 - val_loss: 0.6046
Epoch 56/100
317/317  0s 1ms/step - accuracy: 0.9569 - loss: 0.1342 - val_accuracy: 0.8200 - val_loss: 0.6179
Epoch 57/100
317/317  0s 1ms/step - accuracy: 0.9584 - loss: 0.1330 - val_accuracy: 0.8350 - val_loss: 0.6040
Epoch 58/100
317/317  0s 1ms/step - accuracy: 0.9620 - loss: 0.1266 - val_accuracy: 0.8223 - val_loss: 0.6398
Epoch 59/100
317/317  0s 1ms/step - accuracy: 0.9587 - loss: 0.1314 - val_accuracy: 0.8322 - val_loss: 0.6161
Epoch 60/100
317/317  0s 1ms/step - accuracy: 0.9629 - loss: 0.1177 - val_accuracy: 0.8366 - val_loss: 0.6166
Epoch 61/100
317/317  0s 1ms/step - accuracy: 0.9633 - loss: 0.1183 - val_accuracy: 0.8227 - val_loss: 0.6447
Epoch 62/100
317/317  0s 1ms/step - accuracy: 0.9637 - loss: 0.1208 - val_accuracy: 0.8326 - val_loss: 0.6244
Epoch 63/100
317/317  0s 1ms/step - accuracy: 0.9604 - loss: 0.1245 - val_accuracy: 0.8346 - val_loss: 0.6591
Epoch 64/100
317/317  0s 1ms/step - accuracy: 0.9629 - loss: 0.1260 - val_accuracy: 0.8366 - val_loss: 0.6509
Epoch 65/100
317/317  0s 1ms/step - accuracy: 0.9611 - loss: 0.1246 - val_accuracy: 0.8373 - val_loss: 0.6708
Epoch 66/100
317/317  0s 1ms/step - accuracy: 0.9631 - loss: 0.1189 - val_accuracy: 0.8310 - val_loss: 0.6936
Epoch 67/100
317/317  0s 1ms/step - accuracy: 0.9592 - loss: 0.1239 - val_accuracy: 0.8216 - val_loss: 0.6786
Epoch 68/100
317/317  0s 1ms/step - accuracy: 0.9643 - loss: 0.1230 - val_accuracy: 0.8326 - val_loss: 0.6815
Epoch 69/100
317/317  0s 1ms/step - accuracy: 0.9636 - loss: 0.1216 - val_accuracy: 0.8267 - val_loss: 0.6902
Epoch 70/100
317/317  0s 1ms/step - accuracy: 0.9636 - loss: 0.1147 - val_accuracy: 0.8251 - val_loss: 0.6926
Epoch 71/100
317/317  0s 1ms/step - accuracy: 0.9649 - loss: 0.1131 - val_accuracy: 0.8342 - val_loss: 0.7052
Epoch 72/100
317/317  0s 1ms/step - accuracy: 0.9671 - loss: 0.1105 - val_accuracy: 0.8251 - val_loss: 0.6964
Epoch 73/100
317/317  0s 1ms/step - accuracy: 0.9681 - loss: 0.1057 - val_accuracy: 0.8251 - val_loss: 0.7397
Epoch 74/100
317/317  0s 1ms/step - accuracy: 0.9627 - loss: 0.1106 - val_accuracy: 0.8227 - val_loss: 0.7392
Epoch 75/100
317/317  0s 1ms/step - accuracy: 0.9667 - loss: 0.1124 - val_accuracy: 0.8310 - val_loss: 0.7185
Epoch 76/100
317/317  0s 1ms/step - accuracy: 0.9668 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7144
Epoch 77/100
317/317  0s 1ms/step - accuracy: 0.9672 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7278
Epoch 78/100
317/317  0s 1ms/step - accuracy: 0.9690 - loss: 0.1136 - val_accuracy: 0.8318 - val_loss: 0.7388
Epoch 79/100
317/317  0s 1ms/step - accuracy: 0.9696 - loss: 0.1021 - val_accuracy: 0.8247 - val_loss: 0.7466
Epoch 80/100
317/317  0s 1ms/step - accuracy: 0.9666 - loss: 0.1124 - val_accuracy: 0.8196 - val_loss: 0.7523
Epoch 81/100
317/317  0s 1ms/step - accuracy: 0.9700 - loss: 0.1022 - val_accuracy: 0.8243 - val_loss: 0.7548
Epoch 82/100
317/317  0s 1ms/step - accuracy: 0.9684 - loss: 0.1008 - val_accuracy: 0.8255 - val_loss: 0.7539
Epoch 83/100
317/317  0s 1ms/step - accuracy: 0.9720 - loss: 0.0999 - val_accuracy: 0.8176 - val_loss: 0.7742
Epoch 84/100
317/317  0s 1ms/step - accuracy: 0.9713 - loss: 0.0964 - val_accuracy: 0.8192 - val_loss: 0.7893
Epoch 85/100
317/317  0s 1ms/step - accuracy: 0.9693 - loss: 0.0950 - val_accuracy: 0.8239 - val_loss: 0.8019
Epoch 86/100
317/317  0s 1ms/step - accuracy: 0.9700 - loss: 0.0959 - val_accuracy: 0.8231 - val_loss: 0.8244
Epoch 87/100
317/317  0s 1ms/step - accuracy: 0.9680 - loss: 0.1088 - val_accuracy: 0.8259 - val_loss: 0.8061
Epoch 88/100
317/317  0s 1ms/step - accuracy: 0.9694 - loss: 0.0984 - val_accuracy: 0.8231 - val_loss: 0.7985
Epoch 89/100
317/317  0s 1ms/step - accuracy: 0.9744 - loss: 0.0878 - val_accuracy: 0.8081 - val_loss: 0.8120
Epoch 90/100
317/317  0s 1ms/step - accuracy: 0.9722 - loss: 0.0955 - val_accuracy: 0.8160 - val_loss: 0.8065
Epoch 91/100
317/317  0s 1ms/step - accuracy: 0.9715 - loss: 0.0998 - val_accuracy: 0.8291 - val_loss: 0.7710
Epoch 92/100
317/317  0s 1ms/step - accuracy: 0.9731 - loss: 0.0992 - val_accuracy: 0.8235 - val_loss: 0.8246
Epoch 93/100
317/317  0s 1ms/step - accuracy: 0.9745 - loss: 0.0861 - val_accuracy: 0.8243 - val_loss: 0.8357
Epoch 94/100
317/317  0s 1ms/step - accuracy: 0.9698 - loss: 0.0975 - val_accuracy: 0.8279 - val_loss: 0.8380
Epoch 95/100
317/317  0s 1ms/step - accuracy: 0.9749 - loss: 0.0870 - val_accuracy: 0.8239 - val_loss: 0.8321
Epoch 96/100
317/317  0s 1ms/step - accuracy: 0.9729 - loss: 0.0938 - val_accuracy: 0.8184 - val_loss: 0.8346
Epoch 97/100
317/317  0s 1ms/step - accuracy: 0.9736 - loss: 0.0948 - val_accuracy: 0.8247 - val_loss: 0.8299
Epoch 98/100
317/317  0s 1ms/step - accuracy: 0.9750 - loss: 0.0909 - val_accuracy: 0.8223 - val_loss: 0.8562
Epoch 99/100
317/317  0s 1ms/step - accuracy: 0.9722 - loss: 0.0915 - val_accuracy: 0.8283 - val_loss: 0.8644
Epoch 100/100
317/317  0s 1ms/step - accuracy: 0.9696 - loss: 0.0965 - val_accuracy: 0.8231 - val_loss: 0.8665</code></pre>
</div>
</div>
</section>
<section id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination" class="level3">
<h3 class="anchored" data-anchor-id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination">Modal with with Standard Loss(binary_crossentropy), No discrimination</h3>
<div id="cell-37" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with the custom loss function</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>history2 <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
317/317  1s 1ms/step - accuracy: 0.8989 - loss: 0.2601 - val_accuracy: 0.8551 - val_loss: 0.3496
Epoch 2/10
317/317  0s 782us/step - accuracy: 0.8954 - loss: 0.2608 - val_accuracy: 0.8531 - val_loss: 0.3531
Epoch 3/10
317/317  0s 791us/step - accuracy: 0.9064 - loss: 0.2512 - val_accuracy: 0.8531 - val_loss: 0.3671
Epoch 4/10
317/317  0s 846us/step - accuracy: 0.9090 - loss: 0.2452 - val_accuracy: 0.8468 - val_loss: 0.3754
Epoch 5/10
317/317  0s 786us/step - accuracy: 0.9121 - loss: 0.2368 - val_accuracy: 0.8484 - val_loss: 0.3660
Epoch 6/10
317/317  0s 767us/step - accuracy: 0.9150 - loss: 0.2350 - val_accuracy: 0.8535 - val_loss: 0.3656
Epoch 7/10
317/317  0s 880us/step - accuracy: 0.9145 - loss: 0.2336 - val_accuracy: 0.8468 - val_loss: 0.3764
Epoch 8/10
317/317  0s 729us/step - accuracy: 0.9177 - loss: 0.2211 - val_accuracy: 0.8500 - val_loss: 0.3734
Epoch 9/10
317/317  0s 747us/step - accuracy: 0.9162 - loss: 0.2237 - val_accuracy: 0.8543 - val_loss: 0.3917
Epoch 10/10
317/317  0s 799us/step - accuracy: 0.9165 - loss: 0.2274 - val_accuracy: 0.8496 - val_loss: 0.3897</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_accuracy(histories):</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for accuracy</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add accuracy traces with specified colors</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'accuracy'</span>]))),</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'accuracy'</span>],</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_accuracy'</span>]))),</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_accuracy'</span>],</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for accuracy graph</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Accuracy'</span>,</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Accuracy'</span>,</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(histories):</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for loss</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add loss traces with specified colors</span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'loss'</span>]))),</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'loss'</span>],</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_loss'</span>]))),</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_loss'</span>],</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for loss graph</span></span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Loss'</span>,</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Loss'</span>,</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have history1 and history2 as the history objects from your model training</span></span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>plot_accuracy([(<span class="st">'Custom Loss'</span>, history1), (<span class="st">'Standard Loss'</span>, history2)])</span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>plot_loss([(<span class="st">'Custom Loss'</span>, history1), (<span class="st">'Standard Loss'</span>, history2)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="defining-custom_loss-function-with-standard-lambda-0.5-but-discrimination0" class="level3">
<h3 class="anchored" data-anchor-id="defining-custom_loss-function-with-standard-lambda-0.5-but-discrimination0">DEFINING custom_loss FUNCTION with Standard Lambda = 0.5, but Discrimination!=0</h3>
<div id="cell-40" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract predictions and sensitive attributes</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, predictions)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate discrimination based on sensitive attribute</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&lt;=</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&gt;</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>        sum_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(mask_unpriv)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>        sum_priv <span class="op">=</span> K.<span class="bu">sum</span>(mask_priv)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_unpriv) <span class="op">/</span> (sum_unpriv <span class="op">+</span> epsilon)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_priv) <span class="op">/</span> (sum_priv <span class="op">+</span> epsilon)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug outputs</span></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Standard Loss:", standard_loss, "Discrimination:", discrimination)</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total loss with discrimination penalty</span></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Model compilation</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(lambda_val<span class="op">=</span><span class="fl">0.01</span>),  <span class="co"># Change lambda_val as needed</span></span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the model</span></span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg ansi-bold">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg ansi-bold">In[22], line 38</span>
<span class="ansi-green-fg">     33</span> model<span style="color:rgb(98,98,98)">.</span>compile(optimizer<span style="color:rgb(98,98,98)">=</span>Adam(learning_rate<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.001</span>),
<span class="ansi-green-fg">     34</span>               loss<span style="color:rgb(98,98,98)">=</span>custom_loss(lambda_val<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.01</span>),  <span style="font-style:italic;color:rgb(95,135,135)"># Change lambda_val as needed</span>
<span class="ansi-green-fg">     35</span>               metrics<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">accuracy</span><span style="color:rgb(175,0,0)">'</span>])
<span class="ansi-green-fg">     37</span> <span style="font-style:italic;color:rgb(95,135,135)"># Training the model</span>
<span class="ansi-green-fg ansi-bold">---&gt; 38</span> history1 <span style="color:rgb(98,98,98)">=</span> model<span style="color:rgb(98,98,98)">.</span>fit(X_train_with_sensitive, y_train, epochs<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">100</span>, batch_size<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">32</span>, validation_split<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.2</span>)

<span class="ansi-red-fg ansi-bold">NameError</span>: name 'X_train_with_sensitive' is not defined</pre>
</div>
</div>
</div>
</section>
<section id="defining-custom_loss-function-with-cust-lambda-0.01-but-discrimination0" class="level3">
<h3 class="anchored" data-anchor-id="defining-custom_loss-function-with-cust-lambda-0.01-but-discrimination0">DEFINING custom_loss FUNCTION with cust Lambda = 0.01, but Discrimination!=0</h3>
<div id="cell-42" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> BinaryCrossentropy</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Dense</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(lambda_val<span class="op">=</span><span class="fl">0.001</span>):</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract predictions and sensitive attributes from y_pred</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug prints to check outputs</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Predictions sample:", predictions[:10])</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Sensitive Attr sample:", sensitive_attr[:10])</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, predictions)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine thresholds to convert sensitive attributes to binary</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assuming the negative and positive classes are split around zero</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&lt;=</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&gt;</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>        sum_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(mask_unpriv)</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>        sum_priv <span class="op">=</span> K.<span class="bu">sum</span>(mask_priv)</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Debug prints for mask sums</span></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Sum unprivileged:", sum_unpriv)</span></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Sum privileged:", sum_priv)</span></span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_unpriv) <span class="op">/</span> (sum_unpriv <span class="op">+</span> epsilon)</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_priv) <span class="op">/</span> (sum_priv <span class="op">+</span> epsilon)</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discrimination as the squared difference in probabilities</span></span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Debug print</span></span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Discrimination:", discrimination)</span></span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total loss with discrimination penalty</span></span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train_scaled.shape[<span class="dv">1</span>]  <span class="co"># Number of features</span></span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layers</span></span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>sensitive_inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,))</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Network architecture</span></span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>combined_outputs <span class="op">=</span> tf.keras.layers.concatenate([outputs, sensitive_inputs])</span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>[inputs, sensitive_inputs], outputs<span class="op">=</span>combined_outputs)</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with the custom loss function</span></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(lambda_val <span class="op">=</span> <span class="fl">0.01</span>),</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data with sensitive attribute</span></span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>X_train_with_sensitive <span class="op">=</span> [X_train_scaled, X_train_scaled[:, sensitive_index]]</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>history1_nd <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
317/317  1s 600us/step - accuracy: 0.6550 - loss: 0.7064 - val_accuracy: 0.6609 - val_loss: 0.6894
Epoch 2/100
317/317  0s 374us/step - accuracy: 0.6535 - loss: 0.6854 - val_accuracy: 0.6609 - val_loss: 0.6880
Epoch 3/100
317/317  0s 371us/step - accuracy: 0.6558 - loss: 0.6812 - val_accuracy: 0.6609 - val_loss: 0.6878
Epoch 4/100
317/317  0s 402us/step - accuracy: 0.6569 - loss: 0.6801 - val_accuracy: 0.6609 - val_loss: 0.6883
Epoch 5/100
317/317  0s 409us/step - accuracy: 0.6568 - loss: 0.6789 - val_accuracy: 0.6609 - val_loss: 0.6881
Epoch 6/100
317/317  0s 391us/step - accuracy: 0.6590 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6878
Epoch 7/100
317/317  0s 748us/step - accuracy: 0.6499 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 8/100
317/317  0s 413us/step - accuracy: 0.6574 - loss: 0.6754 - val_accuracy: 0.6609 - val_loss: 0.6888
Epoch 9/100
317/317  0s 393us/step - accuracy: 0.6514 - loss: 0.6772 - val_accuracy: 0.6609 - val_loss: 0.6886
Epoch 10/100
317/317  0s 392us/step - accuracy: 0.6547 - loss: 0.6752 - val_accuracy: 0.6609 - val_loss: 0.6893
Epoch 11/100
317/317  0s 388us/step - accuracy: 0.6602 - loss: 0.6736 - val_accuracy: 0.6609 - val_loss: 0.6887
Epoch 12/100
317/317  0s 392us/step - accuracy: 0.6497 - loss: 0.6741 - val_accuracy: 0.6609 - val_loss: 0.6896
Epoch 13/100
317/317  0s 392us/step - accuracy: 0.6501 - loss: 0.6703 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 14/100
317/317  0s 389us/step - accuracy: 0.6578 - loss: 0.6716 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 15/100
317/317  0s 382us/step - accuracy: 0.6528 - loss: 0.6702 - val_accuracy: 0.6609 - val_loss: 0.6884
Epoch 16/100
317/317  0s 397us/step - accuracy: 0.6607 - loss: 0.6709 - val_accuracy: 0.6609 - val_loss: 0.6890
Epoch 17/100
317/317  0s 377us/step - accuracy: 0.6566 - loss: 0.6708 - val_accuracy: 0.6609 - val_loss: 0.6896
Epoch 18/100
317/317  0s 368us/step - accuracy: 0.6558 - loss: 0.6706 - val_accuracy: 0.6609 - val_loss: 0.6901
Epoch 19/100
317/317  0s 368us/step - accuracy: 0.6534 - loss: 0.6695 - val_accuracy: 0.6609 - val_loss: 0.6898
Epoch 20/100
317/317  0s 443us/step - accuracy: 0.6589 - loss: 0.6673 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 21/100
317/317  0s 374us/step - accuracy: 0.6594 - loss: 0.6697 - val_accuracy: 0.6609 - val_loss: 0.6895
Epoch 22/100
317/317  0s 371us/step - accuracy: 0.6541 - loss: 0.6656 - val_accuracy: 0.6609 - val_loss: 0.6897
Epoch 23/100
317/317  0s 381us/step - accuracy: 0.6504 - loss: 0.6680 - val_accuracy: 0.6609 - val_loss: 0.6898
Epoch 24/100
317/317  0s 371us/step - accuracy: 0.6491 - loss: 0.6642 - val_accuracy: 0.6609 - val_loss: 0.6893
Epoch 25/100
317/317  0s 489us/step - accuracy: 0.6595 - loss: 0.6663 - val_accuracy: 0.6609 - val_loss: 0.6908
Epoch 26/100
317/317  0s 370us/step - accuracy: 0.6486 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 27/100
317/317  0s 383us/step - accuracy: 0.6512 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 28/100
317/317  0s 434us/step - accuracy: 0.6623 - loss: 0.6659 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 29/100
317/317  0s 374us/step - accuracy: 0.6488 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6908
Epoch 30/100
317/317  0s 380us/step - accuracy: 0.6629 - loss: 0.6647 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 31/100
317/317  0s 371us/step - accuracy: 0.6499 - loss: 0.6653 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 32/100
317/317  0s 379us/step - accuracy: 0.6537 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 33/100
317/317  0s 381us/step - accuracy: 0.6560 - loss: 0.6649 - val_accuracy: 0.6609 - val_loss: 0.6893
Epoch 34/100
317/317  0s 367us/step - accuracy: 0.6515 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 35/100
317/317  0s 377us/step - accuracy: 0.6549 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 36/100
317/317  0s 418us/step - accuracy: 0.6498 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 37/100
317/317  0s 378us/step - accuracy: 0.6572 - loss: 0.6641 - val_accuracy: 0.6609 - val_loss: 0.6926
Epoch 38/100
317/317  0s 376us/step - accuracy: 0.6623 - loss: 0.6660 - val_accuracy: 0.6609 - val_loss: 0.6901
Epoch 39/100
317/317  0s 381us/step - accuracy: 0.6571 - loss: 0.6645 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 40/100
317/317  0s 397us/step - accuracy: 0.6571 - loss: 0.6637 - val_accuracy: 0.6609 - val_loss: 0.6909
Epoch 41/100
317/317  0s 380us/step - accuracy: 0.6551 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 42/100
317/317  0s 514us/step - accuracy: 0.6476 - loss: 0.6631 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 43/100
317/317  0s 380us/step - accuracy: 0.6529 - loss: 0.6662 - val_accuracy: 0.6609 - val_loss: 0.6911
Epoch 44/100
317/317  0s 368us/step - accuracy: 0.6571 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6911
Epoch 45/100
317/317  0s 372us/step - accuracy: 0.6486 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6914
Epoch 46/100
317/317  0s 434us/step - accuracy: 0.6591 - loss: 0.6633 - val_accuracy: 0.6609 - val_loss: 0.6913
Epoch 47/100
317/317  0s 381us/step - accuracy: 0.6451 - loss: 0.6622 - val_accuracy: 0.6609 - val_loss: 0.6913
Epoch 48/100
317/317  0s 376us/step - accuracy: 0.6502 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 49/100
317/317  0s 375us/step - accuracy: 0.6517 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6918
Epoch 50/100
317/317  0s 375us/step - accuracy: 0.6593 - loss: 0.6606 - val_accuracy: 0.6609 - val_loss: 0.6911
Epoch 51/100
317/317  0s 368us/step - accuracy: 0.6534 - loss: 0.6638 - val_accuracy: 0.6609 - val_loss: 0.6910
Epoch 52/100
317/317  0s 380us/step - accuracy: 0.6602 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6910
Epoch 53/100
317/317  0s 364us/step - accuracy: 0.6466 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 54/100
317/317  0s 368us/step - accuracy: 0.6599 - loss: 0.6630 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 55/100
317/317  0s 373us/step - accuracy: 0.6499 - loss: 0.6612 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 56/100
317/317  0s 376us/step - accuracy: 0.6580 - loss: 0.6635 - val_accuracy: 0.6609 - val_loss: 0.6913
Epoch 57/100
317/317  0s 377us/step - accuracy: 0.6571 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 58/100
317/317  0s 366us/step - accuracy: 0.6615 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 59/100
317/317  0s 622us/step - accuracy: 0.6572 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 60/100
317/317  0s 370us/step - accuracy: 0.6577 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 61/100
317/317  0s 374us/step - accuracy: 0.6541 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 62/100
317/317  0s 376us/step - accuracy: 0.6614 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 63/100
317/317  0s 376us/step - accuracy: 0.6569 - loss: 0.6602 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 64/100
317/317  0s 365us/step - accuracy: 0.6489 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6905
Epoch 65/100
317/317  0s 379us/step - accuracy: 0.6461 - loss: 0.6604 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 66/100
317/317  0s 379us/step - accuracy: 0.6633 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 67/100
317/317  0s 366us/step - accuracy: 0.6551 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 68/100
317/317  0s 377us/step - accuracy: 0.6567 - loss: 0.6610 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 69/100
317/317  0s 376us/step - accuracy: 0.6503 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6915
Epoch 70/100
317/317  0s 378us/step - accuracy: 0.6579 - loss: 0.6627 - val_accuracy: 0.6609 - val_loss: 0.6905
Epoch 71/100
317/317  0s 365us/step - accuracy: 0.6569 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6915
Epoch 72/100
317/317  0s 370us/step - accuracy: 0.6494 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6918
Epoch 73/100
317/317  0s 378us/step - accuracy: 0.6484 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6916
Epoch 74/100
317/317  0s 416us/step - accuracy: 0.6512 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6909
Epoch 75/100
317/317  0s 551us/step - accuracy: 0.6613 - loss: 0.6588 - val_accuracy: 0.6609 - val_loss: 0.6914
Epoch 76/100
317/317  0s 367us/step - accuracy: 0.6557 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 77/100
317/317  0s 369us/step - accuracy: 0.6592 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 78/100
317/317  0s 376us/step - accuracy: 0.6553 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 79/100
317/317  0s 375us/step - accuracy: 0.6564 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 80/100
317/317  0s 376us/step - accuracy: 0.6525 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 81/100
317/317  0s 372us/step - accuracy: 0.6482 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 82/100
317/317  0s 377us/step - accuracy: 0.6534 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 83/100
317/317  0s 370us/step - accuracy: 0.6580 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 84/100
317/317  0s 370us/step - accuracy: 0.6500 - loss: 0.6596 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 85/100
317/317  0s 367us/step - accuracy: 0.6518 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 86/100
317/317  0s 376us/step - accuracy: 0.6579 - loss: 0.6587 - val_accuracy: 0.6609 - val_loss: 0.6908
Epoch 87/100
317/317  0s 369us/step - accuracy: 0.6646 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6897
Epoch 88/100
317/317  0s 418us/step - accuracy: 0.6484 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 89/100
317/317  0s 366us/step - accuracy: 0.6500 - loss: 0.6595 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 90/100
317/317  0s 532us/step - accuracy: 0.6472 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6915
Epoch 91/100
317/317  0s 375us/step - accuracy: 0.6485 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 92/100
317/317  0s 381us/step - accuracy: 0.6596 - loss: 0.6621 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 93/100
317/317  0s 371us/step - accuracy: 0.6508 - loss: 0.6600 - val_accuracy: 0.6609 - val_loss: 0.6894
Epoch 94/100
317/317  0s 383us/step - accuracy: 0.6520 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6909
Epoch 95/100
317/317  0s 382us/step - accuracy: 0.6587 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6928
Epoch 96/100
317/317  0s 385us/step - accuracy: 0.6538 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6923
Epoch 97/100
317/317  0s 394us/step - accuracy: 0.6521 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6910
Epoch 98/100
317/317  0s 530us/step - accuracy: 0.6471 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6923
Epoch 99/100
317/317  0s 373us/step - accuracy: 0.6484 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 100/100
317/317  0s 381us/step - accuracy: 0.6508 - loss: 0.6590 - val_accuracy: 0.6609 - val_loss: 0.6903</code></pre>
</div>
</div>
</section>
<section id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination-but-discrimination0-in-custom-losses" class="level3">
<h3 class="anchored" data-anchor-id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination-but-discrimination0-in-custom-losses">Modal with with Standard Loss(binary_crossentropy), No discrimination , but Discrimination!=0 in custom losses</h3>
<div id="cell-44" class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train_scaled.shape[<span class="dv">1</span>]  <span class="co"># Number of features</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layers</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>sensitive_inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,))</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Network architecture</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co"># No need to concatenate outputs and sensitive inputs for the loss calculation</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate outputs for predictions and sensitive attributes</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Since we're using binary_crossentropy, we only need the main outputs</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>[inputs, sensitive_inputs], outputs<span class="op">=</span>outputs)</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with binary crossentropy</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data with sensitive attribute</span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>X_train_with_sensitive <span class="op">=</span> [X_train_scaled, X_train_scaled[:, sensitive_index]]</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>history2_nd <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
317/317  1s 598us/step - accuracy: 0.8259 - loss: 0.4073 - val_accuracy: 0.8595 - val_loss: 0.3396
Epoch 2/100
317/317  0s 442us/step - accuracy: 0.8696 - loss: 0.3200 - val_accuracy: 0.8634 - val_loss: 0.3325
Epoch 3/100
317/317  0s 754us/step - accuracy: 0.8746 - loss: 0.3044 - val_accuracy: 0.8630 - val_loss: 0.3334
Epoch 4/100
317/317  0s 399us/step - accuracy: 0.8820 - loss: 0.2964 - val_accuracy: 0.8630 - val_loss: 0.3361
Epoch 5/100
317/317  0s 387us/step - accuracy: 0.8827 - loss: 0.2969 - val_accuracy: 0.8650 - val_loss: 0.3331
Epoch 6/100
317/317  0s 385us/step - accuracy: 0.8857 - loss: 0.2855 - val_accuracy: 0.8646 - val_loss: 0.3304
Epoch 7/100
317/317  0s 374us/step - accuracy: 0.8811 - loss: 0.2898 - val_accuracy: 0.8595 - val_loss: 0.3401
Epoch 8/100
317/317  0s 366us/step - accuracy: 0.8928 - loss: 0.2674 - val_accuracy: 0.8658 - val_loss: 0.3319
Epoch 9/100
317/317  0s 386us/step - accuracy: 0.8897 - loss: 0.2836 - val_accuracy: 0.8646 - val_loss: 0.3381
Epoch 10/100
317/317  0s 375us/step - accuracy: 0.8926 - loss: 0.2663 - val_accuracy: 0.8567 - val_loss: 0.3486
Epoch 11/100
317/317  0s 412us/step - accuracy: 0.8995 - loss: 0.2607 - val_accuracy: 0.8595 - val_loss: 0.3509
Epoch 12/100
317/317  0s 399us/step - accuracy: 0.9013 - loss: 0.2601 - val_accuracy: 0.8598 - val_loss: 0.3511
Epoch 13/100
317/317  0s 364us/step - accuracy: 0.9017 - loss: 0.2531 - val_accuracy: 0.8595 - val_loss: 0.3629
Epoch 14/100
317/317  0s 366us/step - accuracy: 0.9109 - loss: 0.2382 - val_accuracy: 0.8606 - val_loss: 0.3490
Epoch 15/100
317/317  0s 364us/step - accuracy: 0.9070 - loss: 0.2503 - val_accuracy: 0.8535 - val_loss: 0.3574
Epoch 16/100
317/317  0s 444us/step - accuracy: 0.9119 - loss: 0.2331 - val_accuracy: 0.8551 - val_loss: 0.3676
Epoch 17/100
317/317  0s 356us/step - accuracy: 0.9143 - loss: 0.2275 - val_accuracy: 0.8535 - val_loss: 0.3737
Epoch 18/100
317/317  0s 376us/step - accuracy: 0.9121 - loss: 0.2327 - val_accuracy: 0.8575 - val_loss: 0.3660
Epoch 19/100
317/317  0s 563us/step - accuracy: 0.9233 - loss: 0.2215 - val_accuracy: 0.8539 - val_loss: 0.3894
Epoch 20/100
317/317  0s 378us/step - accuracy: 0.9159 - loss: 0.2244 - val_accuracy: 0.8523 - val_loss: 0.3763
Epoch 21/100
317/317  0s 376us/step - accuracy: 0.9230 - loss: 0.2136 - val_accuracy: 0.8555 - val_loss: 0.3882
Epoch 22/100
317/317  0s 385us/step - accuracy: 0.9242 - loss: 0.2074 - val_accuracy: 0.8535 - val_loss: 0.3934
Epoch 23/100
317/317  0s 376us/step - accuracy: 0.9253 - loss: 0.2120 - val_accuracy: 0.8496 - val_loss: 0.3942
Epoch 24/100
317/317  0s 354us/step - accuracy: 0.9266 - loss: 0.2041 - val_accuracy: 0.8464 - val_loss: 0.4131
Epoch 25/100
317/317  0s 383us/step - accuracy: 0.9202 - loss: 0.2132 - val_accuracy: 0.8500 - val_loss: 0.4122
Epoch 26/100
317/317  0s 378us/step - accuracy: 0.9325 - loss: 0.1921 - val_accuracy: 0.8535 - val_loss: 0.4094
Epoch 27/100
317/317  0s 376us/step - accuracy: 0.9351 - loss: 0.1893 - val_accuracy: 0.8504 - val_loss: 0.4226
Epoch 28/100
317/317  0s 351us/step - accuracy: 0.9320 - loss: 0.1931 - val_accuracy: 0.8480 - val_loss: 0.4264
Epoch 29/100
317/317  0s 358us/step - accuracy: 0.9325 - loss: 0.1899 - val_accuracy: 0.8496 - val_loss: 0.4329
Epoch 30/100
317/317  0s 352us/step - accuracy: 0.9317 - loss: 0.1922 - val_accuracy: 0.8500 - val_loss: 0.4303
Epoch 31/100
317/317  0s 352us/step - accuracy: 0.9371 - loss: 0.1796 - val_accuracy: 0.8437 - val_loss: 0.4379
Epoch 32/100
317/317  0s 347us/step - accuracy: 0.9337 - loss: 0.1851 - val_accuracy: 0.8460 - val_loss: 0.4467
Epoch 33/100
317/317  0s 362us/step - accuracy: 0.9348 - loss: 0.1881 - val_accuracy: 0.8445 - val_loss: 0.4406
Epoch 34/100
317/317  0s 383us/step - accuracy: 0.9399 - loss: 0.1758 - val_accuracy: 0.8441 - val_loss: 0.4533
Epoch 35/100
317/317  0s 343us/step - accuracy: 0.9370 - loss: 0.1763 - val_accuracy: 0.8429 - val_loss: 0.4685
Epoch 36/100
317/317  0s 374us/step - accuracy: 0.9420 - loss: 0.1644 - val_accuracy: 0.8437 - val_loss: 0.4601
Epoch 37/100
317/317  0s 373us/step - accuracy: 0.9382 - loss: 0.1685 - val_accuracy: 0.8421 - val_loss: 0.4721
Epoch 38/100
317/317  0s 373us/step - accuracy: 0.9397 - loss: 0.1732 - val_accuracy: 0.8401 - val_loss: 0.4915
Epoch 39/100
317/317  0s 529us/step - accuracy: 0.9463 - loss: 0.1582 - val_accuracy: 0.8354 - val_loss: 0.5038
Epoch 40/100
317/317  0s 361us/step - accuracy: 0.9438 - loss: 0.1608 - val_accuracy: 0.8429 - val_loss: 0.4831
Epoch 41/100
317/317  0s 368us/step - accuracy: 0.9403 - loss: 0.1676 - val_accuracy: 0.8452 - val_loss: 0.4940
Epoch 42/100
317/317  0s 390us/step - accuracy: 0.9445 - loss: 0.1581 - val_accuracy: 0.8393 - val_loss: 0.4956
Epoch 43/100
317/317  0s 346us/step - accuracy: 0.9461 - loss: 0.1537 - val_accuracy: 0.8350 - val_loss: 0.5094
Epoch 44/100
317/317  0s 361us/step - accuracy: 0.9465 - loss: 0.1573 - val_accuracy: 0.8350 - val_loss: 0.5115
Epoch 45/100
317/317  0s 395us/step - accuracy: 0.9467 - loss: 0.1530 - val_accuracy: 0.8409 - val_loss: 0.5131
Epoch 46/100
317/317  0s 362us/step - accuracy: 0.9473 - loss: 0.1580 - val_accuracy: 0.8393 - val_loss: 0.5254
Epoch 47/100
317/317  0s 513us/step - accuracy: 0.9492 - loss: 0.1451 - val_accuracy: 0.8377 - val_loss: 0.5261
Epoch 48/100
317/317  0s 393us/step - accuracy: 0.9517 - loss: 0.1462 - val_accuracy: 0.8247 - val_loss: 0.5360
Epoch 49/100
317/317  0s 377us/step - accuracy: 0.9528 - loss: 0.1422 - val_accuracy: 0.8287 - val_loss: 0.5420
Epoch 50/100
317/317  0s 390us/step - accuracy: 0.9515 - loss: 0.1378 - val_accuracy: 0.8216 - val_loss: 0.5578
Epoch 51/100
317/317  0s 397us/step - accuracy: 0.9508 - loss: 0.1454 - val_accuracy: 0.8373 - val_loss: 0.5463
Epoch 52/100
317/317  0s 387us/step - accuracy: 0.9487 - loss: 0.1405 - val_accuracy: 0.8385 - val_loss: 0.5607
Epoch 53/100
317/317  0s 384us/step - accuracy: 0.9545 - loss: 0.1364 - val_accuracy: 0.8310 - val_loss: 0.5591
Epoch 54/100
317/317  0s 584us/step - accuracy: 0.9557 - loss: 0.1387 - val_accuracy: 0.8366 - val_loss: 0.5878
Epoch 55/100
317/317  0s 382us/step - accuracy: 0.9549 - loss: 0.1338 - val_accuracy: 0.8334 - val_loss: 0.5751
Epoch 56/100
317/317  0s 421us/step - accuracy: 0.9518 - loss: 0.1390 - val_accuracy: 0.8389 - val_loss: 0.5626
Epoch 57/100
317/317  0s 376us/step - accuracy: 0.9583 - loss: 0.1279 - val_accuracy: 0.8389 - val_loss: 0.5940
Epoch 58/100
317/317  0s 345us/step - accuracy: 0.9558 - loss: 0.1300 - val_accuracy: 0.8314 - val_loss: 0.5842
Epoch 59/100
317/317  0s 349us/step - accuracy: 0.9592 - loss: 0.1307 - val_accuracy: 0.8366 - val_loss: 0.5996
Epoch 60/100
317/317  0s 351us/step - accuracy: 0.9568 - loss: 0.1323 - val_accuracy: 0.8326 - val_loss: 0.5967
Epoch 61/100
317/317  0s 347us/step - accuracy: 0.9587 - loss: 0.1232 - val_accuracy: 0.8283 - val_loss: 0.5802
Epoch 62/100
317/317  0s 346us/step - accuracy: 0.9572 - loss: 0.1475 - val_accuracy: 0.8267 - val_loss: 0.6030
Epoch 63/100
317/317  0s 348us/step - accuracy: 0.9628 - loss: 0.1194 - val_accuracy: 0.8279 - val_loss: 0.6128
Epoch 64/100
317/317  0s 368us/step - accuracy: 0.9615 - loss: 0.1162 - val_accuracy: 0.8302 - val_loss: 0.6099
Epoch 65/100
317/317  0s 369us/step - accuracy: 0.9601 - loss: 0.1192 - val_accuracy: 0.8314 - val_loss: 0.6176
Epoch 66/100
317/317  0s 390us/step - accuracy: 0.9592 - loss: 0.1237 - val_accuracy: 0.8298 - val_loss: 0.6224
Epoch 67/100
317/317  0s 355us/step - accuracy: 0.9606 - loss: 0.1188 - val_accuracy: 0.8223 - val_loss: 0.6446
Epoch 68/100
317/317  0s 365us/step - accuracy: 0.9616 - loss: 0.1147 - val_accuracy: 0.8322 - val_loss: 0.6292
Epoch 69/100
317/317  0s 548us/step - accuracy: 0.9624 - loss: 0.1157 - val_accuracy: 0.8330 - val_loss: 0.6557
Epoch 70/100
317/317  0s 365us/step - accuracy: 0.9615 - loss: 0.1165 - val_accuracy: 0.8176 - val_loss: 0.6655
Epoch 71/100
317/317  0s 404us/step - accuracy: 0.9618 - loss: 0.1145 - val_accuracy: 0.8298 - val_loss: 0.6618
Epoch 72/100
317/317  0s 406us/step - accuracy: 0.9617 - loss: 0.1177 - val_accuracy: 0.8018 - val_loss: 0.6880
Epoch 73/100
317/317  0s 368us/step - accuracy: 0.9627 - loss: 0.1134 - val_accuracy: 0.8330 - val_loss: 0.6874
Epoch 74/100
317/317  0s 365us/step - accuracy: 0.9636 - loss: 0.1144 - val_accuracy: 0.8279 - val_loss: 0.6749
Epoch 75/100
317/317  0s 348us/step - accuracy: 0.9638 - loss: 0.1070 - val_accuracy: 0.8239 - val_loss: 0.6767
Epoch 76/100
317/317  0s 377us/step - accuracy: 0.9642 - loss: 0.1097 - val_accuracy: 0.8330 - val_loss: 0.6812
Epoch 77/100
317/317  0s 381us/step - accuracy: 0.9632 - loss: 0.1096 - val_accuracy: 0.8310 - val_loss: 0.6936
Epoch 78/100
317/317  0s 377us/step - accuracy: 0.9667 - loss: 0.1049 - val_accuracy: 0.8255 - val_loss: 0.7077
Epoch 79/100
317/317  0s 376us/step - accuracy: 0.9657 - loss: 0.1040 - val_accuracy: 0.8251 - val_loss: 0.6987
Epoch 80/100
317/317  0s 347us/step - accuracy: 0.9640 - loss: 0.1099 - val_accuracy: 0.8220 - val_loss: 0.7047
Epoch 81/100
317/317  0s 343us/step - accuracy: 0.9661 - loss: 0.1059 - val_accuracy: 0.8243 - val_loss: 0.7189
Epoch 82/100
317/317  0s 493us/step - accuracy: 0.9657 - loss: 0.1057 - val_accuracy: 0.8330 - val_loss: 0.7181
Epoch 83/100
317/317  0s 364us/step - accuracy: 0.9708 - loss: 0.0954 - val_accuracy: 0.8295 - val_loss: 0.7182
Epoch 84/100
317/317  0s 389us/step - accuracy: 0.9676 - loss: 0.1044 - val_accuracy: 0.8247 - val_loss: 0.7164
Epoch 85/100
317/317  0s 372us/step - accuracy: 0.9695 - loss: 0.1019 - val_accuracy: 0.8306 - val_loss: 0.7415
Epoch 86/100
317/317  0s 336us/step - accuracy: 0.9633 - loss: 0.1033 - val_accuracy: 0.8302 - val_loss: 0.7433
Epoch 87/100
317/317  0s 342us/step - accuracy: 0.9687 - loss: 0.1010 - val_accuracy: 0.8302 - val_loss: 0.7664
Epoch 88/100
317/317  0s 372us/step - accuracy: 0.9683 - loss: 0.0958 - val_accuracy: 0.8279 - val_loss: 0.7715
Epoch 89/100
317/317  0s 342us/step - accuracy: 0.9671 - loss: 0.1022 - val_accuracy: 0.8298 - val_loss: 0.7661
Epoch 90/100
317/317  0s 355us/step - accuracy: 0.9731 - loss: 0.0872 - val_accuracy: 0.8397 - val_loss: 0.7873
Epoch 91/100
317/317  0s 380us/step - accuracy: 0.9685 - loss: 0.0973 - val_accuracy: 0.8220 - val_loss: 0.7802
Epoch 92/100
317/317  0s 375us/step - accuracy: 0.9667 - loss: 0.1003 - val_accuracy: 0.8259 - val_loss: 0.7762
Epoch 93/100
317/317  0s 374us/step - accuracy: 0.9679 - loss: 0.0941 - val_accuracy: 0.8259 - val_loss: 0.7889
Epoch 94/100
317/317  0s 391us/step - accuracy: 0.9719 - loss: 0.0876 - val_accuracy: 0.8298 - val_loss: 0.7972
Epoch 95/100
317/317  0s 386us/step - accuracy: 0.9744 - loss: 0.0854 - val_accuracy: 0.8156 - val_loss: 0.8149
Epoch 96/100
317/317  0s 374us/step - accuracy: 0.9703 - loss: 0.0935 - val_accuracy: 0.8302 - val_loss: 0.8087
Epoch 97/100
317/317  0s 345us/step - accuracy: 0.9698 - loss: 0.0894 - val_accuracy: 0.8295 - val_loss: 0.8223
Epoch 98/100
317/317  0s 489us/step - accuracy: 0.9695 - loss: 0.0954 - val_accuracy: 0.8318 - val_loss: 0.8068
Epoch 99/100
317/317  0s 380us/step - accuracy: 0.9722 - loss: 0.0856 - val_accuracy: 0.8338 - val_loss: 0.8178
Epoch 100/100
317/317  0s 377us/step - accuracy: 0.9726 - loss: 0.0874 - val_accuracy: 0.8200 - val_loss: 0.8233</code></pre>
</div>
</div>
<div id="cell-45" class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming history1_nd.history['accuracy'] and history2_nd.history['accuracy'] are available</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co"># These lists should contain the accuracy for each epoch</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(history1_nd.history[<span class="st">'accuracy'</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting accuracy for history1_nd</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(epochs, history1_nd.history[<span class="st">'accuracy'</span>], color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Model 1 Accuracy'</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting accuracy for history2_nd</span></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(epochs, history2_nd.history[<span class="st">'accuracy'</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Model 2 Accuracy'</span>)</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparison of Model Accuracies Across Epochs'</span>)</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="preprocess_disc_bias_files/figure-html/cell-39-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract predictions and sensitive attributes</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, predictions)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate discrimination based on sensitive attribute</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&lt;=</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&gt;</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>        sum_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(mask_unpriv)</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>        sum_priv <span class="op">=</span> K.<span class="bu">sum</span>(mask_priv)</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_unpriv) <span class="op">/</span> (sum_unpriv <span class="op">+</span> epsilon)</span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_priv) <span class="op">/</span> (sum_priv <span class="op">+</span> epsilon)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug outputs</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Standard Loss:", standard_loss, "Discrimination:", discrimination)</span></span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total loss with discrimination penalty</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Model compilation</span></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(lambda_val<span class="op">=</span><span class="fl">0.01</span>),  <span class="co"># Change lambda_val as needed</span></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the model</span></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
317/317  0s 622us/step - accuracy: 0.6523 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 2/5
317/317  0s 440us/step - accuracy: 0.6535 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 3/5
317/317  0s 382us/step - accuracy: 0.6649 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 4/5
317/317  0s 425us/step - accuracy: 0.6511 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 5/5
317/317  0s 416us/step - accuracy: 0.6526 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930</code></pre>
</div>
</div>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming history1_nd and history2_nd have 'accuracy' data from your model training</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracting the accuracy data from each history object</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>accuracy1 <span class="op">=</span> history1_nd.history[<span class="st">'accuracy'</span>]</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>accuracy2 <span class="op">=</span> history2_nd.history[<span class="st">'accuracy'</span>]</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming the number of epochs is determined by the length of the accuracy data</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(accuracy1) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot using Plotly</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure()</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding scatter plot for Model 1</span></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>epochs,</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>accuracy1,</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">'markers'</span>,  <span class="co"># This specifies to use markers (points)</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'Model 1 Accuracy'</span>,  <span class="co"># Name of the series</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'blue'</span>, size<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Marker settings</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding scatter plot for Model 2</span></span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>epochs,</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>accuracy2,</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">'markers'</span>,</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'Model 2 Accuracy'</span>,</span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'red'</span>, size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the layout to add titles and labels</span></span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Comparison of Model Accuracies Across Epochs'</span>,</span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>    xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a>    yaxis_title<span class="op">=</span><span class="st">'Accuracy'</span>,</span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>    template<span class="op">=</span><span class="st">'plotly_white'</span></span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the figure</span></span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="graph" class="level3">
<h3 class="anchored" data-anchor-id="graph">GRAPH</h3>
<div id="cell-50" class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_accuracy(histories):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for accuracy</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add accuracy traces with specified colors</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'accuracy'</span>]))),</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'accuracy'</span>],</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_accuracy'</span>]))),</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_accuracy'</span>],</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for accuracy graph</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Accuracy'</span>,</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Accuracy'</span>,</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(histories):</span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for loss</span></span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add loss traces with specified colors</span></span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'loss'</span>]))),</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'loss'</span>],</span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_loss'</span>]))),</span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_loss'</span>],</span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for loss graph</span></span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Loss'</span>,</span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Loss'</span>,</span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb63-56"><a href="#cb63-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-57"><a href="#cb63-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have history1 and history2 as the history objects from your model training</span></span>
<span id="cb63-58"><a href="#cb63-58" aria-hidden="true" tabindex="-1"></a>plot_accuracy([(<span class="st">'Custom Loss'</span>, history1_nd), (<span class="st">'Standard Loss'</span>, history2)])</span>
<span id="cb63-59"><a href="#cb63-59" aria-hidden="true" tabindex="-1"></a>plot_loss([(<span class="st">'Custom Loss'</span>, history1_nd), (<span class="st">'Standard Loss'</span>, history2)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>lfr_model <span class="op">=</span> LFR(unprivileged_groups<span class="op">=</span>unprivileged_groups, </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>                privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model and transform the dataset</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>lfr_model.fit(dataset_orig_panel19_train)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_lfr <span class="op">=</span> lfr_model.transform(dataset_orig_panel19_train, threshold <span class="op">=</span> <span class="fl">0.22</span>)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_lfr <span class="op">=</span> dataset_orig_panel19_train.align_datasets(dataset_transf_panel19_train_lfr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_distortion_meps(vold, vnew):</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize distortion score</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    distortion_score <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define weights for different categories of attributes</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    sensitive_weight <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    health_status_weight <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    socio_economic_weight <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    behavior_weight <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sensitive attributes</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> [<span class="st">'AGE'</span>, <span class="st">'RACE'</span>, <span class="st">'SEX=1'</span>, <span class="st">'SEX=2'</span>]:</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vold[attr] <span class="op">!=</span> vnew[attr]:</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>            distortion_score <span class="op">+=</span> sensitive_weight</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Health status indicators</span></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    health_attrs <span class="op">=</span> [<span class="st">'PCS42'</span>, <span class="st">'MCS42'</span>, <span class="st">'K6SUM42'</span>, <span class="st">'HIBPDX'</span>, <span class="st">'DIABDX'</span>, <span class="st">'CHDDX'</span>, <span class="st">'ANGIDX'</span>, <span class="st">'MIDX'</span>, <span class="st">'OHRTDX'</span>, <span class="st">'STRKDX'</span>, <span class="st">'EMPHDX'</span>, <span class="st">'CANCERDX'</span>, <span class="st">'JTPAIN'</span>, <span class="st">'ARTHDX'</span>, <span class="st">'ASTHDX'</span>, <span class="st">'ADHDADDX'</span>]</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> health_attrs:</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assuming health status attributes are numerical and a difference in value indicates a change in health status</span></span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>        distortion_score <span class="op">+=</span> health_status_weight <span class="op">*</span> <span class="bu">abs</span>(vold.get(attr, <span class="dv">0</span>) <span class="op">-</span> vnew.get(attr, <span class="dv">0</span>))</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Socioeconomic and environmental factors</span></span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>    socio_attrs <span class="op">=</span> [<span class="st">'REGION=1'</span>, <span class="st">'REGION=2'</span>, <span class="st">'REGION=3'</span>, <span class="st">'REGION=4'</span>, <span class="st">'MARRY'</span>, <span class="st">'FTSTU'</span>, <span class="st">'EMPST'</span>, <span class="st">'POVCAT'</span>, <span class="st">'INSCOV'</span>]</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> socio_attrs:</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vold[attr] <span class="op">!=</span> vnew[attr]:</span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>            distortion_score <span class="op">+=</span> socio_economic_weight</span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Health-related behaviors</span></span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>    behavior_attrs <span class="op">=</span> [<span class="st">'ADSMOK42'</span>, <span class="st">'WLKLIM'</span>, <span class="st">'ACTLIM'</span>, <span class="st">'SOCLIM'</span>, <span class="st">'COGLIM'</span>]</span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> behavior_attrs:</span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vold[attr] <span class="op">!=</span> vnew[attr]:</span>
<span id="cb67-34"><a href="#cb67-34" aria-hidden="true" tabindex="-1"></a>            distortion_score <span class="op">+=</span> behavior_weight</span>
<span id="cb67-35"><a href="#cb67-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-36"><a href="#cb67-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> distortion_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>DI <span class="op">=</span> DisparateImpactRemover()</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_di <span class="op">=</span> DI.fit_transform(dataset_orig_panel19_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-54" class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_rw,</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>test_results_rw<span class="op">=</span>[explainer_transf_panel19_train.mean_difference()</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.consistency()</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.statistical_parity_difference()</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.disparate_impact()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_di,</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>test_results_di<span class="op">=</span>[explainer_transf_panel19_train.mean_difference()</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.consistency()</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.statistical_parity_difference()</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.disparate_impact()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_lfr,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>test_results_lfr<span class="op">=</span>[explainer_transf_panel19_train.mean_difference()</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.consistency()</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.statistical_parity_difference()</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.disparate_impact()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide
  return metric_fun(privileged=False) / metric_fun(privileged=True)</code></pre>
</div>
</div>
<div id="cell-57" class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>test_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',
 'Consistency (Zemel, et al. 2013): [0.83660139]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']</code></pre>
</div>
</div>
<div id="cell-58" class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>test_results_rw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -2.7755575615628914e-17',
 'Consistency (Zemel, et al. 2013): [0.83660139]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -2.7755575615628914e-17',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999999']</code></pre>
</div>
</div>
<div id="cell-59" class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>test_results_lfr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.0',
 'Consistency (Zemel, et al. 2013): [1.]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.0',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): nan']</code></pre>
</div>
</div>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>test_results_di</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',
 'Consistency (Zemel, et al. 2013): [0.83689198]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']</code></pre>
</div>
</div>
<div id="cell-61" class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset_transf_panel19_train_rw.features</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset_transf_panel19_train_rw.labels.ravel()  <span class="co"># Flatten the label array if necessary</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> dataset_transf_panel19_train_rw.instance_weights</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> dataset_transf_panel19_train_rw.feature_names</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>df_rw <span class="op">=</span> pd.DataFrame(features, columns<span class="op">=</span>feature_names)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>df_rw[<span class="st">'label'</span>] <span class="op">=</span> label</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>df_rw[<span class="st">'weights'</span>] <span class="op">=</span> weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-62" class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>df_rw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">weights</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>53.0</td>
<td>1.0</td>
<td>25.93</td>
<td>58.47</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>17459.483776</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>56.0</td>
<td>1.0</td>
<td>20.42</td>
<td>26.57</td>
<td>17.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>14515.313940</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>23.0</td>
<td>1.0</td>
<td>53.12</td>
<td>50.33</td>
<td>7.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>18465.607681</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>21762.696983</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>27.0</td>
<td>0.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>3727.042408</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4909.081729</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>2.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4184.786789</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>54.0</td>
<td>0.0</td>
<td>43.97</td>
<td>42.45</td>
<td>24.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4427.370919</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>73.0</td>
<td>0.0</td>
<td>42.68</td>
<td>43.46</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>6010.846084</td>
</tr>
</tbody>
</table>

<p>15830 rows  140 columns</p>
</div>
</div>
</div>
<div id="cell-63" class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset_orig_panel19_train.features</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset_orig_panel19_train.labels</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> dataset_orig_panel19_train.feature_names</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train_df <span class="op">=</span> pd.DataFrame(features, columns<span class="op">=</span>feature_names)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train_df[<span class="st">'label'</span>] <span class="op">=</span> label</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">EMPST=4</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>53.0</td>
<td>1.0</td>
<td>25.93</td>
<td>58.47</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>56.0</td>
<td>1.0</td>
<td>20.42</td>
<td>26.57</td>
<td>17.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>23.0</td>
<td>1.0</td>
<td>53.12</td>
<td>50.33</td>
<td>7.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>27.0</td>
<td>0.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>2.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>54.0</td>
<td>0.0</td>
<td>43.97</td>
<td>42.45</td>
<td>24.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>73.0</td>
<td>0.0</td>
<td>42.68</td>
<td>43.46</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

<p>15830 rows  139 columns</p>
</div>
</div>
</div>
<div id="cell-64" class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Assuming `dataset_transf_panel19_train_lfr` is your LFR transformed dataset</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="co"># features_lfr = dataset_transf_panel19_train_lfr.features</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="co"># label_lfr = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="co"># feature_names_lfr = dataset_transf_panel19_train_lfr.feature_names</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create a DataFrame</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="co"># df_lfr = pd.DataFrame(features_lfr, columns=feature_names_lfr)</span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="co"># df_lfr['label'] = label_lfr</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset_transf_panel19_train_lfr.features</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset_transf_panel19_train_lfr.labels.ravel()  <span class="co"># Flatten the label array if necessary</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> dataset_transf_panel19_train_lfr.instance_weights</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> dataset_transf_panel19_train_lfr.feature_names</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>df_lfr <span class="op">=</span> pd.DataFrame(features, columns<span class="op">=</span>feature_names)</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>df_lfr[<span class="st">'label'</span>] <span class="op">=</span> label</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>df_lfr[<span class="st">'weights'</span>] <span class="op">=</span> weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-65" class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>df_lfr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">weights</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.364698</td>
<td>0.611075</td>
<td>0.319138</td>
<td>0.516439</td>
<td>0.683071</td>
<td>0.513830</td>
<td>0.556535</td>
<td>0.363196</td>
<td>0.402933</td>
<td>0.589060</td>
<td>...</td>
<td>0.238986</td>
<td>0.553776</td>
<td>0.493602</td>
<td>0.513294</td>
<td>0.386629</td>
<td>0.380919</td>
<td>0.498550</td>
<td>0.363095</td>
<td>0.0</td>
<td>21854.981705</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.379547</td>
<td>0.597793</td>
<td>0.328517</td>
<td>0.502386</td>
<td>0.682940</td>
<td>0.523566</td>
<td>0.577716</td>
<td>0.373460</td>
<td>0.395062</td>
<td>0.600292</td>
<td>...</td>
<td>0.237634</td>
<td>0.551191</td>
<td>0.504359</td>
<td>0.533097</td>
<td>0.374361</td>
<td>0.356593</td>
<td>0.499183</td>
<td>0.369585</td>
<td>0.0</td>
<td>18169.604822</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.345017</td>
<td>0.593873</td>
<td>0.327056</td>
<td>0.509281</td>
<td>0.682566</td>
<td>0.524971</td>
<td>0.551491</td>
<td>0.349275</td>
<td>0.416220</td>
<td>0.575195</td>
<td>...</td>
<td>0.246142</td>
<td>0.570321</td>
<td>0.511401</td>
<td>0.519411</td>
<td>0.420631</td>
<td>0.397081</td>
<td>0.509235</td>
<td>0.382303</td>
<td>0.0</td>
<td>17191.832515</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.370890</td>
<td>0.582912</td>
<td>0.338159</td>
<td>0.494210</td>
<td>0.676912</td>
<td>0.535713</td>
<td>0.577500</td>
<td>0.369280</td>
<td>0.388361</td>
<td>0.583862</td>
<td>...</td>
<td>0.233116</td>
<td>0.577892</td>
<td>0.512044</td>
<td>0.551296</td>
<td>0.382591</td>
<td>0.373165</td>
<td>0.506936</td>
<td>0.374916</td>
<td>0.0</td>
<td>20261.485463</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.377148</td>
<td>0.587833</td>
<td>0.331081</td>
<td>0.493269</td>
<td>0.679347</td>
<td>0.528798</td>
<td>0.582084</td>
<td>0.376288</td>
<td>0.383771</td>
<td>0.602294</td>
<td>...</td>
<td>0.238654</td>
<td>0.558482</td>
<td>0.514984</td>
<td>0.543083</td>
<td>0.382213</td>
<td>0.364254</td>
<td>0.495283</td>
<td>0.377611</td>
<td>0.0</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>0.343260</td>
<td>0.597189</td>
<td>0.324804</td>
<td>0.512213</td>
<td>0.683004</td>
<td>0.522468</td>
<td>0.548023</td>
<td>0.347933</td>
<td>0.417919</td>
<td>0.574523</td>
<td>...</td>
<td>0.246629</td>
<td>0.568838</td>
<td>0.508985</td>
<td>0.514854</td>
<td>0.421901</td>
<td>0.399881</td>
<td>0.508553</td>
<td>0.380749</td>
<td>0.0</td>
<td>4111.315754</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>0.343186</td>
<td>0.596636</td>
<td>0.325113</td>
<td>0.511802</td>
<td>0.682861</td>
<td>0.522864</td>
<td>0.548279</td>
<td>0.347965</td>
<td>0.417634</td>
<td>0.574357</td>
<td>...</td>
<td>0.246577</td>
<td>0.569381</td>
<td>0.509405</td>
<td>0.515495</td>
<td>0.422105</td>
<td>0.400035</td>
<td>0.508671</td>
<td>0.381059</td>
<td>0.0</td>
<td>5415.228173</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>0.364434</td>
<td>0.584266</td>
<td>0.332311</td>
<td>0.494401</td>
<td>0.677981</td>
<td>0.531335</td>
<td>0.573192</td>
<td>0.367677</td>
<td>0.389164</td>
<td>0.590592</td>
<td>...</td>
<td>0.240138</td>
<td>0.570064</td>
<td>0.517921</td>
<td>0.541949</td>
<td>0.397951</td>
<td>0.380818</td>
<td>0.500083</td>
<td>0.381961</td>
<td>0.0</td>
<td>3896.116219</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>0.364365</td>
<td>0.591464</td>
<td>0.330698</td>
<td>0.502169</td>
<td>0.683230</td>
<td>0.527371</td>
<td>0.569395</td>
<td>0.362334</td>
<td>0.406439</td>
<td>0.589670</td>
<td>...</td>
<td>0.242613</td>
<td>0.560312</td>
<td>0.511955</td>
<td>0.530772</td>
<td>0.397606</td>
<td>0.370904</td>
<td>0.505687</td>
<td>0.379358</td>
<td>0.0</td>
<td>4883.851005</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>0.369746</td>
<td>0.591171</td>
<td>0.330650</td>
<td>0.499689</td>
<td>0.681767</td>
<td>0.527452</td>
<td>0.573792</td>
<td>0.367704</td>
<td>0.397916</td>
<td>0.593893</td>
<td>...</td>
<td>0.240415</td>
<td>0.559683</td>
<td>0.511546</td>
<td>0.535024</td>
<td>0.389960</td>
<td>0.368115</td>
<td>0.501997</td>
<td>0.377222</td>
<td>0.0</td>
<td>6630.588948</td>
</tr>
</tbody>
</table>

<p>15830 rows  140 columns</p>
</div>
</div>
</div>
<div id="cell-66" class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming `dataset_transf_panel19_train_di` is your DI transformed dataset</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>features_di <span class="op">=</span> dataset_transf_panel19_train_di.features</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>label_di <span class="op">=</span> dataset_transf_panel19_train_di.labels.ravel()  <span class="co"># Flatten the label array if necessary</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>feature_names_di <span class="op">=</span> dataset_transf_panel19_train_di.feature_names</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>df_di <span class="op">=</span> pd.DataFrame(features_di, columns<span class="op">=</span>feature_names_di)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>df_di[<span class="st">'label'</span>] <span class="op">=</span> label_di</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>df_di</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">EMPST=4</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>53.0</td>
<td>1.0</td>
<td>25.93</td>
<td>58.47</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>56.0</td>
<td>1.0</td>
<td>20.42</td>
<td>26.53</td>
<td>17.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>23.0</td>
<td>1.0</td>
<td>52.92</td>
<td>50.28</td>
<td>7.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>27.0</td>
<td>0.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>25.0</td>
<td>0.0</td>
<td>56.68</td>
<td>62.11</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>25.0</td>
<td>0.0</td>
<td>56.68</td>
<td>62.11</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>2.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>54.0</td>
<td>0.0</td>
<td>43.43</td>
<td>42.45</td>
<td>24.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>73.0</td>
<td>0.0</td>
<td>41.83</td>
<td>43.46</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

<p>15830 rows  139 columns</p>
</div>
</div>
</div>
<div id="cell-67" class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>dataset_orig_panel19_train_df</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features, labels, and weights</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop([<span class="st">'label'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'label'</span>]</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the sample_weight parameter</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to extract the corresponding weights for the training samples</span></span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> X_train.index</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>train_weights <span class="op">=</span> weights[train_indices]</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model.predict_proba(X_train)</span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb87-42"><a href="#cb87-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-43"><a href="#cb87-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb87-44"><a href="#cb87-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-45"><a href="#cb87-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-46"><a href="#cb87-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-47"><a href="#cb87-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb87-48"><a href="#cb87-48" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb87-49"><a href="#cb87-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-50"><a href="#cb87-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb87-51"><a href="#cb87-51" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb87-52"><a href="#cb87-52" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb87-53"><a href="#cb87-53" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb87-54"><a href="#cb87-54" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb87-55"><a href="#cb87-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-56"><a href="#cb87-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb87-57"><a href="#cb87-57" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb87-58"><a href="#cb87-58" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb87-59"><a href="#cb87-59" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb87-60"><a href="#cb87-60" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb87-61"><a href="#cb87-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-62"><a href="#cb87-62" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb87-63"><a href="#cb87-63" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 0.8669456727732154, Training Log Loss: 0.3230202782236161
Testing Accuracy: 0.8619709412507897, Testing Log Loss: 0.335720614452724</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/1117411220.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()</code></pre>
</div>
</div>
<div id="cell-68" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">1</span>).to_csv(<span class="st">'meps.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-69" class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns</span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features, labels, and weights</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_rw.drop([<span class="st">'label'</span>, <span class="st">'weights'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_rw[<span class="st">'label'</span>]</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> df_rw[<span class="st">'weights'</span>]</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>model_rw <span class="op">=</span> LogisticRegression()</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the sample_weight parameter</span></span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to extract the corresponding weights for the training samples</span></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> X_train.index</span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>train_weights <span class="op">=</span> weights[train_indices]</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>model_rw.fit(X_train, y_train, sample_weight<span class="op">=</span>train_weights)</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model_rw.predict(X_train)</span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model_rw.predict(X_test)</span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model_rw.predict_proba(X_train)</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model_rw.predict_proba(X_test)</span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb91-41"><a href="#cb91-41" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb91-42"><a href="#cb91-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-43"><a href="#cb91-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb91-44"><a href="#cb91-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb91-45"><a href="#cb91-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb91-46"><a href="#cb91-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-47"><a href="#cb91-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb91-48"><a href="#cb91-48" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb91-49"><a href="#cb91-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-50"><a href="#cb91-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb91-51"><a href="#cb91-51" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb91-52"><a href="#cb91-52" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb91-53"><a href="#cb91-53" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb91-54"><a href="#cb91-54" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb91-55"><a href="#cb91-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-56"><a href="#cb91-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb91-57"><a href="#cb91-57" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb91-58"><a href="#cb91-58" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb91-59"><a href="#cb91-59" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb91-60"><a href="#cb91-60" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb91-61"><a href="#cb91-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-62"><a href="#cb91-62" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb91-63"><a href="#cb91-63" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 0.8651295009475679, Training Log Loss: 0.33949214668723315
Testing Accuracy: 0.8562855337965888, Testing Log Loss: 0.3484870792605314</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/2225796879.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()</code></pre>
</div>
</div>
<div id="cell-70" class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features, labels, and weights</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_di.drop([<span class="st">'label'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_di[<span class="st">'label'</span>]</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>model_di <span class="op">=</span> LogisticRegression()</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the sample_weight parameter</span></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to extract the corresponding weights for the training samples</span></span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> X_train.index</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>train_weights <span class="op">=</span> weights[train_indices]</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>model_di.fit(X_train, y_train, sample_weight<span class="op">=</span>train_weights)</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model_di.predict(X_train)</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model_di.predict(X_test)</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model_di.predict_proba(X_train)</span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model_di.predict_proba(X_test)</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb94-48"><a href="#cb94-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-49"><a href="#cb94-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb94-50"><a href="#cb94-50" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb94-51"><a href="#cb94-51" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb94-52"><a href="#cb94-52" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb94-53"><a href="#cb94-53" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb94-54"><a href="#cb94-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-55"><a href="#cb94-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb94-56"><a href="#cb94-56" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb94-57"><a href="#cb94-57" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb94-58"><a href="#cb94-58" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb94-59"><a href="#cb94-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb94-60"><a href="#cb94-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-61"><a href="#cb94-61" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb94-62"><a href="#cb94-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 0.8646557169930512, Training Log Loss: 0.33943553174854924
Testing Accuracy: 0.8550221099178774, Testing Log Loss: 0.3481697691277553</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/789110131.py:62: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()</code></pre>
</div>
</div>
<div id="cell-71" class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df_lfr' is your Pandas DataFrame with features and 'label' columns, </span></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="co"># resulting from the LFR transformation</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features and labels</span></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_lfr.drop([<span class="st">'label'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_lfr[<span class="st">'label'</span>]</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y)</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>model_lfr <span class="op">=</span> LogisticRegression()</span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model (no sample weights are needed as the data is already transformed by LFR)</span></span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>model_lfr.fit(X_train, y_train, sample_weight<span class="op">=</span>train_weights)</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model_lfr.predict(X_train)</span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model_lfr.predict(X_test)</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb97-28"><a href="#cb97-28" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model_lfr.predict_proba(X_train)</span>
<span id="cb97-29"><a href="#cb97-29" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model_lfr.predict_proba(X_test)</span>
<span id="cb97-30"><a href="#cb97-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-31"><a href="#cb97-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb97-32"><a href="#cb97-32" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb97-33"><a href="#cb97-33" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb97-34"><a href="#cb97-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-35"><a href="#cb97-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb97-36"><a href="#cb97-36" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb97-37"><a href="#cb97-37" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb97-38"><a href="#cb97-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-39"><a href="#cb97-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb97-40"><a href="#cb97-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb97-41"><a href="#cb97-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb97-42"><a href="#cb97-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-43"><a href="#cb97-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb97-44"><a href="#cb97-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb97-45"><a href="#cb97-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-46"><a href="#cb97-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb97-47"><a href="#cb97-47" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb97-48"><a href="#cb97-48" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb97-49"><a href="#cb97-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb97-50"><a href="#cb97-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb97-51"><a href="#cb97-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-52"><a href="#cb97-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb97-53"><a href="#cb97-53" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb97-54"><a href="#cb97-54" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb97-55"><a href="#cb97-55" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb97-56"><a href="#cb97-56" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb97-57"><a href="#cb97-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-58"><a href="#cb97-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb97-59"><a href="#cb97-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[50], line 21</span>
<span class="ansi-green-fg ansi-bold">     18</span> model_lfr <span style="color:rgb(98,98,98)">=</span> LogisticRegression()
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># Train the model (no sample weights are needed as the data is already transformed by LFR)</span>
<span class="ansi-green-fg">---&gt; 21</span> <span class="ansi-yellow-bg">model_lfr</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">fit</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">X_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">y_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">sample_weight</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">train_weights</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     23</span> <span style="font-style:italic;color:rgb(95,135,135)"># Predictions</span>
<span class="ansi-green-fg ansi-bold">     24</span> y_train_pred <span style="color:rgb(98,98,98)">=</span> model_lfr<span style="color:rgb(98,98,98)">.</span>predict(X_train)

File <span class="ansi-green-fg">/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474</span>, in <span class="ansi-cyan-fg">_fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="ansi-blue-fg">(estimator, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1467</span>     estimator<span style="color:rgb(98,98,98)">.</span>_validate_params()
<span class="ansi-green-fg ansi-bold">   1469</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> config_context(
<span class="ansi-green-fg ansi-bold">   1470</span>     skip_parameter_validation<span style="color:rgb(98,98,98)">=</span>(
<span class="ansi-green-fg ansi-bold">   1471</span>         prefer_skip_nested_validation <span style="font-weight:bold;color:rgb(175,0,255)">or</span> global_skip_validation
<span class="ansi-green-fg ansi-bold">   1472</span>     )
<span class="ansi-green-fg ansi-bold">   1473</span> ):
<span class="ansi-green-fg">-&gt; 1474</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">fit_method</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">estimator</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1246</span>, in <span class="ansi-cyan-fg">LogisticRegression.fit</span><span class="ansi-blue-fg">(self, X, y, sample_weight)</span>
<span class="ansi-green-fg ansi-bold">   1244</span> classes_ <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>classes_
<span class="ansi-green-fg ansi-bold">   1245</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> n_classes <span style="color:rgb(98,98,98)">&lt;</span> <span style="color:rgb(98,98,98)">2</span>:
<span class="ansi-green-fg">-&gt; 1246</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">   1247</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">This solver needs samples of at least 2 classes</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1248</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> in the data, but the data contains only one</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1249</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> class: </span><span style="font-weight:bold;color:rgb(175,95,135)">%r</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1250</span>         <span style="color:rgb(98,98,98)">%</span> classes_[<span style="color:rgb(98,98,98)">0</span>]
<span class="ansi-green-fg ansi-bold">   1251</span>     )
<span class="ansi-green-fg ansi-bold">   1253</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">len</span>(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>classes_) <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(98,98,98)">2</span>:
<span class="ansi-green-fg ansi-bold">   1254</span>     n_classes <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">1</span>

<span class="ansi-red-fg">ValueError</span>: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0</pre>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/srinivassundar98\.github\.io\/Ensuring-Fair-Play\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>