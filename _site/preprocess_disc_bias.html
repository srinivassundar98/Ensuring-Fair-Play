<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DEFINING custom_loss FUNCTION with Standard Lambda = 0.5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d4be639c637f3db3c684c66cefad7e0c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DEFINING custom_loss FUNCTION with Standard Lambda = 0.5</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="cell-1" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime, timedelta</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, <span class="st">'../'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.datasets <span class="im">import</span> MEPSDataset19</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Fairness metrics</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.metrics <span class="im">import</span> BinaryLabelDatasetMetric</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Explainers</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.explainers <span class="im">import</span> MetricTextExplainer</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalers</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Bias mitigation techniques</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> Reweighing,DisparateImpactRemover</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> LFR</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> OptimPreproc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-2" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-3" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19 <span class="op">=</span> MEPSDataset19()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-4" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train <span class="op">=</span> MEPSDataset19()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train.features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>array([[53.  ,  1.  , 25.93, ...,  0.  ,  1.  ,  0.  ],
       [56.  ,  1.  , 20.42, ...,  0.  ,  1.  ,  0.  ],
       [23.  ,  1.  , 53.12, ...,  0.  ,  1.  ,  0.  ],
       ...,
       [ 2.  ,  1.  , -1.  , ...,  0.  ,  1.  ,  0.  ],
       [54.  ,  0.  , 43.97, ...,  0.  ,  1.  ,  0.  ],
       [73.  ,  0.  , 42.68, ...,  0.  ,  1.  ,  0.  ]])</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sens_ind <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sens_attr <span class="op">=</span> dataset_orig_panel19_train.protected_attribute_names[sens_ind]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>privileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>sens_attr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>'RACE'</code></pre>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>privileged_groups</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>[{'RACE': 1.0}]</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>unprivileged_groups</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>[{'RACE': 0.0}]</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>               instance weights features                                    \
                                         protected attribute                 
                                     AGE                RACE  PCS42  MCS42   
instance names                                                               
0                  21854.981705     53.0                 1.0  25.93  58.47   
1                  18169.604822     56.0                 1.0  20.42  26.57   
3                  17191.832515     23.0                 1.0  53.12  50.33   
4                  20261.485463      3.0                 1.0  -1.00  -1.00   
5                      0.000000     27.0                 0.0  -1.00  -1.00   
...                         ...      ...                 ...    ...    ...   
16573               4111.315754     25.0                 0.0  56.71  62.39   
16574               5415.228173     25.0                 0.0  56.71  62.39   
16575               3896.116219      2.0                 1.0  -1.00  -1.00   
16576               4883.851005     54.0                 0.0  43.97  42.45   
16577               6630.588948     73.0                 0.0  42.68  43.46   

                                                            ...          \
                                                            ...           
               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   
instance names                                              ...           
0                  3.0      0.0      1.0      0.0      0.0  ...     1.0   
1                 17.0      0.0      1.0      0.0      0.0  ...     1.0   
3                  7.0      0.0      1.0      0.0      0.0  ...     0.0   
4                 -1.0      0.0      1.0      0.0      0.0  ...     0.0   
5                 -1.0      0.0      0.0      1.0      0.0  ...     0.0   
...                ...      ...      ...      ...      ...  ...     ...   
16573              0.0      0.0      0.0      1.0      0.0  ...     0.0   
16574              0.0      0.0      0.0      1.0      0.0  ...     1.0   
16575             -1.0      0.0      0.0      1.0      0.0  ...     0.0   
16576             24.0      1.0      0.0      0.0      0.0  ...     0.0   
16577              0.0      1.0      0.0      0.0      0.0  ...     1.0   

                                                                               \
                                                                                
               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   
instance names                                                                  
0                   1.0      0.0      0.0      0.0      0.0      0.0      1.0   
1                   0.0      0.0      1.0      0.0      0.0      0.0      1.0   
3                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   
4                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   
5                   0.0      0.0      1.0      0.0      0.0      1.0      0.0   
...                 ...      ...      ...      ...      ...      ...      ...   
16573               1.0      0.0      0.0      0.0      0.0      1.0      0.0   
16574               1.0      0.0      0.0      0.0      0.0      1.0      0.0   
16575               1.0      0.0      0.0      0.0      0.0      0.0      1.0   
16576               0.0      0.0      1.0      0.0      0.0      0.0      1.0   
16577               0.0      0.0      1.0      0.0      0.0      0.0      1.0   

                        labels  
                                
               INSCOV=3         
instance names                  
0                   0.0    1.0  
1                   0.0    1.0  
3                   0.0    0.0  
4                   0.0    0.0  
5                   0.0    0.0  
...                 ...    ...  
16573               0.0    0.0  
16574               0.0    0.0  
16575               0.0    0.0  
16576               0.0    0.0  
16577               0.0    0.0  

[15830 rows x 140 columns]</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>metric_orig_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>        dataset_orig_panel19_train,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        privileged_groups<span class="op">=</span>privileged_groups)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-13" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>explainer_orig_panel19_train <span class="op">=</span> MetricTextExplainer(metric_orig_panel19_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-15" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>test_name<span class="op">=</span>[<span class="st">'Mean Difference'</span>,<span class="st">'Consistency'</span>,<span class="st">'Statistical Parity Difference'</span>,<span class="st">'Disparate Impact'</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>test_definitions<span class="op">=</span>[<span class="st">'difference between mean values of two labels'</span>,<span class="st">'Individual fairness metric that measures how similar the labels are for similar instances.'</span>,<span class="st">'Difference in selection rates.'</span>,<span class="st">'ratio of positive outcomes in the unprivileged group divided by the ratio of positive outcomes in the privileged group.'</span>]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[explainer_orig_panel19_train.mean_difference(),explainer_orig_panel19_train.consistency(),explainer_orig_panel19_train.statistical_parity_difference(),explainer_orig_panel19_train.disparate_impact()]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>test_status<span class="op">=</span>[<span class="st">'Bias Detected'</span>,<span class="st">'Bias Not Detected'</span>,<span class="st">'Bias Detected'</span>,<span class="st">'Bias Detected'</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame({<span class="st">'Test Name'</span>:test_name,<span class="st">'Test Definitions'</span>:test_definitions,<span class="st">'Test Results'</span>:test_results,<span class="st">'Test Status'</span>:test_status})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>test_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',
 'Consistency (Zemel, et al. 2013): [0.83665193]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>RW <span class="op">=</span> Reweighing(unprivileged_groups<span class="op">=</span>unprivileged_groups,privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_rw <span class="op">=</span> RW.fit_transform(dataset_orig_panel19_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-18" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Statistical Parity Difference</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>statistical_parity_difference <span class="op">=</span> metric_orig_panel19_train.statistical_parity_difference()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Statistical Parity Difference (SPD):"</span>, statistical_parity_difference)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Disparate Impact</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>disparate_impact <span class="op">=</span> metric_orig_panel19_train.disparate_impact()</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Disparate Impact (DI):"</span>, disparate_impact)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Statistical Parity Difference (SPD): -0.13507447726478142
Disparate Impact (DI): 0.49826823461176517</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>X_train[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>array([37.  ,  0.  , 48.09, 36.94,  7.  ,  0.  ,  0.  ,  1.  ,  0.  ,
        1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
        0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,
        0.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,
        1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,
        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
        1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,
        0.  ,  0.  ,  1.  ])</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dataset_orig_panel19_train.features</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dataset_orig_panel19_train.labels.ravel()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale features</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Train logistic regression model</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_scaled, y_train)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities on the same scaled training data</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>train_probabilities <span class="op">=</span> model.predict_proba(X_train_scaled)[:, <span class="dv">1</span>]</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculation of discrimination index without modifying dataset structure</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>sens_attr_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_discrimination(X, probabilities, sens_attr_index, unprivileged_val, privileged_val):</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter by sensitive attribute for unprivileged and privileged groups</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    unpriv_indices <span class="op">=</span> X[:, sens_attr_index] <span class="op">==</span> unprivileged_val</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    priv_indices <span class="op">=</span> X[:, sens_attr_index] <span class="op">==</span> privileged_val</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean probabilities for both groups</span></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    mean_prob_unpriv <span class="op">=</span> probabilities[unpriv_indices].mean()</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    mean_prob_priv <span class="op">=</span> probabilities[priv_indices].mean()</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Discrimination index</span></span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> mean_prob_priv <span class="op">-</span> mean_prob_unpriv</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> discrimination</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Define unprivileged and privileged values</span></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>unprivileged_val <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>privileged_val <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute discrimination</span></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>discrimination_index <span class="op">=</span> calculate_discrimination(X_train, train_probabilities, sens_attr_index, unprivileged_val, privileged_val)</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Discrimination Index: </span><span class="sc">{:.4f}</span><span class="st">"</span>.<span class="bu">format</span>(discrimination_index))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Discrimination Index: 0.1315</code></pre>
</div>
</div>
<div id="cell-22" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>X_train_scaled</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>array([[-0.76152525, -0.7401269 , -1.16660499, ..., -1.06074916,
        -0.744093  ,  2.78531021],
       [ 0.08369436, -0.7401269 ,  0.72413337, ..., -1.06074916,
        -0.744093  ,  2.78531021],
       [-0.45012855, -0.7401269 ,  0.94174738, ...,  0.94272994,
        -0.744093  , -0.35902644],
       ...,
       [ 0.79545824,  1.35111965,  0.09517111, ..., -1.06074916,
         1.34391803, -0.35902644],
       [-0.09424661, -0.7401269 ,  1.09658071, ...,  0.94272994,
        -0.744093  , -0.35902644],
       [-0.62806952, -0.7401269 ,  0.4680036 , ...,  0.94272994,
        -0.744093  , -0.35902644]])</code></pre>
</div>
</div>
<div id="cell-23" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)  <span class="co"># Adjust input size to exclude sensitive attribute</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom loss function</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(output, target, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.1</span>, k<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.<span class="bu">sum</span>(mask_unpriv) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> torch.<span class="bu">sum</span>(mask_priv) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> torch.mean(output[mask_unpriv])</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> torch.mean(output[mask_priv])</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)  <span class="co"># Handle cases where one group might be missing</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming your dataset is loaded correctly</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()  <span class="co"># Make sure this conversion is done correctly</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)  <span class="co"># Ensure targets are correctly shaped</span></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]  <span class="co"># Extract the sensitive features</span></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Exclude the sensitive attribute from the main features</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> custom_loss(outputs, targets.squeeze(), sensitive_features)</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Loss: 0.6436237692832947
Epoch 2, Loss: 0.6127090454101562
Epoch 3, Loss: 0.5840549468994141
Epoch 4, Loss: 0.557695209980011
Epoch 5, Loss: 0.5335997343063354
Epoch 6, Loss: 0.5117872953414917
Epoch 7, Loss: 0.49214401841163635
Epoch 8, Loss: 0.47453635931015015
Epoch 9, Loss: 0.45879927277565
Epoch 10, Loss: 0.44474470615386963</code></pre>
</div>
</div>
<div id="cell-24" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom loss function</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(output, target, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.01</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.<span class="bu">sum</span>(mask_unpriv) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> torch.<span class="bu">sum</span>(mask_priv) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> torch.mean(output[mask_unpriv])</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> torch.mean(output[mask_priv])</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)  <span class="co"># Handle cases where one group might be missing</span></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_accuracy(predictions, targets):</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> (predictions <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (predicted_classes <span class="op">==</span> targets).<span class="bu">float</span>().mean()</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation</span></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Test data (assuming it's prepared similarly)</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> torch.tensor(X_test_scaled).<span class="bu">float</span>()</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>test_targets <span class="op">=</span> torch.tensor(y_test).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>test_sensitive_features <span class="op">=</span> test_data[:, <span class="dv">1</span>]</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> torch.cat((test_data[:, :<span class="dv">1</span>], test_data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> custom_loss(outputs, targets.squeeze(), sensitive_features)</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model(test_features)</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">=</span> custom_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, '</span></span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>    model.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.7673807740211487, Train Acc: 24.21%, Test Loss: 0.7409172654151917, Test Acc: 31.46%
Epoch 2, Train Loss: 0.7404283881187439, Train Acc: 32.01%, Test Loss: 0.716571569442749, Test Acc: 41.63%
Epoch 3, Train Loss: 0.7157283425331116, Train Acc: 41.29%, Test Loss: 0.6942327618598938, Test Acc: 53.85%
Epoch 4, Train Loss: 0.693112313747406, Train Acc: 53.62%, Test Loss: 0.6738004684448242, Test Acc: 67.66%
Epoch 5, Train Loss: 0.6722973585128784, Train Acc: 68.04%, Test Loss: 0.6548610329627991, Test Acc: 74.76%
Epoch 6, Train Loss: 0.6529971957206726, Train Acc: 75.09%, Test Loss: 0.637105405330658, Test Acc: 77.51%
Epoch 7, Train Loss: 0.6348973512649536, Train Acc: 78.02%, Test Loss: 0.6203395128250122, Test Acc: 79.41%
Epoch 8, Train Loss: 0.6178111433982849, Train Acc: 79.60%, Test Loss: 0.6044129729270935, Test Acc: 80.51%
Epoch 9, Train Loss: 0.6016006469726562, Train Acc: 80.84%, Test Loss: 0.589247465133667, Test Acc: 81.30%
Epoch 10, Train Loss: 0.5861577391624451, Train Acc: 81.67%, Test Loss: 0.5747091770172119, Test Acc: 81.87%</code></pre>
</div>
</div>
<div id="cell-25" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize binary cross-entropy loss</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_accuracy(predictions, targets):</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> (predictions <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (predicted_classes <span class="op">==</span> targets).<span class="bu">float</span>().mean()</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Test data (assuming it's prepared similarly)</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> torch.tensor(X_test_scaled).<span class="bu">float</span>()</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>test_targets <span class="op">=</span> torch.tensor(y_test).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>test_sensitive_features <span class="op">=</span> test_data[:, <span class="dv">1</span>]</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> torch.cat((test_data[:, :<span class="dv">1</span>], test_data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(outputs, targets.squeeze())</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model(test_features)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">=</span> criterion(test_outputs, test_targets.squeeze())</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, '</span></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>    model.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.7674269676208496, Train Acc: 34.36%, Test Loss: 0.7305501103401184, Test Acc: 39.80%
Epoch 2, Train Loss: 0.7305780053138733, Train Acc: 40.48%, Test Loss: 0.6975762248039246, Test Acc: 49.31%
Epoch 3, Train Loss: 0.6973879337310791, Train Acc: 49.25%, Test Loss: 0.668036162853241, Test Acc: 66.20%
Epoch 4, Train Loss: 0.6675889492034912, Train Acc: 66.80%, Test Loss: 0.6415244936943054, Test Acc: 75.43%
Epoch 5, Train Loss: 0.6408513188362122, Train Acc: 75.71%, Test Loss: 0.61774742603302, Test Acc: 78.68%
Epoch 6, Train Loss: 0.6167588233947754, Train Acc: 78.40%, Test Loss: 0.5962265729904175, Test Acc: 80.39%
Epoch 7, Train Loss: 0.5949429273605347, Train Acc: 80.14%, Test Loss: 0.5766510367393494, Test Acc: 81.40%
Epoch 8, Train Loss: 0.5751211643218994, Train Acc: 81.06%, Test Loss: 0.5587018132209778, Test Acc: 81.84%
Epoch 9, Train Loss: 0.5569527745246887, Train Acc: 81.71%, Test Loss: 0.5421833395957947, Test Acc: 82.25%
Epoch 10, Train Loss: 0.5401964783668518, Train Acc: 82.35%, Test Loss: 0.5268954634666443, Test Acc: 82.72%</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset, random_split</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper function to calculate accuracy</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> binary_accuracy(y_pred, y_true):</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applying threshold to get binary output</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    y_pred_tag <span class="op">=</span> torch.<span class="bu">round</span>(y_pred)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    correct_results_sum <span class="op">=</span> (y_pred_tag <span class="op">==</span> y_true).<span class="bu">sum</span>().<span class="bu">float</span>()</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> correct_results_sum <span class="op">/</span> y_true.shape[<span class="dv">0</span>]</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> torch.<span class="bu">round</span>(acc <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom discrimination loss function</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discrimination_loss(outputs, targets, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.5</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    prob_unpriv <span class="op">=</span> torch.mean(outputs[mask_unpriv])</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    prob_priv <span class="op">=</span> torch.mean(outputs[mask_priv])</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple binary crossentropy loss model for comparison</span></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleBinaryClassifier(nn.Module):</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleBinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare datasets</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(features, targets, sensitive_features)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model and optimizer</span></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> train_loader:</span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> custom_loss:</span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a>            train_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> val_loader:</span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs)</span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> custom_loss:</span>
<span id="cb36-91"><a href="#cb36-91" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> loss.item()</span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a>                val_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average loss and accuracy</span></span>
<span id="cb36-98"><a href="#cb36-98" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb36-99"><a href="#cb36-99" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">, Val Acc: </span><span class="sc">{</span>val_acc<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb36-104"><a href="#cb36-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-105"><a href="#cb36-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with discrimination loss</span></span>
<span id="cb36-106"><a href="#cb36-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-107"><a href="#cb36-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Train another model with only binary crossentropy</span></span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span>discrimination_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: nan, Train Acc: 85.77, Val Loss: nan, Val Acc: 85.97
Epoch 2, Train Loss: nan, Train Acc: 86.90, Val Loss: nan, Val Acc: 86.05
Epoch 3, Train Loss: nan, Train Acc: 87.13, Val Loss: nan, Val Acc: 86.30
Epoch 4, Train Loss: nan, Train Acc: 87.40, Val Loss: nan, Val Acc: 86.24
Epoch 5, Train Loss: nan, Train Acc: 87.93, Val Loss: nan, Val Acc: 86.33
Epoch 6, Train Loss: nan, Train Acc: 88.23, Val Loss: nan, Val Acc: 86.01
Epoch 7, Train Loss: nan, Train Acc: 88.21, Val Loss: nan, Val Acc: 86.25
Epoch 8, Train Loss: nan, Train Acc: 88.37, Val Loss: nan, Val Acc: 86.17
Epoch 9, Train Loss: nan, Train Acc: 88.96, Val Loss: nan, Val Acc: 85.75
Epoch 10, Train Loss: nan, Train Acc: 88.86, Val Loss: nan, Val Acc: 85.99</code></pre>
</div>
</div>
<div id="cell-28" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>simple_model <span class="op">=</span> SimpleBinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>simple_optimizer <span class="op">=</span> optim.Adam(simple_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>train_model(simple_model, simple_optimizer, train_loader, val_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.3526, Train Acc: 85.74, Val Loss: 0.3346, Val Acc: 86.34
Epoch 2, Train Loss: 0.3199, Train Acc: 86.70, Val Loss: 0.3338, Val Acc: 85.76
Epoch 3, Train Loss: 0.3129, Train Acc: 87.31, Val Loss: 0.3343, Val Acc: 86.40
Epoch 4, Train Loss: 0.3095, Train Acc: 87.41, Val Loss: 0.3304, Val Acc: 86.11
Epoch 5, Train Loss: 0.3047, Train Acc: 87.72, Val Loss: 0.3305, Val Acc: 86.55
Epoch 6, Train Loss: 0.2998, Train Acc: 88.05, Val Loss: 0.3306, Val Acc: 86.38
Epoch 7, Train Loss: 0.2960, Train Acc: 88.30, Val Loss: 0.3326, Val Acc: 86.10
Epoch 8, Train Loss: 0.2913, Train Acc: 88.62, Val Loss: 0.3351, Val Acc: 85.94
Epoch 9, Train Loss: 0.2864, Train Acc: 88.95, Val Loss: 0.3380, Val Acc: 86.04
Epoch 10, Train Loss: 0.2806, Train Acc: 89.21, Val Loss: 0.3388, Val Acc: 85.90</code></pre>
</div>
</div>
<div id="cell-29" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset, random_split</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper function to calculate accuracy</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> binary_accuracy(y_pred, y_true):</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applying threshold to get binary output</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    y_pred_tag <span class="op">=</span> torch.<span class="bu">round</span>(y_pred)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    correct_results_sum <span class="op">=</span> (y_pred_tag <span class="op">==</span> y_true).<span class="bu">sum</span>().<span class="bu">float</span>()</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> correct_results_sum <span class="op">/</span> y_true.shape[<span class="dv">0</span>]</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> torch.<span class="bu">round</span>(acc <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom discrimination loss function</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discrimination_loss(outputs, targets, sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.5</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>    prob_unpriv <span class="op">=</span> torch.mean(outputs[mask_unpriv])</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>    prob_priv <span class="op">=</span> torch.mean(outputs[mask_priv])</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple binary crossentropy loss model for comparison</span></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleBinaryClassifier(nn.Module):</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleBinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> data[:, <span class="dv">1</span>]  <span class="co"># Extract the sensitive feature</span></span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Exclude the sensitive attribute</span></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup DataLoader</span></span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(features, targets, sensitive_features)</span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-65"><a href="#cb41-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-66"><a href="#cb41-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model and optimizer</span></span>
<span id="cb41-67"><a href="#cb41-67" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb41-68"><a href="#cb41-68" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb41-69"><a href="#cb41-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-70"><a href="#cb41-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb41-71"><a href="#cb41-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-72"><a href="#cb41-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb41-73"><a href="#cb41-73" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb41-74"><a href="#cb41-74" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-75"><a href="#cb41-75" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-76"><a href="#cb41-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> train_loader:</span>
<span id="cb41-77"><a href="#cb41-77" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb41-78"><a href="#cb41-78" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb41-79"><a href="#cb41-79" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> custom_loss:</span>
<span id="cb41-80"><a href="#cb41-80" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb41-81"><a href="#cb41-81" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb41-82"><a href="#cb41-82" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb41-83"><a href="#cb41-83" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb41-84"><a href="#cb41-84" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb41-85"><a href="#cb41-85" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">+=</span> loss.item()</span>
<span id="cb41-86"><a href="#cb41-86" aria-hidden="true" tabindex="-1"></a>            train_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb41-87"><a href="#cb41-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-88"><a href="#cb41-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb41-89"><a href="#cb41-89" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb41-90"><a href="#cb41-90" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-91"><a href="#cb41-91" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-92"><a href="#cb41-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-93"><a href="#cb41-93" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> inputs, labels, sens <span class="kw">in</span> val_loader:</span>
<span id="cb41-94"><a href="#cb41-94" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(inputs)</span>
<span id="cb41-95"><a href="#cb41-95" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> custom_loss:</span>
<span id="cb41-96"><a href="#cb41-96" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> custom_loss(outputs, labels.squeeze(), sens.squeeze())</span>
<span id="cb41-97"><a href="#cb41-97" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb41-98"><a href="#cb41-98" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> nn.BCELoss()(outputs, labels.squeeze())</span>
<span id="cb41-99"><a href="#cb41-99" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> loss.item()</span>
<span id="cb41-100"><a href="#cb41-100" aria-hidden="true" tabindex="-1"></a>                val_acc <span class="op">+=</span> binary_accuracy(outputs, labels.squeeze()).item()</span>
<span id="cb41-101"><a href="#cb41-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-102"><a href="#cb41-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average loss and accuracy</span></span>
<span id="cb41-103"><a href="#cb41-103" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb41-104"><a href="#cb41-104" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">/=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb41-105"><a href="#cb41-105" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb41-106"><a href="#cb41-106" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">/=</span> <span class="bu">len</span>(val_loader)</span>
<span id="cb41-107"><a href="#cb41-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-108"><a href="#cb41-108" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.4f}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>val_loss<span class="sc">:.4f}</span><span class="ss">, Val Acc: </span><span class="sc">{</span>val_acc<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb41-109"><a href="#cb41-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-110"><a href="#cb41-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with discrimination loss</span></span>
<span id="cb41-111"><a href="#cb41-111" aria-hidden="true" tabindex="-1"></a>train_model(model, optimizer, train_loader, val_loader, custom_loss<span class="op">=</span>discrimination_loss)</span>
<span id="cb41-112"><a href="#cb41-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-113"><a href="#cb41-113" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train another model with only binary crossentropy</span></span>
<span id="cb41-114"><a href="#cb41-114" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: nan, Train Acc: 86.01, Val Loss: nan, Val Acc: 86.56
Epoch 2, Train Loss: nan, Train Acc: 86.83, Val Loss: nan, Val Acc: 86.97
Epoch 3, Train Loss: nan, Train Acc: 87.25, Val Loss: nan, Val Acc: 87.54
Epoch 4, Train Loss: nan, Train Acc: 87.22, Val Loss: nan, Val Acc: 87.04
Epoch 5, Train Loss: nan, Train Acc: 87.67, Val Loss: nan, Val Acc: 87.53
Epoch 6, Train Loss: nan, Train Acc: 87.95, Val Loss: nan, Val Acc: 87.24
Epoch 7, Train Loss: nan, Train Acc: 88.06, Val Loss: nan, Val Acc: 87.11
Epoch 8, Train Loss: nan, Train Acc: 88.62, Val Loss: nan, Val Acc: 86.70
Epoch 9, Train Loss: nan, Train Acc: 88.73, Val Loss: nan, Val Acc: 86.92
Epoch 10, Train Loss: nan, Train Acc: 89.14, Val Loss: nan, Val Acc: 86.97</code></pre>
</div>
</div>
<div id="cell-30" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>simple_model <span class="op">=</span> SimpleBinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>simple_optimizer <span class="op">=</span> optim.Adam(simple_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>train_model(simple_model, simple_optimizer, train_loader, val_loader)  <span class="co"># This time without custom loss</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-31" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Layer, Input, Dense</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> BinaryCrossentropy</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DiscriminationLayer(Layer):</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sensitive_index, k<span class="op">=</span><span class="dv">2</span>, <span class="op">**</span>kwargs):</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sensitive_index <span class="op">=</span> sensitive_index</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> k</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>        y_pred, features <span class="op">=</span> inputs</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> features[:, <span class="va">self</span>.sensitive_index]</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> tf.cast(tf.equal(sensitive_attr, <span class="dv">0</span>), dtype<span class="op">=</span>tf.float32)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> tf.cast(tf.equal(sensitive_attr, <span class="dv">1</span>), dtype<span class="op">=</span>tf.float32)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> tf.reduce_sum(y_pred <span class="op">*</span> mask_unpriv) <span class="op">/</span> (tf.reduce_sum(mask_unpriv) <span class="op">+</span> epsilon)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> tf.reduce_sum(y_pred <span class="op">*</span> mask_priv) <span class="op">/</span> (tf.reduce_sum(mask_priv) <span class="op">+</span> epsilon)</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> tf.<span class="bu">pow</span>((prob_priv <span class="op">-</span> prob_unpriv), <span class="va">self</span>.k)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [y_pred, discrimination]</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(y_true, y_pred_and_discrimination, lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    y_pred, discrimination <span class="op">=</span> y_pred_and_discrimination</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, y_pred)</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standard_loss <span class="op">-</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Model building</span></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>input_features <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Update this to your sensitive attribute index</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(input_features)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>disc_layer <span class="op">=</span> DiscriminationLayer(sensitive_index<span class="op">=</span>sensitive_index)([predictions, input_features])</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>input_features, outputs<span class="op">=</span>disc_layer)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Model compilation and training</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>), loss<span class="op">=</span><span class="kw">lambda</span> y_true, y_pred: custom_loss(y_true, y_pred, <span class="fl">0.5</span>), metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg ansi-bold">OperatorNotAllowedInGraphError</span>            Traceback (most recent call last)
Cell <span class="ansi-green-fg ansi-bold">In[60], line 42</span>
<span class="ansi-green-fg">     40</span> <span style="font-style:italic;color:rgb(95,135,135)"># Model compilation and training</span>
<span class="ansi-green-fg">     41</span> model<span style="color:rgb(98,98,98)">.</span>compile(optimizer<span style="color:rgb(98,98,98)">=</span>Adam(learning_rate<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.001</span>), loss<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">lambda</span> y_true, y_pred: custom_loss(y_true, y_pred, <span style="color:rgb(98,98,98)">0.5</span>), metrics<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">accuracy</span><span style="color:rgb(175,0,0)">'</span>])
<span class="ansi-green-fg ansi-bold">---&gt; 42</span> history <span style="color:rgb(98,98,98)">=</span> model<span style="color:rgb(98,98,98)">.</span>fit(X_train_scaled, y_train, epochs<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">10</span>, batch_size<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">32</span>, validation_split<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.2</span>)

File <span class="ansi-green-fg ansi-bold">c:\Users\srinivas\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122</span>, in <span class="ansi-cyan-fg">filter_traceback.&lt;locals&gt;.error_handler</span><span class="ansi-blue-fg ansi-bold">(*args, **kwargs)</span>
<span class="ansi-green-fg">    119</span>     filtered_tb <span style="color:rgb(98,98,98)">=</span> _process_traceback_frames(e<span style="color:rgb(98,98,98)">.</span>__traceback__)
<span class="ansi-green-fg">    120</span>     <span style="font-style:italic;color:rgb(95,135,135)"># To get the full stack trace, call:</span>
<span class="ansi-green-fg">    121</span>     <span style="font-style:italic;color:rgb(95,135,135)"># `keras.config.disable_traceback_filtering()`</span>
<span class="ansi-green-fg ansi-bold">--&gt; 122</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> e<span style="color:rgb(98,98,98)">.</span>with_traceback(filtered_tb) <span style="font-weight:bold;color:rgb(0,135,0)">from</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg">    123</span> <span style="font-weight:bold;color:rgb(0,135,0)">finally</span>:
<span class="ansi-green-fg">    124</span>     <span style="font-weight:bold;color:rgb(0,135,0)">del</span> filtered_tb

File <span class="ansi-green-fg ansi-bold">c:\Users\srinivas\anaconda3\Lib\site-packages\tensorflow\python\eager\polymorphic_function\autograph_util.py:52</span>, in <span class="ansi-cyan-fg">py_func_from_autograph.&lt;locals&gt;.autograph_handler</span><span class="ansi-blue-fg ansi-bold">(*args, **kwargs)</span>
<span class="ansi-green-fg">     50</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:  <span style="font-style:italic;color:rgb(95,135,135)"># pylint:disable=broad-except</span>
<span class="ansi-green-fg">     51</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">hasattr</span>(e, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">ag_error_metadata</span><span style="color:rgb(175,0,0)">"</span>):
<span class="ansi-green-fg ansi-bold">---&gt; 52</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> e<span style="color:rgb(98,98,98)">.</span>ag_error_metadata<span style="color:rgb(98,98,98)">.</span>to_exception(e)
<span class="ansi-green-fg">     53</span>   <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">     54</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span>

<span class="ansi-red-fg ansi-bold">OperatorNotAllowedInGraphError</span>: in user code:

    File "C:\Users\srinivas\AppData\Local\Temp\ipykernel_19508\3984959856.py", line 26, in custom_loss  *
        y_pred, discrimination = y_pred_and_discrimination

    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.
</pre>
</div>
</div>
</div>
<section id="defining-custom_loss-function-with-custom-lambda-here-0.01" class="level3">
<h3 class="anchored" data-anchor-id="defining-custom_loss-function-with-custom-lambda-here-0.01">DEFINING custom_loss FUNCTION with custom Lambda , here 0.01</h3>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> BinaryCrossentropy</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Dense</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(sensitive_attr, lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Creates a custom loss function that incorporates discrimination penalty.</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co">    sensitive_attr (int): Index of the sensitive attribute in the input features.</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co">    lambda_val (float): Regularization strength for the discrimination penalty.</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co">    loss (function): A loss function that takes (y_true, y_pred).</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, y_pred)</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate discrimination index</span></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We assume sensitive attribute is binary and 0 is unprivileged, 1 is privileged</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(K.equal(sensitive_attr, <span class="dv">0</span>), <span class="st">'float32'</span>)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(K.equal(sensitive_attr, <span class="dv">1</span>), <span class="st">'float32'</span>)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Probabilities of positive class</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prob_unpriv = K.mean(y_pred * mask_unpriv) / K.mean(mask_unpriv)</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prob_priv = K.mean(y_pred * mask_priv) / K.mean(mask_priv)</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.mean(y_pred <span class="op">*</span> mask_unpriv) <span class="op">/</span> (K.mean(mask_unpriv) <span class="op">+</span> epsilon)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.mean(y_pred <span class="op">*</span> mask_priv) <span class="op">/</span> (K.mean(mask_priv) <span class="op">+</span> epsilon)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discrimination as the squared difference in probabilities</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(discrimination)</span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Custom loss calculation</span></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">-</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train_scaled.shape[<span class="dv">1</span>]  <span class="co"># Number of features</span></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model architecture</span></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with the custom loss function</span></span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(sensitive_attr<span class="op">=</span>X_train_scaled[:, sensitive_index], lambda_val<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
Tensor("compile_loss/loss/Square:0", shape=(), dtype=float32)
Tensor("compile_loss/loss/Square:0", shape=(), dtype=float32)
299/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8033 - loss: 0.4365Tensor("compile_loss/loss/Square:0", shape=(), dtype=float32)
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8058 - loss: 0.4327 - val_accuracy: 0.8547 - val_loss: 0.3441
Epoch 2/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8763 - loss: 0.3119 - val_accuracy: 0.8543 - val_loss: 0.3379
Epoch 3/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8794 - loss: 0.2985 - val_accuracy: 0.8587 - val_loss: 0.3372
Epoch 4/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8796 - loss: 0.3065 - val_accuracy: 0.8579 - val_loss: 0.3346
Epoch 5/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8846 - loss: 0.2883 - val_accuracy: 0.8591 - val_loss: 0.3314
Epoch 6/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8845 - loss: 0.2882 - val_accuracy: 0.8563 - val_loss: 0.3369
Epoch 7/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8925 - loss: 0.2760 - val_accuracy: 0.8579 - val_loss: 0.3360
Epoch 8/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8917 - loss: 0.2739 - val_accuracy: 0.8579 - val_loss: 0.3495
Epoch 9/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8951 - loss: 0.2717 - val_accuracy: 0.8512 - val_loss: 0.3459
Epoch 10/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9040 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3561
Epoch 11/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9010 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3470
Epoch 12/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9050 - loss: 0.2490 - val_accuracy: 0.8512 - val_loss: 0.3560
Epoch 13/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9083 - loss: 0.2481 - val_accuracy: 0.8523 - val_loss: 0.3670
Epoch 14/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9054 - loss: 0.2492 - val_accuracy: 0.8539 - val_loss: 0.3552
Epoch 15/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9137 - loss: 0.2301 - val_accuracy: 0.8508 - val_loss: 0.3714
Epoch 16/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9157 - loss: 0.2330 - val_accuracy: 0.8523 - val_loss: 0.3805
Epoch 17/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9144 - loss: 0.2339 - val_accuracy: 0.8500 - val_loss: 0.3780
Epoch 18/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9118 - loss: 0.2302 - val_accuracy: 0.8555 - val_loss: 0.3919
Epoch 19/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9162 - loss: 0.2299 - val_accuracy: 0.8480 - val_loss: 0.3883
Epoch 20/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9226 - loss: 0.2144 - val_accuracy: 0.8460 - val_loss: 0.4096
Epoch 21/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9211 - loss: 0.2217 - val_accuracy: 0.8516 - val_loss: 0.4037
Epoch 22/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9192 - loss: 0.2170 - val_accuracy: 0.8500 - val_loss: 0.4034
Epoch 23/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9270 - loss: 0.2019 - val_accuracy: 0.8413 - val_loss: 0.4156
Epoch 24/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9334 - loss: 0.1965 - val_accuracy: 0.8488 - val_loss: 0.4213
Epoch 25/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9256 - loss: 0.2058 - val_accuracy: 0.8512 - val_loss: 0.4194
Epoch 26/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9230 - loss: 0.2079 - val_accuracy: 0.8437 - val_loss: 0.4259
Epoch 27/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9303 - loss: 0.2013 - val_accuracy: 0.8441 - val_loss: 0.4273
Epoch 28/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9316 - loss: 0.1936 - val_accuracy: 0.8381 - val_loss: 0.4543
Epoch 29/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9313 - loss: 0.2016 - val_accuracy: 0.8468 - val_loss: 0.4473
Epoch 30/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9373 - loss: 0.1827 - val_accuracy: 0.8417 - val_loss: 0.4511
Epoch 31/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9346 - loss: 0.1876 - val_accuracy: 0.8350 - val_loss: 0.4681
Epoch 32/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9367 - loss: 0.1823 - val_accuracy: 0.8441 - val_loss: 0.4578
Epoch 33/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9380 - loss: 0.1823 - val_accuracy: 0.8433 - val_loss: 0.4643
Epoch 34/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9382 - loss: 0.1730 - val_accuracy: 0.8429 - val_loss: 0.4602
Epoch 35/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9407 - loss: 0.1748 - val_accuracy: 0.8397 - val_loss: 0.4727
Epoch 36/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9402 - loss: 0.1778 - val_accuracy: 0.8373 - val_loss: 0.4841
Epoch 37/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.9409 - loss: 0.1663 - val_accuracy: 0.8350 - val_loss: 0.4929
Epoch 38/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9420 - loss: 0.1681 - val_accuracy: 0.8421 - val_loss: 0.4988
Epoch 39/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9475 - loss: 0.1611 - val_accuracy: 0.8429 - val_loss: 0.4904
Epoch 40/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9457 - loss: 0.1633 - val_accuracy: 0.8425 - val_loss: 0.5039
Epoch 41/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9436 - loss: 0.1630 - val_accuracy: 0.8413 - val_loss: 0.5223
Epoch 42/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9502 - loss: 0.1493 - val_accuracy: 0.8452 - val_loss: 0.5219
Epoch 43/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9460 - loss: 0.1563 - val_accuracy: 0.8405 - val_loss: 0.5146
Epoch 44/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9419 - loss: 0.1671 - val_accuracy: 0.8373 - val_loss: 0.6083
Epoch 45/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9563 - loss: 0.1423 - val_accuracy: 0.8441 - val_loss: 0.5172
Epoch 46/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9518 - loss: 0.1563 - val_accuracy: 0.8393 - val_loss: 0.5386
Epoch 47/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9515 - loss: 0.1459 - val_accuracy: 0.8401 - val_loss: 0.5497
Epoch 48/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9506 - loss: 0.1465 - val_accuracy: 0.8267 - val_loss: 0.5693
Epoch 49/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9482 - loss: 0.1613 - val_accuracy: 0.8362 - val_loss: 0.5661
Epoch 50/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9562 - loss: 0.1408 - val_accuracy: 0.8377 - val_loss: 0.5786
Epoch 51/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9569 - loss: 0.1348 - val_accuracy: 0.8287 - val_loss: 0.5823
Epoch 52/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9506 - loss: 0.1394 - val_accuracy: 0.8389 - val_loss: 0.5750
Epoch 53/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9569 - loss: 0.1372 - val_accuracy: 0.8362 - val_loss: 0.5819
Epoch 54/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9597 - loss: 0.1356 - val_accuracy: 0.8298 - val_loss: 0.5924
Epoch 55/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9589 - loss: 0.1328 - val_accuracy: 0.8310 - val_loss: 0.6046
Epoch 56/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9569 - loss: 0.1342 - val_accuracy: 0.8200 - val_loss: 0.6179
Epoch 57/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9584 - loss: 0.1330 - val_accuracy: 0.8350 - val_loss: 0.6040
Epoch 58/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9620 - loss: 0.1266 - val_accuracy: 0.8223 - val_loss: 0.6398
Epoch 59/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9587 - loss: 0.1314 - val_accuracy: 0.8322 - val_loss: 0.6161
Epoch 60/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9629 - loss: 0.1177 - val_accuracy: 0.8366 - val_loss: 0.6166
Epoch 61/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9633 - loss: 0.1183 - val_accuracy: 0.8227 - val_loss: 0.6447
Epoch 62/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9637 - loss: 0.1208 - val_accuracy: 0.8326 - val_loss: 0.6244
Epoch 63/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9604 - loss: 0.1245 - val_accuracy: 0.8346 - val_loss: 0.6591
Epoch 64/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9629 - loss: 0.1260 - val_accuracy: 0.8366 - val_loss: 0.6509
Epoch 65/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9611 - loss: 0.1246 - val_accuracy: 0.8373 - val_loss: 0.6708
Epoch 66/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9631 - loss: 0.1189 - val_accuracy: 0.8310 - val_loss: 0.6936
Epoch 67/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9592 - loss: 0.1239 - val_accuracy: 0.8216 - val_loss: 0.6786
Epoch 68/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9643 - loss: 0.1230 - val_accuracy: 0.8326 - val_loss: 0.6815
Epoch 69/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9636 - loss: 0.1216 - val_accuracy: 0.8267 - val_loss: 0.6902
Epoch 70/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9636 - loss: 0.1147 - val_accuracy: 0.8251 - val_loss: 0.6926
Epoch 71/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9649 - loss: 0.1131 - val_accuracy: 0.8342 - val_loss: 0.7052
Epoch 72/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9671 - loss: 0.1105 - val_accuracy: 0.8251 - val_loss: 0.6964
Epoch 73/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9681 - loss: 0.1057 - val_accuracy: 0.8251 - val_loss: 0.7397
Epoch 74/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9627 - loss: 0.1106 - val_accuracy: 0.8227 - val_loss: 0.7392
Epoch 75/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9667 - loss: 0.1124 - val_accuracy: 0.8310 - val_loss: 0.7185
Epoch 76/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9668 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7144
Epoch 77/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9672 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7278
Epoch 78/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9690 - loss: 0.1136 - val_accuracy: 0.8318 - val_loss: 0.7388
Epoch 79/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9696 - loss: 0.1021 - val_accuracy: 0.8247 - val_loss: 0.7466
Epoch 80/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9666 - loss: 0.1124 - val_accuracy: 0.8196 - val_loss: 0.7523
Epoch 81/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9700 - loss: 0.1022 - val_accuracy: 0.8243 - val_loss: 0.7548
Epoch 82/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9684 - loss: 0.1008 - val_accuracy: 0.8255 - val_loss: 0.7539
Epoch 83/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9720 - loss: 0.0999 - val_accuracy: 0.8176 - val_loss: 0.7742
Epoch 84/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9713 - loss: 0.0964 - val_accuracy: 0.8192 - val_loss: 0.7893
Epoch 85/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9693 - loss: 0.0950 - val_accuracy: 0.8239 - val_loss: 0.8019
Epoch 86/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9700 - loss: 0.0959 - val_accuracy: 0.8231 - val_loss: 0.8244
Epoch 87/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9680 - loss: 0.1088 - val_accuracy: 0.8259 - val_loss: 0.8061
Epoch 88/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9694 - loss: 0.0984 - val_accuracy: 0.8231 - val_loss: 0.7985
Epoch 89/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9744 - loss: 0.0878 - val_accuracy: 0.8081 - val_loss: 0.8120
Epoch 90/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9722 - loss: 0.0955 - val_accuracy: 0.8160 - val_loss: 0.8065
Epoch 91/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9715 - loss: 0.0998 - val_accuracy: 0.8291 - val_loss: 0.7710
Epoch 92/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9731 - loss: 0.0992 - val_accuracy: 0.8235 - val_loss: 0.8246
Epoch 93/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9745 - loss: 0.0861 - val_accuracy: 0.8243 - val_loss: 0.8357
Epoch 94/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9698 - loss: 0.0975 - val_accuracy: 0.8279 - val_loss: 0.8380
Epoch 95/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9749 - loss: 0.0870 - val_accuracy: 0.8239 - val_loss: 0.8321
Epoch 96/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9729 - loss: 0.0938 - val_accuracy: 0.8184 - val_loss: 0.8346
Epoch 97/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9736 - loss: 0.0948 - val_accuracy: 0.8247 - val_loss: 0.8299
Epoch 98/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9750 - loss: 0.0909 - val_accuracy: 0.8223 - val_loss: 0.8562
Epoch 99/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9722 - loss: 0.0915 - val_accuracy: 0.8283 - val_loss: 0.8644
Epoch 100/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9696 - loss: 0.0965 - val_accuracy: 0.8231 - val_loss: 0.8665</code></pre>
</div>
</div>
</section>
<section id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination" class="level3">
<h3 class="anchored" data-anchor-id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination">Modal with with Standard Loss(binary_crossentropy), No discrimination</h3>
<div id="cell-37" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with the custom loss function</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>history2 <span class="op">=</span> model.fit(X_train_scaled, y_train, epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8989 - loss: 0.2601 - val_accuracy: 0.8551 - val_loss: 0.3496
Epoch 2/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 782us/step - accuracy: 0.8954 - loss: 0.2608 - val_accuracy: 0.8531 - val_loss: 0.3531
Epoch 3/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 791us/step - accuracy: 0.9064 - loss: 0.2512 - val_accuracy: 0.8531 - val_loss: 0.3671
Epoch 4/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 846us/step - accuracy: 0.9090 - loss: 0.2452 - val_accuracy: 0.8468 - val_loss: 0.3754
Epoch 5/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 786us/step - accuracy: 0.9121 - loss: 0.2368 - val_accuracy: 0.8484 - val_loss: 0.3660
Epoch 6/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 767us/step - accuracy: 0.9150 - loss: 0.2350 - val_accuracy: 0.8535 - val_loss: 0.3656
Epoch 7/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 880us/step - accuracy: 0.9145 - loss: 0.2336 - val_accuracy: 0.8468 - val_loss: 0.3764
Epoch 8/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 729us/step - accuracy: 0.9177 - loss: 0.2211 - val_accuracy: 0.8500 - val_loss: 0.3734
Epoch 9/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 747us/step - accuracy: 0.9162 - loss: 0.2237 - val_accuracy: 0.8543 - val_loss: 0.3917
Epoch 10/10
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 799us/step - accuracy: 0.9165 - loss: 0.2274 - val_accuracy: 0.8496 - val_loss: 0.3897</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_accuracy(histories):</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for accuracy</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add accuracy traces with specified colors</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'accuracy'</span>]))),</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'accuracy'</span>],</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_accuracy'</span>]))),</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_accuracy'</span>],</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for accuracy graph</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Accuracy'</span>,</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Accuracy'</span>,</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(histories):</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for loss</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add loss traces with specified colors</span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'loss'</span>]))),</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'loss'</span>],</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_loss'</span>]))),</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_loss'</span>],</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for loss graph</span></span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Loss'</span>,</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Loss'</span>,</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have history1 and history2 as the history objects from your model training</span></span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>plot_accuracy([(<span class="st">'Custom Loss'</span>, history1), (<span class="st">'Standard Loss'</span>, history2)])</span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>plot_loss([(<span class="st">'Custom Loss'</span>, history1), (<span class="st">'Standard Loss'</span>, history2)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="defining-custom_loss-function-with-standard-lambda-0.5-but-discrimination0" class="level3">
<h3 class="anchored" data-anchor-id="defining-custom_loss-function-with-standard-lambda-0.5-but-discrimination0">DEFINING custom_loss FUNCTION with Standard Lambda = 0.5, but Discrimination!=0</h3>
<div id="cell-40" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract predictions and sensitive attributes</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, predictions)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate discrimination based on sensitive attribute</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&lt;=</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&gt;</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>        sum_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(mask_unpriv)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>        sum_priv <span class="op">=</span> K.<span class="bu">sum</span>(mask_priv)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_unpriv) <span class="op">/</span> (sum_unpriv <span class="op">+</span> epsilon)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_priv) <span class="op">/</span> (sum_priv <span class="op">+</span> epsilon)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug outputs</span></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Standard Loss:", standard_loss, "Discrimination:", discrimination)</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total loss with discrimination penalty</span></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Model compilation</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(lambda_val<span class="op">=</span><span class="fl">0.01</span>),  <span class="co"># Change lambda_val as needed</span></span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the model</span></span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg ansi-bold">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg ansi-bold">In[22], line 38</span>
<span class="ansi-green-fg">     33</span> model<span style="color:rgb(98,98,98)">.</span>compile(optimizer<span style="color:rgb(98,98,98)">=</span>Adam(learning_rate<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.001</span>),
<span class="ansi-green-fg">     34</span>               loss<span style="color:rgb(98,98,98)">=</span>custom_loss(lambda_val<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.01</span>),  <span style="font-style:italic;color:rgb(95,135,135)"># Change lambda_val as needed</span>
<span class="ansi-green-fg">     35</span>               metrics<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">accuracy</span><span style="color:rgb(175,0,0)">'</span>])
<span class="ansi-green-fg">     37</span> <span style="font-style:italic;color:rgb(95,135,135)"># Training the model</span>
<span class="ansi-green-fg ansi-bold">---&gt; 38</span> history1 <span style="color:rgb(98,98,98)">=</span> model<span style="color:rgb(98,98,98)">.</span>fit(X_train_with_sensitive, y_train, epochs<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">100</span>, batch_size<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">32</span>, validation_split<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.2</span>)

<span class="ansi-red-fg ansi-bold">NameError</span>: name 'X_train_with_sensitive' is not defined</pre>
</div>
</div>
</div>
</section>
<section id="defining-custom_loss-function-with-cust-lambda-0.01-but-discrimination0" class="level3">
<h3 class="anchored" data-anchor-id="defining-custom_loss-function-with-cust-lambda-0.01-but-discrimination0">DEFINING custom_loss FUNCTION with cust Lambda = 0.01, but Discrimination!=0</h3>
<div id="cell-42" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> BinaryCrossentropy</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Dense</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(lambda_val<span class="op">=</span><span class="fl">0.001</span>):</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract predictions and sensitive attributes from y_pred</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug prints to check outputs</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Predictions sample:", predictions[:10])</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Sensitive Attr sample:", sensitive_attr[:10])</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, predictions)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Determine thresholds to convert sensitive attributes to binary</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assuming the negative and positive classes are split around zero</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&lt;=</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&gt;</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>        sum_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(mask_unpriv)</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>        sum_priv <span class="op">=</span> K.<span class="bu">sum</span>(mask_priv)</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Debug prints for mask sums</span></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Sum unprivileged:", sum_unpriv)</span></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Sum privileged:", sum_priv)</span></span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_unpriv) <span class="op">/</span> (sum_unpriv <span class="op">+</span> epsilon)</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_priv) <span class="op">/</span> (sum_priv <span class="op">+</span> epsilon)</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discrimination as the squared difference in probabilities</span></span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Debug print</span></span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Discrimination:", discrimination)</span></span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total loss with discrimination penalty</span></span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train_scaled.shape[<span class="dv">1</span>]  <span class="co"># Number of features</span></span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layers</span></span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>sensitive_inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,))</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Network architecture</span></span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>combined_outputs <span class="op">=</span> tf.keras.layers.concatenate([outputs, sensitive_inputs])</span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>[inputs, sensitive_inputs], outputs<span class="op">=</span>combined_outputs)</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with the custom loss function</span></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(lambda_val <span class="op">=</span> <span class="fl">0.01</span>),</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data with sensitive attribute</span></span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>X_train_with_sensitive <span class="op">=</span> [X_train_scaled, X_train_scaled[:, sensitive_index]]</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>history1_nd <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 600us/step - accuracy: 0.6550 - loss: 0.7064 - val_accuracy: 0.6609 - val_loss: 0.6894
Epoch 2/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6535 - loss: 0.6854 - val_accuracy: 0.6609 - val_loss: 0.6880
Epoch 3/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6558 - loss: 0.6812 - val_accuracy: 0.6609 - val_loss: 0.6878
Epoch 4/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 402us/step - accuracy: 0.6569 - loss: 0.6801 - val_accuracy: 0.6609 - val_loss: 0.6883
Epoch 5/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 409us/step - accuracy: 0.6568 - loss: 0.6789 - val_accuracy: 0.6609 - val_loss: 0.6881
Epoch 6/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 391us/step - accuracy: 0.6590 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6878
Epoch 7/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 748us/step - accuracy: 0.6499 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 8/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 413us/step - accuracy: 0.6574 - loss: 0.6754 - val_accuracy: 0.6609 - val_loss: 0.6888
Epoch 9/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 393us/step - accuracy: 0.6514 - loss: 0.6772 - val_accuracy: 0.6609 - val_loss: 0.6886
Epoch 10/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step - accuracy: 0.6547 - loss: 0.6752 - val_accuracy: 0.6609 - val_loss: 0.6893
Epoch 11/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 388us/step - accuracy: 0.6602 - loss: 0.6736 - val_accuracy: 0.6609 - val_loss: 0.6887
Epoch 12/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step - accuracy: 0.6497 - loss: 0.6741 - val_accuracy: 0.6609 - val_loss: 0.6896
Epoch 13/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step - accuracy: 0.6501 - loss: 0.6703 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 14/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 389us/step - accuracy: 0.6578 - loss: 0.6716 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 15/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.6528 - loss: 0.6702 - val_accuracy: 0.6609 - val_loss: 0.6884
Epoch 16/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step - accuracy: 0.6607 - loss: 0.6709 - val_accuracy: 0.6609 - val_loss: 0.6890
Epoch 17/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6566 - loss: 0.6708 - val_accuracy: 0.6609 - val_loss: 0.6896
Epoch 18/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6558 - loss: 0.6706 - val_accuracy: 0.6609 - val_loss: 0.6901
Epoch 19/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6534 - loss: 0.6695 - val_accuracy: 0.6609 - val_loss: 0.6898
Epoch 20/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 443us/step - accuracy: 0.6589 - loss: 0.6673 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 21/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6594 - loss: 0.6697 - val_accuracy: 0.6609 - val_loss: 0.6895
Epoch 22/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6541 - loss: 0.6656 - val_accuracy: 0.6609 - val_loss: 0.6897
Epoch 23/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6504 - loss: 0.6680 - val_accuracy: 0.6609 - val_loss: 0.6898
Epoch 24/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6491 - loss: 0.6642 - val_accuracy: 0.6609 - val_loss: 0.6893
Epoch 25/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 489us/step - accuracy: 0.6595 - loss: 0.6663 - val_accuracy: 0.6609 - val_loss: 0.6908
Epoch 26/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6486 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 27/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.6512 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 28/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 434us/step - accuracy: 0.6623 - loss: 0.6659 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 29/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6488 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6908
Epoch 30/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6629 - loss: 0.6647 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 31/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6499 - loss: 0.6653 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 32/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 379us/step - accuracy: 0.6537 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 33/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6560 - loss: 0.6649 - val_accuracy: 0.6609 - val_loss: 0.6893
Epoch 34/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 367us/step - accuracy: 0.6515 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 35/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6549 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 36/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 418us/step - accuracy: 0.6498 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 37/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.6572 - loss: 0.6641 - val_accuracy: 0.6609 - val_loss: 0.6926
Epoch 38/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6623 - loss: 0.6660 - val_accuracy: 0.6609 - val_loss: 0.6901
Epoch 39/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6571 - loss: 0.6645 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 40/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step - accuracy: 0.6571 - loss: 0.6637 - val_accuracy: 0.6609 - val_loss: 0.6909
Epoch 41/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6551 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 42/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 514us/step - accuracy: 0.6476 - loss: 0.6631 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 43/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6529 - loss: 0.6662 - val_accuracy: 0.6609 - val_loss: 0.6911
Epoch 44/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6571 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6911
Epoch 45/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.6486 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6914
Epoch 46/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 434us/step - accuracy: 0.6591 - loss: 0.6633 - val_accuracy: 0.6609 - val_loss: 0.6913
Epoch 47/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6451 - loss: 0.6622 - val_accuracy: 0.6609 - val_loss: 0.6913
Epoch 48/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6502 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 49/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6517 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6918
Epoch 50/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6593 - loss: 0.6606 - val_accuracy: 0.6609 - val_loss: 0.6911
Epoch 51/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6534 - loss: 0.6638 - val_accuracy: 0.6609 - val_loss: 0.6910
Epoch 52/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6602 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6910
Epoch 53/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.6466 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 54/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6599 - loss: 0.6630 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 55/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.6499 - loss: 0.6612 - val_accuracy: 0.6609 - val_loss: 0.6906
Epoch 56/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6580 - loss: 0.6635 - val_accuracy: 0.6609 - val_loss: 0.6913
Epoch 57/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6571 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 58/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.6615 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 59/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 622us/step - accuracy: 0.6572 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 60/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6577 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 61/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6541 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 62/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6614 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 63/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6569 - loss: 0.6602 - val_accuracy: 0.6609 - val_loss: 0.6903
Epoch 64/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.6489 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6905
Epoch 65/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 379us/step - accuracy: 0.6461 - loss: 0.6604 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 66/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 379us/step - accuracy: 0.6633 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 67/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.6551 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 68/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6567 - loss: 0.6610 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 69/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6503 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6915
Epoch 70/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.6579 - loss: 0.6627 - val_accuracy: 0.6609 - val_loss: 0.6905
Epoch 71/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.6569 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6915
Epoch 72/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6494 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6918
Epoch 73/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.6484 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6916
Epoch 74/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 416us/step - accuracy: 0.6512 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6909
Epoch 75/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 551us/step - accuracy: 0.6613 - loss: 0.6588 - val_accuracy: 0.6609 - val_loss: 0.6914
Epoch 76/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 367us/step - accuracy: 0.6557 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 77/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step - accuracy: 0.6592 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 78/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6553 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 79/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6564 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 80/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6525 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 81/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.6482 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6912
Epoch 82/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6534 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6900
Epoch 83/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6580 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 84/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6500 - loss: 0.6596 - val_accuracy: 0.6609 - val_loss: 0.6907
Epoch 85/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 367us/step - accuracy: 0.6518 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6899
Epoch 86/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6579 - loss: 0.6587 - val_accuracy: 0.6609 - val_loss: 0.6908
Epoch 87/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step - accuracy: 0.6646 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6897
Epoch 88/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 418us/step - accuracy: 0.6484 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6904
Epoch 89/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.6500 - loss: 0.6595 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 90/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 532us/step - accuracy: 0.6472 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6915
Epoch 91/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6485 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6917
Epoch 92/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6596 - loss: 0.6621 - val_accuracy: 0.6609 - val_loss: 0.6902
Epoch 93/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6508 - loss: 0.6600 - val_accuracy: 0.6609 - val_loss: 0.6894
Epoch 94/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.6520 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6909
Epoch 95/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.6587 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6928
Epoch 96/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 385us/step - accuracy: 0.6538 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6923
Epoch 97/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 394us/step - accuracy: 0.6521 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6910
Epoch 98/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 530us/step - accuracy: 0.6471 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6923
Epoch 99/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.6484 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6891
Epoch 100/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6508 - loss: 0.6590 - val_accuracy: 0.6609 - val_loss: 0.6903</code></pre>
</div>
</div>
</section>
<section id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination-but-discrimination0-in-custom-losses" class="level3">
<h3 class="anchored" data-anchor-id="modal-with-with-standard-lossbinary_crossentropy-no-discrimination-but-discrimination0-in-custom-losses">Modal with with Standard Loss(binary_crossentropy), No discrimination , but Discrimination!=0 in custom losses</h3>
<div id="cell-44" class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> X_train_scaled.shape[<span class="dv">1</span>]  <span class="co"># Number of features</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>sensitive_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layers</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(input_size,))</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>sensitive_inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,))</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Network architecture</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co"># No need to concatenate outputs and sensitive inputs for the loss calculation</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate outputs for predictions and sensitive attributes</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Since we're using binary_crossentropy, we only need the main outputs</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>[inputs, sensitive_inputs], outputs<span class="op">=</span>outputs)</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model with binary crossentropy</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data with sensitive attribute</span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>X_train_with_sensitive <span class="op">=</span> [X_train_scaled, X_train_scaled[:, sensitive_index]]</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>history2_nd <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">100</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 598us/step - accuracy: 0.8259 - loss: 0.4073 - val_accuracy: 0.8595 - val_loss: 0.3396
Epoch 2/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 442us/step - accuracy: 0.8696 - loss: 0.3200 - val_accuracy: 0.8634 - val_loss: 0.3325
Epoch 3/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 754us/step - accuracy: 0.8746 - loss: 0.3044 - val_accuracy: 0.8630 - val_loss: 0.3334
Epoch 4/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 399us/step - accuracy: 0.8820 - loss: 0.2964 - val_accuracy: 0.8630 - val_loss: 0.3361
Epoch 5/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 387us/step - accuracy: 0.8827 - loss: 0.2969 - val_accuracy: 0.8650 - val_loss: 0.3331
Epoch 6/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 385us/step - accuracy: 0.8857 - loss: 0.2855 - val_accuracy: 0.8646 - val_loss: 0.3304
Epoch 7/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.8811 - loss: 0.2898 - val_accuracy: 0.8595 - val_loss: 0.3401
Epoch 8/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.8928 - loss: 0.2674 - val_accuracy: 0.8658 - val_loss: 0.3319
Epoch 9/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 386us/step - accuracy: 0.8897 - loss: 0.2836 - val_accuracy: 0.8646 - val_loss: 0.3381
Epoch 10/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.8926 - loss: 0.2663 - val_accuracy: 0.8567 - val_loss: 0.3486
Epoch 11/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 412us/step - accuracy: 0.8995 - loss: 0.2607 - val_accuracy: 0.8595 - val_loss: 0.3509
Epoch 12/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 399us/step - accuracy: 0.9013 - loss: 0.2601 - val_accuracy: 0.8598 - val_loss: 0.3511
Epoch 13/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.9017 - loss: 0.2531 - val_accuracy: 0.8595 - val_loss: 0.3629
Epoch 14/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.9109 - loss: 0.2382 - val_accuracy: 0.8606 - val_loss: 0.3490
Epoch 15/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.9070 - loss: 0.2503 - val_accuracy: 0.8535 - val_loss: 0.3574
Epoch 16/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 444us/step - accuracy: 0.9119 - loss: 0.2331 - val_accuracy: 0.8551 - val_loss: 0.3676
Epoch 17/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 356us/step - accuracy: 0.9143 - loss: 0.2275 - val_accuracy: 0.8535 - val_loss: 0.3737
Epoch 18/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9121 - loss: 0.2327 - val_accuracy: 0.8575 - val_loss: 0.3660
Epoch 19/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 563us/step - accuracy: 0.9233 - loss: 0.2215 - val_accuracy: 0.8539 - val_loss: 0.3894
Epoch 20/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.9159 - loss: 0.2244 - val_accuracy: 0.8523 - val_loss: 0.3763
Epoch 21/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9230 - loss: 0.2136 - val_accuracy: 0.8555 - val_loss: 0.3882
Epoch 22/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 385us/step - accuracy: 0.9242 - loss: 0.2074 - val_accuracy: 0.8535 - val_loss: 0.3934
Epoch 23/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9253 - loss: 0.2120 - val_accuracy: 0.8496 - val_loss: 0.3942
Epoch 24/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 354us/step - accuracy: 0.9266 - loss: 0.2041 - val_accuracy: 0.8464 - val_loss: 0.4131
Epoch 25/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.9202 - loss: 0.2132 - val_accuracy: 0.8500 - val_loss: 0.4122
Epoch 26/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.9325 - loss: 0.1921 - val_accuracy: 0.8535 - val_loss: 0.4094
Epoch 27/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9351 - loss: 0.1893 - val_accuracy: 0.8504 - val_loss: 0.4226
Epoch 28/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 351us/step - accuracy: 0.9320 - loss: 0.1931 - val_accuracy: 0.8480 - val_loss: 0.4264
Epoch 29/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 358us/step - accuracy: 0.9325 - loss: 0.1899 - val_accuracy: 0.8496 - val_loss: 0.4329
Epoch 30/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 352us/step - accuracy: 0.9317 - loss: 0.1922 - val_accuracy: 0.8500 - val_loss: 0.4303
Epoch 31/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 352us/step - accuracy: 0.9371 - loss: 0.1796 - val_accuracy: 0.8437 - val_loss: 0.4379
Epoch 32/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 347us/step - accuracy: 0.9337 - loss: 0.1851 - val_accuracy: 0.8460 - val_loss: 0.4467
Epoch 33/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 362us/step - accuracy: 0.9348 - loss: 0.1881 - val_accuracy: 0.8445 - val_loss: 0.4406
Epoch 34/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.9399 - loss: 0.1758 - val_accuracy: 0.8441 - val_loss: 0.4533
Epoch 35/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 343us/step - accuracy: 0.9370 - loss: 0.1763 - val_accuracy: 0.8429 - val_loss: 0.4685
Epoch 36/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.9420 - loss: 0.1644 - val_accuracy: 0.8437 - val_loss: 0.4601
Epoch 37/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.9382 - loss: 0.1685 - val_accuracy: 0.8421 - val_loss: 0.4721
Epoch 38/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.9397 - loss: 0.1732 - val_accuracy: 0.8401 - val_loss: 0.4915
Epoch 39/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 529us/step - accuracy: 0.9463 - loss: 0.1582 - val_accuracy: 0.8354 - val_loss: 0.5038
Epoch 40/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 361us/step - accuracy: 0.9438 - loss: 0.1608 - val_accuracy: 0.8429 - val_loss: 0.4831
Epoch 41/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.9403 - loss: 0.1676 - val_accuracy: 0.8452 - val_loss: 0.4940
Epoch 42/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 390us/step - accuracy: 0.9445 - loss: 0.1581 - val_accuracy: 0.8393 - val_loss: 0.4956
Epoch 43/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step - accuracy: 0.9461 - loss: 0.1537 - val_accuracy: 0.8350 - val_loss: 0.5094
Epoch 44/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 361us/step - accuracy: 0.9465 - loss: 0.1573 - val_accuracy: 0.8350 - val_loss: 0.5115
Epoch 45/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 395us/step - accuracy: 0.9467 - loss: 0.1530 - val_accuracy: 0.8409 - val_loss: 0.5131
Epoch 46/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 362us/step - accuracy: 0.9473 - loss: 0.1580 - val_accuracy: 0.8393 - val_loss: 0.5254
Epoch 47/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 513us/step - accuracy: 0.9492 - loss: 0.1451 - val_accuracy: 0.8377 - val_loss: 0.5261
Epoch 48/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 393us/step - accuracy: 0.9517 - loss: 0.1462 - val_accuracy: 0.8247 - val_loss: 0.5360
Epoch 49/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9528 - loss: 0.1422 - val_accuracy: 0.8287 - val_loss: 0.5420
Epoch 50/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 390us/step - accuracy: 0.9515 - loss: 0.1378 - val_accuracy: 0.8216 - val_loss: 0.5578
Epoch 51/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step - accuracy: 0.9508 - loss: 0.1454 - val_accuracy: 0.8373 - val_loss: 0.5463
Epoch 52/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 387us/step - accuracy: 0.9487 - loss: 0.1405 - val_accuracy: 0.8385 - val_loss: 0.5607
Epoch 53/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 384us/step - accuracy: 0.9545 - loss: 0.1364 - val_accuracy: 0.8310 - val_loss: 0.5591
Epoch 54/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 584us/step - accuracy: 0.9557 - loss: 0.1387 - val_accuracy: 0.8366 - val_loss: 0.5878
Epoch 55/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.9549 - loss: 0.1338 - val_accuracy: 0.8334 - val_loss: 0.5751
Epoch 56/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 421us/step - accuracy: 0.9518 - loss: 0.1390 - val_accuracy: 0.8389 - val_loss: 0.5626
Epoch 57/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9583 - loss: 0.1279 - val_accuracy: 0.8389 - val_loss: 0.5940
Epoch 58/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 345us/step - accuracy: 0.9558 - loss: 0.1300 - val_accuracy: 0.8314 - val_loss: 0.5842
Epoch 59/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 349us/step - accuracy: 0.9592 - loss: 0.1307 - val_accuracy: 0.8366 - val_loss: 0.5996
Epoch 60/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 351us/step - accuracy: 0.9568 - loss: 0.1323 - val_accuracy: 0.8326 - val_loss: 0.5967
Epoch 61/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 347us/step - accuracy: 0.9587 - loss: 0.1232 - val_accuracy: 0.8283 - val_loss: 0.5802
Epoch 62/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step - accuracy: 0.9572 - loss: 0.1475 - val_accuracy: 0.8267 - val_loss: 0.6030
Epoch 63/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 348us/step - accuracy: 0.9628 - loss: 0.1194 - val_accuracy: 0.8279 - val_loss: 0.6128
Epoch 64/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.9615 - loss: 0.1162 - val_accuracy: 0.8302 - val_loss: 0.6099
Epoch 65/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step - accuracy: 0.9601 - loss: 0.1192 - val_accuracy: 0.8314 - val_loss: 0.6176
Epoch 66/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 390us/step - accuracy: 0.9592 - loss: 0.1237 - val_accuracy: 0.8298 - val_loss: 0.6224
Epoch 67/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 355us/step - accuracy: 0.9606 - loss: 0.1188 - val_accuracy: 0.8223 - val_loss: 0.6446
Epoch 68/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.9616 - loss: 0.1147 - val_accuracy: 0.8322 - val_loss: 0.6292
Epoch 69/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 548us/step - accuracy: 0.9624 - loss: 0.1157 - val_accuracy: 0.8330 - val_loss: 0.6557
Epoch 70/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.9615 - loss: 0.1165 - val_accuracy: 0.8176 - val_loss: 0.6655
Epoch 71/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 404us/step - accuracy: 0.9618 - loss: 0.1145 - val_accuracy: 0.8298 - val_loss: 0.6618
Epoch 72/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 406us/step - accuracy: 0.9617 - loss: 0.1177 - val_accuracy: 0.8018 - val_loss: 0.6880
Epoch 73/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.9627 - loss: 0.1134 - val_accuracy: 0.8330 - val_loss: 0.6874
Epoch 74/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.9636 - loss: 0.1144 - val_accuracy: 0.8279 - val_loss: 0.6749
Epoch 75/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 348us/step - accuracy: 0.9638 - loss: 0.1070 - val_accuracy: 0.8239 - val_loss: 0.6767
Epoch 76/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9642 - loss: 0.1097 - val_accuracy: 0.8330 - val_loss: 0.6812
Epoch 77/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.9632 - loss: 0.1096 - val_accuracy: 0.8310 - val_loss: 0.6936
Epoch 78/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9667 - loss: 0.1049 - val_accuracy: 0.8255 - val_loss: 0.7077
Epoch 79/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9657 - loss: 0.1040 - val_accuracy: 0.8251 - val_loss: 0.6987
Epoch 80/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 347us/step - accuracy: 0.9640 - loss: 0.1099 - val_accuracy: 0.8220 - val_loss: 0.7047
Epoch 81/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 343us/step - accuracy: 0.9661 - loss: 0.1059 - val_accuracy: 0.8243 - val_loss: 0.7189
Epoch 82/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 493us/step - accuracy: 0.9657 - loss: 0.1057 - val_accuracy: 0.8330 - val_loss: 0.7181
Epoch 83/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.9708 - loss: 0.0954 - val_accuracy: 0.8295 - val_loss: 0.7182
Epoch 84/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 389us/step - accuracy: 0.9676 - loss: 0.1044 - val_accuracy: 0.8247 - val_loss: 0.7164
Epoch 85/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.9695 - loss: 0.1019 - val_accuracy: 0.8306 - val_loss: 0.7415
Epoch 86/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 336us/step - accuracy: 0.9633 - loss: 0.1033 - val_accuracy: 0.8302 - val_loss: 0.7433
Epoch 87/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 342us/step - accuracy: 0.9687 - loss: 0.1010 - val_accuracy: 0.8302 - val_loss: 0.7664
Epoch 88/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.9683 - loss: 0.0958 - val_accuracy: 0.8279 - val_loss: 0.7715
Epoch 89/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 342us/step - accuracy: 0.9671 - loss: 0.1022 - val_accuracy: 0.8298 - val_loss: 0.7661
Epoch 90/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 355us/step - accuracy: 0.9731 - loss: 0.0872 - val_accuracy: 0.8397 - val_loss: 0.7873
Epoch 91/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.9685 - loss: 0.0973 - val_accuracy: 0.8220 - val_loss: 0.7802
Epoch 92/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.9667 - loss: 0.1003 - val_accuracy: 0.8259 - val_loss: 0.7762
Epoch 93/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.9679 - loss: 0.0941 - val_accuracy: 0.8259 - val_loss: 0.7889
Epoch 94/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 391us/step - accuracy: 0.9719 - loss: 0.0876 - val_accuracy: 0.8298 - val_loss: 0.7972
Epoch 95/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 386us/step - accuracy: 0.9744 - loss: 0.0854 - val_accuracy: 0.8156 - val_loss: 0.8149
Epoch 96/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.9703 - loss: 0.0935 - val_accuracy: 0.8302 - val_loss: 0.8087
Epoch 97/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 345us/step - accuracy: 0.9698 - loss: 0.0894 - val_accuracy: 0.8295 - val_loss: 0.8223
Epoch 98/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 489us/step - accuracy: 0.9695 - loss: 0.0954 - val_accuracy: 0.8318 - val_loss: 0.8068
Epoch 99/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.9722 - loss: 0.0856 - val_accuracy: 0.8338 - val_loss: 0.8178
Epoch 100/100
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9726 - loss: 0.0874 - val_accuracy: 0.8200 - val_loss: 0.8233</code></pre>
</div>
</div>
<div id="cell-45" class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming history1_nd.history['accuracy'] and history2_nd.history['accuracy'] are available</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co"># These lists should contain the accuracy for each epoch</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(history1_nd.history[<span class="st">'accuracy'</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting accuracy for history1_nd</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>plt.scatter(epochs, history1_nd.history[<span class="st">'accuracy'</span>], color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Model 1 Accuracy'</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting accuracy for history2_nd</span></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(epochs, history2_nd.history[<span class="st">'accuracy'</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Model 2 Accuracy'</span>)</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparison of Model Accuracies Across Epochs'</span>)</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="preprocess_disc_bias_files/figure-html/cell-39-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_loss(lambda_val<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(y_true, y_pred):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract predictions and sensitive attributes</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        sensitive_attr <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard binary crossentropy loss</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        standard_loss <span class="op">=</span> BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)(y_true, predictions)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate discrimination based on sensitive attribute</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>        mask_unpriv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&lt;=</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        mask_priv <span class="op">=</span> K.cast(sensitive_attr <span class="op">&gt;</span> threshold, <span class="st">'float32'</span>)</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>        sum_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(mask_unpriv)</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>        sum_priv <span class="op">=</span> K.<span class="bu">sum</span>(mask_priv)</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>        prob_unpriv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_unpriv) <span class="op">/</span> (sum_unpriv <span class="op">+</span> epsilon)</span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>        prob_priv <span class="op">=</span> K.<span class="bu">sum</span>(predictions <span class="op">*</span> mask_priv) <span class="op">/</span> (sum_priv <span class="op">+</span> epsilon)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>        discrimination <span class="op">=</span> K.square(prob_priv <span class="op">-</span> prob_unpriv)</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debug outputs</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tf.print("Standard Loss:", standard_loss, "Discrimination:", discrimination)</span></span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total loss with discrimination penalty</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> standard_loss <span class="op">+</span> lambda_val <span class="op">*</span> discrimination</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Model compilation</span></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>custom_loss(lambda_val<span class="op">=</span><span class="fl">0.01</span>),  <span class="co"># Change lambda_val as needed</span></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the model</span></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>history1 <span class="op">=</span> model.fit(X_train_with_sensitive, y_train, epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 622us/step - accuracy: 0.6523 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 2/5
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 440us/step - accuracy: 0.6535 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 3/5
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.6649 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 4/5
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 425us/step - accuracy: 0.6511 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930
Epoch 5/5
317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 416us/step - accuracy: 0.6526 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930</code></pre>
</div>
</div>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming history1_nd and history2_nd have 'accuracy' data from your model training</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracting the accuracy data from each history object</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>accuracy1 <span class="op">=</span> history1_nd.history[<span class="st">'accuracy'</span>]</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>accuracy2 <span class="op">=</span> history2_nd.history[<span class="st">'accuracy'</span>]</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming the number of epochs is determined by the length of the accuracy data</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(accuracy1) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot using Plotly</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure()</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding scatter plot for Model 1</span></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>epochs,</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>accuracy1,</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">'markers'</span>,  <span class="co"># This specifies to use markers (points)</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'Model 1 Accuracy'</span>,  <span class="co"># Name of the series</span></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'blue'</span>, size<span class="op">=</span><span class="dv">8</span>)  <span class="co"># Marker settings</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding scatter plot for Model 2</span></span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>epochs,</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>accuracy2,</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">'markers'</span>,</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">'Model 2 Accuracy'</span>,</span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'red'</span>, size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the layout to add titles and labels</span></span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Comparison of Model Accuracies Across Epochs'</span>,</span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>    xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a>    yaxis_title<span class="op">=</span><span class="st">'Accuracy'</span>,</span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>    template<span class="op">=</span><span class="st">'plotly_white'</span></span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the figure</span></span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="graph" class="level3">
<h3 class="anchored" data-anchor-id="graph">GRAPH</h3>
<div id="cell-50" class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_accuracy(histories):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for accuracy</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add accuracy traces with specified colors</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'accuracy'</span>]))),</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'accuracy'</span>],</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_accuracy'</span>]))),</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_accuracy'</span>],</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Accuracy - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for accuracy graph</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Accuracy'</span>,</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Accuracy'</span>,</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(histories):</span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create figure for loss</span></span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add loss traces with specified colors</span></span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {<span class="st">'Custom Loss'</span>: (<span class="st">'blue'</span>, <span class="st">'red'</span>), <span class="st">'Standard Loss'</span>: (<span class="st">'skyblue'</span>, <span class="st">'pink'</span>)}</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, history <span class="kw">in</span> histories:</span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>        train_color, val_color <span class="op">=</span> color_map[name]</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'loss'</span>]))),</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'loss'</span>],</span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Training Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>train_color)))</span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(x<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(history.history[<span class="st">'val_loss'</span>]))),</span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a>                                 y<span class="op">=</span>history.history[<span class="st">'val_loss'</span>],</span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a>                                 name<span class="op">=</span><span class="ss">f'Validation Loss - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a>                                 mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a>                                 line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span>val_color)))</span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update layout for loss graph</span></span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(title<span class="op">=</span><span class="st">'Training and Validation Loss'</span>,</span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>                      xaxis_title<span class="op">=</span><span class="st">'Epochs'</span>,</span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>                      yaxis_title<span class="op">=</span><span class="st">'Loss'</span>,</span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a>                      legend_title<span class="op">=</span><span class="st">'Metric Type'</span>)</span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb63-56"><a href="#cb63-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-57"><a href="#cb63-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have history1 and history2 as the history objects from your model training</span></span>
<span id="cb63-58"><a href="#cb63-58" aria-hidden="true" tabindex="-1"></a>plot_accuracy([(<span class="st">'Custom Loss'</span>, history1_nd), (<span class="st">'Standard Loss'</span>, history2)])</span>
<span id="cb63-59"><a href="#cb63-59" aria-hidden="true" tabindex="-1"></a>plot_loss([(<span class="st">'Custom Loss'</span>, history1_nd), (<span class="st">'Standard Loss'</span>, history2)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>lfr_model <span class="op">=</span> LFR(unprivileged_groups<span class="op">=</span>unprivileged_groups, </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>                privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model and transform the dataset</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>lfr_model.fit(dataset_orig_panel19_train)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_lfr <span class="op">=</span> lfr_model.transform(dataset_orig_panel19_train, threshold <span class="op">=</span> <span class="fl">0.22</span>)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_lfr <span class="op">=</span> dataset_orig_panel19_train.align_datasets(dataset_transf_panel19_train_lfr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_distortion_meps(vold, vnew):</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize distortion score</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    distortion_score <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define weights for different categories of attributes</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    sensitive_weight <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    health_status_weight <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    socio_economic_weight <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    behavior_weight <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sensitive attributes</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> [<span class="st">'AGE'</span>, <span class="st">'RACE'</span>, <span class="st">'SEX=1'</span>, <span class="st">'SEX=2'</span>]:</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vold[attr] <span class="op">!=</span> vnew[attr]:</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>            distortion_score <span class="op">+=</span> sensitive_weight</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Health status indicators</span></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    health_attrs <span class="op">=</span> [<span class="st">'PCS42'</span>, <span class="st">'MCS42'</span>, <span class="st">'K6SUM42'</span>, <span class="st">'HIBPDX'</span>, <span class="st">'DIABDX'</span>, <span class="st">'CHDDX'</span>, <span class="st">'ANGIDX'</span>, <span class="st">'MIDX'</span>, <span class="st">'OHRTDX'</span>, <span class="st">'STRKDX'</span>, <span class="st">'EMPHDX'</span>, <span class="st">'CANCERDX'</span>, <span class="st">'JTPAIN'</span>, <span class="st">'ARTHDX'</span>, <span class="st">'ASTHDX'</span>, <span class="st">'ADHDADDX'</span>]</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> health_attrs:</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assuming health status attributes are numerical and a difference in value indicates a change in health status</span></span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>        distortion_score <span class="op">+=</span> health_status_weight <span class="op">*</span> <span class="bu">abs</span>(vold.get(attr, <span class="dv">0</span>) <span class="op">-</span> vnew.get(attr, <span class="dv">0</span>))</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Socioeconomic and environmental factors</span></span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>    socio_attrs <span class="op">=</span> [<span class="st">'REGION=1'</span>, <span class="st">'REGION=2'</span>, <span class="st">'REGION=3'</span>, <span class="st">'REGION=4'</span>, <span class="st">'MARRY'</span>, <span class="st">'FTSTU'</span>, <span class="st">'EMPST'</span>, <span class="st">'POVCAT'</span>, <span class="st">'INSCOV'</span>]</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> socio_attrs:</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vold[attr] <span class="op">!=</span> vnew[attr]:</span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>            distortion_score <span class="op">+=</span> socio_economic_weight</span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Health-related behaviors</span></span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>    behavior_attrs <span class="op">=</span> [<span class="st">'ADSMOK42'</span>, <span class="st">'WLKLIM'</span>, <span class="st">'ACTLIM'</span>, <span class="st">'SOCLIM'</span>, <span class="st">'COGLIM'</span>]</span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> behavior_attrs:</span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vold[attr] <span class="op">!=</span> vnew[attr]:</span>
<span id="cb67-34"><a href="#cb67-34" aria-hidden="true" tabindex="-1"></a>            distortion_score <span class="op">+=</span> behavior_weight</span>
<span id="cb67-35"><a href="#cb67-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-36"><a href="#cb67-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> distortion_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>DI <span class="op">=</span> DisparateImpactRemover()</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_di <span class="op">=</span> DI.fit_transform(dataset_orig_panel19_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-54" class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_rw,</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>test_results_rw<span class="op">=</span>[explainer_transf_panel19_train.mean_difference()</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.consistency()</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.statistical_parity_difference()</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.disparate_impact()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_di,</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>test_results_di<span class="op">=</span>[explainer_transf_panel19_train.mean_difference()</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.consistency()</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.statistical_parity_difference()</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.disparate_impact()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>dataset_transf_panel19_train_lfr,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>test_results_lfr<span class="op">=</span>[explainer_transf_panel19_train.mean_difference()</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.consistency()</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.statistical_parity_difference()</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>               ,explainer_transf_panel19_train.disparate_impact()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide
  return metric_fun(privileged=False) / metric_fun(privileged=True)</code></pre>
</div>
</div>
<div id="cell-57" class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>test_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',
 'Consistency (Zemel, et al. 2013): [0.83660139]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']</code></pre>
</div>
</div>
<div id="cell-58" class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>test_results_rw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -2.7755575615628914e-17',
 'Consistency (Zemel, et al. 2013): [0.83660139]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -2.7755575615628914e-17',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999999']</code></pre>
</div>
</div>
<div id="cell-59" class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>test_results_lfr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.0',
 'Consistency (Zemel, et al. 2013): [1.]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.0',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): nan']</code></pre>
</div>
</div>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>test_results_di</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',
 'Consistency (Zemel, et al. 2013): [0.83689198]',
 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',
 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']</code></pre>
</div>
</div>
<div id="cell-61" class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset_transf_panel19_train_rw.features</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset_transf_panel19_train_rw.labels.ravel()  <span class="co"># Flatten the label array if necessary</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> dataset_transf_panel19_train_rw.instance_weights</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> dataset_transf_panel19_train_rw.feature_names</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>df_rw <span class="op">=</span> pd.DataFrame(features, columns<span class="op">=</span>feature_names)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>df_rw[<span class="st">'label'</span>] <span class="op">=</span> label</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>df_rw[<span class="st">'weights'</span>] <span class="op">=</span> weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-62" class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>df_rw</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">weights</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>53.0</td>
<td>1.0</td>
<td>25.93</td>
<td>58.47</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>17459.483776</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>56.0</td>
<td>1.0</td>
<td>20.42</td>
<td>26.57</td>
<td>17.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>14515.313940</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>23.0</td>
<td>1.0</td>
<td>53.12</td>
<td>50.33</td>
<td>7.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>18465.607681</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>21762.696983</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>27.0</td>
<td>0.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>3727.042408</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4909.081729</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>2.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4184.786789</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>54.0</td>
<td>0.0</td>
<td>43.97</td>
<td>42.45</td>
<td>24.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4427.370919</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>73.0</td>
<td>0.0</td>
<td>42.68</td>
<td>43.46</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>6010.846084</td>
</tr>
</tbody>
</table>

<p>15830 rows × 140 columns</p>
</div>
</div>
</div>
<div id="cell-63" class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset_orig_panel19_train.features</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset_orig_panel19_train.labels</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> dataset_orig_panel19_train.feature_names</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train_df <span class="op">=</span> pd.DataFrame(features, columns<span class="op">=</span>feature_names)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train_df[<span class="st">'label'</span>] <span class="op">=</span> label</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">EMPST=4</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>53.0</td>
<td>1.0</td>
<td>25.93</td>
<td>58.47</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>56.0</td>
<td>1.0</td>
<td>20.42</td>
<td>26.57</td>
<td>17.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>23.0</td>
<td>1.0</td>
<td>53.12</td>
<td>50.33</td>
<td>7.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>27.0</td>
<td>0.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>25.0</td>
<td>0.0</td>
<td>56.71</td>
<td>62.39</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>2.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>54.0</td>
<td>0.0</td>
<td>43.97</td>
<td>42.45</td>
<td>24.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>73.0</td>
<td>0.0</td>
<td>42.68</td>
<td>43.46</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

<p>15830 rows × 139 columns</p>
</div>
</div>
</div>
<div id="cell-64" class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Assuming `dataset_transf_panel19_train_lfr` is your LFR transformed dataset</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="co"># features_lfr = dataset_transf_panel19_train_lfr.features</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="co"># label_lfr = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="co"># feature_names_lfr = dataset_transf_panel19_train_lfr.feature_names</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create a DataFrame</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="co"># df_lfr = pd.DataFrame(features_lfr, columns=feature_names_lfr)</span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="co"># df_lfr['label'] = label_lfr</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> dataset_transf_panel19_train_lfr.features</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> dataset_transf_panel19_train_lfr.labels.ravel()  <span class="co"># Flatten the label array if necessary</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> dataset_transf_panel19_train_lfr.instance_weights</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> dataset_transf_panel19_train_lfr.feature_names</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>df_lfr <span class="op">=</span> pd.DataFrame(features, columns<span class="op">=</span>feature_names)</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>df_lfr[<span class="st">'label'</span>] <span class="op">=</span> label</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>df_lfr[<span class="st">'weights'</span>] <span class="op">=</span> weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-65" class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>df_lfr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
<th data-quarto-table-cell-role="th">weights</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.364698</td>
<td>0.611075</td>
<td>0.319138</td>
<td>0.516439</td>
<td>0.683071</td>
<td>0.513830</td>
<td>0.556535</td>
<td>0.363196</td>
<td>0.402933</td>
<td>0.589060</td>
<td>...</td>
<td>0.238986</td>
<td>0.553776</td>
<td>0.493602</td>
<td>0.513294</td>
<td>0.386629</td>
<td>0.380919</td>
<td>0.498550</td>
<td>0.363095</td>
<td>0.0</td>
<td>21854.981705</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.379547</td>
<td>0.597793</td>
<td>0.328517</td>
<td>0.502386</td>
<td>0.682940</td>
<td>0.523566</td>
<td>0.577716</td>
<td>0.373460</td>
<td>0.395062</td>
<td>0.600292</td>
<td>...</td>
<td>0.237634</td>
<td>0.551191</td>
<td>0.504359</td>
<td>0.533097</td>
<td>0.374361</td>
<td>0.356593</td>
<td>0.499183</td>
<td>0.369585</td>
<td>0.0</td>
<td>18169.604822</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.345017</td>
<td>0.593873</td>
<td>0.327056</td>
<td>0.509281</td>
<td>0.682566</td>
<td>0.524971</td>
<td>0.551491</td>
<td>0.349275</td>
<td>0.416220</td>
<td>0.575195</td>
<td>...</td>
<td>0.246142</td>
<td>0.570321</td>
<td>0.511401</td>
<td>0.519411</td>
<td>0.420631</td>
<td>0.397081</td>
<td>0.509235</td>
<td>0.382303</td>
<td>0.0</td>
<td>17191.832515</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.370890</td>
<td>0.582912</td>
<td>0.338159</td>
<td>0.494210</td>
<td>0.676912</td>
<td>0.535713</td>
<td>0.577500</td>
<td>0.369280</td>
<td>0.388361</td>
<td>0.583862</td>
<td>...</td>
<td>0.233116</td>
<td>0.577892</td>
<td>0.512044</td>
<td>0.551296</td>
<td>0.382591</td>
<td>0.373165</td>
<td>0.506936</td>
<td>0.374916</td>
<td>0.0</td>
<td>20261.485463</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.377148</td>
<td>0.587833</td>
<td>0.331081</td>
<td>0.493269</td>
<td>0.679347</td>
<td>0.528798</td>
<td>0.582084</td>
<td>0.376288</td>
<td>0.383771</td>
<td>0.602294</td>
<td>...</td>
<td>0.238654</td>
<td>0.558482</td>
<td>0.514984</td>
<td>0.543083</td>
<td>0.382213</td>
<td>0.364254</td>
<td>0.495283</td>
<td>0.377611</td>
<td>0.0</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>0.343260</td>
<td>0.597189</td>
<td>0.324804</td>
<td>0.512213</td>
<td>0.683004</td>
<td>0.522468</td>
<td>0.548023</td>
<td>0.347933</td>
<td>0.417919</td>
<td>0.574523</td>
<td>...</td>
<td>0.246629</td>
<td>0.568838</td>
<td>0.508985</td>
<td>0.514854</td>
<td>0.421901</td>
<td>0.399881</td>
<td>0.508553</td>
<td>0.380749</td>
<td>0.0</td>
<td>4111.315754</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>0.343186</td>
<td>0.596636</td>
<td>0.325113</td>
<td>0.511802</td>
<td>0.682861</td>
<td>0.522864</td>
<td>0.548279</td>
<td>0.347965</td>
<td>0.417634</td>
<td>0.574357</td>
<td>...</td>
<td>0.246577</td>
<td>0.569381</td>
<td>0.509405</td>
<td>0.515495</td>
<td>0.422105</td>
<td>0.400035</td>
<td>0.508671</td>
<td>0.381059</td>
<td>0.0</td>
<td>5415.228173</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>0.364434</td>
<td>0.584266</td>
<td>0.332311</td>
<td>0.494401</td>
<td>0.677981</td>
<td>0.531335</td>
<td>0.573192</td>
<td>0.367677</td>
<td>0.389164</td>
<td>0.590592</td>
<td>...</td>
<td>0.240138</td>
<td>0.570064</td>
<td>0.517921</td>
<td>0.541949</td>
<td>0.397951</td>
<td>0.380818</td>
<td>0.500083</td>
<td>0.381961</td>
<td>0.0</td>
<td>3896.116219</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>0.364365</td>
<td>0.591464</td>
<td>0.330698</td>
<td>0.502169</td>
<td>0.683230</td>
<td>0.527371</td>
<td>0.569395</td>
<td>0.362334</td>
<td>0.406439</td>
<td>0.589670</td>
<td>...</td>
<td>0.242613</td>
<td>0.560312</td>
<td>0.511955</td>
<td>0.530772</td>
<td>0.397606</td>
<td>0.370904</td>
<td>0.505687</td>
<td>0.379358</td>
<td>0.0</td>
<td>4883.851005</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>0.369746</td>
<td>0.591171</td>
<td>0.330650</td>
<td>0.499689</td>
<td>0.681767</td>
<td>0.527452</td>
<td>0.573792</td>
<td>0.367704</td>
<td>0.397916</td>
<td>0.593893</td>
<td>...</td>
<td>0.240415</td>
<td>0.559683</td>
<td>0.511546</td>
<td>0.535024</td>
<td>0.389960</td>
<td>0.368115</td>
<td>0.501997</td>
<td>0.377222</td>
<td>0.0</td>
<td>6630.588948</td>
</tr>
</tbody>
</table>

<p>15830 rows × 140 columns</p>
</div>
</div>
</div>
<div id="cell-66" class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming `dataset_transf_panel19_train_di` is your DI transformed dataset</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>features_di <span class="op">=</span> dataset_transf_panel19_train_di.features</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>label_di <span class="op">=</span> dataset_transf_panel19_train_di.labels.ravel()  <span class="co"># Flatten the label array if necessary</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>feature_names_di <span class="op">=</span> dataset_transf_panel19_train_di.feature_names</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>df_di <span class="op">=</span> pd.DataFrame(features_di, columns<span class="op">=</span>feature_names_di)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>df_di[<span class="st">'label'</span>] <span class="op">=</span> label_di</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>df_di</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">AGE</th>
<th data-quarto-table-cell-role="th">RACE</th>
<th data-quarto-table-cell-role="th">PCS42</th>
<th data-quarto-table-cell-role="th">MCS42</th>
<th data-quarto-table-cell-role="th">K6SUM42</th>
<th data-quarto-table-cell-role="th">REGION=1</th>
<th data-quarto-table-cell-role="th">REGION=2</th>
<th data-quarto-table-cell-role="th">REGION=3</th>
<th data-quarto-table-cell-role="th">REGION=4</th>
<th data-quarto-table-cell-role="th">SEX=1</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">EMPST=4</th>
<th data-quarto-table-cell-role="th">POVCAT=1</th>
<th data-quarto-table-cell-role="th">POVCAT=2</th>
<th data-quarto-table-cell-role="th">POVCAT=3</th>
<th data-quarto-table-cell-role="th">POVCAT=4</th>
<th data-quarto-table-cell-role="th">POVCAT=5</th>
<th data-quarto-table-cell-role="th">INSCOV=1</th>
<th data-quarto-table-cell-role="th">INSCOV=2</th>
<th data-quarto-table-cell-role="th">INSCOV=3</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>53.0</td>
<td>1.0</td>
<td>25.93</td>
<td>58.47</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>56.0</td>
<td>1.0</td>
<td>20.42</td>
<td>26.53</td>
<td>17.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>23.0</td>
<td>1.0</td>
<td>52.92</td>
<td>50.28</td>
<td>7.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>27.0</td>
<td>0.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15825</td>
<td>25.0</td>
<td>0.0</td>
<td>56.68</td>
<td>62.11</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15826</td>
<td>25.0</td>
<td>0.0</td>
<td>56.68</td>
<td>62.11</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15827</td>
<td>2.0</td>
<td>1.0</td>
<td>-1.00</td>
<td>-1.00</td>
<td>-1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15828</td>
<td>54.0</td>
<td>0.0</td>
<td>43.43</td>
<td>42.45</td>
<td>24.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15829</td>
<td>73.0</td>
<td>0.0</td>
<td>41.83</td>
<td>43.46</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>...</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

<p>15830 rows × 139 columns</p>
</div>
</div>
</div>
<div id="cell-67" class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>dataset_orig_panel19_train_df</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features, labels, and weights</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop([<span class="st">'label'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'label'</span>]</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the sample_weight parameter</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to extract the corresponding weights for the training samples</span></span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> X_train.index</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>train_weights <span class="op">=</span> weights[train_indices]</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model.predict_proba(X_train)</span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb87-42"><a href="#cb87-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-43"><a href="#cb87-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb87-44"><a href="#cb87-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-45"><a href="#cb87-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-46"><a href="#cb87-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-47"><a href="#cb87-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb87-48"><a href="#cb87-48" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb87-49"><a href="#cb87-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-50"><a href="#cb87-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb87-51"><a href="#cb87-51" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb87-52"><a href="#cb87-52" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb87-53"><a href="#cb87-53" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb87-54"><a href="#cb87-54" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb87-55"><a href="#cb87-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-56"><a href="#cb87-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb87-57"><a href="#cb87-57" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb87-58"><a href="#cb87-58" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb87-59"><a href="#cb87-59" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb87-60"><a href="#cb87-60" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb87-61"><a href="#cb87-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-62"><a href="#cb87-62" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb87-63"><a href="#cb87-63" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 0.8669456727732154, Training Log Loss: 0.3230202782236161
Testing Accuracy: 0.8619709412507897, Testing Log Loss: 0.335720614452724</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/1117411220.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()</code></pre>
</div>
</div>
<div id="cell-68" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">1</span>).to_csv(<span class="st">'meps.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-69" class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns</span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features, labels, and weights</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_rw.drop([<span class="st">'label'</span>, <span class="st">'weights'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_rw[<span class="st">'label'</span>]</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> df_rw[<span class="st">'weights'</span>]</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>model_rw <span class="op">=</span> LogisticRegression()</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the sample_weight parameter</span></span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to extract the corresponding weights for the training samples</span></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> X_train.index</span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>train_weights <span class="op">=</span> weights[train_indices]</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>model_rw.fit(X_train, y_train, sample_weight<span class="op">=</span>train_weights)</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model_rw.predict(X_train)</span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model_rw.predict(X_test)</span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model_rw.predict_proba(X_train)</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model_rw.predict_proba(X_test)</span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb91-41"><a href="#cb91-41" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb91-42"><a href="#cb91-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-43"><a href="#cb91-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb91-44"><a href="#cb91-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb91-45"><a href="#cb91-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb91-46"><a href="#cb91-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-47"><a href="#cb91-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb91-48"><a href="#cb91-48" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb91-49"><a href="#cb91-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-50"><a href="#cb91-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb91-51"><a href="#cb91-51" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb91-52"><a href="#cb91-52" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb91-53"><a href="#cb91-53" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb91-54"><a href="#cb91-54" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb91-55"><a href="#cb91-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-56"><a href="#cb91-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb91-57"><a href="#cb91-57" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb91-58"><a href="#cb91-58" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb91-59"><a href="#cb91-59" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb91-60"><a href="#cb91-60" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb91-61"><a href="#cb91-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-62"><a href="#cb91-62" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb91-63"><a href="#cb91-63" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 0.8651295009475679, Training Log Loss: 0.33949214668723315
Testing Accuracy: 0.8562855337965888, Testing Log Loss: 0.3484870792605314</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/2225796879.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()</code></pre>
</div>
</div>
<div id="cell-70" class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns</span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features, labels, and weights</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_di.drop([<span class="st">'label'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_di[<span class="st">'label'</span>]</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>model_di <span class="op">=</span> LogisticRegression()</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the sample_weight parameter</span></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to extract the corresponding weights for the training samples</span></span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> X_train.index</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>train_weights <span class="op">=</span> weights[train_indices]</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>model_di.fit(X_train, y_train, sample_weight<span class="op">=</span>train_weights)</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model_di.predict(X_train)</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model_di.predict(X_test)</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model_di.predict_proba(X_train)</span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model_di.predict_proba(X_test)</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb94-48"><a href="#cb94-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-49"><a href="#cb94-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb94-50"><a href="#cb94-50" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb94-51"><a href="#cb94-51" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb94-52"><a href="#cb94-52" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb94-53"><a href="#cb94-53" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb94-54"><a href="#cb94-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-55"><a href="#cb94-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb94-56"><a href="#cb94-56" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb94-57"><a href="#cb94-57" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb94-58"><a href="#cb94-58" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb94-59"><a href="#cb94-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb94-60"><a href="#cb94-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-61"><a href="#cb94-61" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb94-62"><a href="#cb94-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 0.8646557169930512, Training Log Loss: 0.33943553174854924
Testing Accuracy: 0.8550221099178774, Testing Log Loss: 0.3481697691277553</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/789110131.py:62: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()</code></pre>
</div>
</div>
<div id="cell-71" class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'df_lfr' is your Pandas DataFrame with features and 'label' columns, </span></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="co"># resulting from the LFR transformation</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the DataFrame into features and labels</span></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_lfr.drop([<span class="st">'label'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_lfr[<span class="st">'label'</span>]</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y)</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model</span></span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>model_lfr <span class="op">=</span> LogisticRegression()</span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model (no sample weights are needed as the data is already transformed by LFR)</span></span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>model_lfr.fit(X_train, y_train, sample_weight<span class="op">=</span>train_weights)</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a>y_train_pred <span class="op">=</span> model_lfr.predict(X_train)</span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model_lfr.predict(X_test)</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability predictions for log loss calculation</span></span>
<span id="cb97-28"><a href="#cb97-28" aria-hidden="true" tabindex="-1"></a>y_train_pred_proba <span class="op">=</span> model_lfr.predict_proba(X_train)</span>
<span id="cb97-29"><a href="#cb97-29" aria-hidden="true" tabindex="-1"></a>y_test_pred_proba <span class="op">=</span> model_lfr.predict_proba(X_test)</span>
<span id="cb97-30"><a href="#cb97-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-31"><a href="#cb97-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb97-32"><a href="#cb97-32" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, y_train_pred)</span>
<span id="cb97-33"><a href="#cb97-33" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_test_pred)</span>
<span id="cb97-34"><a href="#cb97-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-35"><a href="#cb97-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate log loss</span></span>
<span id="cb97-36"><a href="#cb97-36" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> log_loss(y_train, y_train_pred_proba)</span>
<span id="cb97-37"><a href="#cb97-37" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> log_loss(y_test, y_test_pred_proba)</span>
<span id="cb97-38"><a href="#cb97-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-39"><a href="#cb97-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Print errors and losses</span></span>
<span id="cb97-40"><a href="#cb97-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">, Training Log Loss: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb97-41"><a href="#cb97-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">, Testing Log Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb97-42"><a href="#cb97-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-43"><a href="#cb97-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb97-44"><a href="#cb97-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb97-45"><a href="#cb97-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-46"><a href="#cb97-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy plot</span></span>
<span id="cb97-47"><a href="#cb97-47" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb97-48"><a href="#cb97-48" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Accuracy'</span>, <span class="st">'Test Accuracy'</span>], [train_accuracy, test_accuracy], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb97-49"><a href="#cb97-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb97-50"><a href="#cb97-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Accuracy'</span>)</span>
<span id="cb97-51"><a href="#cb97-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-52"><a href="#cb97-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Log Loss plot</span></span>
<span id="cb97-53"><a href="#cb97-53" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb97-54"><a href="#cb97-54" aria-hidden="true" tabindex="-1"></a>plt.bar([<span class="st">'Train Log Loss'</span>, <span class="st">'Test Log Loss'</span>], [train_loss, test_loss], color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'green'</span>])</span>
<span id="cb97-55"><a href="#cb97-55" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Loss'</span>)</span>
<span id="cb97-56"><a href="#cb97-56" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Train vs Test Log Loss'</span>)</span>
<span id="cb97-57"><a href="#cb97-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-58"><a href="#cb97-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb97-59"><a href="#cb97-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[50], line 21</span>
<span class="ansi-green-fg ansi-bold">     18</span> model_lfr <span style="color:rgb(98,98,98)">=</span> LogisticRegression()
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># Train the model (no sample weights are needed as the data is already transformed by LFR)</span>
<span class="ansi-green-fg">---&gt; 21</span> <span class="ansi-yellow-bg">model_lfr</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">fit</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">X_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">y_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">sample_weight</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">train_weights</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     23</span> <span style="font-style:italic;color:rgb(95,135,135)"># Predictions</span>
<span class="ansi-green-fg ansi-bold">     24</span> y_train_pred <span style="color:rgb(98,98,98)">=</span> model_lfr<span style="color:rgb(98,98,98)">.</span>predict(X_train)

File <span class="ansi-green-fg">/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474</span>, in <span class="ansi-cyan-fg">_fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="ansi-blue-fg">(estimator, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1467</span>     estimator<span style="color:rgb(98,98,98)">.</span>_validate_params()
<span class="ansi-green-fg ansi-bold">   1469</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> config_context(
<span class="ansi-green-fg ansi-bold">   1470</span>     skip_parameter_validation<span style="color:rgb(98,98,98)">=</span>(
<span class="ansi-green-fg ansi-bold">   1471</span>         prefer_skip_nested_validation <span style="font-weight:bold;color:rgb(175,0,255)">or</span> global_skip_validation
<span class="ansi-green-fg ansi-bold">   1472</span>     )
<span class="ansi-green-fg ansi-bold">   1473</span> ):
<span class="ansi-green-fg">-&gt; 1474</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">fit_method</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">estimator</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1246</span>, in <span class="ansi-cyan-fg">LogisticRegression.fit</span><span class="ansi-blue-fg">(self, X, y, sample_weight)</span>
<span class="ansi-green-fg ansi-bold">   1244</span> classes_ <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>classes_
<span class="ansi-green-fg ansi-bold">   1245</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> n_classes <span style="color:rgb(98,98,98)">&lt;</span> <span style="color:rgb(98,98,98)">2</span>:
<span class="ansi-green-fg">-&gt; 1246</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">   1247</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">This solver needs samples of at least 2 classes</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1248</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> in the data, but the data contains only one</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1249</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> class: </span><span style="font-weight:bold;color:rgb(175,95,135)">%r</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1250</span>         <span style="color:rgb(98,98,98)">%</span> classes_[<span style="color:rgb(98,98,98)">0</span>]
<span class="ansi-green-fg ansi-bold">   1251</span>     )
<span class="ansi-green-fg ansi-bold">   1253</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">len</span>(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>classes_) <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(98,98,98)">2</span>:
<span class="ansi-green-fg ansi-bold">   1254</span>     n_classes <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">1</span>

<span class="ansi-red-fg">ValueError</span>: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0</pre>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/srinivassundar98\.github\.io\/Ensuring-Fair-Play\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>