[
  {
    "objectID": "visualizations.html",
    "href": "visualizations.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\nfrom aif360.datasets import MEPSDataset20\nfrom aif360.datasets import MEPSDataset21\nfrom aif360.datasets import GermanDataset\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Bias mitigation techniques\nfrom aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\nfrom aif360.algorithms.preprocessing import LFR\nfrom aif360.algorithms.preprocessing import OptimPreproc\nfrom sklearn.model_selection import train_test_split\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n  warn_deprecated('vmap', 'torch.vmap')\n\n\n\ndataset_orig_panel19_train = MEPSDataset19()\ndataset_orig_panel19_train.features\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\nmetric_orig_panel19_train = BinaryLabelDatasetMetric(\n        dataset_orig_panel19_train,\n        unprivileged_groups=unprivileged_groups,\n        privileged_groups=privileged_groups)\n\n\nfrom aif360.datasets import MEPSDataset19\n\ndata_orig_sex=MEPSDataset19()\n\n\nsens_ind = 0\nsens_attr = data_orig_sex.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    data_orig_sex.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    data_orig_sex.privileged_protected_attributes[sens_ind]]\n\n\nimport pandas as pd\n# Combining features and labels into one DataFrame for correlation analysis\nfull_df = pd.read_csv('MEPS_FINAL.csv')\n\n\nimport numpy as np\nnp.random.seed(42)\n\ndata_orig_sex_train, data_orig_sex_test = data_orig_sex.split([0.7], shuffle=True)\n\nprint(\"Perpetrator Sex :\",data_orig_sex_train.features.shape)\nprint(\"Perpetrator Sex :\",data_orig_sex_test.features.shape)\n\nPerpetrator Sex : (11081, 138)\nPerpetrator Sex : (4749, 138)\n\n\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Configuring the model to allow for incremental training\nmodel = LogisticRegression(solver='liblinear')\n\n# Arrays to store metrics for plotting\ntrain_losses = []\ntest_losses = []\ntrain_accuracies = []\ntest_accuracies = []\n\n# Simulating training over multiple iterations to mimic epochs\niterations = 30\nfor i in range(iterations):\n    model.fit(data_orig_sex_train.features, data_orig_sex_train.labels.ravel(), \n              sample_weight=data_orig_sex_train.instance_weights)\n    # Predict probabilities for calculating log loss\n    y_pred_train_prob = model.predict_proba(data_orig_sex_train.features)\n    y_pred_test_prob = model.predict_proba(data_orig_sex_test.features)\n    \n    # Calculate accuracy\n    y_pred_train = model.predict(data_orig_sex_train.features)\n    y_pred_test = model.predict(data_orig_sex_test.features)\n    train_accuracy = accuracy_score(data_orig_sex_train.labels, y_pred_train)\n    test_accuracy = accuracy_score(data_orig_sex_test.labels, y_pred_test)\n    \n    # Calculate log loss\n    train_loss = log_loss(data_orig_sex_train.labels, y_pred_train_prob, \n                          sample_weight=data_orig_sex_train.instance_weights)\n    test_loss = log_loss(data_orig_sex_test.labels, y_pred_test_prob, \n                         sample_weight=data_orig_sex_test.instance_weights)\n    \n    # Store metrics\n    train_losses.append(train_loss)\n    test_losses.append(test_loss)\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)\n\n# Visualization of the metrics over iterations\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(iterations), y=train_losses, mode='lines+markers', name='Train Loss'))\nfig.add_trace(go.Scatter(x=np.arange(iterations), y=test_losses, mode='lines+markers', name='Test Loss'))\nfig.update_layout(title='Training and Test Loss Over Iterations', xaxis_title='Iteration', yaxis_title='Log Loss')\nfig.show()\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(iterations), y=train_accuracies, mode='lines+markers', name='Train Accuracy'))\nfig.add_trace(go.Scatter(x=np.arange(iterations), y=test_accuracies, mode='lines+markers', name='Test Accuracy'))\nfig.update_layout(title='Training and Test Accuracy Over Iterations', xaxis_title='Iteration', yaxis_title='Accuracy')\nfig.show()\n\n# Feature Importance (if applicable and model converged)\nif hasattr(model, 'coef_'):\n    importances = model.coef_[0]\n    fig = go.Figure([go.Bar(x=[f'Feature {i}' for i in range(len(importances))], y=importances)])\n    fig.update_layout(title='Feature Importance', xaxis_title='Feature', yaxis_title='Coefficient Value')\n    fig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport plotly.figure_factory as ff\n\n# Assuming y_pred_test is obtained from model.predict(X_test_scaled)\ncm = confusion_matrix(data_orig_sex_test.labels, y_pred_test)\n\n# Plotting using Plotly's Figure Factory\nfig = ff.create_annotated_heatmap(z=cm, x=[str(x) for x in model.classes_], y=[str(x) for x in model.classes_],\n                                  colorscale='Viridis', showscale=True)\nfig.update_layout(title='Confusion Matrix', xaxis_title='Predicted Label', yaxis_title='True Label')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "preprocessing_bias_pr.html",
    "href": "preprocessing_bias_pr.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\nfrom aif360.datasets import CompasDataset\n\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Bias mitigation techniques\nfrom aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\nfrom aif360.algorithms.preprocessing import LFR\nfrom aif360.algorithms.preprocessing import OptimPreproc\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_functorch/deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n  warn_deprecated('vmap', 'torch.vmap')\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n# dataset_orig_compas = MEPSDataset19()\n\n\ndataset_orig_compas = CompasDataset()\n\nWARNING:root:Missing Data: 5 rows removed from CompasDataset.\n\n\n\n# dataset_orig_panel19_train = MEPSDataset19()\n\n\ndataset_orig_compas_train = CompasDataset()\n\nWARNING:root:Missing Data: 5 rows removed from CompasDataset.\n\n\n\ntype(dataset_orig_compas_train)\n\n\naif360.datasets.compas_dataset.CompasDataset\n\n\n\nprint(dataset_orig_compas_train.feature_names)\n\n['sex', 'age', 'race', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'age_cat=25 - 45', 'age_cat=Greater than 45', 'age_cat=Less than 25', 'c_charge_degree=F', 'c_charge_degree=M', 'c_charge_desc=Abuse Without Great Harm', 'c_charge_desc=Agg Abuse Elderlly/Disabled Adult', 'c_charge_desc=Agg Assault W/int Com Fel Dome', 'c_charge_desc=Agg Battery Grt/Bod/Harm', 'c_charge_desc=Agg Fleeing and Eluding', 'c_charge_desc=Agg Fleeing/Eluding High Speed', 'c_charge_desc=Aggr Child Abuse-Torture,Punish', 'c_charge_desc=Aggrav Battery w/Deadly Weapon', 'c_charge_desc=Aggrav Child Abuse-Agg Battery', 'c_charge_desc=Aggrav Child Abuse-Causes Harm', 'c_charge_desc=Aggrav Stalking After Injunctn', 'c_charge_desc=Aggravated Assault', 'c_charge_desc=Aggravated Assault W/Dead Weap', 'c_charge_desc=Aggravated Assault W/dead Weap', 'c_charge_desc=Aggravated Assault W/o Firearm', 'c_charge_desc=Aggravated Assault w/Firearm', 'c_charge_desc=Aggravated Battery', 'c_charge_desc=Aggravated Battery (Firearm)', 'c_charge_desc=Aggravated Battery (Firearm/Actual Possession)', 'c_charge_desc=Aggravated Battery / Pregnant', 'c_charge_desc=Aggravated Battery On 65/Older', 'c_charge_desc=Aide/Abet Prostitution Lewdness', 'c_charge_desc=Aiding Escape', 'c_charge_desc=Alcoholic Beverage Violation-FL', 'c_charge_desc=Armed Trafficking in Cannabis', 'c_charge_desc=Arson in the First Degree', 'c_charge_desc=Assault', 'c_charge_desc=Assault Law Enforcement Officer', 'c_charge_desc=Att Burgl Conv Occp', 'c_charge_desc=Att Burgl Struc/Conv Dwel/Occp', 'c_charge_desc=Att Burgl Unoccupied Dwel', 'c_charge_desc=Att Tamper w/Physical Evidence', 'c_charge_desc=Attempt Armed Burglary Dwell', 'c_charge_desc=Attempted Burg/Convey/Unocc', 'c_charge_desc=Attempted Burg/struct/unocc', 'c_charge_desc=Attempted Deliv Control Subst', 'c_charge_desc=Attempted Robbery  No Weapon', 'c_charge_desc=Attempted Robbery  Weapon', 'c_charge_desc=Battery', 'c_charge_desc=Battery Emergency Care Provide', 'c_charge_desc=Battery On A Person Over 65', 'c_charge_desc=Battery On Fire Fighter', 'c_charge_desc=Battery On Parking Enfor Speci', 'c_charge_desc=Battery Spouse Or Girlfriend', 'c_charge_desc=Battery on Law Enforc Officer', 'c_charge_desc=Battery on a Person Over 65', 'c_charge_desc=Bribery Athletic Contests', 'c_charge_desc=Burgl Dwel/Struct/Convey Armed', 'c_charge_desc=Burglary Assault/Battery Armed', 'c_charge_desc=Burglary Conveyance Armed', 'c_charge_desc=Burglary Conveyance Assault/Bat', 'c_charge_desc=Burglary Conveyance Occupied', 'c_charge_desc=Burglary Conveyance Unoccup', 'c_charge_desc=Burglary Dwelling Armed', 'c_charge_desc=Burglary Dwelling Assault/Batt', 'c_charge_desc=Burglary Dwelling Occupied', 'c_charge_desc=Burglary Structure Assault/Batt', 'c_charge_desc=Burglary Structure Occupied', 'c_charge_desc=Burglary Structure Unoccup', 'c_charge_desc=Burglary Unoccupied Dwelling', 'c_charge_desc=Burglary With Assault/battery', 'c_charge_desc=Carjacking w/o Deadly Weapon', 'c_charge_desc=Carjacking with a Firearm', 'c_charge_desc=Carry Open/Uncov Bev In Pub', 'c_charge_desc=Carrying A Concealed Weapon', 'c_charge_desc=Carrying Concealed Firearm', 'c_charge_desc=Cash Item w/Intent to Defraud', 'c_charge_desc=Child Abuse', 'c_charge_desc=Computer Pornography', 'c_charge_desc=Consp Traff Oxycodone  4g&gt;&lt;14g', 'c_charge_desc=Conspiracy Dealing Stolen Prop', 'c_charge_desc=Consume Alcoholic Bev Pub', 'c_charge_desc=Contradict Statement', 'c_charge_desc=Contribute Delinquency Of A Minor', 'c_charge_desc=Corrupt Public Servant', 'c_charge_desc=Counterfeit Lic Plates/Sticker', 'c_charge_desc=Crim Attempt/Solic/Consp', 'c_charge_desc=Crim Use of Personal ID Info', 'c_charge_desc=Crimin Mischief Damage $1000+', 'c_charge_desc=Criminal Mischief', 'c_charge_desc=Criminal Mischief Damage &lt;$200', 'c_charge_desc=Criminal Mischief&gt;$200&lt;$1000', 'c_charge_desc=Crlty Twrd Child Urge Oth Act', 'c_charge_desc=Cruelty Toward Child', 'c_charge_desc=Cruelty to Animals', 'c_charge_desc=Culpable Negligence', 'c_charge_desc=D.U.I. Serious Bodily Injury', 'c_charge_desc=DOC/Cause Public Danger', 'c_charge_desc=DUI - Enhanced', 'c_charge_desc=DUI - Property Damage/Personal Injury', 'c_charge_desc=DUI Blood Alcohol Above 0.20', 'c_charge_desc=DUI Level 0.15 Or Minor In Veh', 'c_charge_desc=DUI Property Damage/Injury', 'c_charge_desc=DUI- Enhanced', 'c_charge_desc=DUI/Property Damage/Persnl Inj', 'c_charge_desc=DWI w/Inj Susp Lic / Habit Off', 'c_charge_desc=DWLS Canceled Disqul 1st Off', 'c_charge_desc=DWLS Susp/Cancel Revoked', 'c_charge_desc=Dealing in Stolen Property', 'c_charge_desc=Defrauding Innkeeper', 'c_charge_desc=Defrauding Innkeeper $300/More', 'c_charge_desc=Del 3,4 Methylenedioxymethcath', 'c_charge_desc=Del Cannabis At/Near Park', 'c_charge_desc=Del Cannabis For Consideration', 'c_charge_desc=Del of JWH-250 2-Methox 1-Pentyl', 'c_charge_desc=Deliver 3,4 Methylenediox', 'c_charge_desc=Deliver Alprazolam', 'c_charge_desc=Deliver Cannabis', 'c_charge_desc=Deliver Cannabis 1000FTSch', 'c_charge_desc=Deliver Cocaine', 'c_charge_desc=Deliver Cocaine 1000FT Church', 'c_charge_desc=Deliver Cocaine 1000FT Park', 'c_charge_desc=Deliver Cocaine 1000FT School', 'c_charge_desc=Deliver Cocaine 1000FT Store', 'c_charge_desc=Delivery Of Drug Paraphernalia', 'c_charge_desc=Delivery of 5-Fluoro PB-22', 'c_charge_desc=Delivery of Heroin', 'c_charge_desc=Depriv LEO of Protect/Communic', 'c_charge_desc=Disorderly Conduct', 'c_charge_desc=Disorderly Intoxication', 'c_charge_desc=Disrupting School Function', 'c_charge_desc=Drivg While Lic Suspd/Revk/Can', 'c_charge_desc=Driving License Suspended', 'c_charge_desc=Driving Under The Influence', 'c_charge_desc=Driving While License Revoked', 'c_charge_desc=Escape', 'c_charge_desc=Exhibition Weapon School Prop', 'c_charge_desc=Expired DL More Than 6 Months', 'c_charge_desc=Exposes Culpable Negligence', 'c_charge_desc=Extradition/Defendants', 'c_charge_desc=Fabricating Physical Evidence', 'c_charge_desc=Fail Register Vehicle', 'c_charge_desc=Fail Sex Offend Report Bylaw', 'c_charge_desc=Fail To Obey Police Officer', 'c_charge_desc=Fail To Redeliv Hire/Leas Prop', 'c_charge_desc=Failure To Pay Taxi Cab Charge', 'c_charge_desc=Failure To Return Hired Vehicle', 'c_charge_desc=False 911 Call', 'c_charge_desc=False Bomb Report', 'c_charge_desc=False Imprisonment', 'c_charge_desc=False Info LEO During Invest', 'c_charge_desc=False Motor Veh Insurance Card', 'c_charge_desc=False Name By Person Arrest', 'c_charge_desc=False Ownership Info/Pawn Item', 'c_charge_desc=Falsely Impersonating Officer', 'c_charge_desc=Fel Drive License Perm Revoke', 'c_charge_desc=Felon in Pos of Firearm or Amm', 'c_charge_desc=Felony Batt(Great Bodily Harm)', 'c_charge_desc=Felony Battery', 'c_charge_desc=Felony Battery (Dom Strang)', 'c_charge_desc=Felony Battery w/Prior Convict', 'c_charge_desc=Felony Committing Prostitution', 'c_charge_desc=Felony DUI (level 3)', 'c_charge_desc=Felony DUI - Enhanced', 'c_charge_desc=Felony Driving While Lic Suspd', 'c_charge_desc=Felony Petit Theft', 'c_charge_desc=Felony/Driving Under Influence', 'c_charge_desc=Fighting/Baiting Animals', 'c_charge_desc=Fleeing Or Attmp Eluding A Leo', 'c_charge_desc=Fleeing or Eluding a LEO', 'c_charge_desc=Forging Bank Bills/Promis Note', 'c_charge_desc=Fraudulent Use of Credit Card', 'c_charge_desc=Grand Theft (Motor Vehicle)', 'c_charge_desc=Grand Theft Dwell Property', 'c_charge_desc=Grand Theft Firearm', 'c_charge_desc=Grand Theft in the 1st Degree', 'c_charge_desc=Grand Theft in the 3rd Degree', 'c_charge_desc=Grand Theft of a Fire Extinquisher', 'c_charge_desc=Grand Theft of the 2nd Degree', 'c_charge_desc=Grand Theft on 65 Yr or Older', 'c_charge_desc=Harass Witness/Victm/Informnt', 'c_charge_desc=Harm Public Servant Or Family', 'c_charge_desc=Hiring with Intent to Defraud', 'c_charge_desc=Imperson Public Officer or Emplyee', 'c_charge_desc=Interfere W/Traf Cont Dev RR', 'c_charge_desc=Interference with Custody', 'c_charge_desc=Intoxicated/Safety Of Another', 'c_charge_desc=Introduce Contraband Into Jail', 'c_charge_desc=Issuing a Worthless Draft', 'c_charge_desc=Kidnapping / Domestic Violence', 'c_charge_desc=Lease For Purpose Trafficking', 'c_charge_desc=Leave Acc/Attend Veh/More $50', 'c_charge_desc=Leave Accd/Attend Veh/Less $50', 'c_charge_desc=Leaving Acc/Unattended Veh', 'c_charge_desc=Leaving the Scene of Accident', 'c_charge_desc=Lewd Act Presence Child 16-', 'c_charge_desc=Lewd or Lascivious Molestation', 'c_charge_desc=Lewd/Lasc Battery Pers 12+/&lt;16', 'c_charge_desc=Lewd/Lasc Exhib Presence &lt;16yr', 'c_charge_desc=Lewd/Lasciv Molest Elder Persn', 'c_charge_desc=Lewdness Violation', 'c_charge_desc=License Suspended Revoked', 'c_charge_desc=Littering', 'c_charge_desc=Live on Earnings of Prostitute', 'c_charge_desc=Lve/Scen/Acc/Veh/Prop/Damage', 'c_charge_desc=Manage Busn W/O City Occup Lic', 'c_charge_desc=Manslaughter W/Weapon/Firearm', 'c_charge_desc=Manufacture Cannabis', 'c_charge_desc=Misuse Of 911 Or E911 System', 'c_charge_desc=Money Launder 100K or More Dols', 'c_charge_desc=Murder In 2nd Degree W/firearm', 'c_charge_desc=Murder in the First Degree', 'c_charge_desc=Neglect Child / Bodily Harm', 'c_charge_desc=Neglect Child / No Bodily Harm', 'c_charge_desc=Neglect/Abuse Elderly Person', 'c_charge_desc=Obstruct Fire Equipment', 'c_charge_desc=Obstruct Officer W/Violence', 'c_charge_desc=Obtain Control Substance By Fraud', 'c_charge_desc=Offer Agree Secure For Lewd Act', 'c_charge_desc=Offer Agree Secure/Lewd Act', 'c_charge_desc=Offn Against Intellectual Prop', 'c_charge_desc=Open Carrying Of Weapon', 'c_charge_desc=Oper Motorcycle W/O Valid DL', 'c_charge_desc=Operating W/O Valid License', 'c_charge_desc=Opert With Susp DL 2nd Offens', 'c_charge_desc=PL/Unlaw Use Credit Card', 'c_charge_desc=Petit Theft', 'c_charge_desc=Petit Theft $100- $300', 'c_charge_desc=Pos Cannabis For Consideration', 'c_charge_desc=Pos Cannabis W/Intent Sel/Del', 'c_charge_desc=Pos Methylenedioxymethcath W/I/D/S', 'c_charge_desc=Poss 3,4 MDMA (Ecstasy)', 'c_charge_desc=Poss Alprazolam W/int Sell/Del', 'c_charge_desc=Poss Anti-Shoplifting Device', 'c_charge_desc=Poss Cntrft Contr Sub w/Intent', 'c_charge_desc=Poss Cocaine/Intent To Del/Sel', 'c_charge_desc=Poss Contr Subst W/o Prescript', 'c_charge_desc=Poss Counterfeit Payment Inst', 'c_charge_desc=Poss Drugs W/O A Prescription', 'c_charge_desc=Poss F/Arm Delinq', 'c_charge_desc=Poss Firearm W/Altered ID#', 'c_charge_desc=Poss Meth/Diox/Meth/Amp (MDMA)', 'c_charge_desc=Poss Of 1,4-Butanediol', 'c_charge_desc=Poss Of Controlled Substance', 'c_charge_desc=Poss Of RX Without RX', 'c_charge_desc=Poss Oxycodone W/Int/Sell/Del', 'c_charge_desc=Poss Pyrrolidinobutiophenone', 'c_charge_desc=Poss Pyrrolidinovalerophenone', 'c_charge_desc=Poss Pyrrolidinovalerophenone W/I/D/S', 'c_charge_desc=Poss Similitude of Drivers Lic', 'c_charge_desc=Poss Tetrahydrocannabinols', 'c_charge_desc=Poss Unlaw Issue Driver Licenc', 'c_charge_desc=Poss Unlaw Issue Id', 'c_charge_desc=Poss Wep Conv Felon', 'c_charge_desc=Poss of Cocaine W/I/D/S 1000FT Park', 'c_charge_desc=Poss of Firearm by Convic Felo', 'c_charge_desc=Poss of Methylethcathinone', 'c_charge_desc=Poss/Sell/Del Cocaine 1000FT Sch', 'c_charge_desc=Poss/Sell/Del/Man Amobarbital', 'c_charge_desc=Poss/pur/sell/deliver Cocaine', 'c_charge_desc=Poss3,4 Methylenedioxymethcath', 'c_charge_desc=Posses/Disply Susp/Revk/Frd DL', 'c_charge_desc=Possess Cannabis 1000FTSch', 'c_charge_desc=Possess Cannabis/20 Grams Or Less', 'c_charge_desc=Possess Controlled Substance', 'c_charge_desc=Possess Countrfeit Credit Card', 'c_charge_desc=Possess Drug Paraphernalia', 'c_charge_desc=Possess Mot Veh W/Alt Vin #', 'c_charge_desc=Possess Tobacco Product Under 18', 'c_charge_desc=Possess Weapon On School Prop', 'c_charge_desc=Possess w/I/Utter Forged Bills', 'c_charge_desc=Possession Burglary Tools', 'c_charge_desc=Possession Child Pornography', 'c_charge_desc=Possession Firearm School Prop', 'c_charge_desc=Possession Of 3,4Methylenediox', 'c_charge_desc=Possession Of Alprazolam', 'c_charge_desc=Possession Of Amphetamine', 'c_charge_desc=Possession Of Anabolic Steroid', 'c_charge_desc=Possession Of Buprenorphine', 'c_charge_desc=Possession Of Carisoprodol', 'c_charge_desc=Possession Of Clonazepam', 'c_charge_desc=Possession Of Cocaine', 'c_charge_desc=Possession Of Diazepam', 'c_charge_desc=Possession Of Fentanyl', 'c_charge_desc=Possession Of Heroin', 'c_charge_desc=Possession Of Methamphetamine', 'c_charge_desc=Possession Of Paraphernalia', 'c_charge_desc=Possession Of Phentermine', 'c_charge_desc=Possession of Alcohol Under 21', 'c_charge_desc=Possession of Benzylpiperazine', 'c_charge_desc=Possession of Butylone', 'c_charge_desc=Possession of Cannabis', 'c_charge_desc=Possession of Cocaine', 'c_charge_desc=Possession of Codeine', 'c_charge_desc=Possession of Ethylone', 'c_charge_desc=Possession of Hydrocodone', 'c_charge_desc=Possession of Hydromorphone', 'c_charge_desc=Possession of LSD', 'c_charge_desc=Possession of Methadone', 'c_charge_desc=Possession of Morphine', 'c_charge_desc=Possession of Oxycodone', 'c_charge_desc=Possession of XLR11', 'c_charge_desc=Principal In The First Degree', 'c_charge_desc=Prostitution', 'c_charge_desc=Prostitution/Lewd Act Assignation', 'c_charge_desc=Prostitution/Lewdness/Assign', 'c_charge_desc=Prowling/Loitering', 'c_charge_desc=Purchase Cannabis', 'c_charge_desc=Purchase/P/W/Int Cannabis', 'c_charge_desc=Reckless Driving', 'c_charge_desc=Refuse Submit Blood/Breath Test', 'c_charge_desc=Refuse to Supply DNA Sample', 'c_charge_desc=Resist Officer w/Violence', 'c_charge_desc=Resist/Obstruct W/O Violence', 'c_charge_desc=Retail Theft $300 1st Offense', 'c_charge_desc=Retail Theft $300 2nd Offense', 'c_charge_desc=Ride Tri-Rail Without Paying', 'c_charge_desc=Robbery / No Weapon', 'c_charge_desc=Robbery / Weapon', 'c_charge_desc=Robbery Sudd Snatch No Weapon', 'c_charge_desc=Robbery W/Deadly Weapon', 'c_charge_desc=Robbery W/Firearm', 'c_charge_desc=Sale/Del Cannabis At/Near Scho', 'c_charge_desc=Sale/Del Counterfeit Cont Subs', 'c_charge_desc=Sel/Pur/Mfr/Del Control Substa', 'c_charge_desc=Sell or Offer for Sale Counterfeit Goods', 'c_charge_desc=Sell/Man/Del Pos/w/int Heroin', 'c_charge_desc=Sex Batt Faml/Cust Vict 12-17Y', 'c_charge_desc=Sex Battery Deft 18+/Vict 11-', 'c_charge_desc=Sex Offender Fail Comply W/Law', 'c_charge_desc=Sexual Battery / Vict 12 Yrs +', 'c_charge_desc=Sexual Performance by a Child', 'c_charge_desc=Shoot In Occupied Dwell', 'c_charge_desc=Shoot Into Vehicle', 'c_charge_desc=Simulation of Legal Process', 'c_charge_desc=Solic to Commit Battery', 'c_charge_desc=Solicit Deliver Cocaine', 'c_charge_desc=Solicit Purchase Cocaine', 'c_charge_desc=Solicit To Deliver Cocaine', 'c_charge_desc=Solicitation On Felony 3 Deg', 'c_charge_desc=Soliciting For Prostitution', 'c_charge_desc=Sound Articles Over 100', 'c_charge_desc=Stalking', 'c_charge_desc=Stalking (Aggravated)', 'c_charge_desc=Strong Armed  Robbery', 'c_charge_desc=Structuring Transactions', 'c_charge_desc=Susp Drivers Lic 1st Offense', 'c_charge_desc=Tamper With Victim', 'c_charge_desc=Tamper With Witness', 'c_charge_desc=Tamper With Witness/Victim/CI', 'c_charge_desc=Tampering With Physical Evidence', 'c_charge_desc=Tampering with a Victim', 'c_charge_desc=Theft/To Deprive', 'c_charge_desc=Threat Public Servant', 'c_charge_desc=Throw Deadly Missile Into Veh', 'c_charge_desc=Throw In Occupied Dwell', 'c_charge_desc=Throw Missile Into Pub/Priv Dw', 'c_charge_desc=Traff In Cocaine &lt;400g&gt;150 Kil', 'c_charge_desc=Traffic Counterfeit Cred Cards', 'c_charge_desc=Traffick Amphetamine 28g&gt;&lt;200g', 'c_charge_desc=Traffick Oxycodone     4g&gt;&lt;14g', 'c_charge_desc=Trans/Harm/Material to a Minor', 'c_charge_desc=Trespass On School Grounds', 'c_charge_desc=Trespass Other Struct/Conve', 'c_charge_desc=Trespass Private Property', 'c_charge_desc=Trespass Property w/Dang Weap', 'c_charge_desc=Trespass Struct/Conveyance', 'c_charge_desc=Trespass Structure w/Dang Weap', 'c_charge_desc=Trespass Structure/Conveyance', 'c_charge_desc=Trespassing/Construction Site', 'c_charge_desc=Tresspass Struct/Conveyance', 'c_charge_desc=Tresspass in Structure or Conveyance', 'c_charge_desc=Unauth C/P/S Sounds&gt;1000/Audio', 'c_charge_desc=Unauth Poss ID Card or DL', 'c_charge_desc=Unauthorized Interf w/Railroad', 'c_charge_desc=Unl/Disturb Education/Instui', 'c_charge_desc=Unlaw Lic Use/Disply Of Others', 'c_charge_desc=Unlaw LicTag/Sticker Attach', 'c_charge_desc=Unlaw Use False Name/Identity', 'c_charge_desc=Unlawful Conveyance of Fuel', 'c_charge_desc=Unlicensed Telemarketing', 'c_charge_desc=Use Computer for Child Exploit', 'c_charge_desc=Use Of 2 Way Device To Fac Fel', 'c_charge_desc=Use Scanning Device to Defraud', 'c_charge_desc=Use of Anti-Shoplifting Device', 'c_charge_desc=Uttering Forged Bills', 'c_charge_desc=Uttering Forged Credit Card', 'c_charge_desc=Uttering Worthless Check +$150', 'c_charge_desc=Uttering a Forged Instrument', 'c_charge_desc=Video Voyeur-&lt;24Y on Child &gt;16', 'c_charge_desc=Viol Injunct Domestic Violence', 'c_charge_desc=Viol Injunction Protect Dom Vi', 'c_charge_desc=Viol Pretrial Release Dom Viol', 'c_charge_desc=Viol Prot Injunc Repeat Viol', 'c_charge_desc=Violation License Restrictions', 'c_charge_desc=Violation Of Boater Safety Id', 'c_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking', 'c_charge_desc=Voyeurism', 'c_charge_desc=arrest case no charge']\n\n\n\ndataset_orig_compas_train.features\n\narray([[ 0., 69.,  0., ...,  0.,  0.,  0.],\n       [ 0., 34.,  0., ...,  0.,  0.,  0.],\n       [ 0., 24.,  0., ...,  0.,  0.,  0.],\n       ...,\n       [ 0., 57.,  0., ...,  0.,  0.,  0.],\n       [ 1., 33.,  0., ...,  0.,  0.,  0.],\n       [ 1., 23.,  0., ...,  0.,  0.,  0.]])\n\n\n\n# sens_ind = 0\n# sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n# unprivileged_groups = [{sens_attr: v} for v in\n#                     dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n# privileged_groups = [{sens_attr: v} for v in\n#                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\nsens_ind = 1\nsens_attr_comp = dataset_orig_compas_train.protected_attribute_names[sens_ind]\nunprivileged_groups_compas = [{sens_attr_comp: v} for v in\n                    dataset_orig_compas_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups_compas = [{sens_attr_comp: v} for v in\n                    dataset_orig_compas_train.privileged_protected_attributes[sens_ind]]\n\n\nsens_attr_comp\n\n'race'\n\n\n\nprivileged_groups_compas\n\n[{'race': 1.0}]\n\n\n\nunprivileged_groups_compas\n\n[{'race': 0.0}]\n\n\n\nmetric_orig_panel19_train = BinaryLabelDatasetMetric(\n        dataset_orig_compas_train,\n        unprivileged_groups=unprivileged_groups_compas,\n        privileged_groups=privileged_groups_compas)\n\n\nexplainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n\n\ntest_name=['Mean Difference','Consistency','Statistical Parity Difference','Disparate Impact']\ntest_definitions=['difference between mean values of two labels','Individual fairness metric that measures how similar the labels are for similar instances.','Difference in selection rates.','ratio of positive outcomes in the unprivileged group divided by the ratio of positive outcomes in the privileged group.']\ntest_results=[explainer_orig_panel19_train.mean_difference(),explainer_orig_panel19_train.consistency(),explainer_orig_panel19_train.statistical_parity_difference(),explainer_orig_panel19_train.disparate_impact()]\ntest_status=['Bias Detected','Bias Not Detected','Bias Detected','Bias Detected']\ndf=pd.DataFrame({'Test Name':test_name,'Test Definitions':test_definitions,'Test Results':test_results,'Test Status':test_status})\n\n\ntest_results\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.09713793951315464',\n 'Consistency (Zemel, et al. 2013): [0.67630939]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.09713793951315464',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.8403836674666473']\n\n\n\nRW = Reweighing(unprivileged_groups=unprivileged_groups_compas,privileged_groups=privileged_groups_compas)\ndataset_transf_compas_train_rw = RW.fit_transform(dataset_orig_compas_train)\n\n\nlfr_model = LFR(unprivileged_groups=unprivileged_groups_compas, \n                privileged_groups=privileged_groups_compas)\n\n# Fit the model and transform the dataset\nlfr_model.fit(dataset_orig_compas_train)\ndataset_transf_compas_train_lfr = lfr_model.transform(dataset_orig_compas_train, threshold = 0.1)\ndataset_transf_compas_train_lfr = dataset_orig_compas_train.align_datasets(dataset_transf_compas_train_lfr)\n\n\n# import numpy as np\n\n# def get_distortion_meps(vold, vnew):\n#     # Initialize distortion score\n#     distortion_score = 0.0\n\n#     # Define weights for different categories of attributes\n#     sensitive_weight = 3.0\n#     health_status_weight = 2.0\n#     socio_economic_weight = 1.0\n#     behavior_weight = 1.5\n\n#     # Sensitive attributes\n#     for attr in ['sex', 'age', 'race']:\n#         if vold[attr] != vnew[attr]:\n#             distortion_score += sensitive_weight\n\n#     # Health status indicators\n#     health_attrs = ['PCS42', 'MCS42', 'K6SUM42', 'HIBPDX', 'DIABDX', 'CHDDX', 'ANGIDX', 'MIDX', 'OHRTDX', 'STRKDX', 'EMPHDX', 'CANCERDX', 'JTPAIN', 'ARTHDX', 'ASTHDX', 'ADHDADDX']\n#     for attr in health_attrs:\n#         # Assuming health status attributes are numerical and a difference in value indicates a change in health status\n#         distortion_score += health_status_weight * abs(vold.get(attr, 0) - vnew.get(attr, 0))\n\n#     # Socioeconomic and environmental factors\n#     socio_attrs = ['REGION=1', 'REGION=2', 'REGION=3', 'REGION=4', 'MARRY', 'FTSTU', 'EMPST', 'POVCAT', 'INSCOV']\n#     for attr in socio_attrs:\n#         if vold[attr] != vnew[attr]:\n#             distortion_score += socio_economic_weight\n\n#     # Health-related behaviors\n#     behavior_attrs = ['ADSMOK42', 'WLKLIM', 'ACTLIM', 'SOCLIM', 'COGLIM']\n#     for attr in behavior_attrs:\n#         if vold[attr] != vnew[attr]:\n#             distortion_score += behavior_weight\n\n#     return distortion_score\n\n\ndef get_distortion_compas(vold, vnew):\n    # Initialize distortion score\n    distortion_score = 0.0\n\n    # Define weights for different categories of attributes\n    sensitive_weight = 3.0\n    criminal_history_weight = 2.0\n    age_category_weight = 1.5\n    charge_degree_weight = 1.0\n    specific_charge_weight = 0.5  # Lower weight as there are many specific charges\n\n    # Sensitive attributes\n    for attr in ['sex', 'age', 'race']:\n        if vold[attr] != vnew[attr]:\n            distortion_score += sensitive_weight\n\n    # Criminal history indicators\n    criminal_attrs = ['juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count']\n    for attr in criminal_attrs:\n        distortion_score += criminal_history_weight * abs(vold.get(attr, 0) - vnew.get(attr, 0))\n\n    # Age category\n    age_cats = ['age_cat=25 - 45', 'age_cat=Greater than 45', 'age_cat=Less than 25']\n    for attr in age_cats:\n        if vold.get(attr, 0) != vnew.get(attr, 0):\n            distortion_score += age_category_weight\n\n    # Charge degree\n    charge_degrees = ['c_charge_degree=F', 'c_charge_degree=M']\n    for attr in charge_degrees:\n        if vold.get(attr, 0) != vnew.get(attr, 0):\n            distortion_score += charge_degree_weight\n\n    # Specific charges, just check if there's any change\n    charge_changes = sum(vold.get(attr, 0) != vnew.get(attr, 0) for attr in vold if attr.startswith('c_charge_desc='))\n    distortion_score += specific_charge_weight * charge_changes\n\n    return distortion_score\n\n\n# DI = DisparateImpactRemover()\n# dataset_transf_compas_train_di = DI.fit_transform(dataset_orig_panel19_train)\n\n\nDI = DisparateImpactRemover()\ndataset_transf_compas_train_di = DI.fit_transform(dataset_orig_compas_train)\n\n\n# metric_transf_panel19_train = BinaryLabelDatasetMetric(\n# dataset_transf_panel19_train_rw,\n# unprivileged_groups=unprivileged_groups,\n# privileged_groups=privileged_groups)\n# explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n# test_results_rw=[explainer_transf_panel19_train.mean_difference()\n#                ,explainer_transf_panel19_train.consistency()\n#                ,explainer_transf_panel19_train.statistical_parity_difference()\n#                ,explainer_transf_panel19_train.disparate_impact()]\n\n\nmetric_transf_compas_train = BinaryLabelDatasetMetric(\ndataset_transf_compas_train_rw,\nunprivileged_groups=unprivileged_groups_compas,\nprivileged_groups=privileged_groups_compas)\nexplainer_transf_compas_train = MetricTextExplainer(metric_transf_compas_train)\ntest_results_rw=[explainer_transf_compas_train.mean_difference()\n               ,explainer_transf_compas_train.consistency()\n               ,explainer_transf_compas_train.statistical_parity_difference()\n               ,explainer_transf_compas_train.disparate_impact()]\n\n\n# metric_transf_panel19_train = BinaryLabelDatasetMetric(\n# dataset_transf_panel19_train_di,\n# unprivileged_groups=unprivileged_groups,\n# privileged_groups=privileged_groups)\n# explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n# test_results_di=[explainer_transf_panel19_train.mean_difference()\n#                ,explainer_transf_panel19_train.consistency()\n#                ,explainer_transf_panel19_train.statistical_parity_difference()\n#                ,explainer_transf_panel19_train.disparate_impact()]\n\n\nmetric_transf_compas_train = BinaryLabelDatasetMetric(\ndataset_transf_compas_train_di,\nunprivileged_groups=unprivileged_groups_compas,\nprivileged_groups=privileged_groups_compas)\nexplainer_transf_compas_train = MetricTextExplainer(metric_transf_compas_train)\ntest_results_di=[explainer_transf_compas_train.mean_difference()\n               ,explainer_transf_compas_train.consistency()\n               ,explainer_transf_compas_train.statistical_parity_difference()\n               ,explainer_transf_compas_train.disparate_impact()]\n\n\n# metric_transf_panel19_train = BinaryLabelDatasetMetric(\n# dataset_transf_panel19_train_lfr,\n# unprivileged_groups=unprivileged_groups,\n# privileged_groups=privileged_groups)\n# explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n# test_results_lfr=[explainer_transf_panel19_train.mean_difference()\n#                ,explainer_transf_panel19_train.consistency()\n#                ,explainer_transf_panel19_train.statistical_parity_difference()\n#                ,explainer_transf_panel19_train.disparate_impact()]\n\n\nmetric_transf_compas_train = BinaryLabelDatasetMetric(\ndataset_transf_compas_train_lfr,\nunprivileged_groups=unprivileged_groups_compas,\nprivileged_groups=privileged_groups_compas)\nexplainer_transf_compas_train = MetricTextExplainer(metric_transf_compas_train)\ntest_results_lfr=[explainer_transf_compas_train.mean_difference()\n               ,explainer_transf_compas_train.consistency()\n               ,explainer_transf_compas_train.statistical_parity_difference()\n               ,explainer_transf_compas_train.disparate_impact()]\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide\n  return metric_fun(privileged=False) / metric_fun(privileged=True)\n\n\n\ntest_results\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.09713793951315464',\n 'Consistency (Zemel, et al. 2013): [0.67630939]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.09713793951315464',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.8403836674666473']\n\n\n\ntest_results_rw\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 1.1102230246251565e-16',\n 'Consistency (Zemel, et al. 2013): [0.67630939]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 1.1102230246251565e-16',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.0000000000000002']\n\n\n\ntest_results_lfr\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.0',\n 'Consistency (Zemel, et al. 2013): [1.]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.0',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): nan']\n\n\n\ntest_results_di\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.09713793951315464',\n 'Consistency (Zemel, et al. 2013): [0.66774769]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.09713793951315464',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.8403836674666473']\n\n\n\n# features = dataset_transf_panel19_train_rw.features\n# label = dataset_transf_panel19_train_rw.labels.ravel()  # Flatten the label array if necessary\n# weights = dataset_transf_panel19_train_rw.instance_weights\n# feature_names = dataset_transf_panel19_train_rw.feature_names\n# df_rw = pd.DataFrame(features, columns=feature_names)\n# df_rw['label'] = label\n# df_rw['weights'] = weights\n\n\nfeatures = dataset_transf_compas_train_rw.features\nlabel = dataset_transf_compas_train_rw.labels.ravel()  # Flatten the label array if necessary\nweights = dataset_transf_compas_train_rw.instance_weights\nfeature_names = dataset_transf_compas_train_rw.feature_names\ndf_rw = pd.DataFrame(features, columns=feature_names)\ndf_rw['label'] = label\ndf_rw['weights'] = weights\n\n\ndf_rw\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nage_cat=25 - 45\nage_cat=Greater than 45\nage_cat=Less than 25\n...\nc_charge_desc=Viol Injunction Protect Dom Vi\nc_charge_desc=Viol Pretrial Release Dom Viol\nc_charge_desc=Viol Prot Injunc Repeat Viol\nc_charge_desc=Violation License Restrictions\nc_charge_desc=Violation Of Boater Safety Id\nc_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking\nc_charge_desc=Voyeurism\nc_charge_desc=arrest case no charge\nlabel\nweights\n\n\n\n\n0\n0.0\n69.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.064676\n\n\n1\n0.0\n34.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.932297\n\n\n2\n0.0\n24.0\n0.0\n0.0\n0.0\n1.0\n4.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.932297\n\n\n3\n0.0\n44.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.064676\n\n\n4\n0.0\n41.0\n1.0\n0.0\n0.0\n0.0\n14.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.163658\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6162\n0.0\n23.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.064676\n\n\n6163\n0.0\n23.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.064676\n\n\n6164\n0.0\n57.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.064676\n\n\n6165\n1.0\n33.0\n0.0\n0.0\n0.0\n0.0\n3.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.064676\n\n\n6166\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.932297\n\n\n\n\n6167 rows × 403 columns\n\n\n\n\nfeatures = dataset_orig_panel19_train.features\nlabel = dataset_orig_panel19_train.labels\nfeature_names = dataset_orig_panel19_train.feature_names\ndataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\ndataset_orig_panel19_train_df['label'] = label\ndataset_orig_panel19_train_df\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15826\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15828\n54.0\n0.0\n43.97\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15829\n73.0\n0.0\n42.68\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n15830 rows × 139 columns\n\n\n\n\n# # Assuming `dataset_transf_panel19_train_lfr` is your LFR transformed dataset\n# features_lfr = dataset_transf_panel19_train_lfr.features\n# label_lfr = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary\n# feature_names_lfr = dataset_transf_panel19_train_lfr.feature_names\n\n# # Create a DataFrame\n# df_lfr = pd.DataFrame(features_lfr, columns=feature_names_lfr)\n# df_lfr['label'] = label_lfr\n\n\nfeatures = dataset_transf_compas_train_lfr.features\nlabel = dataset_transf_compas_train_lfr.labels.ravel()  # Flatten the label array if necessary\nweights = dataset_transf_compas_train_lfr.instance_weights\nfeature_names = dataset_transf_compas_train_lfr.feature_names\ndf_lfr = pd.DataFrame(features, columns=feature_names)\ndf_lfr['label'] = label\ndf_lfr['weights'] = weights\n\n\ndf_lfr\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nage_cat=25 - 45\nage_cat=Greater than 45\nage_cat=Less than 25\n...\nc_charge_desc=Viol Injunction Protect Dom Vi\nc_charge_desc=Viol Pretrial Release Dom Viol\nc_charge_desc=Viol Prot Injunc Repeat Viol\nc_charge_desc=Violation License Restrictions\nc_charge_desc=Violation Of Boater Safety Id\nc_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking\nc_charge_desc=Voyeurism\nc_charge_desc=arrest case no charge\nlabel\nweights\n\n\n\n\n0\n0.521109\n0.625498\n0.716708\n0.432452\n0.533850\n0.582301\n0.445268\n0.535120\n0.653046\n0.358865\n...\n0.545693\n0.468199\n0.480423\n0.303290\n0.520715\n0.533933\n0.408816\n0.398183\n1.0\n1.0\n\n\n1\n0.523721\n0.618111\n0.719904\n0.434879\n0.531925\n0.582281\n0.440209\n0.538981\n0.647618\n0.353103\n...\n0.541876\n0.470039\n0.481520\n0.298589\n0.526050\n0.535778\n0.411733\n0.393008\n1.0\n1.0\n\n\n2\n0.521773\n0.618395\n0.721806\n0.438287\n0.533270\n0.582259\n0.444720\n0.532756\n0.644685\n0.352959\n...\n0.546447\n0.470344\n0.484824\n0.298436\n0.524345\n0.530726\n0.411325\n0.392385\n1.0\n1.0\n\n\n3\n0.522205\n0.624398\n0.716270\n0.432044\n0.534721\n0.583117\n0.442769\n0.535952\n0.651580\n0.355646\n...\n0.545672\n0.469955\n0.480438\n0.299492\n0.522338\n0.536884\n0.408060\n0.392491\n1.0\n1.0\n\n\n4\n0.516227\n0.620342\n0.726517\n0.444027\n0.531929\n0.579084\n0.457035\n0.522022\n0.644180\n0.361615\n...\n0.551542\n0.464862\n0.489937\n0.307846\n0.517226\n0.515481\n0.413552\n0.405947\n1.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6162\n0.525380\n0.615405\n0.719757\n0.431285\n0.530067\n0.580368\n0.434852\n0.542806\n0.648857\n0.352716\n...\n0.536541\n0.467504\n0.480691\n0.295075\n0.525745\n0.540234\n0.412472\n0.387391\n1.0\n1.0\n\n\n6163\n0.524266\n0.616218\n0.720508\n0.433675\n0.531036\n0.581009\n0.437997\n0.539684\n0.647388\n0.352701\n...\n0.539669\n0.468499\n0.482022\n0.296207\n0.525471\n0.537118\n0.412211\n0.389155\n1.0\n1.0\n\n\n6164\n0.521599\n0.625426\n0.716216\n0.432378\n0.534636\n0.583074\n0.444325\n0.535372\n0.652339\n0.357115\n...\n0.546255\n0.469557\n0.480299\n0.301559\n0.521685\n0.535433\n0.408159\n0.395590\n1.0\n1.0\n\n\n6165\n0.521538\n0.623086\n0.718106\n0.434657\n0.535028\n0.583082\n0.444573\n0.533102\n0.648802\n0.354578\n...\n0.547564\n0.470453\n0.482579\n0.298797\n0.522425\n0.534039\n0.408527\n0.391653\n1.0\n1.0\n\n\n6166\n0.522265\n0.616323\n0.722428\n0.435223\n0.530786\n0.579325\n0.441727\n0.534815\n0.646657\n0.355179\n...\n0.541484\n0.465963\n0.484714\n0.297372\n0.522076\n0.532218\n0.412622\n0.390216\n1.0\n1.0\n\n\n\n\n6167 rows × 403 columns\n\n\n\n\n# Assuming `dataset_transf_panel19_train_di` is your DI transformed dataset\nfeatures_di = dataset_transf_panel19_train_di.features\nlabel_di = dataset_transf_panel19_train_di.labels.ravel()  # Flatten the label array if necessary\nfeature_names_di = dataset_transf_panel19_train_di.feature_names\n\n# Create a DataFrame\ndf_di = pd.DataFrame(features_di, columns=feature_names_di)\ndf_di['label'] = label_di\ndf_di\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n56.0\n1.0\n20.42\n26.53\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n23.0\n1.0\n52.92\n50.28\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.68\n62.11\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15826\n25.0\n0.0\n56.68\n62.11\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15828\n54.0\n0.0\n43.43\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15829\n73.0\n0.0\n41.83\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n15830 rows × 139 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\ndf=dataset_orig_panel19_train_df\n# Split the DataFrame into features, labels, and weights\n\nX = df.drop(['label'], axis=1)\ny = df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model.predict_proba(X_train)\ny_test_pred_proba = model.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8678932406822489, Training Log Loss: 0.3226097093813417\nTesting Accuracy: 0.8610233733417562, Testing Log Loss: 0.33582932969930285\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_28792\\1117411220.py:63: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n  plt.show()\n\n\n\ndf.head(1).to_csv('meps.csv')\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n\n# Split the DataFrame into features, labels, and weights\nX = df_rw.drop(['label', 'weights'], axis=1)\ny = df_rw['label']\nweights = df_rw['weights']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel_rw = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel_rw.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_rw.predict(X_train)\ny_test_pred = model_rw.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_rw.predict_proba(X_train)\ny_test_pred_proba = model_rw.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8663929248262792, Training Log Loss: 0.3352042516295803\nTesting Accuracy: 0.8578648136449779, Testing Log Loss: 0.3442758714994835\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_28792\\2225796879.py:63: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n  plt.show()\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n\n# Split the DataFrame into features, labels, and weights\nX = df_di.drop(['label'], axis=1)\ny = df_di['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel_di = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel_di.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_di.predict(X_train)\ny_test_pred = model_di.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_di.predict_proba(X_train)\ny_test_pred_proba = model_di.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8656822488945041, Training Log Loss: 0.3377688207025086\nTesting Accuracy: 0.8572331017056223, Testing Log Loss: 0.3468531284456185\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_28792\\789110131.py:62: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n  plt.show()\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df_lfr' is your Pandas DataFrame with features and 'label' columns, \n# resulting from the LFR transformation\n\n# Split the DataFrame into features and labels\nX = df_lfr.drop(['label'], axis=1)\ny = df_lfr['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Initialize the model\nmodel_lfr = LogisticRegression()\n\n# Train the model (no sample weights are needed as the data is already transformed by LFR)\nmodel_lfr.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_lfr.predict(X_train)\ny_test_pred = model_lfr.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_lfr.predict_proba(X_train)\ny_test_pred_proba = model_lfr.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.5713044851547694, Training Log Loss: 0.6770592058568611\nTesting Accuracy: 0.5713834491471889, Testing Log Loss: 0.6836813366149022\n\n\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_28792\\3296956902.py:59: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n  plt.show()"
  },
  {
    "objectID": "postprocessing.html",
    "href": "postprocessing.html",
    "title": "",
    "section": "",
    "text": "%matplotlib inline\n# data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\nfrom time import time\n\n# Graphs libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nplt.style.use('seaborn-white')\nimport seaborn as sns\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom plotly import tools\n\n# Libraries to study\nfrom aif360.datasets import StandardDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\nfrom aif360.algorithms.preprocessing import LFR, Reweighing\nfrom aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\nfrom aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n\n# ML libraries\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\n\n# Design libraries\nfrom IPython.display import Markdown, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom aif360.datasets import MEPSDataset19\n\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_36416\\1330569207.py:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-white')\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n\n\n\n\ndataset_orig_panel19_train,dataset_orig_panel19_test = MEPSDataset19().split([0.7], shuffle=True)\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\ndata_orig_test_pred = dataset_orig_panel19_test.copy(deepcopy=True)\n# Prediction with the original RandomForest model\nscores = np.zeros_like(dataset_orig_panel19_test.labels)\n\n\nfeatures = dataset_orig_panel19_train.features\nlabel = dataset_orig_panel19_train.labels\nfeature_names = dataset_orig_panel19_train.feature_names\ndataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\ndataset_orig_panel19_train_df['label'] = label\ndataset_orig_panel19_train_df\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\n\n\n\n\n0\n9.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n1\n85.0\n0.0\n27.95\n48.73\n5.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n7.0\n0.0\n-1.00\n-1.00\n-1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n54.0\n0.0\n54.88\n57.41\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n65.0\n0.0\n40.02\n45.85\n5.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11076\n24.0\n1.0\n60.47\n50.65\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n11077\n11.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n11078\n23.0\n0.0\n57.84\n54.35\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n11079\n71.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n11080\n32.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n11081 rows × 139 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\ndf=dataset_orig_panel19_train_df\n# Split the DataFrame into features, labels, and weights\n\nX = df.drop(['label'], axis=1)\ny = df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model.predict_proba(X_train)\ny_test_pred_proba = model.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8621389891696751, Training Log Loss: 0.3308866299546088\nTesting Accuracy: 0.8633288227334236, Testing Log Loss: 0.3319915144634525\n\n\n\n\n\n\n\n\n\n\nscores = model.predict_proba(dataset_orig_panel19_test.features)[:,1].reshape(-1,1)\ndata_orig_test_pred.scores = scores\n\npreds = np.zeros_like(dataset_orig_panel19_test.labels)\n\n\npreds = model.predict(dataset_orig_panel19_test.features).reshape(-1,1)\ndata_orig_test_pred.labels = preds\n\ndef format_probs(probs1):\n    probs1 = np.array(probs1)\n    probs0 = np.array(1-probs1)\n    return np.concatenate((probs0, probs1), axis=1)\n\n\ncost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n\nCPP = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n                                     unprivileged_groups = unprivileged_groups,\n                                     cost_constraint=cost_constraint,\n                                     seed=42)\n\nCPP = CPP.fit(dataset_orig_panel19_test, data_orig_test_pred)\ndata_transf_test_pred = CPP.predict(data_orig_test_pred)\n\n\ndata_transf_test_pred\n\n               instance weights features                                    \\\n                                         protected attribute                 \n                                     AGE                RACE  PCS42  MCS42   \ninstance names                                                               \n13357               9766.309748     47.0                 1.0  44.67  58.50   \n1622                3687.136625     15.0                 0.0  -1.00  -1.00   \n10655               3162.965899     35.0                 0.0  48.79  46.12   \n13275                  0.000000      2.0                 0.0  -1.00  -1.00   \n16189              10437.058559     39.0                 0.0  53.71  53.48   \n...                         ...      ...                 ...    ...    ...   \n3594                3137.852722      8.0                 0.0  -1.00  -1.00   \n6600               12813.157945      5.0                 0.0  -1.00  -1.00   \n5029                8074.453027     36.0                 0.0  50.32  54.80   \n7917                1915.014430     43.0                 0.0  51.29  55.90   \n7105                3116.633473     40.0                 0.0  60.74  53.51   \n\n                                                            ...          \\\n                                                            ...           \n               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \ninstance names                                              ...           \n13357              0.0      1.0      0.0      0.0      0.0  ...     0.0   \n1622              -1.0      0.0      0.0      0.0      1.0  ...     0.0   \n10655              0.0      0.0      0.0      1.0      0.0  ...     1.0   \n13275             -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n16189              3.0      0.0      1.0      0.0      0.0  ...     0.0   \n...                ...      ...      ...      ...      ...  ...     ...   \n3594              -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n6600              -1.0      0.0      0.0      0.0      1.0  ...     0.0   \n5029               4.0      0.0      0.0      1.0      0.0  ...     1.0   \n7917               4.0      0.0      0.0      1.0      0.0  ...     0.0   \n7105               1.0      0.0      0.0      1.0      0.0  ...     1.0   \n\n                                                                               \\\n                                                                                \n               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \ninstance names                                                                  \n13357               0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n1622                0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n10655               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n13275               0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n16189               0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n...                 ...      ...      ...      ...      ...      ...      ...   \n3594                1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n6600                0.0      0.0      0.0      0.0      1.0      0.0      1.0   \n5029                0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n7917                0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n7105                1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n\n                        labels  \n                                \n               INSCOV=3         \ninstance names                  \n13357               0.0    0.0  \n1622                0.0    0.0  \n10655               0.0    0.0  \n13275               0.0    0.0  \n16189               0.0    0.0  \n...                 ...    ...  \n3594                0.0    0.0  \n6600                0.0    0.0  \n5029                0.0    0.0  \n7917                1.0    0.0  \n7105                0.0    0.0  \n\n[4749 rows x 140 columns]\n\n\n\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n\nmetric_transf_panel19_test = BinaryLabelDatasetMetric(\ndata_transf_test_pred,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_cpp = MetricTextExplainer(metric_transf_panel19_test)\ntest_results_cpp=[explainer_transf_panel19_cpp.mean_difference()\n               ,explainer_transf_panel19_cpp.consistency()\n               ,explainer_transf_panel19_cpp.statistical_parity_difference()\n               ,explainer_transf_panel19_cpp.disparate_impact()]\n\n\ntest_results_cpp\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.011554085192856722',\n 'Consistency (Zemel, et al. 2013): [0.9505159]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.011554085192856722',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.29280637210065']\n\n\n\nROC = RejectOptionClassification(privileged_groups = privileged_groups,\n                             unprivileged_groups = unprivileged_groups)\n\nROC = ROC.fit(dataset_orig_panel19_test, data_orig_test_pred)\ndata_transf_test_pred_roc = ROC.predict(data_orig_test_pred)\n\n\nmetric_transf_panel19_test = BinaryLabelDatasetMetric(\ndata_transf_test_pred_roc,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_roc = MetricTextExplainer(metric_transf_panel19_test)\ntest_results_roc=[explainer_transf_panel19_roc.mean_difference()\n               ,explainer_transf_panel19_roc.consistency()\n               ,explainer_transf_panel19_roc.statistical_parity_difference()\n               ,explainer_transf_panel19_roc.disparate_impact()]\n\n\ntest_results_roc\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.040726123105935985',\n 'Consistency (Zemel, et al. 2013): [0.85171615]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.040726123105935985',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.8861379636018648']"
  },
  {
    "objectID": "metrics (2) (1) (3).html",
    "href": "metrics (2) (1) (3).html",
    "title": "",
    "section": "",
    "text": "::: {#cell-0 .cell _uuid=‘832049268e597621b8995d308611ce2dac7e471c’ execution_count=81}\n%matplotlib inline\n# data manipulation libraries\nimport pandas as pd\nimport numpy as np\nfrom time import time\n# Graphs libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nplt.style.use('seaborn-v0_8-whitegrid')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n# Libraries to study\n\nfrom aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing\nfrom aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\n\n# ML libraries\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n# Design libraries\nfrom IPython.display import Markdown, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom datetime import datetime, timedelta\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\n\n\n:::\n\ndataset_orig_panel19=MEPSDataset19()\n\n\nnum_datapoints = dataset_orig_panel19.features.shape\nnum_datapoints\n\n(15830, 138)\n\n\n\nnum_datapoints = dataset_orig_panel19.features.shape[0]\nprint(f\"Number of datapoints in the dataset: {num_datapoints}\")\n\nNumber of datapoints in the dataset: 15830\n\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19.privileged_protected_attributes[sens_ind]]\n\n\nSplit into train and test set\n\nnp.random.seed(42)\ndata_orig_sex_train, data_orig_sex_test = dataset_orig_panel19.split([0.7], shuffle=True)\nprint(\"Perpetrator Sex :\",data_orig_sex_train.features.shape)\nprint(\"Perpetrator Sex :\",data_orig_sex_test.features.shape)\n\nPerpetrator Sex : (11081, 138)\nPerpetrator Sex : (4749, 138)\n\n\n\n\nTraining the model : Logistic Regression\n\nrf_orig_sex = LogisticRegression().fit(data_orig_sex_train.features, \n                     data_orig_sex_train.labels.ravel(), \n                     sample_weight=data_orig_sex_train.instance_weights)\n\n\n\nPredict on test set\n\nX_test_sex = data_orig_sex_test.features\ny_test_sex = data_orig_sex_test.labels.ravel()\n\n\n\nMETRICS\n\ndef get_attributes(data, selected_attr=None):\n    return privileged_groups, unprivileged_groups\n\n\ndef get_model_performance(X_test, y_true, y_pred, probs):\n    accuracy = accuracy_score(y_true, y_pred)\n    matrix = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    return accuracy, matrix, f1, fpr, tpr, roc_auc\n\ndef plot_confusion_matrix(matrix):\n    plt.figure(figsize=(8, 6))    \n    sns.heatmap(matrix, annot=True, cmap='OrRd', fmt='g')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\ndef plot_roc_curve(fpr, tpr, roc_auc):\n    fig = px.area(\n        x=fpr, y=tpr,\n        title=f'ROC curve ( Area = {roc_auc:.2f})',\n        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n        width=600, height=600\n    )\n    fig.add_shape(\n        type='line', line=dict(dash='dash', color='navy', width=2),\n        x0=0, x1=1, y0=0, y1=1\n    )\n    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n    fig.update_xaxes(constrain='domain')\n    fig.add_vrect(x0=0, x1=0.5, fillcolor=\"red\", opacity=0.1, line_width=0)  \n    fig.add_vrect(x0=0.5, x1=1, fillcolor=\"green\", opacity=0.1, line_width=0) \n    fig.show()\n\n\n\nACCURACY and F1 SCORE\n\ndef model_performance(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    display(Markdown('#### Accuracy of the model :'))\n    print(accuracy)\n    display(Markdown('#### F1 score of the model :'))\n    print(f1)\n\nmodel_performance(rf_orig_sex, data_orig_sex_test.features, y_test_sex)\n\nAccuracy of the model :\n\n\n0.866919351442409\n\n\nF1 score of the model :\n\n\n0.5182926829268293\n\n\n\n\nROC CURVE\n\ndef ROC_Plot(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    loss = log_loss(y_true, probs)\n    accuracy = accuracy_score(y_true, y_pred)\n    accuracy1, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    plot_roc_curve(fpr, tpr, roc_auc)\nROC_Plot(rf_orig_sex, data_orig_sex_test.features, y_test_sex)\n\n                                                \n\n\n\n\nCONFUSION MATRIX\n\ndef Confusion_Matrix(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    plot_confusion_matrix(matrix)\nConfusion_Matrix(rf_orig_sex, data_orig_sex_test.features, y_test_sex)\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom IPython.display import display, Markdown\n\ndef plot_fair_metrics(fair_metrics):\n    sns.set(style=\"whitegrid\", palette=\"muted\")\n    cols = fair_metrics.columns.values\n    metrics_df = fair_metrics.iloc[1:]  # Exclude the 'objective' row for plotting\n    objective_values = fair_metrics.loc['objective']\n    fig, axs = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n    colors = sns.color_palette(\"husl\", n_colors=len(cols))\n    for i, col in enumerate(cols):\n        bar_plot = sns.barplot(x=metrics_df.index, y=metrics_df[col], ax=axs[i], palette=[colors[i]])\n        xlim = axs[i].get_xlim()\n        axs[i].plot(xlim, [objective_values[col]] * 2, 'r--', label='Objective')\n        axs[i].set_xlim(xlim)  # Ensure full width of the plot\n        bound = [-0.1, 0.1] if i &lt; 3 else [0.8, 1.2] if i == 3 else [0, 0.25]\n        axs[i].add_patch(patches.Rectangle((xlim[0], bound[0]), xlim[1] - xlim[0], bound[1] - bound[0], color='GREEN', alpha=0.2))\n        # ----- Modify here to only show the calculated value (first value of the bar)\n        if len(bar_plot.patches) &gt; 0:  # Check if there are any bars to annotate\n            bar = bar_plot.patches[0]  # Get the first bar object\n            height = bar.get_height()  # Get the height of the bar\n            axs[i].annotate(f\"{height:.2f}\", (bar.get_x() + bar.get_width() / 2, height),\n                             ha='center', va='center', fontsize=10, color='black', xytext=(0, 10),\n                             textcoords='offset points')  # Annotate the height value above the bar\n        axs[i].set_title(col.replace('_', ' ').title(), fontsize=16)\n        axs[i].set_xlabel('')\n        axs[i].set_ylabel('Metric Value' if i == 0 else '')\n    plt.suptitle('Fairness Metrics Overview', fontsize=24, y=1.05)\n    plt.tight_layout()\n    plt.legend()\n    plt.show()\n\n\n\ndef fair_metrics():\n    dataset = data_orig_sex_test\n    pred = rf_orig_sex.predict(dataset.features)  \n    dataset_pred = dataset.copy()\n    dataset_pred.labels = pred\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }    \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n    return fair_metrics\nfair = fair_metrics()\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair)\ndisplay(fair)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical_parity_difference\nequal_opportunity_difference\naverage_abs_odds_difference\ndisparate_impact\ntheil_index\n\n\n\n\nobjective\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\nRACE\n-0.115925\n-0.181738\n0.110251\n0.352207\n0.115688\n\n\n\n\n\n\n\n\n\nDEFINING FUNCTIONS WHICH ARE USED LATER\n\nalgo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\ndef add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n    # Using loc to add a new row to the DataFrame directly\n    algo_metrics.loc[name] = [model, fair_metrics, preds, probs]\n    return algo_metrics\n\n\ndata_orig_test = data_orig_sex_test\ndata_orig_train = data_orig_sex_train\nrf = rf_orig_sex\n\nprobs = rf.predict_proba(data_orig_test.features)\npreds = rf.predict(data_orig_test.features)\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, rf, fair, preds, probs, 'Origin')\n\n\n\nREWEIGHING\n\nprivileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\nRW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n# RW.fit(data_orig_train)\ndata_transf_train = RW.fit_transform(data_orig_train)\n\n# Train and save the model\nrf_transf = LogisticRegression().fit(data_transf_train.features, \n                     data_transf_train.labels.ravel(), \n                     sample_weight=data_transf_train.instance_weights)\n\ndata_transf_test = RW.transform(data_orig_test)\n\n\n\n\ndef fair_metrics_rw():\n    dataset = data_transf_test\n    pred = rf_transf.predict(dataset.features)\n    \n    dataset_pred = dataset.copy()\n    dataset_pred.labels = pred\n\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    \n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }\n        \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n\n    # print('fair_metrics')\n    # print(fair_metrics)\n    # print(type(fair_metrics))\n\n    return fair_metrics\n\nfair = fair_metrics_rw()\n\n\n\n\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair)\ndisplay(fair)\n\n\ntime elapsed : 0.76s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical_parity_difference\nequal_opportunity_difference\naverage_abs_odds_difference\ndisparate_impact\ntheil_index\n\n\n\n\nobjective\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\nRACE\n0.005033\n0.021989\n0.013078\n1.039227\n0.113078\n\n\n\n\n\n\n\n\ny_test_sex_rw = data_transf_test.labels.ravel()\n\nmodel_performance(rf_transf, data_transf_test.features, y_test_sex_rw)\nROC_Plot(rf_transf, data_transf_test.features, y_test_sex_rw)\nConfusion_Matrix(rf_transf, data_transf_test.features, y_test_sex)\n\nAccuracy of the model :\n\n\n0.863129079806275\n\n\nF1 score of the model :\n\n\n0.5283018867924528\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\nDISPARATE IMPACT REMOVER\n\nprivileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\nDIR = DisparateImpactRemover(repair_level=1.0)\ndata_transf_train = DIR.fit_transform(data_orig_train)\n\n# Train and save the model\nrf_transf_DI = LogisticRegression().fit(data_transf_train.features, \n                     data_transf_train.labels.ravel(), \n                     sample_weight=data_transf_train.instance_weights)\n\ndata_transf_test_DI = DIR.fit_transform(data_orig_test)\n\n\ndef fair_metrics_DI():\n    dataset = data_transf_test_DI\n    pred = rf_transf_DI.predict(dataset.features)\n    \n    dataset_pred = dataset.copy()\n    dataset_pred.labels = pred\n\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    \n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }\n        \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n\n    return fair_metrics\n\nfair_DI = fair_metrics_DI()\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf_DI, fair_DI, preds, probs, 'DISPARATE IMPACT REMOVER')\nprint('time elapsed : %.2fs'%(time()-t0))\n\n\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair_DI)\ndisplay(fair_DI)\n\ntime elapsed : 2.23s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical_parity_difference\nequal_opportunity_difference\naverage_abs_odds_difference\ndisparate_impact\ntheil_index\n\n\n\n\nobjective\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\nRACE\n-0.119065\n-0.189487\n0.114261\n0.352467\n0.114273\n\n\n\n\n\n\n\n\ny_test_sex_DI = data_transf_test_DI.labels.ravel()\n\nmodel_performance(rf_transf_DI, data_transf_test_DI.features, y_test_sex_DI)\nROC_Plot(rf_transf_DI, data_transf_test_DI.features, y_test_sex_DI)\nConfusion_Matrix(rf_transf_DI, data_transf_test_DI.features, y_test_sex_DI)\n\nAccuracy of the model :\n\n\n0.8681827753211202\n\n\nF1 score of the model :\n\n\n0.5250379362670713\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\nADVERSARIAL DEBIASING\n\ndata_orig_test.features\n\narray([[46.  ,  1.  , 54.39, ...,  1.  ,  0.  ,  0.  ],\n       [31.  ,  1.  , 56.15, ...,  1.  ,  0.  ,  0.  ],\n       [59.  ,  1.  , 54.8 , ...,  1.  ,  0.  ,  0.  ],\n       ...,\n       [53.  ,  1.  , 31.76, ...,  0.  ,  1.  ,  0.  ],\n       [33.  ,  0.  , 57.76, ...,  1.  ,  0.  ,  0.  ],\n       [21.  ,  0.  , 41.44, ...,  1.  ,  0.  ,  0.  ]])\n\n\n\nprivileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['RACE'])\nt0 = time()\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n# Now you can use Session as you did in TensorFlow 1.x\nsess2 = tf.Session()\n\ndebiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n                          unprivileged_groups = unprivileged_groups,\n                          scope_name='debiased_classifier',\n                          num_epochs=10,\n                          debias=True,\n                          sess=sess2)\n\ndebiased_model.fit(data_orig_train)\n\n\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n\n\n2024-05-04 21:07:17.221926: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n\n\nepoch 0; iter: 0; batch classifier loss: 1.608079; batch adversarial loss: 0.854341\nepoch 1; iter: 0; batch classifier loss: 0.603297; batch adversarial loss: 0.701015\nepoch 2; iter: 0; batch classifier loss: 0.441963; batch adversarial loss: 0.690415\nepoch 3; iter: 0; batch classifier loss: 0.279543; batch adversarial loss: 0.662025\nepoch 4; iter: 0; batch classifier loss: 0.343660; batch adversarial loss: 0.679853\nepoch 5; iter: 0; batch classifier loss: 0.305189; batch adversarial loss: 0.665114\nepoch 6; iter: 0; batch classifier loss: 0.396766; batch adversarial loss: 0.659516\nepoch 7; iter: 0; batch classifier loss: 0.272041; batch adversarial loss: 0.669291\nepoch 8; iter: 0; batch classifier loss: 0.359243; batch adversarial loss: 0.658095\nepoch 9; iter: 0; batch classifier loss: 0.288321; batch adversarial loss: 0.644946\n\n\n&lt;aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x3576a3a90&gt;\n\n\n\n\ndef fair_metrics_AD():\n    dataset = data_orig_test\n    pred = debiased_model.predict(data_orig_test).labels \n    \n    dataset_pred = dataset.copy()\n    dataset_pred.labels = pred\n\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    \n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }\n        \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n    return fair_metrics,pred\n\nfair,pred = fair_metrics_AD()\n\n\n\ndata_pred = debiased_model.predict(data_orig_test)\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, debiased_model, fair, data_pred.labels, data_pred.scores, 'AdvDebiasing')\nprint('time elapsed : %.2fs'%(time()-t0))\n\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair)\ndisplay(fair)\n\n\ntime elapsed : 10.46s\n\n\n\n\n\n\n\n\n\nstatistical_parity_difference\nequal_opportunity_difference\naverage_abs_odds_difference\ndisparate_impact\ntheil_index\n\n\n\n\nobjective\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\nRACE\n-0.049186\n-0.034582\n0.017979\n0.564731\n0.126952\n\n\n\n\n\n\n\n\n\nDone Debugging :D\n\ndef Confusion_Matrix_ad(model, X_test, y_true):\n    y_pred = model.predict(data_orig_test).labels.flatten()\n    # print(y_pred)\n    pos_probs = model.predict(X_test).scores\n    negative_class_probs = 1 - pos_probs\n\n# Combine to form the new array with both classes\n    probs = np.hstack((negative_class_probs, pos_probs))\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    plot_confusion_matrix(matrix)\n\n    \n\n\ndef ROC_Plot_ad(model, X_test, y_true):\n    y_pred = model.predict(data_orig_test).labels.flatten()\n    # print(y_pred)\n    pos_probs = model.predict(X_test).scores\n    negative_class_probs = 1 - pos_probs\n\n# Combine to form the new array with both classes\n    probs = np.hstack((negative_class_probs, pos_probs))\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    plot_roc_curve(fpr, tpr, roc_auc)\n\n\ny_test_sex_ad = data_orig_test.labels.ravel()\n\n# model_performance(debiased_model, data_orig_train.features, y_test_sex_rw)\nROC_Plot_ad(debiased_model, data_orig_test, y_test_sex_ad)\n# Confusion_Matrix(debiased_model, data_orig_train.features, y_test_sex)\nConfusion_Matrix_ad(debiased_model, data_orig_test, y_test_sex_ad)\n\n\n\n\nPREJUDICE REMOVER\n\nt0 = time()\ndebiased_model_PR = PrejudiceRemover(sensitive_attr=\"RACE\", eta = 25.0)\ndebiased_model_PR.fit(data_orig_train)\n\n\n\ndef fair_metrics_PR():\n    dataset = data_orig_train\n    pred = debiased_model_PR.predict(data_orig_train).labels \n    \n    dataset_pred = dataset.copy()\n    dataset_pred.labels = pred\n\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    \n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }\n        \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n\n    print('fair_metrics')\n    print(fair_metrics)\n    print(type(fair_metrics))\n\n    return fair_metrics\n\nfair = fair_metrics_PR()\n\n\n\ndata_pred = debiased_model_PR.predict(data_orig_test)\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, debiased_model_PR, fair, data_pred.labels, data_pred.scores, 'Prejudice Remover')\nprint('time elapsed : %.2fs'%(time()-t0))\n\n\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair)\ndisplay(fair)\n\n\n\n\nDone Debugging\n\ndef Confusion_Matrix_pr(model, X_test, y_true):\n    y_pred = model.predict(data_orig_test).labels.flatten()\n    print(y_pred)\n    pos_probs = model.predict(X_test).scores\n    negative_class_probs = 1 - pos_probs\n# Combine to form the new array with both classes\n    probs = np.hstack((negative_class_probs, pos_probs))\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    plot_confusion_matrix(matrix)\n\n    \n\n\ndef ROC_Plot_pr(model, X_test, y_true):\n    y_pred = model.predict(data_orig_test).labels.flatten()\n    print(y_pred)\n    pos_probs = model.predict(X_test).scores\n    negative_class_probs = 1 - pos_probs\n# Combine to form the new array with both classes\n    probs = np.hstack((negative_class_probs, pos_probs))\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    plot_roc_curve(fpr, tpr, roc_auc)\n\n\n\ny_test_sex_pr = data_orig_test.labels.ravel()\n# model_performance(debiased_model_PR, data_orig_train.features, y_test_sex_rw)\nROC_Plot_pr(debiased_model_PR, data_orig_test, y_test_sex_pr)\nConfusion_Matrix_pr(debiased_model_PR, data_orig_test, y_test_sex_pr)\n\n\n\nLOSS FUNCTION WITH DISCRIMINATION\n\ndataset_orig_panel19_train = MEPSDataset19()\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data\nX = dataset_orig_panel19_train.features\ny = dataset_orig_panel19_train.labels.ravel()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = X_train\nX_test_scaled = X_test\n\n# Train logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the same scaled training data\ntrain_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n# Calculation of discrimination index without modifying dataset structure\nsens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n\n\n# Define unprivileged and privileged values\nunprivileged_val = 0.0\nprivileged_val = 1.0\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)  # Removed sigmoid to get raw logits\n        return x.squeeze()\n\n# Custom loss function that considers discrimination\ndef discrimination_loss(output, target, sensitive_features, lambda_val=100, k=2):\n    criterion = nn.BCEWithLogitsLoss()\n    standard_loss = criterion(output, target)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    prob_unpriv = torch.sigmoid(output[mask_unpriv]).mean()\n    prob_priv = torch.sigmoid(output[mask_priv]).mean()\n    discrimination = lambda_val * (prob_priv - prob_unpriv) ** k\n    loss_val = (1 + discrimination) * standard_loss\n\n    return loss_val, discrimination.item()\n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions.sigmoid() &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\ndef predict(model, input_tensor):\n    model.eval()\n    with torch.no_grad():\n        output = model(input_tensor)\n        predicted_class = (output.sigmoid() &gt;= 0.5).int()\n        return predicted_class.item()\n\ndef predict_proba(model, input_tensor):\n    model.eval()\n    with torch.no_grad():\n        output = model(input_tensor)\n        predicted_prob = output.sigmoid()\n        return (1 - predicted_prob).item(), predicted_prob.item()\n\n# Data preparation\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\nsensitive_features = torch.tensor((data[:, 1].numpy() &gt; 0.5).astype(float)).float()\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = torch.tensor((test_data[:, 1].numpy() &gt; 0.5).astype(float)).float()\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nfinal_epoch_predictions=None\nfinal_epoch_probabilities=None\n# Training and testing loop\nfinal_epoch=100\nmodel.train()\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model(features)\n    loss, discrimination = discrimination_loss(outputs, targets.squeeze(), sensitive_features)\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n\n    # Model evaluation\n    model.eval()\n    with torch.no_grad():\n        test_outputs = model(test_features)\n        test_loss, test_discrimination = discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n        if epoch == final_epoch - 1:\n            final_epoch_predictions=(test_outputs.sigmoid() &gt;= 0.5).int()\n            prob_positive = test_outputs.sigmoid()\n            prob_negative = 1 - prob_positive\n\n            # Stack the probabilities to create a 2D tensor with both class probabilities\n            final_epoch_probabilities = torch.stack((prob_negative, prob_positive), dim=1)\n\n\n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, Train Discrimination: {discrimination}, '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%, Test Discrimination: {test_discrimination}')\n    model.train()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom IPython.display import display, Markdown\n\ndef plot_fair_metrics(fair_metrics):\n    sns.set(style=\"whitegrid\", palette=\"muted\")\n    \n    cols = fair_metrics.columns.values\n    metrics_df = fair_metrics.iloc[1:]  # Exclude the 'objective' row for plotting\n    objective_values = fair_metrics.loc['objective']\n\n    fig, axs = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n    colors = sns.color_palette(\"husl\", n_colors=len(cols))\n\n    for i, col in enumerate(cols):\n        bar_plot = sns.barplot(x=metrics_df.index, y=metrics_df[col], ax=axs[i], palette=[colors[i]])\n\n        xlim = axs[i].get_xlim()\n        axs[i].plot(xlim, [objective_values[col]] * 2, 'r--', label='Objective')\n        axs[i].set_xlim(xlim)  # Ensure full width of the plot\n\n        bound = [-0.1, 0.1] if i &lt; 3 else [0.8, 1.2] if i == 3 else [0, 0.25]\n        axs[i].add_patch(patches.Rectangle((xlim[0], bound[0]), xlim[1] - xlim[0], bound[1] - bound[0], color='GREEN', alpha=0.2))\n\n        # ----- Modify here to only show the calculated value (first value of the bar)\n        if len(bar_plot.patches) &gt; 0:  # Check if there are any bars to annotate\n            bar = bar_plot.patches[0]  # Get the first bar object\n            height = bar.get_height()  # Get the height of the bar\n            axs[i].annotate(f\"{height:.2f}\", (bar.get_x() + bar.get_width() / 2, height),\n                             ha='center', va='center', fontsize=10, color='black', xytext=(0, 10),\n                             textcoords='offset points')  # Annotate the height value above the bar\n\n        axs[i].set_title(col.replace('_', ' ').title(), fontsize=16)\n        axs[i].set_xlabel('')\n        axs[i].set_ylabel('Metric Value' if i == 0 else '')\n\n    plt.suptitle('Fairness Metrics Overview', fontsize=24, y=1.05)\n    plt.tight_layout()\n    plt.legend()\n\n    plt.show()\n\n\n# dataset_orig_panel19=MEPSDataset19()\ndata_orig_sex_train, data_orig_sex_test = dataset_orig_panel19.split([0.7], shuffle=True)\n\ndef fair_metrics():\n    dataset=data_orig_sex_test\n    pred=final_epoch_predictions.numpy()\n    pred_proba=final_epoch_probabilities.numpy()\n    dataset_pred = data_orig_sex_test\n    dataset_pred.labels = pred\n\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    \n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }\n        \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n\n    # print('fair_metrics')\n    # print(fair_metrics)\n    # print(type(fair_metrics))\n\n    return fair_metrics\n\nfair = fair_metrics()\n\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair)\ndisplay(fair)\n\n\n# dataset=data_orig_sex_test\n# pred=final_epoch_predictions.numpy()\n# pred_proba=final_epoch_probabilities.numpy()\n# dataset_pred = data_orig_sex_test\n# dataset_pred.labels = pred\n\n\n\n# def ROC_Plot_disc(X_test, y_true,y_pred,pred_proba):\n#     #y_pred = model.predict(X_test)\n#     #probs = model.predict_proba(X_test)\n#     accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n#     plot_roc_curve(fpr, tpr, roc_auc)\n    \n# ROC_Plot_disc(data_orig_sex_test.features, data_orig_sex_test.labels,pred,predict_proba)\n-0.123118       0.      0.104548\n\n\n# # lambda = 100, k =2\n\n\n# Model = ['BASE MODEL', 'REWEIGHING','DISPARATE IMPACT REMOVER','ADVERSARIAL DEBIASING', 'PREJUDICE REMOVER','MODEL WITH DISC IN LOSS FUNCTION']\n# stat = ['-0.11843','0.001649','-0.121844','-0.053664','-0.024841','0.000414']\n# eq_op = ['-0.197133','0.010389','-0.207568','-0.043402','0.009017','0.00000']\n# abs_odds = ['0.116694','0.006653','0.123159','0.02403','0.006213','0.00000']\n# disp_imp = ['0.353492','1.012789','0.33208','0.547452','0.59515','1.005809']\n# theil = ['0.114186','0.113549','0.115176','0.126379','0.152623','0.00000']\n\n# FinalMetrics_df = pd.DataFrame({\n#     'MODEL NAME': Model,\n#     'STATISTICAL_PARITY_DIFFERENCE': stat,\n#     'EQUAL_OPPORTUNITY_DIFFERENCE': eq_op,\n#     'AVERAGE_ABS_ODDS_DIFFERENCE': abs_odds,\n#     'DISPARATE_IMPACT': disp_imp,\n#     'THEIL_INDEX' : theil,\n# })\n\n# FinalMetrics_df\n\n\n\n\n\n\n\n\nMODEL NAME\nSTATISTICAL_PARITY_DIFFERENCE\nEQUAL_OPPORTUNITY_DIFFERENCE\nAVERAGE_ABS_ODDS_DIFFERENCE\nDISPARATE_IMPACT\nTHEIL_INDEX\n\n\n\n\n0\nBASE MODEL\n-0.11843\n-0.197133\n0.116694\n0.353492\n0.114186\n\n\n1\nREWEIGHING\n0.001649\n0.010389\n0.006653\n1.012789\n0.113549\n\n\n2\nDISPARATE IMPACT REMOVER\n-0.121844\n-0.207568\n0.123159\n0.33208\n0.115176\n\n\n3\nADVERSARIAL DEBIASING\n-0.053664\n-0.043402\n0.02403\n0.547452\n0.126379\n\n\n4\nPREJUDICE REMOVER\n-0.024841\n0.009017\n0.006213\n0.59515\n0.152623\n\n\n5\nMODEL WITH DISC IN LOSS FUNCTION\n0.000414\n0.00000\n0.00000\n1.005809\n0.00000\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Defining the models and their metrics\nModel = ['BASE MODEL', 'REWEIGHING', 'DISPARATE IMPACT REMOVER', 'ADVERSARIAL DEBIASING', 'PREJUDICE REMOVER', 'MODEL WITH DISC IN LOSS FUNCTION']\nstat = [-0.11843, 0.001649, -0.121844, -0.053664, -0.024841, 0.000414]\neq_op = [-0.197133, 0.010389, -0.207568, -0.043402, 0.009017, 0.00000]\nabs_odds = [0.116694, 0.006653, 0.123159, 0.02403, 0.006213, 0.00000]\ndisp_imp = [0.353492, 1.012789, 0.33208, 0.547452, 0.59515, 1.005809]\ntheil = [0.114186, 0.113549, 0.115176, 0.126379, 0.152623, 0.00000]\n\n# Creating DataFrame with the model names as the index\nFinalMetrics_df = pd.DataFrame({\n    'STATISTICAL_PARITY_DIFFERENCE': stat,\n    'EQUAL_OPPORTUNITY_DIFFERENCE': eq_op,\n    'AVERAGE_ABS_ODDS_DIFFERENCE': abs_odds,\n    'DISPARATE_IMPACT': disp_imp,\n    'THEIL_INDEX': theil\n}, index=Model)\n\nFinalMetrics_df\n\n\n\n\n\n\n\n\nSTATISTICAL_PARITY_DIFFERENCE\nEQUAL_OPPORTUNITY_DIFFERENCE\nAVERAGE_ABS_ODDS_DIFFERENCE\nDISPARATE_IMPACT\nTHEIL_INDEX\n\n\n\n\nBASE MODEL\n-0.118430\n-0.197133\n0.116694\n0.353492\n0.114186\n\n\nREWEIGHING\n0.001649\n0.010389\n0.006653\n1.012789\n0.113549\n\n\nDISPARATE IMPACT REMOVER\n-0.121844\n-0.207568\n0.123159\n0.332080\n0.115176\n\n\nADVERSARIAL DEBIASING\n-0.053664\n-0.043402\n0.024030\n0.547452\n0.126379\n\n\nPREJUDICE REMOVER\n-0.024841\n0.009017\n0.006213\n0.595150\n0.152623\n\n\nMODEL WITH DISC IN LOSS FUNCTION\n0.000414\n0.000000\n0.000000\n1.005809\n0.000000\n\n\n\n\n\n\n\n\nimport plotly.graph_objs as go\n\ndef plot_fair_metrics_plotly2(fair_metrics):\n    # Prepare data\n    data = fair_metrics.values.T  # Transpose to get metrics as columns\n    cols = fair_metrics.columns\n    index = fair_metrics.index\n\n    # Create heatmap\n    fig = go.Figure(data=go.Heatmap(\n        z=data,\n        x=index,  # Models on the x-axis\n        y=cols,  # Metrics on the y-axis\n        colorscale='RdBu',  # Red to blue color scale, good for centered data\n        zmid=0  # Center the color scale at zero\n    ))\n\n    # Update layout\n    fig.update_layout(\n        title='Fairness Metrics Across Models',\n        xaxis_nticks=36,\n        yaxis=dict(title='Metric', autorange='reversed'),  # Reverse to show the top as the start\n        xaxis=dict(title='Models'),\n        plot_bgcolor='white'\n    )\n    fig.show()\n\ndef plot_fair_metrics_plotly(fair_metrics):\n    bottom = [-1, -1, -1, 0, 0]\n    max_valid = [0.1, 0.1, 0.1, 1.2, 0.25]\n    min_valid = [-0.1, -0.1, -0.1, 0.8, 0]\n    cols = fair_metrics.columns.values\n\n    for i in range(0, 5):\n        col = cols[i]\n\n        x, y = (fair_metrics[col].values, fair_metrics.index)\n        colors = []\n        for v in x:\n            color = '#e74c3c' if v &lt; min_valid[i] or v &gt; max_valid[i] else '#2ecc71'\n            colors.append(color)\n\n        trace = go.Bar(x=x, y=y, marker=dict(color=colors)\n                       , opacity=0.9, orientation='h')\n\n        layout = go.Layout(barmode='group',\n                           title=col,\n                           xaxis=dict(range=[bottom[i], bottom[i] + 2]),\n                           yaxis=go.layout.YAxis(automargin=True),\n                           shapes=[\n                               {\n                                   'type': 'line',\n                                   'x0': min_valid[i],\n                                   'y0': -1,\n                                   'x1': min_valid[i],\n                                   'y1': len(y),\n                                   'line': {\n                                       'color': 'rgb(0, 0, 0)',\n                                       'width': 2,\n                                   },\n                               }, {\n                                   'type': 'line',\n                                   'x0': max_valid[i],\n                                   'y0': -1,\n                                   'x1': max_valid[i],\n                                   'y1': len(y),\n                                   'line': {\n                                       'color': 'rgb(0, 0, 0)',\n                                       'width': 2,\n                                   },\n                               }])\n        fig = go.Figure([trace], layout=layout)\n        py.iplot(fig)\n\n\ndef plot_score_fair_metrics(score):\n    display(score.sort_values(['nb_valid', 'score'], ascending=[0, 1]))\n    score.sort_values(['nb_valid', 'score'], ascending=[1, 0], inplace=True)\n\n    gold, silver, bronze, other = ('#FFA400', '#bdc3c7', '#cd7f32', '#3498db')\n    colors = [gold if i == 0 else silver if i == 1 else bronze if i == 2 else other for i in range(0, len(score))]\n    colors = [c for c in reversed(colors)]\n\n    x, y = (score['score'].values, score.index)\n\n    trace = go.Bar(x=x, y=y, marker=dict(color=colors)\n                   , opacity=0.9, orientation='h')\n    layout = go.Layout(barmode='group',\n                       title='Fairest algorithm',\n                       yaxis=go.layout.YAxis(automargin=True))\n    fig = go.Figure([trace], layout=layout)\n    py.iplot(fig)\n    \n\ndef score_fair_metrics(fair):\n    objective = [0, 0, 0, 1, 0]\n    max_valid = [0.1, 0.1, 0.1, 1.2, 0.25]\n    min_valid = [-0.1, -0.1, -0.1, 0.8, 0]\n\n    nb_valid = np.sum(((fair.values &gt; min_valid) * (fair.values &lt; max_valid)), axis=1)\n    score = np.sum(np.abs(fair.values - objective), axis=1)\n    score = np.array([score, nb_valid])\n\n    score = pd.DataFrame(data=score.transpose(), columns=['score', 'nb_valid'], index=fair.index)\n    return score\n\n\ndef score_all_attr(algo_metrics):\n    attributes = algo_metrics.loc['Origin', 'fair_metrics'].index.values[1:]\n\n    all_scores = np.zeros((len(algo_metrics), 2))\n    for attr in attributes:\n        df_metrics = pd.DataFrame(columns=algo_metrics.loc['Origin', 'fair_metrics'].columns.values)\n        for fair in algo_metrics.loc[:, 'fair_metrics']:\n            df_metrics = df_metrics.append(fair.loc[attr], ignore_index=True)\n        all_scores = all_scores + score_fair_metrics(df_metrics).values\n\n    final = pd.DataFrame(data=all_scores, columns=['score', 'nb_valid'], index=algo_metrics.index)\n    return final\n\n\ndisplay(FinalMetrics_df)\nplot_fair_metrics_plotly(FinalMetrics_df)\nplot_fair_metrics_plotly2(FinalMetrics_df)\nscore = score_fair_metrics(FinalMetrics_df)\nplot_score_fair_metrics(score.dropna())\n\n\n\n\n\n\n\n\nSTATISTICAL_PARITY_DIFFERENCE\nEQUAL_OPPORTUNITY_DIFFERENCE\nAVERAGE_ABS_ODDS_DIFFERENCE\nDISPARATE_IMPACT\nTHEIL_INDEX\n\n\n\n\nBASE MODEL\n-0.118430\n-0.197133\n0.116694\n0.353492\n0.114186\n\n\nREWEIGHING\n0.001649\n0.010389\n0.006653\n1.012789\n0.113549\n\n\nDISPARATE IMPACT REMOVER\n-0.121844\n-0.207568\n0.123159\n0.332080\n0.115176\n\n\nADVERSARIAL DEBIASING\n-0.053664\n-0.043402\n0.024030\n0.547452\n0.126379\n\n\nPREJUDICE REMOVER\n-0.024841\n0.009017\n0.006213\n0.595150\n0.152623\n\n\nMODEL WITH DISC IN LOSS FUNCTION\n0.000414\n0.000000\n0.000000\n1.005809\n0.000000\n\n\n\n\n\n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n\n\n\n\n\n\n\nscore\nnb_valid\n\n\n\n\nREWEIGHING\n0.145029\n5.0\n\n\nMODEL WITH DISC IN LOSS FUNCTION\n0.006223\n4.0\n\n\nPREJUDICE REMOVER\n0.597544\n4.0\n\n\nADVERSARIAL DEBIASING\n0.700023\n4.0\n\n\nBASE MODEL\n1.192951\n1.0\n\n\nDISPARATE IMPACT REMOVER\n1.235667\n1.0"
  },
  {
    "objectID": "inprocessing.html",
    "href": "inprocessing.html",
    "title": "",
    "section": "",
    "text": "from aif360.algorithms.inprocessing import PrejudiceRemover,AdversarialDebiasing\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\nfrom aif360.datasets import MEPSDataset20\nfrom aif360.datasets import MEPSDataset21\nfrom aif360.datasets import GermanDataset\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n\n\n\ndataset_orig_panel19 = MEPSDataset19()\n\n\ndataset_orig_panel19_train, dataset_orig_panel19_val = dataset_orig_panel19.split([0.7], shuffle=True)\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\ndef test(dataset, model, thresh, unprivileged_groups, privileged_groups):\n    y_val_pred_prob = model.predict(dataset).scores\n    pos_ind = 0  # Assuming scores are for the favorable outcome\n\n    # Ensure y_val_pred_prob is two-dimensional\n    if y_val_pred_prob.ndim == 1:\n        # If y_val_pred_prob is one-dimensional, use it directly\n        y_val_pred = (y_val_pred_prob &gt; thresh).astype(int)\n    else:\n        # Use pos_ind to index the second dimension\n        y_val_pred = (y_val_pred_prob[:, pos_ind] &gt; thresh).astype(int)\n\n    # Continue with the evaluation using y_val_pred\n    # Calculate accuracy, fairness metrics, etc., based on y_val_pred\n\n    return {\n        'accuracy': None,  # Replace with actual accuracy calculation\n        'fairness_metric': None  # Replace with actual fairness metric calculation\n    }\n\n\nfeatures = dataset_orig_panel19_train.features\nlabel = dataset_orig_panel19_train.labels\nfeature_names = dataset_orig_panel19_train.feature_names\ndataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\ndataset_orig_panel19_train_df['RACE2']=dataset_orig_panel19_train_df['RACE']\ndataset_orig_panel19_train_df.drop('RACE',axis=1,inplace=True)\ndataset_orig_panel19_train_df.rename(columns={'RACE2': 'RACE'},inplace=True)\ndataset_orig_panel19_train_df['label'] = label\n\n\ndataset_orig_panel19_train_df\n\n\n\n\n\n\n\n\nAGE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\nSEX=2\n...\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nRACE\nlabel\n\n\n\n\n0\n57.0\n54.80\n60.12\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n29.0\n57.47\n54.69\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n53.0\n52.40\n62.66\n2.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n55.0\n-1.00\n-1.00\n-1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n\n\n4\n51.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11076\n35.0\n53.03\n43.53\n4.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n11077\n17.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n\n\n11078\n58.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n11079\n36.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n11080\n73.0\n34.95\n61.93\n3.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n11081 rows × 139 columns\n\n\n\n\nfrom aif360.datasets import StandardDataset\n\n# Define which column is the label and which one is the protected attribute\nlabel_name = 'label'  # or whatever the name of your label column is\nprotected_attribute_name = 'RACE'  # or your protected attribute column name\n\n# Create the aif360 dataset\naif360_dataset = StandardDataset(\n    df=dataset_orig_panel19_train_df,\n    label_name=label_name,\n    favorable_classes=[1],  # assuming 1 is the favorable class\n    protected_attribute_names=[protected_attribute_name],\n    privileged_classes=[[1.0]],  # assuming 'White' is the privileged class\n)\n\n\naif360_dataset_train,aif360_dataset_test=aif360_dataset.split([0.7], shuffle=True)\n\n\naif360_dataset_train\n\n               instance weights features                                 \\\n                                                                          \n                                     AGE  PCS42  MCS42 K6SUM42 REGION=1   \ninstance names                                                            \n3965                        1.0     66.0  40.09  48.40    -9.0      0.0   \n1752                        1.0     28.0  55.66  50.01     2.0      0.0   \n284                         1.0      5.0  -1.00  -1.00    -1.0      0.0   \n1558                        1.0     51.0  45.64  62.40     1.0      0.0   \n4408                        1.0     76.0  16.34  53.12     0.0      0.0   \n...                         ...      ...    ...    ...     ...      ...   \n5186                        1.0     52.0  53.42  44.83     3.0      1.0   \n7196                        1.0     14.0  -1.00  -1.00    -1.0      1.0   \n3134                        1.0     72.0  57.78  50.82     0.0      0.0   \n2473                        1.0     27.0  53.17  54.96     1.0      0.0   \n11045                       1.0     74.0  44.26  56.09     4.0      0.0   \n\n                                                 ...                    \\\n                                                 ...                     \n               REGION=2 REGION=3 REGION=4 SEX=1  ... POVCAT=1 POVCAT=2   \ninstance names                                   ...                     \n3965                0.0      1.0      0.0   0.0  ...      0.0      0.0   \n1752                1.0      0.0      0.0   1.0  ...      0.0      0.0   \n284                 0.0      1.0      0.0   0.0  ...      0.0      0.0   \n1558                0.0      1.0      0.0   1.0  ...      0.0      0.0   \n4408                0.0      0.0      1.0   1.0  ...      0.0      1.0   \n...                 ...      ...      ...   ...  ...      ...      ...   \n5186                0.0      0.0      0.0   0.0  ...      0.0      0.0   \n7196                0.0      0.0      0.0   1.0  ...      0.0      1.0   \n3134                0.0      1.0      0.0   1.0  ...      1.0      0.0   \n2473                0.0      1.0      0.0   1.0  ...      0.0      0.0   \n11045               0.0      1.0      0.0   1.0  ...      1.0      0.0   \n\n                                                                      \\\n                                                                       \n               POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2 INSCOV=3   \ninstance names                                                         \n3965                0.0      1.0      0.0      1.0      0.0      0.0   \n1752                0.0      0.0      1.0      1.0      0.0      0.0   \n284                 1.0      0.0      0.0      0.0      1.0      0.0   \n1558                0.0      0.0      1.0      0.0      0.0      1.0   \n4408                0.0      0.0      0.0      0.0      1.0      0.0   \n...                 ...      ...      ...      ...      ...      ...   \n5186                1.0      0.0      0.0      0.0      0.0      1.0   \n7196                0.0      0.0      0.0      0.0      1.0      0.0   \n3134                0.0      0.0      0.0      0.0      1.0      0.0   \n2473                0.0      0.0      1.0      1.0      0.0      0.0   \n11045               0.0      0.0      0.0      0.0      1.0      0.0   \n\n                                   labels  \n               protected attribute         \n                              RACE         \ninstance names                             \n3965                           0.0    1.0  \n1752                           1.0    0.0  \n284                            0.0    0.0  \n1558                           0.0    0.0  \n4408                           0.0    1.0  \n...                            ...    ...  \n5186                           0.0    0.0  \n7196                           0.0    0.0  \n3134                           0.0    1.0  \n2473                           0.0    0.0  \n11045                          1.0    0.0  \n\n[7756 rows x 140 columns]\n\n\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\npr=PrejudiceRemover(sensitive_attr='RACE', eta=25.0)\ndset_scaled_trn = aif360_dataset_train.copy()\ndset_scaled_trn.features = scaler.fit_transform(dset_scaled_trn.features)\n\npr_fitted = pr.fit(dset_scaled_trn)\nprint(pr_fitted)\naccs = []\nthresholds = np.linspace(0.01, 0.5, 10)\n\ndset_val = aif360_dataset_test.copy()\ndset_val.features = scaler.transform(dset_val.features)\n\n##################### STEP 1 TRAINING WITH INPROCESSING #####################\npr_pred_prob = pr_fitted.predict(dset_val).scores\nprint(pr_pred_prob)\n##################### STEP 2 PICKING THRESHOLD WITH VALIDATION DATA #####################\nfor threshold in thresholds:\n    dset_val_pred = dset_val.copy()\n    dset_val_pred.labels = (pr_pred_prob[:, 0] &gt; threshold).astype(np.float64)\n\n    metric = ClassificationMetric(\n                dset_val, dset_val_pred,\n                unprivileged_groups = unprivileged_groups,\n                privileged_groups=privileged_groups)\n    accs.append((metric.true_positive_rate() + metric.true_negative_rate()) / 2)\n\n\npr_val_best_idx = np.argmax(accs)\nbest_threshold = thresholds[pr_val_best_idx]\n\n##################### STEP 3 TEST DATA #####################\ndset_tst = aif360_dataset_test.copy()\ndset_tst.features = scaler.transform(dset_tst.features)\n\npr_pred_prob = pr_fitted.predict(dset_tst).scores\n\n\ndset_tst_pred = dset_tst.copy()\ndset_tst_pred.labels = (pr_pred_prob[:, 0] &gt; best_threshold).astype(np.float64)\n\nmetric = ClassificationMetric(\n            dset_tst, dset_tst_pred,\n            unprivileged_groups = unprivileged_groups,\n            privileged_groups   = privileged_groups)\ntest_acc = (metric.true_positive_rate() + metric.true_negative_rate()) / 2 ## no built in balanced error rate\ntest_disp_impact = metric.disparate_impact()\n\nprint(\"Testing accuracy with ETA %0.2f = %0.2f\\n Disparate impact %0.2f\" % (25.0, test_acc, test_disp_impact))\n\n&lt;aif360.algorithms.inprocessing.prejudice_remover.PrejudiceRemover object at 0x0000029A6D939510&gt;\nSTDOUT: b'Model loaded.\\r\\nData shape: (3325, 139)\\r\\nFeatures shape: (3325, 138), Labels shape: (3325,)\\r\\nPredictions shape: (3325, 2)\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\n'\nSTDERR: b''\n(3325, 5)\n[[0.45902625]\n [0.22868268]\n [0.03929404]\n ...\n [0.03253647]\n [0.55638827]\n [0.45576392]]\nSTDOUT: b'Model loaded.\\r\\nData shape: (3325, 139)\\r\\nFeatures shape: (3325, 138), Labels shape: (3325,)\\r\\nPredictions shape: (3325, 2)\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\nWriting prediction for sample 3324: class 0\\r\\n'\nSTDERR: b''\n(3325, 5)\nTesting accuracy with ETA 25.00 = 0.73\n Disparate impact 0.69\n\n\n\nimport tensorflow as tf\n\ntf.compat.v1.disable_eager_execution()\n\n# Now you can use tf.Session like in TensorFlow 1.x\nsession = tf.compat.v1.Session()\n\ndebiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n                          unprivileged_groups = unprivileged_groups,\n                          scope_name='debiased_classifier',\n                          num_epochs=10,\n                          debias=True,\n                          sess=session)\n\ndebiased_model.fit(dataset_orig_panel19_train)\n\n#fair = get_fair_metrics_and_plot(data_orig_test, debiased_model, plot=False, model_aif=True)\ndata_pred = debiased_model.predict(dataset_orig_panel19_val)\n\nWARNING:tensorflow:From C:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_34168\\105869548.py:3: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n\nWARNING:tensorflow:From c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n\nepoch 0; iter: 0; batch classifier loss: 1.724557; batch adversarial loss: 0.708830\nepoch 1; iter: 0; batch classifier loss: 0.604499; batch adversarial loss: 0.681954\nepoch 2; iter: 0; batch classifier loss: 0.321243; batch adversarial loss: 0.682153\nepoch 3; iter: 0; batch classifier loss: 0.300242; batch adversarial loss: 0.670921\nepoch 4; iter: 0; batch classifier loss: 0.346055; batch adversarial loss: 0.662858\nepoch 5; iter: 0; batch classifier loss: 0.332033; batch adversarial loss: 0.650315\nepoch 6; iter: 0; batch classifier loss: 0.281160; batch adversarial loss: 0.655163\nepoch 7; iter: 0; batch classifier loss: 0.349158; batch adversarial loss: 0.624533\nepoch 8; iter: 0; batch classifier loss: 0.324309; batch adversarial loss: 0.624527\nepoch 9; iter: 0; batch classifier loss: 0.308793; batch adversarial loss: 0.611027\n\n\n\nmetric_transf_panel19_train = BinaryLabelDatasetMetric(\ndata_pred,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_ad=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\ntest_results_ad\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.061699987002589565',\n 'Consistency (Zemel, et al. 2013): [0.95712782]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.061699987002589565',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.4779928304847973']\n\n\n\n\nfrom aif360.algorithms.inprocessing import MetaFairClassifier\nfrom aif360.algorithms.inprocessing import GerryFairClassifier\n\n\nMC = MetaFairClassifier(tau=0, sensitive_attr=\"RACE\", type=\"fdr\")\n\n\nmc_model = MC.fit(dataset_orig_panel19_train)\n\n\ndataset_transf_panel19_train_MC = mc_model.predict(dataset_orig_panel19_train)\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n  prob_y_1 = (prob_1_1 + prob_1_0) / total\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n  prob_z_0 = (prob_m1_0 + prob_1_0) / total\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n  prob_z_1 = (prob_m1_1 + prob_1_1) / total\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n  probc_m1_0 = prob_m1_0 / total\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n  probc_m1_1 = prob_m1_1 / total\n\n\n\ndataset_transf_panel19_test_MC = mc_model.predict(dataset_orig_panel19_val)\n\n\nclassified_metric_bias_test = BinaryLabelDatasetMetric(dataset_transf_panel19_test_MC,\n                                                   unprivileged_groups=unprivileged_groups,\n                                                   privileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_mc=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\ntest_results_mc\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.061699987002589565',\n 'Consistency (Zemel, et al. 2013): [0.95712782]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.061699987002589565',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.4779928304847973']\n\n\n\nGC = GerryFairClassifier(C= 100, printflag=True, gamma=.005, fairness_def='FP',\n             max_iters=10, heatmapflag=False)\n\n\n# fit method\ngfc_model = GC.fit(dataset_orig_panel19_train, early_termination=True)\n\n# predict method. If threshold in (0, 1) produces binary predictions\ndataset_yhat = gfc_model.predict(dataset_orig_panel19_train, threshold=False)\n\niteration: 1, error: 0.1330204855157477, fairness violation: 0.006406795146862778, violated group size: 0.27019222091868966\niteration: 2, error: 0.15003158559696778, fairness violation: 0.003234917463099615, violated group size: 0.5602382456456998\niteration: 3, error: 0.15552146316517765, fairness violation: 0.0021776249018452265, violated group size: 0.27019222091868966\niteration: 4, error: 0.15808591282375237, fairness violation: 0.001656858593635089, violated group size: 0.27019222091868966\niteration: 5, error: 0.15953433805613212, fairness violation: 0.00133852634139932, violated group size: 0.5602382456456998\niteration: 6, error: 0.16042475107541435, fairness violation: 0.0011465989153143603, violated group size: 0.27019222091868966\niteration: 7, error: 0.1609834078925316, fairness violation: 0.0010182053575703398, violated group size: 0.5602382456456998\niteration: 8, error: 0.16136855879433265, fairness violation: 0.0009219101892623255, violated group size: 0.27019222091868966\niteration: 9, error: 0.1616480662595634, fairness violation: 0.0008537786390468312, violated group size: 0.5602382456456998\n\n\n\n# auditing \nfrom aif360.algorithms.inprocessing.gerryfair.clean import array_to_tuple\n\ngerry_metric = BinaryLabelDatasetMetric(dataset_orig_panel19_train)\ngamma_disparity = gerry_metric.rich_subgroup(array_to_tuple(dataset_yhat.labels), 'FP')\nprint(gamma_disparity)\n\n0.0008537786390468316\n\n\n\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn import linear_model\nimport pickle\n\n\n# set to 10 iterations for fast running of notebook - set &gt;= 1000 when running real experiments\n# tests learning with different hypothesis classes\npareto_iters = 10\ndef multiple_classifiers_pareto(dataset, gamma_list=[0.002, 0.005, 0.01], save_results=True, iters=pareto_iters):\n\n    ln_predictor = linear_model.LinearRegression()\n    svm_predictor = svm.LinearSVR()\n    tree_predictor = tree.DecisionTreeRegressor(max_depth=3)\n    kernel_predictor = KernelRidge(alpha=1.0, gamma=1.0, kernel='rbf')\n    predictor_dict = {'Linear': {'predictor': ln_predictor, 'iters': iters},\n                       'SVR': {'predictor': svm_predictor, 'iters': iters},\n                       'Tree': {'predictor': tree_predictor, 'iters': iters},\n                       'Kernel': {'predictor': kernel_predictor, 'iters': iters}}\n    #predictor_dict = {'Linear': {'predictor': ln_predictor, 'iters': iters}}\n\n    results_dict = {}\n\n    for pred in predictor_dict:\n        print('Curr Predictor: {}'.format(pred))\n        predictor = predictor_dict[pred]['predictor']\n        max_iters = predictor_dict[pred]['iters']\n        fair_clf = GerryFairClassifier(C=100, printflag=True, gamma=1, predictor=predictor, max_iters=max_iters)\n        fair_clf.printflag = False\n        fair_clf.max_iters=max_iters\n        errors, fp_violations, fn_violations = fair_clf.pareto(dataset, gamma_list)\n        results_dict[pred] = {'errors': errors, 'fp_violations': fp_violations, 'fn_violations': fn_violations}\n    if save_results:\n        print('path:'+'results_dict_' + str(gamma_list) + '_gammas' + str(gamma_list) + '.pkl', 'wb')\n        pickle.dump(results_dict, open('results_dict_' + str(gamma_list) + '_gammas' + str(gamma_list) + '.pkl', 'wb'))\n\nmultiple_classifiers_pareto(dataset_orig_panel19_train)\n\nCurr Predictor: Linear\nCurr Predictor: SVR\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\nCurr Predictor: Tree\nCurr Predictor: Kernel\npath:results_dict_[0.002, 0.005, 0.01]_gammas[0.002, 0.005, 0.01].pkl wb\n\n\n\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details."
  },
  {
    "objectID": "fdm.html",
    "href": "fdm.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nlabel_encoder = LabelEncoder()\nscaler = StandardScaler()\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\npd.set_option('display.max_columns', None)\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom aif360.datasets import CompasDataset\nimport pandas as pd\nfrom bias_processor import BiasMitigator\n\n\ndataset2 = CompasDataset()\n\nWARNING:root:Missing Data: 5 rows removed from CompasDataset.\n\n\n\nfeatures = dataset2.features\nfeature_names = dataset2.feature_names\ndataset2_df = pd.DataFrame(features, columns=feature_names)\n\n\ndataset2_df\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nage_cat=25 - 45\nage_cat=Greater than 45\nage_cat=Less than 25\nc_charge_degree=F\nc_charge_degree=M\nc_charge_desc=Abuse Without Great Harm\nc_charge_desc=Agg Abuse Elderlly/Disabled Adult\nc_charge_desc=Agg Assault W/int Com Fel Dome\nc_charge_desc=Agg Battery Grt/Bod/Harm\nc_charge_desc=Agg Fleeing and Eluding\nc_charge_desc=Agg Fleeing/Eluding High Speed\nc_charge_desc=Aggr Child Abuse-Torture,Punish\nc_charge_desc=Aggrav Battery w/Deadly Weapon\nc_charge_desc=Aggrav Child Abuse-Agg Battery\nc_charge_desc=Aggrav Child Abuse-Causes Harm\nc_charge_desc=Aggrav Stalking After Injunctn\nc_charge_desc=Aggravated Assault\nc_charge_desc=Aggravated Assault W/Dead Weap\nc_charge_desc=Aggravated Assault W/dead Weap\nc_charge_desc=Aggravated Assault W/o Firearm\nc_charge_desc=Aggravated Assault w/Firearm\nc_charge_desc=Aggravated Battery\nc_charge_desc=Aggravated Battery (Firearm)\nc_charge_desc=Aggravated Battery (Firearm/Actual Possession)\nc_charge_desc=Aggravated Battery / Pregnant\nc_charge_desc=Aggravated Battery On 65/Older\nc_charge_desc=Aide/Abet Prostitution Lewdness\nc_charge_desc=Aiding Escape\nc_charge_desc=Alcoholic Beverage Violation-FL\nc_charge_desc=Armed Trafficking in Cannabis\nc_charge_desc=Arson in the First Degree\nc_charge_desc=Assault\nc_charge_desc=Assault Law Enforcement Officer\nc_charge_desc=Att Burgl Conv Occp\nc_charge_desc=Att Burgl Struc/Conv Dwel/Occp\nc_charge_desc=Att Burgl Unoccupied Dwel\nc_charge_desc=Att Tamper w/Physical Evidence\nc_charge_desc=Attempt Armed Burglary Dwell\nc_charge_desc=Attempted Burg/Convey/Unocc\nc_charge_desc=Attempted Burg/struct/unocc\nc_charge_desc=Attempted Deliv Control Subst\nc_charge_desc=Attempted Robbery No Weapon\nc_charge_desc=Attempted Robbery Weapon\nc_charge_desc=Battery\nc_charge_desc=Battery Emergency Care Provide\nc_charge_desc=Battery On A Person Over 65\nc_charge_desc=Battery On Fire Fighter\nc_charge_desc=Battery On Parking Enfor Speci\nc_charge_desc=Battery Spouse Or Girlfriend\nc_charge_desc=Battery on Law Enforc Officer\nc_charge_desc=Battery on a Person Over 65\nc_charge_desc=Bribery Athletic Contests\nc_charge_desc=Burgl Dwel/Struct/Convey Armed\nc_charge_desc=Burglary Assault/Battery Armed\nc_charge_desc=Burglary Conveyance Armed\nc_charge_desc=Burglary Conveyance Assault/Bat\nc_charge_desc=Burglary Conveyance Occupied\nc_charge_desc=Burglary Conveyance Unoccup\nc_charge_desc=Burglary Dwelling Armed\nc_charge_desc=Burglary Dwelling Assault/Batt\nc_charge_desc=Burglary Dwelling Occupied\nc_charge_desc=Burglary Structure Assault/Batt\nc_charge_desc=Burglary Structure Occupied\nc_charge_desc=Burglary Structure Unoccup\nc_charge_desc=Burglary Unoccupied Dwelling\nc_charge_desc=Burglary With Assault/battery\nc_charge_desc=Carjacking w/o Deadly Weapon\nc_charge_desc=Carjacking with a Firearm\nc_charge_desc=Carry Open/Uncov Bev In Pub\nc_charge_desc=Carrying A Concealed Weapon\nc_charge_desc=Carrying Concealed Firearm\nc_charge_desc=Cash Item w/Intent to Defraud\nc_charge_desc=Child Abuse\nc_charge_desc=Computer Pornography\nc_charge_desc=Consp Traff Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Conspiracy Dealing Stolen Prop\nc_charge_desc=Consume Alcoholic Bev Pub\nc_charge_desc=Contradict Statement\nc_charge_desc=Contribute Delinquency Of A Minor\nc_charge_desc=Corrupt Public Servant\nc_charge_desc=Counterfeit Lic Plates/Sticker\nc_charge_desc=Crim Attempt/Solic/Consp\nc_charge_desc=Crim Use of Personal ID Info\nc_charge_desc=Crimin Mischief Damage $1000+\nc_charge_desc=Criminal Mischief\nc_charge_desc=Criminal Mischief Damage &lt;$200\nc_charge_desc=Criminal Mischief&gt;$200&lt;$1000\nc_charge_desc=Crlty Twrd Child Urge Oth Act\nc_charge_desc=Cruelty Toward Child\nc_charge_desc=Cruelty to Animals\nc_charge_desc=Culpable Negligence\nc_charge_desc=D.U.I. Serious Bodily Injury\nc_charge_desc=DOC/Cause Public Danger\nc_charge_desc=DUI - Enhanced\nc_charge_desc=DUI - Property Damage/Personal Injury\nc_charge_desc=DUI Blood Alcohol Above 0.20\nc_charge_desc=DUI Level 0.15 Or Minor In Veh\nc_charge_desc=DUI Property Damage/Injury\nc_charge_desc=DUI- Enhanced\nc_charge_desc=DUI/Property Damage/Persnl Inj\nc_charge_desc=DWI w/Inj Susp Lic / Habit Off\nc_charge_desc=DWLS Canceled Disqul 1st Off\nc_charge_desc=DWLS Susp/Cancel Revoked\nc_charge_desc=Dealing in Stolen Property\nc_charge_desc=Defrauding Innkeeper\nc_charge_desc=Defrauding Innkeeper $300/More\nc_charge_desc=Del 3,4 Methylenedioxymethcath\nc_charge_desc=Del Cannabis At/Near Park\nc_charge_desc=Del Cannabis For Consideration\nc_charge_desc=Del of JWH-250 2-Methox 1-Pentyl\nc_charge_desc=Deliver 3,4 Methylenediox\nc_charge_desc=Deliver Alprazolam\nc_charge_desc=Deliver Cannabis\nc_charge_desc=Deliver Cannabis 1000FTSch\nc_charge_desc=Deliver Cocaine\nc_charge_desc=Deliver Cocaine 1000FT Church\nc_charge_desc=Deliver Cocaine 1000FT Park\nc_charge_desc=Deliver Cocaine 1000FT School\nc_charge_desc=Deliver Cocaine 1000FT Store\nc_charge_desc=Delivery Of Drug Paraphernalia\nc_charge_desc=Delivery of 5-Fluoro PB-22\nc_charge_desc=Delivery of Heroin\nc_charge_desc=Depriv LEO of Protect/Communic\nc_charge_desc=Disorderly Conduct\nc_charge_desc=Disorderly Intoxication\nc_charge_desc=Disrupting School Function\nc_charge_desc=Drivg While Lic Suspd/Revk/Can\nc_charge_desc=Driving License Suspended\nc_charge_desc=Driving Under The Influence\nc_charge_desc=Driving While License Revoked\nc_charge_desc=Escape\nc_charge_desc=Exhibition Weapon School Prop\nc_charge_desc=Expired DL More Than 6 Months\nc_charge_desc=Exposes Culpable Negligence\nc_charge_desc=Extradition/Defendants\nc_charge_desc=Fabricating Physical Evidence\nc_charge_desc=Fail Register Vehicle\nc_charge_desc=Fail Sex Offend Report Bylaw\nc_charge_desc=Fail To Obey Police Officer\nc_charge_desc=Fail To Redeliv Hire/Leas Prop\nc_charge_desc=Failure To Pay Taxi Cab Charge\nc_charge_desc=Failure To Return Hired Vehicle\nc_charge_desc=False 911 Call\nc_charge_desc=False Bomb Report\nc_charge_desc=False Imprisonment\nc_charge_desc=False Info LEO During Invest\nc_charge_desc=False Motor Veh Insurance Card\nc_charge_desc=False Name By Person Arrest\nc_charge_desc=False Ownership Info/Pawn Item\nc_charge_desc=Falsely Impersonating Officer\nc_charge_desc=Fel Drive License Perm Revoke\nc_charge_desc=Felon in Pos of Firearm or Amm\nc_charge_desc=Felony Batt(Great Bodily Harm)\nc_charge_desc=Felony Battery\nc_charge_desc=Felony Battery (Dom Strang)\nc_charge_desc=Felony Battery w/Prior Convict\nc_charge_desc=Felony Committing Prostitution\nc_charge_desc=Felony DUI (level 3)\nc_charge_desc=Felony DUI - Enhanced\nc_charge_desc=Felony Driving While Lic Suspd\nc_charge_desc=Felony Petit Theft\nc_charge_desc=Felony/Driving Under Influence\nc_charge_desc=Fighting/Baiting Animals\nc_charge_desc=Fleeing Or Attmp Eluding A Leo\nc_charge_desc=Fleeing or Eluding a LEO\nc_charge_desc=Forging Bank Bills/Promis Note\nc_charge_desc=Fraudulent Use of Credit Card\nc_charge_desc=Grand Theft (Motor Vehicle)\nc_charge_desc=Grand Theft Dwell Property\nc_charge_desc=Grand Theft Firearm\nc_charge_desc=Grand Theft in the 1st Degree\nc_charge_desc=Grand Theft in the 3rd Degree\nc_charge_desc=Grand Theft of a Fire Extinquisher\nc_charge_desc=Grand Theft of the 2nd Degree\nc_charge_desc=Grand Theft on 65 Yr or Older\nc_charge_desc=Harass Witness/Victm/Informnt\nc_charge_desc=Harm Public Servant Or Family\nc_charge_desc=Hiring with Intent to Defraud\nc_charge_desc=Imperson Public Officer or Emplyee\nc_charge_desc=Interfere W/Traf Cont Dev RR\nc_charge_desc=Interference with Custody\nc_charge_desc=Intoxicated/Safety Of Another\nc_charge_desc=Introduce Contraband Into Jail\nc_charge_desc=Issuing a Worthless Draft\nc_charge_desc=Kidnapping / Domestic Violence\nc_charge_desc=Lease For Purpose Trafficking\nc_charge_desc=Leave Acc/Attend Veh/More $50\nc_charge_desc=Leave Accd/Attend Veh/Less $50\nc_charge_desc=Leaving Acc/Unattended Veh\nc_charge_desc=Leaving the Scene of Accident\nc_charge_desc=Lewd Act Presence Child 16-\nc_charge_desc=Lewd or Lascivious Molestation\nc_charge_desc=Lewd/Lasc Battery Pers 12+/&lt;16\nc_charge_desc=Lewd/Lasc Exhib Presence &lt;16yr\nc_charge_desc=Lewd/Lasciv Molest Elder Persn\nc_charge_desc=Lewdness Violation\nc_charge_desc=License Suspended Revoked\nc_charge_desc=Littering\nc_charge_desc=Live on Earnings of Prostitute\nc_charge_desc=Lve/Scen/Acc/Veh/Prop/Damage\nc_charge_desc=Manage Busn W/O City Occup Lic\nc_charge_desc=Manslaughter W/Weapon/Firearm\nc_charge_desc=Manufacture Cannabis\nc_charge_desc=Misuse Of 911 Or E911 System\nc_charge_desc=Money Launder 100K or More Dols\nc_charge_desc=Murder In 2nd Degree W/firearm\nc_charge_desc=Murder in the First Degree\nc_charge_desc=Neglect Child / Bodily Harm\nc_charge_desc=Neglect Child / No Bodily Harm\nc_charge_desc=Neglect/Abuse Elderly Person\nc_charge_desc=Obstruct Fire Equipment\nc_charge_desc=Obstruct Officer W/Violence\nc_charge_desc=Obtain Control Substance By Fraud\nc_charge_desc=Offer Agree Secure For Lewd Act\nc_charge_desc=Offer Agree Secure/Lewd Act\nc_charge_desc=Offn Against Intellectual Prop\nc_charge_desc=Open Carrying Of Weapon\nc_charge_desc=Oper Motorcycle W/O Valid DL\nc_charge_desc=Operating W/O Valid License\nc_charge_desc=Opert With Susp DL 2nd Offens\nc_charge_desc=PL/Unlaw Use Credit Card\nc_charge_desc=Petit Theft\nc_charge_desc=Petit Theft $100- $300\nc_charge_desc=Pos Cannabis For Consideration\nc_charge_desc=Pos Cannabis W/Intent Sel/Del\nc_charge_desc=Pos Methylenedioxymethcath W/I/D/S\nc_charge_desc=Poss 3,4 MDMA (Ecstasy)\nc_charge_desc=Poss Alprazolam W/int Sell/Del\nc_charge_desc=Poss Anti-Shoplifting Device\nc_charge_desc=Poss Cntrft Contr Sub w/Intent\nc_charge_desc=Poss Cocaine/Intent To Del/Sel\nc_charge_desc=Poss Contr Subst W/o Prescript\nc_charge_desc=Poss Counterfeit Payment Inst\nc_charge_desc=Poss Drugs W/O A Prescription\nc_charge_desc=Poss F/Arm Delinq\nc_charge_desc=Poss Firearm W/Altered ID#\nc_charge_desc=Poss Meth/Diox/Meth/Amp (MDMA)\nc_charge_desc=Poss Of 1,4-Butanediol\nc_charge_desc=Poss Of Controlled Substance\nc_charge_desc=Poss Of RX Without RX\nc_charge_desc=Poss Oxycodone W/Int/Sell/Del\nc_charge_desc=Poss Pyrrolidinobutiophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone W/I/D/S\nc_charge_desc=Poss Similitude of Drivers Lic\nc_charge_desc=Poss Tetrahydrocannabinols\nc_charge_desc=Poss Unlaw Issue Driver Licenc\nc_charge_desc=Poss Unlaw Issue Id\nc_charge_desc=Poss Wep Conv Felon\nc_charge_desc=Poss of Cocaine W/I/D/S 1000FT Park\nc_charge_desc=Poss of Firearm by Convic Felo\nc_charge_desc=Poss of Methylethcathinone\nc_charge_desc=Poss/Sell/Del Cocaine 1000FT Sch\nc_charge_desc=Poss/Sell/Del/Man Amobarbital\nc_charge_desc=Poss/pur/sell/deliver Cocaine\nc_charge_desc=Poss3,4 Methylenedioxymethcath\nc_charge_desc=Posses/Disply Susp/Revk/Frd DL\nc_charge_desc=Possess Cannabis 1000FTSch\nc_charge_desc=Possess Cannabis/20 Grams Or Less\nc_charge_desc=Possess Controlled Substance\nc_charge_desc=Possess Countrfeit Credit Card\nc_charge_desc=Possess Drug Paraphernalia\nc_charge_desc=Possess Mot Veh W/Alt Vin #\nc_charge_desc=Possess Tobacco Product Under 18\nc_charge_desc=Possess Weapon On School Prop\nc_charge_desc=Possess w/I/Utter Forged Bills\nc_charge_desc=Possession Burglary Tools\nc_charge_desc=Possession Child Pornography\nc_charge_desc=Possession Firearm School Prop\nc_charge_desc=Possession Of 3,4Methylenediox\nc_charge_desc=Possession Of Alprazolam\nc_charge_desc=Possession Of Amphetamine\nc_charge_desc=Possession Of Anabolic Steroid\nc_charge_desc=Possession Of Buprenorphine\nc_charge_desc=Possession Of Carisoprodol\nc_charge_desc=Possession Of Clonazepam\nc_charge_desc=Possession Of Cocaine\nc_charge_desc=Possession Of Diazepam\nc_charge_desc=Possession Of Fentanyl\nc_charge_desc=Possession Of Heroin\nc_charge_desc=Possession Of Methamphetamine\nc_charge_desc=Possession Of Paraphernalia\nc_charge_desc=Possession Of Phentermine\nc_charge_desc=Possession of Alcohol Under 21\nc_charge_desc=Possession of Benzylpiperazine\nc_charge_desc=Possession of Butylone\nc_charge_desc=Possession of Cannabis\nc_charge_desc=Possession of Cocaine\nc_charge_desc=Possession of Codeine\nc_charge_desc=Possession of Ethylone\nc_charge_desc=Possession of Hydrocodone\nc_charge_desc=Possession of Hydromorphone\nc_charge_desc=Possession of LSD\nc_charge_desc=Possession of Methadone\nc_charge_desc=Possession of Morphine\nc_charge_desc=Possession of Oxycodone\nc_charge_desc=Possession of XLR11\nc_charge_desc=Principal In The First Degree\nc_charge_desc=Prostitution\nc_charge_desc=Prostitution/Lewd Act Assignation\nc_charge_desc=Prostitution/Lewdness/Assign\nc_charge_desc=Prowling/Loitering\nc_charge_desc=Purchase Cannabis\nc_charge_desc=Purchase/P/W/Int Cannabis\nc_charge_desc=Reckless Driving\nc_charge_desc=Refuse Submit Blood/Breath Test\nc_charge_desc=Refuse to Supply DNA Sample\nc_charge_desc=Resist Officer w/Violence\nc_charge_desc=Resist/Obstruct W/O Violence\nc_charge_desc=Retail Theft $300 1st Offense\nc_charge_desc=Retail Theft $300 2nd Offense\nc_charge_desc=Ride Tri-Rail Without Paying\nc_charge_desc=Robbery / No Weapon\nc_charge_desc=Robbery / Weapon\nc_charge_desc=Robbery Sudd Snatch No Weapon\nc_charge_desc=Robbery W/Deadly Weapon\nc_charge_desc=Robbery W/Firearm\nc_charge_desc=Sale/Del Cannabis At/Near Scho\nc_charge_desc=Sale/Del Counterfeit Cont Subs\nc_charge_desc=Sel/Pur/Mfr/Del Control Substa\nc_charge_desc=Sell or Offer for Sale Counterfeit Goods\nc_charge_desc=Sell/Man/Del Pos/w/int Heroin\nc_charge_desc=Sex Batt Faml/Cust Vict 12-17Y\nc_charge_desc=Sex Battery Deft 18+/Vict 11-\nc_charge_desc=Sex Offender Fail Comply W/Law\nc_charge_desc=Sexual Battery / Vict 12 Yrs +\nc_charge_desc=Sexual Performance by a Child\nc_charge_desc=Shoot In Occupied Dwell\nc_charge_desc=Shoot Into Vehicle\nc_charge_desc=Simulation of Legal Process\nc_charge_desc=Solic to Commit Battery\nc_charge_desc=Solicit Deliver Cocaine\nc_charge_desc=Solicit Purchase Cocaine\nc_charge_desc=Solicit To Deliver Cocaine\nc_charge_desc=Solicitation On Felony 3 Deg\nc_charge_desc=Soliciting For Prostitution\nc_charge_desc=Sound Articles Over 100\nc_charge_desc=Stalking\nc_charge_desc=Stalking (Aggravated)\nc_charge_desc=Strong Armed Robbery\nc_charge_desc=Structuring Transactions\nc_charge_desc=Susp Drivers Lic 1st Offense\nc_charge_desc=Tamper With Victim\nc_charge_desc=Tamper With Witness\nc_charge_desc=Tamper With Witness/Victim/CI\nc_charge_desc=Tampering With Physical Evidence\nc_charge_desc=Tampering with a Victim\nc_charge_desc=Theft/To Deprive\nc_charge_desc=Threat Public Servant\nc_charge_desc=Throw Deadly Missile Into Veh\nc_charge_desc=Throw In Occupied Dwell\nc_charge_desc=Throw Missile Into Pub/Priv Dw\nc_charge_desc=Traff In Cocaine &lt;400g&gt;150 Kil\nc_charge_desc=Traffic Counterfeit Cred Cards\nc_charge_desc=Traffick Amphetamine 28g&gt;&lt;200g\nc_charge_desc=Traffick Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Trans/Harm/Material to a Minor\nc_charge_desc=Trespass On School Grounds\nc_charge_desc=Trespass Other Struct/Conve\nc_charge_desc=Trespass Private Property\nc_charge_desc=Trespass Property w/Dang Weap\nc_charge_desc=Trespass Struct/Conveyance\nc_charge_desc=Trespass Structure w/Dang Weap\nc_charge_desc=Trespass Structure/Conveyance\nc_charge_desc=Trespassing/Construction Site\nc_charge_desc=Tresspass Struct/Conveyance\nc_charge_desc=Tresspass in Structure or Conveyance\nc_charge_desc=Unauth C/P/S Sounds&gt;1000/Audio\nc_charge_desc=Unauth Poss ID Card or DL\nc_charge_desc=Unauthorized Interf w/Railroad\nc_charge_desc=Unl/Disturb Education/Instui\nc_charge_desc=Unlaw Lic Use/Disply Of Others\nc_charge_desc=Unlaw LicTag/Sticker Attach\nc_charge_desc=Unlaw Use False Name/Identity\nc_charge_desc=Unlawful Conveyance of Fuel\nc_charge_desc=Unlicensed Telemarketing\nc_charge_desc=Use Computer for Child Exploit\nc_charge_desc=Use Of 2 Way Device To Fac Fel\nc_charge_desc=Use Scanning Device to Defraud\nc_charge_desc=Use of Anti-Shoplifting Device\nc_charge_desc=Uttering Forged Bills\nc_charge_desc=Uttering Forged Credit Card\nc_charge_desc=Uttering Worthless Check +$150\nc_charge_desc=Uttering a Forged Instrument\nc_charge_desc=Video Voyeur-&lt;24Y on Child &gt;16\nc_charge_desc=Viol Injunct Domestic Violence\nc_charge_desc=Viol Injunction Protect Dom Vi\nc_charge_desc=Viol Pretrial Release Dom Viol\nc_charge_desc=Viol Prot Injunc Repeat Viol\nc_charge_desc=Violation License Restrictions\nc_charge_desc=Violation Of Boater Safety Id\nc_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking\nc_charge_desc=Voyeurism\nc_charge_desc=arrest case no charge\n\n\n\n\n0\n0.0\n69.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n34.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n24.0\n0.0\n0.0\n0.0\n1.0\n4.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n44.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n41.0\n1.0\n0.0\n0.0\n0.0\n14.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6162\n0.0\n23.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6163\n0.0\n23.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6164\n0.0\n57.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6165\n1.0\n33.0\n0.0\n0.0\n0.0\n0.0\n3.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6166\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n6167 rows × 401 columns\n\n\n\n\ncompas = pd.read_csv(\"cox-violent-parsed.csv\")\ncompas_aif = pd.read_csv(\"compas_dataset.csv\")\n\n\nrow,column=compas.shape\nprint(f\"The compas dataset contain {row} rows and {column} columns\")\n\nThe compas dataset contain 18316 rows and 52 columns\n\n\n\ncompas.head()\n\n\n\n\n\n\n\n\nid\nname\nfirst\nlast\ncompas_screening_date\nsex\ndob\nage\nage_cat\nrace\njuv_fel_count\ndecile_score\njuv_misd_count\njuv_other_count\npriors_count\ndays_b_screening_arrest\nc_jail_in\nc_jail_out\nc_case_number\nc_offense_date\nc_arrest_date\nc_days_from_compas\nc_charge_degree\nc_charge_desc\nis_recid\nr_case_number\nr_charge_degree\nr_days_from_arrest\nr_offense_date\nr_charge_desc\nr_jail_in\nr_jail_out\nviolent_recid\nis_violent_recid\nvr_case_number\nvr_charge_degree\nvr_offense_date\nvr_charge_desc\ntype_of_assessment\ndecile_score.1\nscore_text\nscreening_date\nv_type_of_assessment\nv_decile_score\nv_score_text\nv_screening_date\nin_custody\nout_custody\npriors_count.1\nstart\nend\nevent\n\n\n\n\n0\n1.0\nmiguel hernandez\nmiguel\nhernandez\n14/08/2013\nMale\n18/04/1947\n69\nGreater than 45\nOther\n0\n1\n0\n0\n0\n-1.0\n13/08/2013 6:03\n14/08/2013 5:41\n13011352CF10A\n13/08/2013\nNaN\n1.0\n(F3)\nAggravated Assault w/Firearm\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\nNaN\nNaN\nRisk of Recidivism\n1\nLow\n14/08/2013\nRisk of Violence\n1\nLow\n14/08/2013\n07/07/2014\n14/07/2014\n0\n0\n327\n0\n\n\n1\n2.0\nmiguel hernandez\nmiguel\nhernandez\n14/08/2013\nMale\n18/04/1947\n69\nGreater than 45\nOther\n0\n1\n0\n0\n0\n-1.0\n13/08/2013 6:03\n14/08/2013 5:41\n13011352CF10A\n13/08/2013\nNaN\n1.0\n(F3)\nAggravated Assault w/Firearm\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\nNaN\nNaN\nRisk of Recidivism\n1\nLow\n14/08/2013\nRisk of Violence\n1\nLow\n14/08/2013\n07/07/2014\n14/07/2014\n0\n334\n961\n0\n\n\n2\n3.0\nmichael ryan\nmichael\nryan\n31/12/2014\nMale\n06/02/1985\n31\n25 - 45\nCaucasian\n0\n5\n0\n0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\nNaN\nNaN\nRisk of Recidivism\n5\nMedium\n31/12/2014\nRisk of Violence\n2\nLow\n31/12/2014\n30/12/2014\n03/01/2015\n0\n3\n457\n0\n\n\n3\n4.0\nkevon dixon\nkevon\ndixon\n27/01/2013\nMale\n22/01/1982\n34\n25 - 45\nAfrican-American\n0\n3\n0\n0\n0\n-1.0\n26/01/2013 3:45\n05/02/2013 5:36\n13001275CF10A\n26/01/2013\nNaN\n1.0\n(F3)\nFelony Battery w/Prior Convict\n1\n13009779CF10A\n(F3)\nNaN\n05/07/2013\nFelony Battery (Dom Strang)\nNaN\nNaN\nNaN\n1\n13009779CF10A\n(F3)\n05/07/2013\nFelony Battery (Dom Strang)\nRisk of Recidivism\n3\nLow\n27/01/2013\nRisk of Violence\n1\nLow\n27/01/2013\n26/01/2013\n05/02/2013\n0\n9\n159\n1\n\n\n4\n5.0\ned philo\ned\nphilo\n14/04/2013\nMale\n14/05/1991\n24\nLess than 25\nAfrican-American\n0\n4\n0\n1\n4\n-1.0\n13/04/2013 4:58\n14/04/2013 7:02\n13005330CF10A\n13/04/2013\nNaN\n1.0\n(F3)\nPossession of Cocaine\n1\n13011511MM10A\n(M1)\n0.0\n16/06/2013\nDriving Under The Influence\n16/06/2013\n16/06/2013\nNaN\n0\nNaN\nNaN\nNaN\nNaN\nRisk of Recidivism\n4\nLow\n14/04/2013\nRisk of Violence\n3\nLow\n14/04/2013\n16/06/2013\n16/06/2013\n4\n0\n63\n0\n\n\n\n\n\n\n\n\ndataset2_df.head()\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nage_cat=25 - 45\nage_cat=Greater than 45\nage_cat=Less than 25\nc_charge_degree=F\nc_charge_degree=M\nc_charge_desc=Abuse Without Great Harm\nc_charge_desc=Agg Abuse Elderlly/Disabled Adult\nc_charge_desc=Agg Assault W/int Com Fel Dome\nc_charge_desc=Agg Battery Grt/Bod/Harm\nc_charge_desc=Agg Fleeing and Eluding\nc_charge_desc=Agg Fleeing/Eluding High Speed\nc_charge_desc=Aggr Child Abuse-Torture,Punish\nc_charge_desc=Aggrav Battery w/Deadly Weapon\nc_charge_desc=Aggrav Child Abuse-Agg Battery\nc_charge_desc=Aggrav Child Abuse-Causes Harm\nc_charge_desc=Aggrav Stalking After Injunctn\nc_charge_desc=Aggravated Assault\nc_charge_desc=Aggravated Assault W/Dead Weap\nc_charge_desc=Aggravated Assault W/dead Weap\nc_charge_desc=Aggravated Assault W/o Firearm\nc_charge_desc=Aggravated Assault w/Firearm\nc_charge_desc=Aggravated Battery\nc_charge_desc=Aggravated Battery (Firearm)\nc_charge_desc=Aggravated Battery (Firearm/Actual Possession)\nc_charge_desc=Aggravated Battery / Pregnant\nc_charge_desc=Aggravated Battery On 65/Older\nc_charge_desc=Aide/Abet Prostitution Lewdness\nc_charge_desc=Aiding Escape\nc_charge_desc=Alcoholic Beverage Violation-FL\nc_charge_desc=Armed Trafficking in Cannabis\nc_charge_desc=Arson in the First Degree\nc_charge_desc=Assault\nc_charge_desc=Assault Law Enforcement Officer\nc_charge_desc=Att Burgl Conv Occp\nc_charge_desc=Att Burgl Struc/Conv Dwel/Occp\nc_charge_desc=Att Burgl Unoccupied Dwel\nc_charge_desc=Att Tamper w/Physical Evidence\nc_charge_desc=Attempt Armed Burglary Dwell\nc_charge_desc=Attempted Burg/Convey/Unocc\nc_charge_desc=Attempted Burg/struct/unocc\nc_charge_desc=Attempted Deliv Control Subst\nc_charge_desc=Attempted Robbery No Weapon\nc_charge_desc=Attempted Robbery Weapon\nc_charge_desc=Battery\nc_charge_desc=Battery Emergency Care Provide\nc_charge_desc=Battery On A Person Over 65\nc_charge_desc=Battery On Fire Fighter\nc_charge_desc=Battery On Parking Enfor Speci\nc_charge_desc=Battery Spouse Or Girlfriend\nc_charge_desc=Battery on Law Enforc Officer\nc_charge_desc=Battery on a Person Over 65\nc_charge_desc=Bribery Athletic Contests\nc_charge_desc=Burgl Dwel/Struct/Convey Armed\nc_charge_desc=Burglary Assault/Battery Armed\nc_charge_desc=Burglary Conveyance Armed\nc_charge_desc=Burglary Conveyance Assault/Bat\nc_charge_desc=Burglary Conveyance Occupied\nc_charge_desc=Burglary Conveyance Unoccup\nc_charge_desc=Burglary Dwelling Armed\nc_charge_desc=Burglary Dwelling Assault/Batt\nc_charge_desc=Burglary Dwelling Occupied\nc_charge_desc=Burglary Structure Assault/Batt\nc_charge_desc=Burglary Structure Occupied\nc_charge_desc=Burglary Structure Unoccup\nc_charge_desc=Burglary Unoccupied Dwelling\nc_charge_desc=Burglary With Assault/battery\nc_charge_desc=Carjacking w/o Deadly Weapon\nc_charge_desc=Carjacking with a Firearm\nc_charge_desc=Carry Open/Uncov Bev In Pub\nc_charge_desc=Carrying A Concealed Weapon\nc_charge_desc=Carrying Concealed Firearm\nc_charge_desc=Cash Item w/Intent to Defraud\nc_charge_desc=Child Abuse\nc_charge_desc=Computer Pornography\nc_charge_desc=Consp Traff Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Conspiracy Dealing Stolen Prop\nc_charge_desc=Consume Alcoholic Bev Pub\nc_charge_desc=Contradict Statement\nc_charge_desc=Contribute Delinquency Of A Minor\nc_charge_desc=Corrupt Public Servant\nc_charge_desc=Counterfeit Lic Plates/Sticker\nc_charge_desc=Crim Attempt/Solic/Consp\nc_charge_desc=Crim Use of Personal ID Info\nc_charge_desc=Crimin Mischief Damage $1000+\nc_charge_desc=Criminal Mischief\nc_charge_desc=Criminal Mischief Damage &lt;$200\nc_charge_desc=Criminal Mischief&gt;$200&lt;$1000\nc_charge_desc=Crlty Twrd Child Urge Oth Act\nc_charge_desc=Cruelty Toward Child\nc_charge_desc=Cruelty to Animals\nc_charge_desc=Culpable Negligence\nc_charge_desc=D.U.I. Serious Bodily Injury\nc_charge_desc=DOC/Cause Public Danger\nc_charge_desc=DUI - Enhanced\nc_charge_desc=DUI - Property Damage/Personal Injury\nc_charge_desc=DUI Blood Alcohol Above 0.20\nc_charge_desc=DUI Level 0.15 Or Minor In Veh\nc_charge_desc=DUI Property Damage/Injury\nc_charge_desc=DUI- Enhanced\nc_charge_desc=DUI/Property Damage/Persnl Inj\nc_charge_desc=DWI w/Inj Susp Lic / Habit Off\nc_charge_desc=DWLS Canceled Disqul 1st Off\nc_charge_desc=DWLS Susp/Cancel Revoked\nc_charge_desc=Dealing in Stolen Property\nc_charge_desc=Defrauding Innkeeper\nc_charge_desc=Defrauding Innkeeper $300/More\nc_charge_desc=Del 3,4 Methylenedioxymethcath\nc_charge_desc=Del Cannabis At/Near Park\nc_charge_desc=Del Cannabis For Consideration\nc_charge_desc=Del of JWH-250 2-Methox 1-Pentyl\nc_charge_desc=Deliver 3,4 Methylenediox\nc_charge_desc=Deliver Alprazolam\nc_charge_desc=Deliver Cannabis\nc_charge_desc=Deliver Cannabis 1000FTSch\nc_charge_desc=Deliver Cocaine\nc_charge_desc=Deliver Cocaine 1000FT Church\nc_charge_desc=Deliver Cocaine 1000FT Park\nc_charge_desc=Deliver Cocaine 1000FT School\nc_charge_desc=Deliver Cocaine 1000FT Store\nc_charge_desc=Delivery Of Drug Paraphernalia\nc_charge_desc=Delivery of 5-Fluoro PB-22\nc_charge_desc=Delivery of Heroin\nc_charge_desc=Depriv LEO of Protect/Communic\nc_charge_desc=Disorderly Conduct\nc_charge_desc=Disorderly Intoxication\nc_charge_desc=Disrupting School Function\nc_charge_desc=Drivg While Lic Suspd/Revk/Can\nc_charge_desc=Driving License Suspended\nc_charge_desc=Driving Under The Influence\nc_charge_desc=Driving While License Revoked\nc_charge_desc=Escape\nc_charge_desc=Exhibition Weapon School Prop\nc_charge_desc=Expired DL More Than 6 Months\nc_charge_desc=Exposes Culpable Negligence\nc_charge_desc=Extradition/Defendants\nc_charge_desc=Fabricating Physical Evidence\nc_charge_desc=Fail Register Vehicle\nc_charge_desc=Fail Sex Offend Report Bylaw\nc_charge_desc=Fail To Obey Police Officer\nc_charge_desc=Fail To Redeliv Hire/Leas Prop\nc_charge_desc=Failure To Pay Taxi Cab Charge\nc_charge_desc=Failure To Return Hired Vehicle\nc_charge_desc=False 911 Call\nc_charge_desc=False Bomb Report\nc_charge_desc=False Imprisonment\nc_charge_desc=False Info LEO During Invest\nc_charge_desc=False Motor Veh Insurance Card\nc_charge_desc=False Name By Person Arrest\nc_charge_desc=False Ownership Info/Pawn Item\nc_charge_desc=Falsely Impersonating Officer\nc_charge_desc=Fel Drive License Perm Revoke\nc_charge_desc=Felon in Pos of Firearm or Amm\nc_charge_desc=Felony Batt(Great Bodily Harm)\nc_charge_desc=Felony Battery\nc_charge_desc=Felony Battery (Dom Strang)\nc_charge_desc=Felony Battery w/Prior Convict\nc_charge_desc=Felony Committing Prostitution\nc_charge_desc=Felony DUI (level 3)\nc_charge_desc=Felony DUI - Enhanced\nc_charge_desc=Felony Driving While Lic Suspd\nc_charge_desc=Felony Petit Theft\nc_charge_desc=Felony/Driving Under Influence\nc_charge_desc=Fighting/Baiting Animals\nc_charge_desc=Fleeing Or Attmp Eluding A Leo\nc_charge_desc=Fleeing or Eluding a LEO\nc_charge_desc=Forging Bank Bills/Promis Note\nc_charge_desc=Fraudulent Use of Credit Card\nc_charge_desc=Grand Theft (Motor Vehicle)\nc_charge_desc=Grand Theft Dwell Property\nc_charge_desc=Grand Theft Firearm\nc_charge_desc=Grand Theft in the 1st Degree\nc_charge_desc=Grand Theft in the 3rd Degree\nc_charge_desc=Grand Theft of a Fire Extinquisher\nc_charge_desc=Grand Theft of the 2nd Degree\nc_charge_desc=Grand Theft on 65 Yr or Older\nc_charge_desc=Harass Witness/Victm/Informnt\nc_charge_desc=Harm Public Servant Or Family\nc_charge_desc=Hiring with Intent to Defraud\nc_charge_desc=Imperson Public Officer or Emplyee\nc_charge_desc=Interfere W/Traf Cont Dev RR\nc_charge_desc=Interference with Custody\nc_charge_desc=Intoxicated/Safety Of Another\nc_charge_desc=Introduce Contraband Into Jail\nc_charge_desc=Issuing a Worthless Draft\nc_charge_desc=Kidnapping / Domestic Violence\nc_charge_desc=Lease For Purpose Trafficking\nc_charge_desc=Leave Acc/Attend Veh/More $50\nc_charge_desc=Leave Accd/Attend Veh/Less $50\nc_charge_desc=Leaving Acc/Unattended Veh\nc_charge_desc=Leaving the Scene of Accident\nc_charge_desc=Lewd Act Presence Child 16-\nc_charge_desc=Lewd or Lascivious Molestation\nc_charge_desc=Lewd/Lasc Battery Pers 12+/&lt;16\nc_charge_desc=Lewd/Lasc Exhib Presence &lt;16yr\nc_charge_desc=Lewd/Lasciv Molest Elder Persn\nc_charge_desc=Lewdness Violation\nc_charge_desc=License Suspended Revoked\nc_charge_desc=Littering\nc_charge_desc=Live on Earnings of Prostitute\nc_charge_desc=Lve/Scen/Acc/Veh/Prop/Damage\nc_charge_desc=Manage Busn W/O City Occup Lic\nc_charge_desc=Manslaughter W/Weapon/Firearm\nc_charge_desc=Manufacture Cannabis\nc_charge_desc=Misuse Of 911 Or E911 System\nc_charge_desc=Money Launder 100K or More Dols\nc_charge_desc=Murder In 2nd Degree W/firearm\nc_charge_desc=Murder in the First Degree\nc_charge_desc=Neglect Child / Bodily Harm\nc_charge_desc=Neglect Child / No Bodily Harm\nc_charge_desc=Neglect/Abuse Elderly Person\nc_charge_desc=Obstruct Fire Equipment\nc_charge_desc=Obstruct Officer W/Violence\nc_charge_desc=Obtain Control Substance By Fraud\nc_charge_desc=Offer Agree Secure For Lewd Act\nc_charge_desc=Offer Agree Secure/Lewd Act\nc_charge_desc=Offn Against Intellectual Prop\nc_charge_desc=Open Carrying Of Weapon\nc_charge_desc=Oper Motorcycle W/O Valid DL\nc_charge_desc=Operating W/O Valid License\nc_charge_desc=Opert With Susp DL 2nd Offens\nc_charge_desc=PL/Unlaw Use Credit Card\nc_charge_desc=Petit Theft\nc_charge_desc=Petit Theft $100- $300\nc_charge_desc=Pos Cannabis For Consideration\nc_charge_desc=Pos Cannabis W/Intent Sel/Del\nc_charge_desc=Pos Methylenedioxymethcath W/I/D/S\nc_charge_desc=Poss 3,4 MDMA (Ecstasy)\nc_charge_desc=Poss Alprazolam W/int Sell/Del\nc_charge_desc=Poss Anti-Shoplifting Device\nc_charge_desc=Poss Cntrft Contr Sub w/Intent\nc_charge_desc=Poss Cocaine/Intent To Del/Sel\nc_charge_desc=Poss Contr Subst W/o Prescript\nc_charge_desc=Poss Counterfeit Payment Inst\nc_charge_desc=Poss Drugs W/O A Prescription\nc_charge_desc=Poss F/Arm Delinq\nc_charge_desc=Poss Firearm W/Altered ID#\nc_charge_desc=Poss Meth/Diox/Meth/Amp (MDMA)\nc_charge_desc=Poss Of 1,4-Butanediol\nc_charge_desc=Poss Of Controlled Substance\nc_charge_desc=Poss Of RX Without RX\nc_charge_desc=Poss Oxycodone W/Int/Sell/Del\nc_charge_desc=Poss Pyrrolidinobutiophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone W/I/D/S\nc_charge_desc=Poss Similitude of Drivers Lic\nc_charge_desc=Poss Tetrahydrocannabinols\nc_charge_desc=Poss Unlaw Issue Driver Licenc\nc_charge_desc=Poss Unlaw Issue Id\nc_charge_desc=Poss Wep Conv Felon\nc_charge_desc=Poss of Cocaine W/I/D/S 1000FT Park\nc_charge_desc=Poss of Firearm by Convic Felo\nc_charge_desc=Poss of Methylethcathinone\nc_charge_desc=Poss/Sell/Del Cocaine 1000FT Sch\nc_charge_desc=Poss/Sell/Del/Man Amobarbital\nc_charge_desc=Poss/pur/sell/deliver Cocaine\nc_charge_desc=Poss3,4 Methylenedioxymethcath\nc_charge_desc=Posses/Disply Susp/Revk/Frd DL\nc_charge_desc=Possess Cannabis 1000FTSch\nc_charge_desc=Possess Cannabis/20 Grams Or Less\nc_charge_desc=Possess Controlled Substance\nc_charge_desc=Possess Countrfeit Credit Card\nc_charge_desc=Possess Drug Paraphernalia\nc_charge_desc=Possess Mot Veh W/Alt Vin #\nc_charge_desc=Possess Tobacco Product Under 18\nc_charge_desc=Possess Weapon On School Prop\nc_charge_desc=Possess w/I/Utter Forged Bills\nc_charge_desc=Possession Burglary Tools\nc_charge_desc=Possession Child Pornography\nc_charge_desc=Possession Firearm School Prop\nc_charge_desc=Possession Of 3,4Methylenediox\nc_charge_desc=Possession Of Alprazolam\nc_charge_desc=Possession Of Amphetamine\nc_charge_desc=Possession Of Anabolic Steroid\nc_charge_desc=Possession Of Buprenorphine\nc_charge_desc=Possession Of Carisoprodol\nc_charge_desc=Possession Of Clonazepam\nc_charge_desc=Possession Of Cocaine\nc_charge_desc=Possession Of Diazepam\nc_charge_desc=Possession Of Fentanyl\nc_charge_desc=Possession Of Heroin\nc_charge_desc=Possession Of Methamphetamine\nc_charge_desc=Possession Of Paraphernalia\nc_charge_desc=Possession Of Phentermine\nc_charge_desc=Possession of Alcohol Under 21\nc_charge_desc=Possession of Benzylpiperazine\nc_charge_desc=Possession of Butylone\nc_charge_desc=Possession of Cannabis\nc_charge_desc=Possession of Cocaine\nc_charge_desc=Possession of Codeine\nc_charge_desc=Possession of Ethylone\nc_charge_desc=Possession of Hydrocodone\nc_charge_desc=Possession of Hydromorphone\nc_charge_desc=Possession of LSD\nc_charge_desc=Possession of Methadone\nc_charge_desc=Possession of Morphine\nc_charge_desc=Possession of Oxycodone\nc_charge_desc=Possession of XLR11\nc_charge_desc=Principal In The First Degree\nc_charge_desc=Prostitution\nc_charge_desc=Prostitution/Lewd Act Assignation\nc_charge_desc=Prostitution/Lewdness/Assign\nc_charge_desc=Prowling/Loitering\nc_charge_desc=Purchase Cannabis\nc_charge_desc=Purchase/P/W/Int Cannabis\nc_charge_desc=Reckless Driving\nc_charge_desc=Refuse Submit Blood/Breath Test\nc_charge_desc=Refuse to Supply DNA Sample\nc_charge_desc=Resist Officer w/Violence\nc_charge_desc=Resist/Obstruct W/O Violence\nc_charge_desc=Retail Theft $300 1st Offense\nc_charge_desc=Retail Theft $300 2nd Offense\nc_charge_desc=Ride Tri-Rail Without Paying\nc_charge_desc=Robbery / No Weapon\nc_charge_desc=Robbery / Weapon\nc_charge_desc=Robbery Sudd Snatch No Weapon\nc_charge_desc=Robbery W/Deadly Weapon\nc_charge_desc=Robbery W/Firearm\nc_charge_desc=Sale/Del Cannabis At/Near Scho\nc_charge_desc=Sale/Del Counterfeit Cont Subs\nc_charge_desc=Sel/Pur/Mfr/Del Control Substa\nc_charge_desc=Sell or Offer for Sale Counterfeit Goods\nc_charge_desc=Sell/Man/Del Pos/w/int Heroin\nc_charge_desc=Sex Batt Faml/Cust Vict 12-17Y\nc_charge_desc=Sex Battery Deft 18+/Vict 11-\nc_charge_desc=Sex Offender Fail Comply W/Law\nc_charge_desc=Sexual Battery / Vict 12 Yrs +\nc_charge_desc=Sexual Performance by a Child\nc_charge_desc=Shoot In Occupied Dwell\nc_charge_desc=Shoot Into Vehicle\nc_charge_desc=Simulation of Legal Process\nc_charge_desc=Solic to Commit Battery\nc_charge_desc=Solicit Deliver Cocaine\nc_charge_desc=Solicit Purchase Cocaine\nc_charge_desc=Solicit To Deliver Cocaine\nc_charge_desc=Solicitation On Felony 3 Deg\nc_charge_desc=Soliciting For Prostitution\nc_charge_desc=Sound Articles Over 100\nc_charge_desc=Stalking\nc_charge_desc=Stalking (Aggravated)\nc_charge_desc=Strong Armed Robbery\nc_charge_desc=Structuring Transactions\nc_charge_desc=Susp Drivers Lic 1st Offense\nc_charge_desc=Tamper With Victim\nc_charge_desc=Tamper With Witness\nc_charge_desc=Tamper With Witness/Victim/CI\nc_charge_desc=Tampering With Physical Evidence\nc_charge_desc=Tampering with a Victim\nc_charge_desc=Theft/To Deprive\nc_charge_desc=Threat Public Servant\nc_charge_desc=Throw Deadly Missile Into Veh\nc_charge_desc=Throw In Occupied Dwell\nc_charge_desc=Throw Missile Into Pub/Priv Dw\nc_charge_desc=Traff In Cocaine &lt;400g&gt;150 Kil\nc_charge_desc=Traffic Counterfeit Cred Cards\nc_charge_desc=Traffick Amphetamine 28g&gt;&lt;200g\nc_charge_desc=Traffick Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Trans/Harm/Material to a Minor\nc_charge_desc=Trespass On School Grounds\nc_charge_desc=Trespass Other Struct/Conve\nc_charge_desc=Trespass Private Property\nc_charge_desc=Trespass Property w/Dang Weap\nc_charge_desc=Trespass Struct/Conveyance\nc_charge_desc=Trespass Structure w/Dang Weap\nc_charge_desc=Trespass Structure/Conveyance\nc_charge_desc=Trespassing/Construction Site\nc_charge_desc=Tresspass Struct/Conveyance\nc_charge_desc=Tresspass in Structure or Conveyance\nc_charge_desc=Unauth C/P/S Sounds&gt;1000/Audio\nc_charge_desc=Unauth Poss ID Card or DL\nc_charge_desc=Unauthorized Interf w/Railroad\nc_charge_desc=Unl/Disturb Education/Instui\nc_charge_desc=Unlaw Lic Use/Disply Of Others\nc_charge_desc=Unlaw LicTag/Sticker Attach\nc_charge_desc=Unlaw Use False Name/Identity\nc_charge_desc=Unlawful Conveyance of Fuel\nc_charge_desc=Unlicensed Telemarketing\nc_charge_desc=Use Computer for Child Exploit\nc_charge_desc=Use Of 2 Way Device To Fac Fel\nc_charge_desc=Use Scanning Device to Defraud\nc_charge_desc=Use of Anti-Shoplifting Device\nc_charge_desc=Uttering Forged Bills\nc_charge_desc=Uttering Forged Credit Card\nc_charge_desc=Uttering Worthless Check +$150\nc_charge_desc=Uttering a Forged Instrument\nc_charge_desc=Video Voyeur-&lt;24Y on Child &gt;16\nc_charge_desc=Viol Injunct Domestic Violence\nc_charge_desc=Viol Injunction Protect Dom Vi\nc_charge_desc=Viol Pretrial Release Dom Viol\nc_charge_desc=Viol Prot Injunc Repeat Viol\nc_charge_desc=Violation License Restrictions\nc_charge_desc=Violation Of Boater Safety Id\nc_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking\nc_charge_desc=Voyeurism\nc_charge_desc=arrest case no charge\n\n\n\n\n0\n0.0\n69.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n34.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n24.0\n0.0\n0.0\n0.0\n1.0\n4.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n44.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n41.0\n1.0\n0.0\n0.0\n0.0\n14.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ncompas.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18316 entries, 0 to 18315\nData columns (total 52 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       11001 non-null  float64\n 1   name                     18316 non-null  object \n 2   first                    18316 non-null  object \n 3   last                     18316 non-null  object \n 4   compas_screening_date    18316 non-null  object \n 5   sex                      18316 non-null  object \n 6   dob                      18316 non-null  object \n 7   age                      18316 non-null  int64  \n 8   age_cat                  18316 non-null  object \n 9   race                     18316 non-null  object \n 10  juv_fel_count            18316 non-null  int64  \n 11  decile_score             18316 non-null  int64  \n 12  juv_misd_count           18316 non-null  int64  \n 13  juv_other_count          18316 non-null  int64  \n 14  priors_count             18316 non-null  int64  \n 15  days_b_screening_arrest  17019 non-null  float64\n 16  c_jail_in                17019 non-null  object \n 17  c_jail_out               17019 non-null  object \n 18  c_case_number            17449 non-null  object \n 19  c_offense_date           14364 non-null  object \n 20  c_arrest_date            3085 non-null   object \n 21  c_days_from_compas       17449 non-null  float64\n 22  c_charge_degree          17449 non-null  object \n 23  c_charge_desc            17435 non-null  object \n 24  is_recid                 18316 non-null  int64  \n 25  r_case_number            8417 non-null   object \n 26  r_charge_degree          8417 non-null   object \n 27  r_days_from_arrest       6359 non-null   float64\n 28  r_offense_date           8417 non-null   object \n 29  r_charge_desc            8277 non-null   object \n 30  r_jail_in                6359 non-null   object \n 31  r_jail_out               6359 non-null   object \n 32  violent_recid            0 non-null      float64\n 33  is_violent_recid         18316 non-null  int64  \n 34  vr_case_number           1339 non-null   object \n 35  vr_charge_degree         1339 non-null   object \n 36  vr_offense_date          1339 non-null   object \n 37  vr_charge_desc           1339 non-null   object \n 38  type_of_assessment       18316 non-null  object \n 39  decile_score.1           18316 non-null  int64  \n 40  score_text               18293 non-null  object \n 41  screening_date           18316 non-null  object \n 42  v_type_of_assessment     18316 non-null  object \n 43  v_decile_score           18316 non-null  int64  \n 44  v_score_text             18310 non-null  object \n 45  v_screening_date         18316 non-null  object \n 46  in_custody               17722 non-null  object \n 47  out_custody              17722 non-null  object \n 48  priors_count.1           18316 non-null  int64  \n 49  start                    18316 non-null  int64  \n 50  end                      18316 non-null  int64  \n 51  event                    18316 non-null  int64  \ndtypes: float64(5), int64(14), object(33)\nmemory usage: 7.3+ MB\n\n\n\n\n#This drop({[]}) will remove all the column listed that are not needed for the analysis\n\ncompas = compas.drop(['last','first','out_custody','in_custody','c_offense_date','decile_score.1','priors_count.1','c_case_number','days_b_screening_arrest','start','end','event','screening_date','c_case_number','juv_other_count','juv_misd_count','juv_fel_count','r_days_from_arrest','id','r_charge_degree','r_offense_date', 'vr_case_number','r_case_number','r_jail_out','c_arrest_date','r_charge_desc','r_jail_in', 'violent_recid','vr_charge_degree','vr_offense_date','vr_charge_desc'], axis=1)\n\n\ndataset2_df.head()\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nage_cat=25 - 45\nage_cat=Greater than 45\nage_cat=Less than 25\nc_charge_degree=F\nc_charge_degree=M\nc_charge_desc=Abuse Without Great Harm\nc_charge_desc=Agg Abuse Elderlly/Disabled Adult\nc_charge_desc=Agg Assault W/int Com Fel Dome\nc_charge_desc=Agg Battery Grt/Bod/Harm\nc_charge_desc=Agg Fleeing and Eluding\nc_charge_desc=Agg Fleeing/Eluding High Speed\nc_charge_desc=Aggr Child Abuse-Torture,Punish\nc_charge_desc=Aggrav Battery w/Deadly Weapon\nc_charge_desc=Aggrav Child Abuse-Agg Battery\nc_charge_desc=Aggrav Child Abuse-Causes Harm\nc_charge_desc=Aggrav Stalking After Injunctn\nc_charge_desc=Aggravated Assault\nc_charge_desc=Aggravated Assault W/Dead Weap\nc_charge_desc=Aggravated Assault W/dead Weap\nc_charge_desc=Aggravated Assault W/o Firearm\nc_charge_desc=Aggravated Assault w/Firearm\nc_charge_desc=Aggravated Battery\nc_charge_desc=Aggravated Battery (Firearm)\nc_charge_desc=Aggravated Battery (Firearm/Actual Possession)\nc_charge_desc=Aggravated Battery / Pregnant\nc_charge_desc=Aggravated Battery On 65/Older\nc_charge_desc=Aide/Abet Prostitution Lewdness\nc_charge_desc=Aiding Escape\nc_charge_desc=Alcoholic Beverage Violation-FL\nc_charge_desc=Armed Trafficking in Cannabis\nc_charge_desc=Arson in the First Degree\nc_charge_desc=Assault\nc_charge_desc=Assault Law Enforcement Officer\nc_charge_desc=Att Burgl Conv Occp\nc_charge_desc=Att Burgl Struc/Conv Dwel/Occp\nc_charge_desc=Att Burgl Unoccupied Dwel\nc_charge_desc=Att Tamper w/Physical Evidence\nc_charge_desc=Attempt Armed Burglary Dwell\nc_charge_desc=Attempted Burg/Convey/Unocc\nc_charge_desc=Attempted Burg/struct/unocc\nc_charge_desc=Attempted Deliv Control Subst\nc_charge_desc=Attempted Robbery No Weapon\nc_charge_desc=Attempted Robbery Weapon\nc_charge_desc=Battery\nc_charge_desc=Battery Emergency Care Provide\nc_charge_desc=Battery On A Person Over 65\nc_charge_desc=Battery On Fire Fighter\nc_charge_desc=Battery On Parking Enfor Speci\nc_charge_desc=Battery Spouse Or Girlfriend\nc_charge_desc=Battery on Law Enforc Officer\nc_charge_desc=Battery on a Person Over 65\nc_charge_desc=Bribery Athletic Contests\nc_charge_desc=Burgl Dwel/Struct/Convey Armed\nc_charge_desc=Burglary Assault/Battery Armed\nc_charge_desc=Burglary Conveyance Armed\nc_charge_desc=Burglary Conveyance Assault/Bat\nc_charge_desc=Burglary Conveyance Occupied\nc_charge_desc=Burglary Conveyance Unoccup\nc_charge_desc=Burglary Dwelling Armed\nc_charge_desc=Burglary Dwelling Assault/Batt\nc_charge_desc=Burglary Dwelling Occupied\nc_charge_desc=Burglary Structure Assault/Batt\nc_charge_desc=Burglary Structure Occupied\nc_charge_desc=Burglary Structure Unoccup\nc_charge_desc=Burglary Unoccupied Dwelling\nc_charge_desc=Burglary With Assault/battery\nc_charge_desc=Carjacking w/o Deadly Weapon\nc_charge_desc=Carjacking with a Firearm\nc_charge_desc=Carry Open/Uncov Bev In Pub\nc_charge_desc=Carrying A Concealed Weapon\nc_charge_desc=Carrying Concealed Firearm\nc_charge_desc=Cash Item w/Intent to Defraud\nc_charge_desc=Child Abuse\nc_charge_desc=Computer Pornography\nc_charge_desc=Consp Traff Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Conspiracy Dealing Stolen Prop\nc_charge_desc=Consume Alcoholic Bev Pub\nc_charge_desc=Contradict Statement\nc_charge_desc=Contribute Delinquency Of A Minor\nc_charge_desc=Corrupt Public Servant\nc_charge_desc=Counterfeit Lic Plates/Sticker\nc_charge_desc=Crim Attempt/Solic/Consp\nc_charge_desc=Crim Use of Personal ID Info\nc_charge_desc=Crimin Mischief Damage $1000+\nc_charge_desc=Criminal Mischief\nc_charge_desc=Criminal Mischief Damage &lt;$200\nc_charge_desc=Criminal Mischief&gt;$200&lt;$1000\nc_charge_desc=Crlty Twrd Child Urge Oth Act\nc_charge_desc=Cruelty Toward Child\nc_charge_desc=Cruelty to Animals\nc_charge_desc=Culpable Negligence\nc_charge_desc=D.U.I. Serious Bodily Injury\nc_charge_desc=DOC/Cause Public Danger\nc_charge_desc=DUI - Enhanced\nc_charge_desc=DUI - Property Damage/Personal Injury\nc_charge_desc=DUI Blood Alcohol Above 0.20\nc_charge_desc=DUI Level 0.15 Or Minor In Veh\nc_charge_desc=DUI Property Damage/Injury\nc_charge_desc=DUI- Enhanced\nc_charge_desc=DUI/Property Damage/Persnl Inj\nc_charge_desc=DWI w/Inj Susp Lic / Habit Off\nc_charge_desc=DWLS Canceled Disqul 1st Off\nc_charge_desc=DWLS Susp/Cancel Revoked\nc_charge_desc=Dealing in Stolen Property\nc_charge_desc=Defrauding Innkeeper\nc_charge_desc=Defrauding Innkeeper $300/More\nc_charge_desc=Del 3,4 Methylenedioxymethcath\nc_charge_desc=Del Cannabis At/Near Park\nc_charge_desc=Del Cannabis For Consideration\nc_charge_desc=Del of JWH-250 2-Methox 1-Pentyl\nc_charge_desc=Deliver 3,4 Methylenediox\nc_charge_desc=Deliver Alprazolam\nc_charge_desc=Deliver Cannabis\nc_charge_desc=Deliver Cannabis 1000FTSch\nc_charge_desc=Deliver Cocaine\nc_charge_desc=Deliver Cocaine 1000FT Church\nc_charge_desc=Deliver Cocaine 1000FT Park\nc_charge_desc=Deliver Cocaine 1000FT School\nc_charge_desc=Deliver Cocaine 1000FT Store\nc_charge_desc=Delivery Of Drug Paraphernalia\nc_charge_desc=Delivery of 5-Fluoro PB-22\nc_charge_desc=Delivery of Heroin\nc_charge_desc=Depriv LEO of Protect/Communic\nc_charge_desc=Disorderly Conduct\nc_charge_desc=Disorderly Intoxication\nc_charge_desc=Disrupting School Function\nc_charge_desc=Drivg While Lic Suspd/Revk/Can\nc_charge_desc=Driving License Suspended\nc_charge_desc=Driving Under The Influence\nc_charge_desc=Driving While License Revoked\nc_charge_desc=Escape\nc_charge_desc=Exhibition Weapon School Prop\nc_charge_desc=Expired DL More Than 6 Months\nc_charge_desc=Exposes Culpable Negligence\nc_charge_desc=Extradition/Defendants\nc_charge_desc=Fabricating Physical Evidence\nc_charge_desc=Fail Register Vehicle\nc_charge_desc=Fail Sex Offend Report Bylaw\nc_charge_desc=Fail To Obey Police Officer\nc_charge_desc=Fail To Redeliv Hire/Leas Prop\nc_charge_desc=Failure To Pay Taxi Cab Charge\nc_charge_desc=Failure To Return Hired Vehicle\nc_charge_desc=False 911 Call\nc_charge_desc=False Bomb Report\nc_charge_desc=False Imprisonment\nc_charge_desc=False Info LEO During Invest\nc_charge_desc=False Motor Veh Insurance Card\nc_charge_desc=False Name By Person Arrest\nc_charge_desc=False Ownership Info/Pawn Item\nc_charge_desc=Falsely Impersonating Officer\nc_charge_desc=Fel Drive License Perm Revoke\nc_charge_desc=Felon in Pos of Firearm or Amm\nc_charge_desc=Felony Batt(Great Bodily Harm)\nc_charge_desc=Felony Battery\nc_charge_desc=Felony Battery (Dom Strang)\nc_charge_desc=Felony Battery w/Prior Convict\nc_charge_desc=Felony Committing Prostitution\nc_charge_desc=Felony DUI (level 3)\nc_charge_desc=Felony DUI - Enhanced\nc_charge_desc=Felony Driving While Lic Suspd\nc_charge_desc=Felony Petit Theft\nc_charge_desc=Felony/Driving Under Influence\nc_charge_desc=Fighting/Baiting Animals\nc_charge_desc=Fleeing Or Attmp Eluding A Leo\nc_charge_desc=Fleeing or Eluding a LEO\nc_charge_desc=Forging Bank Bills/Promis Note\nc_charge_desc=Fraudulent Use of Credit Card\nc_charge_desc=Grand Theft (Motor Vehicle)\nc_charge_desc=Grand Theft Dwell Property\nc_charge_desc=Grand Theft Firearm\nc_charge_desc=Grand Theft in the 1st Degree\nc_charge_desc=Grand Theft in the 3rd Degree\nc_charge_desc=Grand Theft of a Fire Extinquisher\nc_charge_desc=Grand Theft of the 2nd Degree\nc_charge_desc=Grand Theft on 65 Yr or Older\nc_charge_desc=Harass Witness/Victm/Informnt\nc_charge_desc=Harm Public Servant Or Family\nc_charge_desc=Hiring with Intent to Defraud\nc_charge_desc=Imperson Public Officer or Emplyee\nc_charge_desc=Interfere W/Traf Cont Dev RR\nc_charge_desc=Interference with Custody\nc_charge_desc=Intoxicated/Safety Of Another\nc_charge_desc=Introduce Contraband Into Jail\nc_charge_desc=Issuing a Worthless Draft\nc_charge_desc=Kidnapping / Domestic Violence\nc_charge_desc=Lease For Purpose Trafficking\nc_charge_desc=Leave Acc/Attend Veh/More $50\nc_charge_desc=Leave Accd/Attend Veh/Less $50\nc_charge_desc=Leaving Acc/Unattended Veh\nc_charge_desc=Leaving the Scene of Accident\nc_charge_desc=Lewd Act Presence Child 16-\nc_charge_desc=Lewd or Lascivious Molestation\nc_charge_desc=Lewd/Lasc Battery Pers 12+/&lt;16\nc_charge_desc=Lewd/Lasc Exhib Presence &lt;16yr\nc_charge_desc=Lewd/Lasciv Molest Elder Persn\nc_charge_desc=Lewdness Violation\nc_charge_desc=License Suspended Revoked\nc_charge_desc=Littering\nc_charge_desc=Live on Earnings of Prostitute\nc_charge_desc=Lve/Scen/Acc/Veh/Prop/Damage\nc_charge_desc=Manage Busn W/O City Occup Lic\nc_charge_desc=Manslaughter W/Weapon/Firearm\nc_charge_desc=Manufacture Cannabis\nc_charge_desc=Misuse Of 911 Or E911 System\nc_charge_desc=Money Launder 100K or More Dols\nc_charge_desc=Murder In 2nd Degree W/firearm\nc_charge_desc=Murder in the First Degree\nc_charge_desc=Neglect Child / Bodily Harm\nc_charge_desc=Neglect Child / No Bodily Harm\nc_charge_desc=Neglect/Abuse Elderly Person\nc_charge_desc=Obstruct Fire Equipment\nc_charge_desc=Obstruct Officer W/Violence\nc_charge_desc=Obtain Control Substance By Fraud\nc_charge_desc=Offer Agree Secure For Lewd Act\nc_charge_desc=Offer Agree Secure/Lewd Act\nc_charge_desc=Offn Against Intellectual Prop\nc_charge_desc=Open Carrying Of Weapon\nc_charge_desc=Oper Motorcycle W/O Valid DL\nc_charge_desc=Operating W/O Valid License\nc_charge_desc=Opert With Susp DL 2nd Offens\nc_charge_desc=PL/Unlaw Use Credit Card\nc_charge_desc=Petit Theft\nc_charge_desc=Petit Theft $100- $300\nc_charge_desc=Pos Cannabis For Consideration\nc_charge_desc=Pos Cannabis W/Intent Sel/Del\nc_charge_desc=Pos Methylenedioxymethcath W/I/D/S\nc_charge_desc=Poss 3,4 MDMA (Ecstasy)\nc_charge_desc=Poss Alprazolam W/int Sell/Del\nc_charge_desc=Poss Anti-Shoplifting Device\nc_charge_desc=Poss Cntrft Contr Sub w/Intent\nc_charge_desc=Poss Cocaine/Intent To Del/Sel\nc_charge_desc=Poss Contr Subst W/o Prescript\nc_charge_desc=Poss Counterfeit Payment Inst\nc_charge_desc=Poss Drugs W/O A Prescription\nc_charge_desc=Poss F/Arm Delinq\nc_charge_desc=Poss Firearm W/Altered ID#\nc_charge_desc=Poss Meth/Diox/Meth/Amp (MDMA)\nc_charge_desc=Poss Of 1,4-Butanediol\nc_charge_desc=Poss Of Controlled Substance\nc_charge_desc=Poss Of RX Without RX\nc_charge_desc=Poss Oxycodone W/Int/Sell/Del\nc_charge_desc=Poss Pyrrolidinobutiophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone W/I/D/S\nc_charge_desc=Poss Similitude of Drivers Lic\nc_charge_desc=Poss Tetrahydrocannabinols\nc_charge_desc=Poss Unlaw Issue Driver Licenc\nc_charge_desc=Poss Unlaw Issue Id\nc_charge_desc=Poss Wep Conv Felon\nc_charge_desc=Poss of Cocaine W/I/D/S 1000FT Park\nc_charge_desc=Poss of Firearm by Convic Felo\nc_charge_desc=Poss of Methylethcathinone\nc_charge_desc=Poss/Sell/Del Cocaine 1000FT Sch\nc_charge_desc=Poss/Sell/Del/Man Amobarbital\nc_charge_desc=Poss/pur/sell/deliver Cocaine\nc_charge_desc=Poss3,4 Methylenedioxymethcath\nc_charge_desc=Posses/Disply Susp/Revk/Frd DL\nc_charge_desc=Possess Cannabis 1000FTSch\nc_charge_desc=Possess Cannabis/20 Grams Or Less\nc_charge_desc=Possess Controlled Substance\nc_charge_desc=Possess Countrfeit Credit Card\nc_charge_desc=Possess Drug Paraphernalia\nc_charge_desc=Possess Mot Veh W/Alt Vin #\nc_charge_desc=Possess Tobacco Product Under 18\nc_charge_desc=Possess Weapon On School Prop\nc_charge_desc=Possess w/I/Utter Forged Bills\nc_charge_desc=Possession Burglary Tools\nc_charge_desc=Possession Child Pornography\nc_charge_desc=Possession Firearm School Prop\nc_charge_desc=Possession Of 3,4Methylenediox\nc_charge_desc=Possession Of Alprazolam\nc_charge_desc=Possession Of Amphetamine\nc_charge_desc=Possession Of Anabolic Steroid\nc_charge_desc=Possession Of Buprenorphine\nc_charge_desc=Possession Of Carisoprodol\nc_charge_desc=Possession Of Clonazepam\nc_charge_desc=Possession Of Cocaine\nc_charge_desc=Possession Of Diazepam\nc_charge_desc=Possession Of Fentanyl\nc_charge_desc=Possession Of Heroin\nc_charge_desc=Possession Of Methamphetamine\nc_charge_desc=Possession Of Paraphernalia\nc_charge_desc=Possession Of Phentermine\nc_charge_desc=Possession of Alcohol Under 21\nc_charge_desc=Possession of Benzylpiperazine\nc_charge_desc=Possession of Butylone\nc_charge_desc=Possession of Cannabis\nc_charge_desc=Possession of Cocaine\nc_charge_desc=Possession of Codeine\nc_charge_desc=Possession of Ethylone\nc_charge_desc=Possession of Hydrocodone\nc_charge_desc=Possession of Hydromorphone\nc_charge_desc=Possession of LSD\nc_charge_desc=Possession of Methadone\nc_charge_desc=Possession of Morphine\nc_charge_desc=Possession of Oxycodone\nc_charge_desc=Possession of XLR11\nc_charge_desc=Principal In The First Degree\nc_charge_desc=Prostitution\nc_charge_desc=Prostitution/Lewd Act Assignation\nc_charge_desc=Prostitution/Lewdness/Assign\nc_charge_desc=Prowling/Loitering\nc_charge_desc=Purchase Cannabis\nc_charge_desc=Purchase/P/W/Int Cannabis\nc_charge_desc=Reckless Driving\nc_charge_desc=Refuse Submit Blood/Breath Test\nc_charge_desc=Refuse to Supply DNA Sample\nc_charge_desc=Resist Officer w/Violence\nc_charge_desc=Resist/Obstruct W/O Violence\nc_charge_desc=Retail Theft $300 1st Offense\nc_charge_desc=Retail Theft $300 2nd Offense\nc_charge_desc=Ride Tri-Rail Without Paying\nc_charge_desc=Robbery / No Weapon\nc_charge_desc=Robbery / Weapon\nc_charge_desc=Robbery Sudd Snatch No Weapon\nc_charge_desc=Robbery W/Deadly Weapon\nc_charge_desc=Robbery W/Firearm\nc_charge_desc=Sale/Del Cannabis At/Near Scho\nc_charge_desc=Sale/Del Counterfeit Cont Subs\nc_charge_desc=Sel/Pur/Mfr/Del Control Substa\nc_charge_desc=Sell or Offer for Sale Counterfeit Goods\nc_charge_desc=Sell/Man/Del Pos/w/int Heroin\nc_charge_desc=Sex Batt Faml/Cust Vict 12-17Y\nc_charge_desc=Sex Battery Deft 18+/Vict 11-\nc_charge_desc=Sex Offender Fail Comply W/Law\nc_charge_desc=Sexual Battery / Vict 12 Yrs +\nc_charge_desc=Sexual Performance by a Child\nc_charge_desc=Shoot In Occupied Dwell\nc_charge_desc=Shoot Into Vehicle\nc_charge_desc=Simulation of Legal Process\nc_charge_desc=Solic to Commit Battery\nc_charge_desc=Solicit Deliver Cocaine\nc_charge_desc=Solicit Purchase Cocaine\nc_charge_desc=Solicit To Deliver Cocaine\nc_charge_desc=Solicitation On Felony 3 Deg\nc_charge_desc=Soliciting For Prostitution\nc_charge_desc=Sound Articles Over 100\nc_charge_desc=Stalking\nc_charge_desc=Stalking (Aggravated)\nc_charge_desc=Strong Armed Robbery\nc_charge_desc=Structuring Transactions\nc_charge_desc=Susp Drivers Lic 1st Offense\nc_charge_desc=Tamper With Victim\nc_charge_desc=Tamper With Witness\nc_charge_desc=Tamper With Witness/Victim/CI\nc_charge_desc=Tampering With Physical Evidence\nc_charge_desc=Tampering with a Victim\nc_charge_desc=Theft/To Deprive\nc_charge_desc=Threat Public Servant\nc_charge_desc=Throw Deadly Missile Into Veh\nc_charge_desc=Throw In Occupied Dwell\nc_charge_desc=Throw Missile Into Pub/Priv Dw\nc_charge_desc=Traff In Cocaine &lt;400g&gt;150 Kil\nc_charge_desc=Traffic Counterfeit Cred Cards\nc_charge_desc=Traffick Amphetamine 28g&gt;&lt;200g\nc_charge_desc=Traffick Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Trans/Harm/Material to a Minor\nc_charge_desc=Trespass On School Grounds\nc_charge_desc=Trespass Other Struct/Conve\nc_charge_desc=Trespass Private Property\nc_charge_desc=Trespass Property w/Dang Weap\nc_charge_desc=Trespass Struct/Conveyance\nc_charge_desc=Trespass Structure w/Dang Weap\nc_charge_desc=Trespass Structure/Conveyance\nc_charge_desc=Trespassing/Construction Site\nc_charge_desc=Tresspass Struct/Conveyance\nc_charge_desc=Tresspass in Structure or Conveyance\nc_charge_desc=Unauth C/P/S Sounds&gt;1000/Audio\nc_charge_desc=Unauth Poss ID Card or DL\nc_charge_desc=Unauthorized Interf w/Railroad\nc_charge_desc=Unl/Disturb Education/Instui\nc_charge_desc=Unlaw Lic Use/Disply Of Others\nc_charge_desc=Unlaw LicTag/Sticker Attach\nc_charge_desc=Unlaw Use False Name/Identity\nc_charge_desc=Unlawful Conveyance of Fuel\nc_charge_desc=Unlicensed Telemarketing\nc_charge_desc=Use Computer for Child Exploit\nc_charge_desc=Use Of 2 Way Device To Fac Fel\nc_charge_desc=Use Scanning Device to Defraud\nc_charge_desc=Use of Anti-Shoplifting Device\nc_charge_desc=Uttering Forged Bills\nc_charge_desc=Uttering Forged Credit Card\nc_charge_desc=Uttering Worthless Check +$150\nc_charge_desc=Uttering a Forged Instrument\nc_charge_desc=Video Voyeur-&lt;24Y on Child &gt;16\nc_charge_desc=Viol Injunct Domestic Violence\nc_charge_desc=Viol Injunction Protect Dom Vi\nc_charge_desc=Viol Pretrial Release Dom Viol\nc_charge_desc=Viol Prot Injunc Repeat Viol\nc_charge_desc=Violation License Restrictions\nc_charge_desc=Violation Of Boater Safety Id\nc_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking\nc_charge_desc=Voyeurism\nc_charge_desc=arrest case no charge\n\n\n\n\n0\n0.0\n69.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n34.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n24.0\n0.0\n0.0\n0.0\n1.0\n4.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n44.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n41.0\n1.0\n0.0\n0.0\n0.0\n14.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\ndataset2_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6167 entries, 0 to 6166\nColumns: 401 entries, sex to c_charge_desc=arrest case no charge\ndtypes: float64(401)\nmemory usage: 18.9 MB\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nage\nrace\njuv_fel_count\njuv_misd_count\njuv_other_count\npriors_count\nage_cat=25 - 45\nage_cat=Greater than 45\nage_cat=Less than 25\nc_charge_degree=F\nc_charge_degree=M\nc_charge_desc=Abuse Without Great Harm\nc_charge_desc=Agg Abuse Elderlly/Disabled Adult\nc_charge_desc=Agg Assault W/int Com Fel Dome\nc_charge_desc=Agg Battery Grt/Bod/Harm\nc_charge_desc=Agg Fleeing and Eluding\nc_charge_desc=Agg Fleeing/Eluding High Speed\nc_charge_desc=Aggr Child Abuse-Torture,Punish\nc_charge_desc=Aggrav Battery w/Deadly Weapon\nc_charge_desc=Aggrav Child Abuse-Agg Battery\nc_charge_desc=Aggrav Child Abuse-Causes Harm\nc_charge_desc=Aggrav Stalking After Injunctn\nc_charge_desc=Aggravated Assault\nc_charge_desc=Aggravated Assault W/Dead Weap\nc_charge_desc=Aggravated Assault W/dead Weap\nc_charge_desc=Aggravated Assault W/o Firearm\nc_charge_desc=Aggravated Assault w/Firearm\nc_charge_desc=Aggravated Battery\nc_charge_desc=Aggravated Battery (Firearm)\nc_charge_desc=Aggravated Battery (Firearm/Actual Possession)\nc_charge_desc=Aggravated Battery / Pregnant\nc_charge_desc=Aggravated Battery On 65/Older\nc_charge_desc=Aide/Abet Prostitution Lewdness\nc_charge_desc=Aiding Escape\nc_charge_desc=Alcoholic Beverage Violation-FL\nc_charge_desc=Armed Trafficking in Cannabis\nc_charge_desc=Arson in the First Degree\nc_charge_desc=Assault\nc_charge_desc=Assault Law Enforcement Officer\nc_charge_desc=Att Burgl Conv Occp\nc_charge_desc=Att Burgl Struc/Conv Dwel/Occp\nc_charge_desc=Att Burgl Unoccupied Dwel\nc_charge_desc=Att Tamper w/Physical Evidence\nc_charge_desc=Attempt Armed Burglary Dwell\nc_charge_desc=Attempted Burg/Convey/Unocc\nc_charge_desc=Attempted Burg/struct/unocc\nc_charge_desc=Attempted Deliv Control Subst\nc_charge_desc=Attempted Robbery No Weapon\nc_charge_desc=Attempted Robbery Weapon\nc_charge_desc=Battery\nc_charge_desc=Battery Emergency Care Provide\nc_charge_desc=Battery On A Person Over 65\nc_charge_desc=Battery On Fire Fighter\nc_charge_desc=Battery On Parking Enfor Speci\nc_charge_desc=Battery Spouse Or Girlfriend\nc_charge_desc=Battery on Law Enforc Officer\nc_charge_desc=Battery on a Person Over 65\nc_charge_desc=Bribery Athletic Contests\nc_charge_desc=Burgl Dwel/Struct/Convey Armed\nc_charge_desc=Burglary Assault/Battery Armed\nc_charge_desc=Burglary Conveyance Armed\nc_charge_desc=Burglary Conveyance Assault/Bat\nc_charge_desc=Burglary Conveyance Occupied\nc_charge_desc=Burglary Conveyance Unoccup\nc_charge_desc=Burglary Dwelling Armed\nc_charge_desc=Burglary Dwelling Assault/Batt\nc_charge_desc=Burglary Dwelling Occupied\nc_charge_desc=Burglary Structure Assault/Batt\nc_charge_desc=Burglary Structure Occupied\nc_charge_desc=Burglary Structure Unoccup\nc_charge_desc=Burglary Unoccupied Dwelling\nc_charge_desc=Burglary With Assault/battery\nc_charge_desc=Carjacking w/o Deadly Weapon\nc_charge_desc=Carjacking with a Firearm\nc_charge_desc=Carry Open/Uncov Bev In Pub\nc_charge_desc=Carrying A Concealed Weapon\nc_charge_desc=Carrying Concealed Firearm\nc_charge_desc=Cash Item w/Intent to Defraud\nc_charge_desc=Child Abuse\nc_charge_desc=Computer Pornography\nc_charge_desc=Consp Traff Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Conspiracy Dealing Stolen Prop\nc_charge_desc=Consume Alcoholic Bev Pub\nc_charge_desc=Contradict Statement\nc_charge_desc=Contribute Delinquency Of A Minor\nc_charge_desc=Corrupt Public Servant\nc_charge_desc=Counterfeit Lic Plates/Sticker\nc_charge_desc=Crim Attempt/Solic/Consp\nc_charge_desc=Crim Use of Personal ID Info\nc_charge_desc=Crimin Mischief Damage $1000+\nc_charge_desc=Criminal Mischief\nc_charge_desc=Criminal Mischief Damage &lt;$200\nc_charge_desc=Criminal Mischief&gt;$200&lt;$1000\nc_charge_desc=Crlty Twrd Child Urge Oth Act\nc_charge_desc=Cruelty Toward Child\nc_charge_desc=Cruelty to Animals\nc_charge_desc=Culpable Negligence\nc_charge_desc=D.U.I. Serious Bodily Injury\nc_charge_desc=DOC/Cause Public Danger\nc_charge_desc=DUI - Enhanced\nc_charge_desc=DUI - Property Damage/Personal Injury\nc_charge_desc=DUI Blood Alcohol Above 0.20\nc_charge_desc=DUI Level 0.15 Or Minor In Veh\nc_charge_desc=DUI Property Damage/Injury\nc_charge_desc=DUI- Enhanced\nc_charge_desc=DUI/Property Damage/Persnl Inj\nc_charge_desc=DWI w/Inj Susp Lic / Habit Off\nc_charge_desc=DWLS Canceled Disqul 1st Off\nc_charge_desc=DWLS Susp/Cancel Revoked\nc_charge_desc=Dealing in Stolen Property\nc_charge_desc=Defrauding Innkeeper\nc_charge_desc=Defrauding Innkeeper $300/More\nc_charge_desc=Del 3,4 Methylenedioxymethcath\nc_charge_desc=Del Cannabis At/Near Park\nc_charge_desc=Del Cannabis For Consideration\nc_charge_desc=Del of JWH-250 2-Methox 1-Pentyl\nc_charge_desc=Deliver 3,4 Methylenediox\nc_charge_desc=Deliver Alprazolam\nc_charge_desc=Deliver Cannabis\nc_charge_desc=Deliver Cannabis 1000FTSch\nc_charge_desc=Deliver Cocaine\nc_charge_desc=Deliver Cocaine 1000FT Church\nc_charge_desc=Deliver Cocaine 1000FT Park\nc_charge_desc=Deliver Cocaine 1000FT School\nc_charge_desc=Deliver Cocaine 1000FT Store\nc_charge_desc=Delivery Of Drug Paraphernalia\nc_charge_desc=Delivery of 5-Fluoro PB-22\nc_charge_desc=Delivery of Heroin\nc_charge_desc=Depriv LEO of Protect/Communic\nc_charge_desc=Disorderly Conduct\nc_charge_desc=Disorderly Intoxication\nc_charge_desc=Disrupting School Function\nc_charge_desc=Drivg While Lic Suspd/Revk/Can\nc_charge_desc=Driving License Suspended\nc_charge_desc=Driving Under The Influence\nc_charge_desc=Driving While License Revoked\nc_charge_desc=Escape\nc_charge_desc=Exhibition Weapon School Prop\nc_charge_desc=Expired DL More Than 6 Months\nc_charge_desc=Exposes Culpable Negligence\nc_charge_desc=Extradition/Defendants\nc_charge_desc=Fabricating Physical Evidence\nc_charge_desc=Fail Register Vehicle\nc_charge_desc=Fail Sex Offend Report Bylaw\nc_charge_desc=Fail To Obey Police Officer\nc_charge_desc=Fail To Redeliv Hire/Leas Prop\nc_charge_desc=Failure To Pay Taxi Cab Charge\nc_charge_desc=Failure To Return Hired Vehicle\nc_charge_desc=False 911 Call\nc_charge_desc=False Bomb Report\nc_charge_desc=False Imprisonment\nc_charge_desc=False Info LEO During Invest\nc_charge_desc=False Motor Veh Insurance Card\nc_charge_desc=False Name By Person Arrest\nc_charge_desc=False Ownership Info/Pawn Item\nc_charge_desc=Falsely Impersonating Officer\nc_charge_desc=Fel Drive License Perm Revoke\nc_charge_desc=Felon in Pos of Firearm or Amm\nc_charge_desc=Felony Batt(Great Bodily Harm)\nc_charge_desc=Felony Battery\nc_charge_desc=Felony Battery (Dom Strang)\nc_charge_desc=Felony Battery w/Prior Convict\nc_charge_desc=Felony Committing Prostitution\nc_charge_desc=Felony DUI (level 3)\nc_charge_desc=Felony DUI - Enhanced\nc_charge_desc=Felony Driving While Lic Suspd\nc_charge_desc=Felony Petit Theft\nc_charge_desc=Felony/Driving Under Influence\nc_charge_desc=Fighting/Baiting Animals\nc_charge_desc=Fleeing Or Attmp Eluding A Leo\nc_charge_desc=Fleeing or Eluding a LEO\nc_charge_desc=Forging Bank Bills/Promis Note\nc_charge_desc=Fraudulent Use of Credit Card\nc_charge_desc=Grand Theft (Motor Vehicle)\nc_charge_desc=Grand Theft Dwell Property\nc_charge_desc=Grand Theft Firearm\nc_charge_desc=Grand Theft in the 1st Degree\nc_charge_desc=Grand Theft in the 3rd Degree\nc_charge_desc=Grand Theft of a Fire Extinquisher\nc_charge_desc=Grand Theft of the 2nd Degree\nc_charge_desc=Grand Theft on 65 Yr or Older\nc_charge_desc=Harass Witness/Victm/Informnt\nc_charge_desc=Harm Public Servant Or Family\nc_charge_desc=Hiring with Intent to Defraud\nc_charge_desc=Imperson Public Officer or Emplyee\nc_charge_desc=Interfere W/Traf Cont Dev RR\nc_charge_desc=Interference with Custody\nc_charge_desc=Intoxicated/Safety Of Another\nc_charge_desc=Introduce Contraband Into Jail\nc_charge_desc=Issuing a Worthless Draft\nc_charge_desc=Kidnapping / Domestic Violence\nc_charge_desc=Lease For Purpose Trafficking\nc_charge_desc=Leave Acc/Attend Veh/More $50\nc_charge_desc=Leave Accd/Attend Veh/Less $50\nc_charge_desc=Leaving Acc/Unattended Veh\nc_charge_desc=Leaving the Scene of Accident\nc_charge_desc=Lewd Act Presence Child 16-\nc_charge_desc=Lewd or Lascivious Molestation\nc_charge_desc=Lewd/Lasc Battery Pers 12+/&lt;16\nc_charge_desc=Lewd/Lasc Exhib Presence &lt;16yr\nc_charge_desc=Lewd/Lasciv Molest Elder Persn\nc_charge_desc=Lewdness Violation\nc_charge_desc=License Suspended Revoked\nc_charge_desc=Littering\nc_charge_desc=Live on Earnings of Prostitute\nc_charge_desc=Lve/Scen/Acc/Veh/Prop/Damage\nc_charge_desc=Manage Busn W/O City Occup Lic\nc_charge_desc=Manslaughter W/Weapon/Firearm\nc_charge_desc=Manufacture Cannabis\nc_charge_desc=Misuse Of 911 Or E911 System\nc_charge_desc=Money Launder 100K or More Dols\nc_charge_desc=Murder In 2nd Degree W/firearm\nc_charge_desc=Murder in the First Degree\nc_charge_desc=Neglect Child / Bodily Harm\nc_charge_desc=Neglect Child / No Bodily Harm\nc_charge_desc=Neglect/Abuse Elderly Person\nc_charge_desc=Obstruct Fire Equipment\nc_charge_desc=Obstruct Officer W/Violence\nc_charge_desc=Obtain Control Substance By Fraud\nc_charge_desc=Offer Agree Secure For Lewd Act\nc_charge_desc=Offer Agree Secure/Lewd Act\nc_charge_desc=Offn Against Intellectual Prop\nc_charge_desc=Open Carrying Of Weapon\nc_charge_desc=Oper Motorcycle W/O Valid DL\nc_charge_desc=Operating W/O Valid License\nc_charge_desc=Opert With Susp DL 2nd Offens\nc_charge_desc=PL/Unlaw Use Credit Card\nc_charge_desc=Petit Theft\nc_charge_desc=Petit Theft $100- $300\nc_charge_desc=Pos Cannabis For Consideration\nc_charge_desc=Pos Cannabis W/Intent Sel/Del\nc_charge_desc=Pos Methylenedioxymethcath W/I/D/S\nc_charge_desc=Poss 3,4 MDMA (Ecstasy)\nc_charge_desc=Poss Alprazolam W/int Sell/Del\nc_charge_desc=Poss Anti-Shoplifting Device\nc_charge_desc=Poss Cntrft Contr Sub w/Intent\nc_charge_desc=Poss Cocaine/Intent To Del/Sel\nc_charge_desc=Poss Contr Subst W/o Prescript\nc_charge_desc=Poss Counterfeit Payment Inst\nc_charge_desc=Poss Drugs W/O A Prescription\nc_charge_desc=Poss F/Arm Delinq\nc_charge_desc=Poss Firearm W/Altered ID#\nc_charge_desc=Poss Meth/Diox/Meth/Amp (MDMA)\nc_charge_desc=Poss Of 1,4-Butanediol\nc_charge_desc=Poss Of Controlled Substance\nc_charge_desc=Poss Of RX Without RX\nc_charge_desc=Poss Oxycodone W/Int/Sell/Del\nc_charge_desc=Poss Pyrrolidinobutiophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone\nc_charge_desc=Poss Pyrrolidinovalerophenone W/I/D/S\nc_charge_desc=Poss Similitude of Drivers Lic\nc_charge_desc=Poss Tetrahydrocannabinols\nc_charge_desc=Poss Unlaw Issue Driver Licenc\nc_charge_desc=Poss Unlaw Issue Id\nc_charge_desc=Poss Wep Conv Felon\nc_charge_desc=Poss of Cocaine W/I/D/S 1000FT Park\nc_charge_desc=Poss of Firearm by Convic Felo\nc_charge_desc=Poss of Methylethcathinone\nc_charge_desc=Poss/Sell/Del Cocaine 1000FT Sch\nc_charge_desc=Poss/Sell/Del/Man Amobarbital\nc_charge_desc=Poss/pur/sell/deliver Cocaine\nc_charge_desc=Poss3,4 Methylenedioxymethcath\nc_charge_desc=Posses/Disply Susp/Revk/Frd DL\nc_charge_desc=Possess Cannabis 1000FTSch\nc_charge_desc=Possess Cannabis/20 Grams Or Less\nc_charge_desc=Possess Controlled Substance\nc_charge_desc=Possess Countrfeit Credit Card\nc_charge_desc=Possess Drug Paraphernalia\nc_charge_desc=Possess Mot Veh W/Alt Vin #\nc_charge_desc=Possess Tobacco Product Under 18\nc_charge_desc=Possess Weapon On School Prop\nc_charge_desc=Possess w/I/Utter Forged Bills\nc_charge_desc=Possession Burglary Tools\nc_charge_desc=Possession Child Pornography\nc_charge_desc=Possession Firearm School Prop\nc_charge_desc=Possession Of 3,4Methylenediox\nc_charge_desc=Possession Of Alprazolam\nc_charge_desc=Possession Of Amphetamine\nc_charge_desc=Possession Of Anabolic Steroid\nc_charge_desc=Possession Of Buprenorphine\nc_charge_desc=Possession Of Carisoprodol\nc_charge_desc=Possession Of Clonazepam\nc_charge_desc=Possession Of Cocaine\nc_charge_desc=Possession Of Diazepam\nc_charge_desc=Possession Of Fentanyl\nc_charge_desc=Possession Of Heroin\nc_charge_desc=Possession Of Methamphetamine\nc_charge_desc=Possession Of Paraphernalia\nc_charge_desc=Possession Of Phentermine\nc_charge_desc=Possession of Alcohol Under 21\nc_charge_desc=Possession of Benzylpiperazine\nc_charge_desc=Possession of Butylone\nc_charge_desc=Possession of Cannabis\nc_charge_desc=Possession of Cocaine\nc_charge_desc=Possession of Codeine\nc_charge_desc=Possession of Ethylone\nc_charge_desc=Possession of Hydrocodone\nc_charge_desc=Possession of Hydromorphone\nc_charge_desc=Possession of LSD\nc_charge_desc=Possession of Methadone\nc_charge_desc=Possession of Morphine\nc_charge_desc=Possession of Oxycodone\nc_charge_desc=Possession of XLR11\nc_charge_desc=Principal In The First Degree\nc_charge_desc=Prostitution\nc_charge_desc=Prostitution/Lewd Act Assignation\nc_charge_desc=Prostitution/Lewdness/Assign\nc_charge_desc=Prowling/Loitering\nc_charge_desc=Purchase Cannabis\nc_charge_desc=Purchase/P/W/Int Cannabis\nc_charge_desc=Reckless Driving\nc_charge_desc=Refuse Submit Blood/Breath Test\nc_charge_desc=Refuse to Supply DNA Sample\nc_charge_desc=Resist Officer w/Violence\nc_charge_desc=Resist/Obstruct W/O Violence\nc_charge_desc=Retail Theft $300 1st Offense\nc_charge_desc=Retail Theft $300 2nd Offense\nc_charge_desc=Ride Tri-Rail Without Paying\nc_charge_desc=Robbery / No Weapon\nc_charge_desc=Robbery / Weapon\nc_charge_desc=Robbery Sudd Snatch No Weapon\nc_charge_desc=Robbery W/Deadly Weapon\nc_charge_desc=Robbery W/Firearm\nc_charge_desc=Sale/Del Cannabis At/Near Scho\nc_charge_desc=Sale/Del Counterfeit Cont Subs\nc_charge_desc=Sel/Pur/Mfr/Del Control Substa\nc_charge_desc=Sell or Offer for Sale Counterfeit Goods\nc_charge_desc=Sell/Man/Del Pos/w/int Heroin\nc_charge_desc=Sex Batt Faml/Cust Vict 12-17Y\nc_charge_desc=Sex Battery Deft 18+/Vict 11-\nc_charge_desc=Sex Offender Fail Comply W/Law\nc_charge_desc=Sexual Battery / Vict 12 Yrs +\nc_charge_desc=Sexual Performance by a Child\nc_charge_desc=Shoot In Occupied Dwell\nc_charge_desc=Shoot Into Vehicle\nc_charge_desc=Simulation of Legal Process\nc_charge_desc=Solic to Commit Battery\nc_charge_desc=Solicit Deliver Cocaine\nc_charge_desc=Solicit Purchase Cocaine\nc_charge_desc=Solicit To Deliver Cocaine\nc_charge_desc=Solicitation On Felony 3 Deg\nc_charge_desc=Soliciting For Prostitution\nc_charge_desc=Sound Articles Over 100\nc_charge_desc=Stalking\nc_charge_desc=Stalking (Aggravated)\nc_charge_desc=Strong Armed Robbery\nc_charge_desc=Structuring Transactions\nc_charge_desc=Susp Drivers Lic 1st Offense\nc_charge_desc=Tamper With Victim\nc_charge_desc=Tamper With Witness\nc_charge_desc=Tamper With Witness/Victim/CI\nc_charge_desc=Tampering With Physical Evidence\nc_charge_desc=Tampering with a Victim\nc_charge_desc=Theft/To Deprive\nc_charge_desc=Threat Public Servant\nc_charge_desc=Throw Deadly Missile Into Veh\nc_charge_desc=Throw In Occupied Dwell\nc_charge_desc=Throw Missile Into Pub/Priv Dw\nc_charge_desc=Traff In Cocaine &lt;400g&gt;150 Kil\nc_charge_desc=Traffic Counterfeit Cred Cards\nc_charge_desc=Traffick Amphetamine 28g&gt;&lt;200g\nc_charge_desc=Traffick Oxycodone 4g&gt;&lt;14g\nc_charge_desc=Trans/Harm/Material to a Minor\nc_charge_desc=Trespass On School Grounds\nc_charge_desc=Trespass Other Struct/Conve\nc_charge_desc=Trespass Private Property\nc_charge_desc=Trespass Property w/Dang Weap\nc_charge_desc=Trespass Struct/Conveyance\nc_charge_desc=Trespass Structure w/Dang Weap\nc_charge_desc=Trespass Structure/Conveyance\nc_charge_desc=Trespassing/Construction Site\nc_charge_desc=Tresspass Struct/Conveyance\nc_charge_desc=Tresspass in Structure or Conveyance\nc_charge_desc=Unauth C/P/S Sounds&gt;1000/Audio\nc_charge_desc=Unauth Poss ID Card or DL\nc_charge_desc=Unauthorized Interf w/Railroad\nc_charge_desc=Unl/Disturb Education/Instui\nc_charge_desc=Unlaw Lic Use/Disply Of Others\nc_charge_desc=Unlaw LicTag/Sticker Attach\nc_charge_desc=Unlaw Use False Name/Identity\nc_charge_desc=Unlawful Conveyance of Fuel\nc_charge_desc=Unlicensed Telemarketing\nc_charge_desc=Use Computer for Child Exploit\nc_charge_desc=Use Of 2 Way Device To Fac Fel\nc_charge_desc=Use Scanning Device to Defraud\nc_charge_desc=Use of Anti-Shoplifting Device\nc_charge_desc=Uttering Forged Bills\nc_charge_desc=Uttering Forged Credit Card\nc_charge_desc=Uttering Worthless Check +$150\nc_charge_desc=Uttering a Forged Instrument\nc_charge_desc=Video Voyeur-&lt;24Y on Child &gt;16\nc_charge_desc=Viol Injunct Domestic Violence\nc_charge_desc=Viol Injunction Protect Dom Vi\nc_charge_desc=Viol Pretrial Release Dom Viol\nc_charge_desc=Viol Prot Injunc Repeat Viol\nc_charge_desc=Violation License Restrictions\nc_charge_desc=Violation Of Boater Safety Id\nc_charge_desc=Violation of Injunction Order/Stalking/Cyberstalking\nc_charge_desc=Voyeurism\nc_charge_desc=arrest case no charge\n\n\n\n\n0\n0.0\n69.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n34.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n24.0\n0.0\n0.0\n0.0\n1.0\n4.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n44.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n41.0\n1.0\n0.0\n0.0\n0.0\n14.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6167 entries, 0 to 6166\nColumns: 401 entries, sex to c_charge_desc=arrest case no charge\ndtypes: float64(401)\nmemory usage: 18.9 MB\n\n\n\n\n#This is to change the data type from object to date\n\ncompas['c_jail_in'] = pd.to_datetime(compas['c_jail_in'])\ncompas['c_jail_out'] = pd.to_datetime(compas['c_jail_out'])\ncompas['days_in_jail'] = abs((compas['c_jail_out'] - compas['c_jail_in']).dt.days)\ncompas['compas_screening_date'] = pd.to_datetime(compas['compas_screening_date'])\ncompas['v_screening_date'] = pd.to_datetime(compas['v_screening_date'])\n\n\ncompas.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18316 entries, 0 to 18315\nData columns (total 23 columns):\n #   Column                 Non-Null Count  Dtype         \n---  ------                 --------------  -----         \n 0   name                   18316 non-null  object        \n 1   compas_screening_date  18316 non-null  datetime64[ns]\n 2   sex                    18316 non-null  object        \n 3   dob                    18316 non-null  object        \n 4   age                    18316 non-null  int64         \n 5   age_cat                18316 non-null  object        \n 6   race                   18316 non-null  object        \n 7   decile_score           18316 non-null  int64         \n 8   priors_count           18316 non-null  int64         \n 9   c_jail_in              17019 non-null  datetime64[ns]\n 10  c_jail_out             17019 non-null  datetime64[ns]\n 11  c_days_from_compas     17449 non-null  float64       \n 12  c_charge_degree        17449 non-null  object        \n 13  c_charge_desc          17435 non-null  object        \n 14  is_recid               18316 non-null  int64         \n 15  is_violent_recid       18316 non-null  int64         \n 16  type_of_assessment     18316 non-null  object        \n 17  score_text             18293 non-null  object        \n 18  v_type_of_assessment   18316 non-null  object        \n 19  v_decile_score         18316 non-null  int64         \n 20  v_score_text           18310 non-null  object        \n 21  v_screening_date       18316 non-null  datetime64[ns]\n 22  days_in_jail           17019 non-null  float64       \ndtypes: datetime64[ns](4), float64(2), int64(6), object(11)\nmemory usage: 3.2+ MB\n\n\n\n#data.isnull().sum() is used to get the number of missing records in each column\n\nprint(\"The sum of Null values in each columns \")\n\ncompas.isnull().sum()\n\nThe sum of Null values in each columns \n\n\nname                        0\ncompas_screening_date       0\nsex                         0\ndob                         0\nage                         0\nage_cat                     0\nrace                        0\ndecile_score                0\npriors_count                0\nc_jail_in                1297\nc_jail_out               1297\nc_days_from_compas        867\nc_charge_degree           867\nc_charge_desc             881\nis_recid                    0\nis_violent_recid            0\ntype_of_assessment          0\nscore_text                 23\nv_type_of_assessment        0\nv_decile_score              0\nv_score_text                6\nv_screening_date            0\ndays_in_jail             1297\ndtype: int64\n\n\n\n# Impute missing values for numerical variables\nnumeric_cols = ['c_days_from_compas', 'v_decile_score']\nfor col in numeric_cols:\n    compas[col].fillna(compas[col].median(), inplace=True)\n\n# Impute missing values for categorical variables\ncategorical_cols = ['c_charge_degree', 'score_text', 'v_score_text', 'c_jail_in', 'c_jail_out', 'c_charge_desc', 'days_in_jail']\nfor col in categorical_cols:\n    compas[col].fillna(compas[col].mode()[0], inplace=True)\n\n# Check if there are any missing values remaining\nprint(compas.isnull().sum())\n\nname                     0\ncompas_screening_date    0\nsex                      0\ndob                      0\nage                      0\nage_cat                  0\nrace                     0\ndecile_score             0\npriors_count             0\nc_jail_in                0\nc_jail_out               0\nc_days_from_compas       0\nc_charge_degree          0\nc_charge_desc            0\nis_recid                 0\nis_violent_recid         0\ntype_of_assessment       0\nscore_text               0\nv_type_of_assessment     0\nv_decile_score           0\nv_score_text             0\nv_screening_date         0\ndays_in_jail             0\ndtype: int64\n\n\nThe code above replace the missing value in numerical column with the column’s median while it does the same in categorical column using the column’s mode\n\ncompas.describe()\n\n\n\n\n\n\n\n\ncompas_screening_date\nage\ndecile_score\npriors_count\nc_jail_in\nc_jail_out\nc_days_from_compas\nis_recid\nis_violent_recid\nv_decile_score\nv_screening_date\ndays_in_jail\n\n\n\n\ncount\n18316\n18316.000000\n18316.000000\n18316.000000\n18316\n18316\n18316.000000\n18316.000000\n18316.000000\n18316.000000\n18316\n18316.000000\n\n\nmean\n2013-11-22 23:28:18.973574912\n34.019273\n4.997052\n3.913191\n2013-11-06 20:46:19.209434624\n2013-12-06 14:03:25.459707392\n55.010919\n0.414774\n0.073105\n4.022822\n2013-11-22 23:28:18.973574912\n23.293514\n\n\nmin\n2013-01-01 00:00:00\n18.000000\n-1.000000\n0.000000\n2013-01-01 01:31:00\n2013-01-02 01:12:00\n0.000000\n-1.000000\n0.000000\n-1.000000\n2013-01-01 00:00:00\n0.000000\n\n\n25%\n2013-04-22 00:00:00\n25.000000\n2.000000\n0.000000\n2013-03-30 01:04:00\n2013-04-30 07:29:00\n1.000000\n0.000000\n0.000000\n2.000000\n2013-04-22 00:00:00\n0.000000\n\n\n50%\n2013-11-08 00:00:00\n31.000000\n5.000000\n2.000000\n2013-10-18 01:38:00\n2013-11-13 01:00:00\n1.000000\n0.000000\n0.000000\n4.000000\n2013-11-08 00:00:00\n1.000000\n\n\n75%\n2014-05-29 00:00:00\n41.000000\n8.000000\n5.000000\n2014-05-15 10:48:00\n2014-06-10 07:35:00\n2.000000\n1.000000\n0.000000\n6.000000\n2014-05-29 00:00:00\n14.000000\n\n\nmax\n2014-12-31 00:00:00\n96.000000\n10.000000\n43.000000\n2016-03-11 10:26:00\n2020-01-01 00:00:00\n9485.000000\n1.000000\n1.000000\n10.000000\n2014-12-31 00:00:00\n2152.000000\n\n\nstd\nNaN\n11.667811\n2.937569\n5.299864\nNaN\nNaN\n310.610219\n0.576449\n0.260317\n2.614189\nNaN\n64.917074\n\n\n\n\n\n\n\n\nData_Visualisation\n\n# Plot histogram of age\nplt.figure(figsize=(8, 6))\nsns.histplot(data=compas, x='age', bins=30, kde=True)\nplt.title('Histogram of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot histogram of age\nplt.figure(figsize=(8, 6))\nsns.histplot(data=compas, x='race', bins=30, kde=True)\nplt.title('Histogram of race')\nplt.xlabel('race')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot bar mean decile score of age range\n\ndecile_mean_age = compas.groupby(\"age_cat\")[\"decile_score\"].mean().reset_index()\n\nplt.figure(figsize=(7, 5))\nsns.barplot(data=decile_mean_age, x=\"age_cat\", y=\"decile_score\", \n            palette=sns.color_palette(\"hsv\", len(decile_mean_age)))\n\nplt.ylabel(\"Decile score mean\")\nplt.xlabel(\"\")\nplt.title(\"Mean decile score by age range\")\n\n# Add percentage labels to each bar\nfor index, row in decile_mean_age.iterrows():\n    plt.text(index, row['decile_score'], f\"{row['decile_score']:.2f}%\", \n             color='black', ha=\"center\")\n\nplt.show(block=False)\n\n\n\n\n\n\n\n\n\n# Plot Count number of cases by sex and age\n\nplt.figure(figsize=(8, 5)) \nax = sns.countplot(data=compas, x=\"sex\", hue=\"age_cat\", palette=sns.color_palette(\"hsv\", 3)) \nplt.title(\"Number of cases by sex and age\") \nplt.xlabel(\"\") \n\n# Calculate total count for each category\ntotal_counts = compas.groupby(['sex', 'age_cat']).size().reset_index(name='total_count')\n\n# Add percentage labels to each bar\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()/2., height + 0.1, f'{100 * height / total_counts[\"total_count\"].sum():.1f}%', ha=\"center\")\n\nplt.show(block=False)\n\n\n\n\n\n\n\n\n\n\n# Calculate percentage of each race\nrace_percentage = compas['race'].value_counts(normalize=True) * 100\n\n# Sort races by count in descending order\nrace_percentage = race_percentage.sort_values(ascending=False)\n\n# Visualize race distribution with percentage labels\nplt.figure(figsize=(8, 6))\nsns.barplot(x=race_percentage.index, y=race_percentage.values, palette=\"Blues_r\")\n\n# Add percentage labels to each bar\nfor i, val in enumerate(race_percentage.values):\n    plt.text(i, val + 0.5, f\"{val:.2f}%\", ha=\"center\")\n\nplt.title('Distribution of Racial Groups')\nplt.xlabel('Race')\nplt.ylabel('Percentage')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define a list of racial groups\nracial_groups = ['African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other']\n\n# Set the style of seaborn\nsns.set(style=\"whitegrid\")\n\n# Create a grid of count plots for each race's decile scores\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()  # Flatten the axes array for easier iteration\n\nfor i, race in enumerate(racial_groups):\n    # Filter the dataframe for the current racial group\n    race_data = compas[compas['race'] == race]\n    \n    # Create a count plot for the decile scores of the current racial group\n    sns.countplot(x='decile_score', data=race_data, ax=axes[i], palette='husl')\n    axes[i].set_title(race)\n    axes[i].set_xlabel('Decile Score')\n    axes[i].set_ylabel('Count')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define a list of racial groups\nracial_groups = ['African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other']\n\n# Set the style of seaborn\nsns.set(style=\"whitegrid\")\n\n# Create a grid of count plots for each race's v_decile_scores\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()  # Flatten the axes array for easier iteration\n\nfor i, race in enumerate(racial_groups):\n    # Filter the dataframe for the current racial group\n    race_data = compas[compas['race'] == race]\n    \n    # Create a count plot for the v_decile_scores of the current racial group\n    sns.countplot(x='v_decile_score', data=race_data, ax=axes[i], palette='husl')\n    axes[i].set_title(race)\n    axes[i].set_xlabel('v_decile_score')\n    axes[i].set_ylabel('Count')\n\n# Adjust layout\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nsns.pairplot(compas[['age', 'decile_score', 'priors_count', 'c_days_from_compas', 'is_violent_recid', 'v_decile_score', 'days_in_jail', 'is_recid']])\nplt.show()\n\n\n\n\n\n\n\n\n\n\ncompas[\"sex\"].replace({'Male': 1, 'Female': 0}, inplace=True)\n\n\n\n# Filter out object variables\nnumeric_variables = compas.select_dtypes(include=['int64', 'float64'])\n\n# Compute the correlation matrix\ncorr_matrix = numeric_variables.corr()\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix of Numeric Variables')\nplt.show()"
  },
  {
    "objectID": "data_preproc.html",
    "href": "data_preproc.html",
    "title": "",
    "section": "",
    "text": "def default_preprocessing(df):\n    \"\"\"Custom preprocessing function to prepare MEPS data, focusing on specified features.\"\"\"\n    def race(row):\n        if row['HISPANX'] == 2 and row['RACEV2X'] == 1:\n            return 'White'\n        return 'Non-White'\n    def race2(row):\n        if row['HISPANX'] == 2 and row['RACEV2X'] == 1:\n            return 'White'\n        elif row['RACEV2X'] == 2:\n            return 'Black'\n        elif row['RACEV2X'] == 3:\n            return 'American Indian'\n        elif row['RACEV2X'] == 4:\n            return 'Asian Indian'\n        elif row['RACEV2X'] == 5:\n            return 'Chinese'\n        elif row['RACEV2X'] == 6:\n            return 'Filipino'\n        elif row['RACEV2X'] == 10:\n            return 'Other'\n        elif row['RACEV2X'] == 12:\n            return 'Multiplle'\n        else:\n            return 'NA'\n    # Apply transformations\n    df['RACE'] = df.apply(race, axis=1)\n    df['RACE_EXP'] = df.apply(race2, axis=1)\n\n    df = df[df['PANEL'] == 19]  # Restrict to Panel 19\n\n    # Rename columns as necessary\n    rename_dict = {\n        'FTSTU53X': 'FTSTU', 'ACTDTY53': 'ACTDTY', 'HONRDC53': 'HONRDC',\n        'RTHLTH53': 'RTHLTH', 'MNHLTH53': 'MNHLTH', 'CHBRON53': 'CHBRON', \n        'JTPAIN53': 'JTPAIN', 'PREGNT53': 'PREGNT', 'WLKLIM53': 'WLKLIM', \n        'ACTLIM53': 'ACTLIM', 'SOCLIM53': 'SOCLIM', 'COGLIM53': 'COGLIM', \n        'EMPST53': 'EMPST', 'REGION53': 'REGION', 'MARRY53X': 'MARRY', \n        'AGE53X': 'AGE', 'POVCAT15': 'POVCAT', 'INSCOV15': 'INSCOV'\n    }\n    df.rename(columns=rename_dict, inplace=True)\n\n    # Calculate UTILIZATION\n    def utilization(row):\n        return row['OBTOTV15'] + row['OPTOTV15'] + row['ERTOT15'] + row['IPNGTD15'] + row['HHTOTD15']\n\n    df['UTILIZATION'] = df.apply(utilization, axis=1)\n    df['UTILIZATION'] = (df['UTILIZATION'] &gt;= 10).astype(float)\n\n    # Filter rows based on valid values\n    valid_criteria = {\n        'REGION': 0, 'AGE': 0, 'MARRY': 0, 'ASTHDX': 0,\n        'FTSTU': -1, 'ACTDTY': -1, 'HONRDC': -1, 'RTHLTH': -1, 'MNHLTH': -1,\n        'HIBPDX': -1, 'CHDDX': -1, 'ANGIDX': -1, 'EDUCYR': -1, 'HIDEG': -1,\n        'MIDX': -1, 'OHRTDX': -1, 'STRKDX': -1, 'EMPHDX': -1, 'CHBRON': -1,\n        'CHOLDX': -1, 'CANCERDX': -1, 'DIABDX': -1, 'JTPAIN': -1, 'ARTHDX': -1,\n        'ARTHTYPE': -1, 'ASTHDX': -1, 'ADHDADDX': -1, 'PREGNT': -1, 'WLKLIM': -1,\n        'ACTLIM': -1, 'SOCLIM': -1, 'COGLIM': -1, 'DFHEAR42': -1, 'DFSEE42': -1,\n        'ADSMOK42': -1, 'PHQ242': -1, 'EMPST': -1, 'POVCAT': -1, 'INSCOV': -1\n    }\n\n    for column, cutoff in valid_criteria.items():\n        df = df[df[column] &gt;= cutoff]\n\n    # Specify the features to keep\n    features_to_keep = [\n        'REGION', 'AGE', 'SEX', 'RACE', 'MARRY', 'FTSTU', 'ACTDTY', 'HONRDC', 'RTHLTH', 'MNHLTH',\n        'HIBPDX', 'CHDDX', 'ANGIDX', 'MIDX', 'OHRTDX', 'STRKDX', 'EMPHDX', 'CHBRON', 'CHOLDX',\n        'CANCERDX', 'DIABDX', 'JTPAIN', 'ARTHDX', 'ARTHTYPE', 'ASTHDX', 'ADHDADDX', 'PREGNT',\n        'WLKLIM', 'ACTLIM', 'SOCLIM', 'COGLIM', 'DFHEAR42', 'DFSEE42', 'ADSMOK42', 'PCS42',\n        'MCS42', 'K6SUM42', 'PHQ242', 'EMPST', 'POVCAT', 'INSCOV', 'UTILIZATION', 'PERWT15F','RACE_EXP'\n    ]\n\n    # Drop all other features not listed\n    features_to_drop = [col for col in df.columns if col not in features_to_keep]\n    df.drop(columns=features_to_drop, inplace=True)\n\n    return df\n\n\nfilepath = 'h181.csv'\ndf = pd.read_csv(filepath)\ndf_transformed = default_preprocessing(df)\ndf_transformed.head()\n\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_30680\\2599504375.py:41: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.rename(columns=rename_dict, inplace=True)\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_30680\\2599504375.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['UTILIZATION'] = df.apply(utilization, axis=1)\nC:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_30680\\2599504375.py:48: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['UTILIZATION'] = (df['UTILIZATION'] &gt;= 10).astype(float)\n\n\n\n\n\n\n\n\n\nREGION\nAGE\nSEX\nMARRY\nFTSTU\nACTDTY\nHONRDC\nRTHLTH\nMNHLTH\nHIBPDX\n...\nMCS42\nK6SUM42\nPHQ242\nEMPST\nPOVCAT\nINSCOV\nPERWT15F\nRACE\nRACE_EXP\nUTILIZATION\n\n\n\n\n0\n2\n53\n1\n5\n-1\n2\n2\n4\n3\n1\n...\n58.47\n3\n0\n4\n1\n2\n21854.981705\nWhite\nWhite\n1.0\n\n\n1\n2\n56\n2\n3\n-1\n2\n2\n4\n3\n1\n...\n26.57\n17\n6\n4\n3\n2\n18169.604822\nWhite\nWhite\n1.0\n\n\n3\n2\n23\n2\n5\n3\n2\n2\n1\n1\n2\n...\n50.33\n7\n0\n1\n2\n2\n17191.832515\nWhite\nWhite\n0.0\n\n\n4\n2\n3\n1\n6\n-1\n3\n3\n1\n3\n-1\n...\n-1.00\n-1\n-1\n-1\n2\n2\n20261.485463\nWhite\nWhite\n0.0\n\n\n5\n3\n27\n1\n1\n-1\n1\n4\n2\n1\n2\n...\n-1.00\n-1\n-1\n1\n3\n1\n0.000000\nNon-White\nMultiplle\n0.0\n\n\n\n\n5 rows × 44 columns\n\n\n\n\ndf_transformed.columns\n\nIndex(['REGION', 'AGE', 'SEX', 'MARRY', 'FTSTU', 'ACTDTY', 'HONRDC', 'RTHLTH',\n       'MNHLTH', 'HIBPDX', 'CHDDX', 'ANGIDX', 'MIDX', 'OHRTDX', 'STRKDX',\n       'EMPHDX', 'CHBRON', 'CHOLDX', 'CANCERDX', 'DIABDX', 'JTPAIN', 'ARTHDX',\n       'ARTHTYPE', 'ASTHDX', 'ADHDADDX', 'PREGNT', 'WLKLIM', 'ACTLIM',\n       'SOCLIM', 'COGLIM', 'DFHEAR42', 'DFSEE42', 'ADSMOK42', 'PCS42', 'MCS42',\n       'K6SUM42', 'PHQ242', 'EMPST', 'POVCAT', 'INSCOV', 'PERWT15F', 'RACE',\n       'RACE_EXP', 'UTILIZATION'],\n      dtype='object')\n\n\n\nlen(df_transformed)\n\n15830\n\n\n\ndf_transformed.to_csv('MEPS_FINAL.csv',index=False)\n\n\ndf_transformed.select_dtypes(include=[float]).info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 15830 entries, 0 to 16577\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PCS42        15830 non-null  float64\n 1   MCS42        15830 non-null  float64\n 2   PERWT15F     15830 non-null  float64\n 3   UTILIZATION  15830 non-null  float64\ndtypes: float64(4)\nmemory usage: 618.4 KB"
  },
  {
    "objectID": "bias_vis.html",
    "href": "bias_vis.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\nfrom aif360.datasets import MEPSDataset20\nfrom aif360.datasets import MEPSDataset21\nfrom aif360.datasets import GermanDataset\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Bias mitigation techniques\nfrom aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\nfrom aif360.algorithms.preprocessing import LFR\nfrom aif360.algorithms.preprocessing import OptimPreproc\nfrom sklearn.model_selection import train_test_split\n\n\ndataset_orig_panel19 = MEPSDataset19()\n\n\ndataset_orig_panel19_train = MEPSDataset19()\n\n\ndataset_orig_panel19_train.features\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\nsens_attr\n\n\nprivileged_groups\n\n\nunprivileged_groups\n\n\ndataset_orig_panel19_train\n\n\nmetric_orig_panel19_train = BinaryLabelDatasetMetric(\n        dataset_orig_panel19_train,\n        unprivileged_groups=unprivileged_groups,\n        privileged_groups=privileged_groups)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n# Prepare data\nX = dataset_orig_panel19_train.features\ny = dataset_orig_panel19_train.labels.ravel()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the same scaled training data\ntrain_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n# Calculation of discrimination index without modifying dataset structure\nsens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\ndef calculate_discrimination(X, probabilities, sens_attr_index, unprivileged_val, privileged_val):\n    # Filter by sensitive attribute for unprivileged and privileged groups\n    unpriv_indices = X[:, sens_attr_index] == unprivileged_val\n    priv_indices = X[:, sens_attr_index] == privileged_val\n    \n    # Calculate mean probabilities for both groups\n    mean_prob_unpriv = probabilities[unpriv_indices].mean()\n    mean_prob_priv = probabilities[priv_indices].mean()\n    \n    # Discrimination index\n    discrimination = mean_prob_priv - mean_prob_unpriv\n    return discrimination\n\n# Define unprivileged and privileged values\nunprivileged_val = 0.0\nprivileged_val = 1.0\n\n# Compute discrimination\ndiscrimination_index = calculate_discrimination(X_train, train_probabilities, sens_attr_index, unprivileged_val, privileged_val)\nprint(\"Discrimination Index: {:.4f}\".format(discrimination_index))\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\ndef custom_loss(lambda_val=0.001):\n    def loss(y_true, y_pred):\n        # Extract predictions and sensitive attributes from y_pred\n        predictions = y_pred[:, 0]\n        sensitive_attr = y_pred[:, 1]\n\n        # Debug prints to check outputs\n        # tf.print(\"Predictions sample:\", predictions[:10])\n        # tf.print(\"Sensitive Attr sample:\", sensitive_attr[:10])\n\n        # Standard binary crossentropy loss\n        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n        \n        # Determine thresholds to convert sensitive attributes to binary\n        # Assuming the negative and positive classes are split around zero\n        threshold = 0\n        mask_unpriv = K.cast(sensitive_attr &lt;= threshold, 'float32')\n        mask_priv = K.cast(sensitive_attr &gt; threshold, 'float32')\n\n        epsilon = 1e-8\n        sum_unpriv = K.sum(mask_unpriv)\n        sum_priv = K.sum(mask_priv)\n\n        # # Debug prints for mask sums\n        # tf.print(\"Sum unprivileged:\", sum_unpriv)\n        # tf.print(\"Sum privileged:\", sum_priv)\n\n        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n\n        # Discrimination as the squared difference in probabilities\n        discrimination = K.square(prob_priv - prob_unpriv)\n\n        # # Debug print\n        # tf.print(\"Discrimination:\", discrimination)\n\n        # Total loss with discrimination penalty\n        return standard_loss + lambda_val * discrimination\n    \n    return loss\n\n\n\n# Model parameters\ninput_size = X_train_scaled.shape[1]  # Number of features\nsensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n# Input layers\ninputs = Input(shape=(input_size,))\nsensitive_inputs = Input(shape=(1,))\n\n# Network architecture\nx = Dense(64, activation='relu')(inputs)\noutputs = Dense(1, activation='sigmoid')(x)\ncombined_outputs = tf.keras.layers.concatenate([outputs, sensitive_inputs])\n\nmodel = Model(inputs=[inputs, sensitive_inputs], outputs=combined_outputs)\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=custom_loss(lambda_val = 0.01),\n              metrics=['accuracy'])\n\n# Prepare data with sensitive attribute\nX_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n\n# Train the model\nhistory1_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\n\n\n# Model parameters\ninput_size = X_train_scaled.shape[1]  # Number of features\nsensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n# Input layers\ninputs = Input(shape=(input_size,))\nsensitive_inputs = Input(shape=(1,))\n\n# Network architecture\nx = Dense(64, activation='relu')(inputs)\noutputs = Dense(1, activation='sigmoid')(x)\n\n# No need to concatenate outputs and sensitive inputs for the loss calculation\n# Separate outputs for predictions and sensitive attributes\n# Since we're using binary_crossentropy, we only need the main outputs\nmodel = Model(inputs=[inputs, sensitive_inputs], outputs=outputs)\n\n# Compile the model with binary crossentropy\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Prepare data with sensitive attribute\nX_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n\n# Train the model\nhistory2_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)"
  },
  {
    "objectID": "bias_2.html",
    "href": "bias_2.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom aif360.datasets import BinaryLabelDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\nfrom aif360.explainers import MetricTextExplainer\nfrom aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover, LFR\nfrom sklearn.preprocessing import StandardScaler\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n  warn_deprecated('vmap', 'torch.vmap')\n\n\n\nfrom bias_processor import *\n\n\ndf = pd.read_csv('MEPSDataset19.csv')\nbias = BiasMitigator()\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nUTILIZATION\n\n\n\n\n0\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n1\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n3\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n4\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n5\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 140 columns\n\n\n\n\nprotected = [{'RACE': 1.0}]\nunprotected = [{'RACE': 0.0}]\n\n\nsens_attr = 'RACE'\nlabel = 'UTILIZATION'\n\n\ndf2 = bias.compute_bias_metrics(df, unprotected, protected, sens_attr, label)\ndf2.head()\n\n\n\n\n\n\n\n\nMean Difference\nConsistency\nStatistical Parity Difference\nDisparate Impact\nMean Difference Explanation\nConsistency Explanation\nStatistical Parity Difference Explanation\nDisparate Impact Explanation\n\n\n\n\nValues\n-0.130083\n[0.8247378395451697]\n-0.130083\n0.490478\nMean difference (mean label value on unprivile...\nConsistency (Zemel, et al. 2013): [0.82473784]\nStatistical parity difference (probability of ...\nDisparate impact (probability of favorable out...\n\n\n\n\n\n\n\n\n# threshold=0.22\ndf3 = bias.preprocess(df, 'reweighing', sens_attr, unprotected, protected, label)\ndf3.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\n...\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\nweights\n\n\n\n\n0\n0.0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.672529\n\n\n1\n1.0\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.672529\n\n\n2\n3.0\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.112267\n\n\n3\n4.0\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.112267\n\n\n4\n5.0\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.946869\n\n\n\n\n5 rows × 141 columns\n\n\n\n\ndf4=bias.compute_bias_metrics(df3, unprotected, protected, sens_attr, label='label')\n\n\ndf4\n\n\n\n\n\n\n\n\nMean Difference\nConsistency\nStatistical Parity Difference\nDisparate Impact\nMean Difference Explanation\nConsistency Explanation\nStatistical Parity Difference Explanation\nDisparate Impact Explanation\n\n\n\n\nValues\n-0.130083\n[0.8248389134554664]\n-0.130083\n0.490478\nMean difference (mean label value on unprivile...\nConsistency (Zemel, et al. 2013): [0.82483891]\nStatistical parity difference (probability of ...\nDisparate impact (probability of favorable out..."
  },
  {
    "objectID": "dataset_desc.html",
    "href": "dataset_desc.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\n\n\nfile_path = 'MEPS_FINAL.csv'\nmeps_data = pd.read_csv(file_path)\nmeps_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 15830 entries, 0 to 15829\nData columns (total 44 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   REGION       15830 non-null  int64  \n 1   AGE          15830 non-null  int64  \n 2   SEX          15830 non-null  int64  \n 3   MARRY        15830 non-null  int64  \n 4   FTSTU        15830 non-null  int64  \n 5   ACTDTY       15830 non-null  int64  \n 6   HONRDC       15830 non-null  int64  \n 7   RTHLTH       15830 non-null  int64  \n 8   MNHLTH       15830 non-null  int64  \n 9   HIBPDX       15830 non-null  int64  \n 10  CHDDX        15830 non-null  int64  \n 11  ANGIDX       15830 non-null  int64  \n 12  MIDX         15830 non-null  int64  \n 13  OHRTDX       15830 non-null  int64  \n 14  STRKDX       15830 non-null  int64  \n 15  EMPHDX       15830 non-null  int64  \n 16  CHBRON       15830 non-null  int64  \n 17  CHOLDX       15830 non-null  int64  \n 18  CANCERDX     15830 non-null  int64  \n 19  DIABDX       15830 non-null  int64  \n 20  JTPAIN       15830 non-null  int64  \n 21  ARTHDX       15830 non-null  int64  \n 22  ARTHTYPE     15830 non-null  int64  \n 23  ASTHDX       15830 non-null  int64  \n 24  ADHDADDX     15830 non-null  int64  \n 25  PREGNT       15830 non-null  int64  \n 26  WLKLIM       15830 non-null  int64  \n 27  ACTLIM       15830 non-null  int64  \n 28  SOCLIM       15830 non-null  int64  \n 29  COGLIM       15830 non-null  int64  \n 30  DFHEAR42     15830 non-null  int64  \n 31  DFSEE42      15830 non-null  int64  \n 32  ADSMOK42     15830 non-null  int64  \n 33  PCS42        15830 non-null  float64\n 34  MCS42        15830 non-null  float64\n 35  K6SUM42      15830 non-null  int64  \n 36  PHQ242       15830 non-null  int64  \n 37  EMPST        15830 non-null  int64  \n 38  POVCAT       15830 non-null  int64  \n 39  INSCOV       15830 non-null  int64  \n 40  PERWT15F     15830 non-null  float64\n 41  RACE         15830 non-null  object \n 42  RACE_EXP     10988 non-null  object \n 43  UTILIZATION  15830 non-null  float64\ndtypes: float64(4), int64(38), object(2)\nmemory usage: 5.3+ MB\n\n\n\nmeps_data.head()\n\n\n\n\n\n\n\n\nREGION\nAGE\nSEX\nMARRY\nFTSTU\nACTDTY\nHONRDC\nRTHLTH\nMNHLTH\nHIBPDX\n...\nMCS42\nK6SUM42\nPHQ242\nEMPST\nPOVCAT\nINSCOV\nPERWT15F\nRACE\nRACE_EXP\nUTILIZATION\n\n\n\n\n0\n2\n53\n1\n5\n-1\n2\n2\n4\n3\n1\n...\n58.47\n3\n0\n4\n1\n2\n21854.981705\nWhite\nWhite\n1.0\n\n\n1\n2\n56\n2\n3\n-1\n2\n2\n4\n3\n1\n...\n26.57\n17\n6\n4\n3\n2\n18169.604822\nWhite\nWhite\n1.0\n\n\n2\n2\n23\n2\n5\n3\n2\n2\n1\n1\n2\n...\n50.33\n7\n0\n1\n2\n2\n17191.832515\nWhite\nWhite\n0.0\n\n\n3\n2\n3\n1\n6\n-1\n3\n3\n1\n3\n-1\n...\n-1.00\n-1\n-1\n-1\n2\n2\n20261.485463\nWhite\nWhite\n0.0\n\n\n4\n3\n27\n1\n1\n-1\n1\n4\n2\n1\n2\n...\n-1.00\n-1\n-1\n1\n3\n1\n0.000000\nNon-White\nMultiplle\n0.0\n\n\n\n\n5 rows × 44 columns\n\n\n\n\nimport plotly.express as px\n\n# Assuming 'RACE' is properly coded and needs mapping if required\nrace_counts = meps_data['RACE_EXP'].value_counts().reset_index()\nrace_counts.columns = ['Race', 'Count']\n\n# Plotting the race distribution\nfig = px.bar(race_counts, x='Race', y='Count', title='Distribution of Races in the MEPS Dataset')\nfig.update_xaxes(type='category')  # Ensuring the x-axis is treated as categorical\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Assuming 'RACE' is properly coded and needs mapping if required\nrace_counts = meps_data['RACE'].value_counts().reset_index()\nrace_counts.columns = ['Race', 'Count']\n\n# Plotting the race distribution\nfig = px.bar(race_counts, x='Race', y='Count', title='Distribution of Races in the MEPS Dataset')\nfig.update_xaxes(type='category')  # Ensuring the x-axis is treated as categorical\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Assuming meps_data is your DataFrame and is already loaded\n\n# Histogram for PCS42 where values are greater than 0\nfig_pcs = px.histogram(meps_data[meps_data['PCS42'] &gt; 0], x='PCS42', title='Distribution of PCS42 Scores')\nfig_pcs.show()\n\n# Histogram for MCS42 where values are greater than 0\nfig_mcs = px.histogram(meps_data[meps_data['MCS42'] &gt; 0], x='MCS42', title='Distribution of MCS42 Scores')\nfig_mcs.show()\n\n# Histogram for K6SUM42 where values are greater than 0\nfig_k6 = px.histogram(meps_data[meps_data['K6SUM42'] &gt; 0], x='K6SUM42', title='Distribution of K6SUM42 Scores')\nfig_k6.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\ndf=meps_data\n\nDistribution Understanding: These histograms provide a visual representation of how the scores are distributed across the dataset. For example, you can see if most people have high or low scores, or if the scores are normally distributed. Identify Skewness and Outliers: Histograms help identify if the data is skewed to the left or right, and if there are any outliers. For example, a lot of data piled up at one end of the histogram might suggest skewness. Health Score Insights: Specifically for PCS42 and MCS42, these might represent physical and mental health scores respectively. The histograms will show whether the population in the dataset generally reports good or poor health. High scores clustered around higher values might suggest better health conditions, while a spread towards lower values could indicate prevalent health issues. Mental Health Indicator (K6SUM42): Given that K6SUM42 might measure psychological distress, its histogram can be particularly insightful in public health studies. It helps to understand the mental health burden within the study population.\n\nimport plotly.express as px\n\n# Filter the DataFrame to include only PCS42 and MCS42 values greater than 0\nfiltered_df = df[(df['PCS42'] &gt; 0) & (df['MCS42'] &gt; 0)]\n\n# Box plot for PCS42 by RACE with values &gt; 0\nfig_pcs = px.box(filtered_df, x='RACE', y='PCS42', color='RACE', \n                 title='Physical Component Summary (PCS42) by Race')\nfig_pcs.show()\n\n# Box plot for MCS42 by RACE with values &gt; 0\nfig_mcs = px.box(filtered_df, x='RACE', y='MCS42', color='RACE', \n                 title='Mental Component Summary (MCS42) by Race')\nfig_mcs.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Filter the DataFrame to include only PCS42 and MCS42 values greater than 0\nfiltered_df = df[(df['PCS42'] &gt; 0) & (df['MCS42'] &gt; 0)]\n\n# Box plot for PCS42 by RACE with values &gt; 0\nfig_pcs = px.box(filtered_df, x='RACE_EXP', y='PCS42', color='RACE', \n                 title='Physical Component Summary (PCS42) by Race')\nfig_pcs.show()\n\n# Box plot for MCS42 by RACE with values &gt; 0\nfig_mcs = px.box(filtered_df, x='RACE_EXP', y='MCS42', color='RACE', \n                 title='Mental Component Summary (MCS42) by Race')\nfig_mcs.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport pandas as pd\nimport plotly.express as px\ndf=meps_data\n# Create a crosstab to visualize counts\ncrosstab = pd.crosstab(df['RACE'], df['UTILIZATION'])\n\n# Plotting\nfig = px.bar(crosstab, title='Count of Healthcare Utilization by Race',\n             labels={'value': 'Count', 'RACE': 'Race', 'UTILIZATION': 'Utilization (&gt;10 visits = 1)'})\nfig.update_layout(barmode='group', xaxis_title='Race', yaxis_title='Count')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Assuming 'SEX' is coded as 1 for male and 2 for female in your dataset\nfig = px.histogram(df, x='RACE_EXP', color='UTILIZATION', facet_col='SEX', barmode='group',\n                   category_orders={\"RACE\": [\"White\", \"Non-White\"], \"UTILIZATION\": [0, 1], \"SEX\": [1, 2]},\n                   labels={'UTILIZATION': 'Healthcare Utilization (&gt;10 visits = 1)', 'RACE': 'Race', 'SEX': 'Gender'},\n                   title='Healthcare Utilization by Race and Gender')\nfig.update_layout(xaxis_title='Race', yaxis_title='Count')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Assuming 'SEX' is coded as 1 for male and 2 for female in your dataset\nfig = px.histogram(df, x='RACE', color='UTILIZATION', facet_col='SEX', barmode='group',\n                   category_orders={\"RACE\": [\"White\", \"Non-White\"], \"UTILIZATION\": [0, 1], \"SEX\": [1, 2]},\n                   labels={'UTILIZATION': 'Healthcare Utilization (&gt;10 visits = 1)', 'RACE': 'Race', 'SEX': 'Gender'},\n                   title='Healthcare Utilization by Race and Gender')\nfig.update_layout(xaxis_title='Race', yaxis_title='Count')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfig = px.parallel_categories(df, dimensions=['RACE', 'UTILIZATION', 'SEX'],\n                             color=\"UTILIZATION\", color_continuous_scale=px.colors.sequential.Inferno,\n                             labels={'RACE': 'Race', 'UTILIZATION': 'Utilization', 'SEX': 'Gender'})\nfig.update_layout(title='Multi-dimensional Exploration of Healthcare Utilization')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfig = px.parallel_categories(df, dimensions=['RACE_EXP', 'UTILIZATION'],\n                             color=\"UTILIZATION\", color_continuous_scale=px.colors.sequential.Inferno,\n                             labels={'RACE': 'Race', 'UTILIZATION': 'Utilization'})\nfig.update_layout(title='Multi-dimensional Exploration of Healthcare Utilization')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Calculate percentages for the stacked bar chart\nutilization_percent = df.groupby(['RACE_EXP', 'UTILIZATION']).size().unstack().apply(lambda x: x/x.sum() * 100, axis=1)\n\nfig = px.bar(utilization_percent, title='Percentage of Healthcare Utilization by Race',\n             labels={'value': 'Percentage', 'variable': 'Utilization (&gt;10 visits = 1)'},\n             text_auto=True)\nfig.update_layout(yaxis_title='Percentage (%)', xaxis_title='Race',\n                  legend_title='Healthcare Utilization')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Calculate percentages for the stacked bar chart\nutilization_percent = df.groupby(['RACE', 'UTILIZATION']).size().unstack().apply(lambda x: x/x.sum() * 100, axis=1)\n\nfig = px.bar(utilization_percent, title='Percentage of Healthcare Utilization by Race',\n             labels={'value': 'Percentage', 'variable': 'Utilization (&gt;10 visits = 1)'},\n             text_auto=True)\nfig.update_layout(yaxis_title='Percentage (%)', xaxis_title='Race',\n                  legend_title='Healthcare Utilization')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Assuming df is your DataFrame and is already loaded with the 'AGE' column\nfig = px.histogram(df, x='AGE', nbins=50, title='Distribution of Age')\nfig.update_layout(xaxis_title='Age', yaxis_title='Count',\n                  bargap=0)  # Adjust the gap between bars for better visibility\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Assuming df is your DataFrame already loaded\n# Convert UTILIZATION to a string to treat it as a categorical variable\ndf['UTILIZATION'] = df['UTILIZATION'].astype(str)\ndf=df[df['RACE_EXP']!='Unknown']\n\n# Check if 'RACE_EXP' needs any cleanup or is ready to use\nprint(df['RACE_EXP'].unique())  # Print unique values to check the data\n\n# Now create the Sunburst chart\nfig = px.sunburst(df, path=['RACE', 'RACE_EXP', 'UTILIZATION'], \n                  title='',\n                  color='RACE_EXP',\n                      # Color by Utilization for visual distinction\n                  width=600, height=600,\n                  color_discrete_sequence=px.colors.qualitative.Set3)  # Using Set3 palette for distinct coloring\n\n\n# Show the figure\nfig.show()\n\n['White' 'Multiplle' 'Chinese' 'Other' 'Black' 'American Indian'\n 'Filipino' 'Asian Indian']\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "disc_loss_fn.html",
    "href": "disc_loss_fn.html",
    "title": "Discrimination Loss Fn",
    "section": "",
    "text": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n  warn_deprecated('vmap', 'torch.vmap')\ndataset_orig_panel19_train = MEPSDataset19()\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\ndataset_orig_panel19_train.feature_names[1]\n\n'RACE'\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data\nX = dataset_orig_panel19_train.features\ny = dataset_orig_panel19_train.labels.ravel()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = X_train\nX_test_scaled = X_test\n\n# Train logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the same scaled training data\ntrain_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n# Calculation of discrimination index without modifying dataset structure\nsens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n\n\n# Define unprivileged and privileged values\nunprivileged_val = 0.0\nprivileged_val = 1.0\n\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Custom loss function\ndef discrimination_loss(output, target, sensitive_features, lambda_val=100, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(output, target)\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    #discrimination=torch.abs(prob_priv)\n    prob_unpriv = torch.mean(output[mask_unpriv])\n    prob_priv = torch.mean(output[mask_priv])\n    discrimination = lambda_val * (prob_priv - prob_unpriv) ** k\n    loss_val=(1 + lambda_val * discrimination) * standard_loss\n    return loss_val,discrimination.item() \n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\n\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\n\n# Correctly preparing the sensitive features\nthreshold = 0.5  # Adjust the threshold according to your specific case\nsensitive_features = torch.tensor((data[:, 1].numpy() &gt; threshold).astype(float)).float()\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\n# Assuming similar preparation for test data\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = torch.tensor((test_data[:, 1].numpy() &gt; threshold).astype(float)).float()\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel2 = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model2.parameters(), lr=0.01)\ntrain_losses, train_accuracies, train_discriminations,train_fairness = [], [], [],[]\ntest_losses, test_accuracies, test_discriminations,test_fairness = [], [], [],[]\n\n# Training loop\nmodel2.train()\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model2(features)\n    loss, discrimination = discrimination_loss(outputs, targets.squeeze(), sensitive_features)\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluation on test data\n    model2.eval()\n    with torch.no_grad():\n        test_outputs = model2(test_features)\n        test_loss,test_discrimination = discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n    \n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, Train Discrimination: {discrimination} '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%,Test Discrimination: {test_discrimination}')\n    train_losses.append(loss.item())\n    train_accuracies.append(train_accuracy.item() * 100)\n    train_discriminations.append(discrimination)\n    fairness=1-discrimination\n    train_fairness.append(fairness)\n    test_fairness.append(fairness)\n    test_losses.append(test_loss.item())\n    test_accuracies.append(test_accuracy.item() * 100)\n    test_discriminations.append(test_discrimination)\n    model2.train()\n\nEpoch 1, Train Loss: 2.673090934753418, Train Acc: 39.25%, Train Discrimination: 0.022887492552399635 Test Loss: 61.42726135253906, Test Acc: 58.78%,Test Discrimination: 0.7591049075126648\nEpoch 2, Train Loss: 38.965476989746094, Train Acc: 58.88%, Train Discrimination: 0.483476847410202 Test Loss: 33.263851165771484, Test Acc: 81.62%,Test Discrimination: 0.3896600008010864\nEpoch 3, Train Loss: 20.107635498046875, Train Acc: 82.00%, Train Discrimination: 0.23562787473201752 Test Loss: 15.587469100952148, Test Acc: 82.66%,Test Discrimination: 0.17575567960739136\nEpoch 4, Train Loss: 9.372156143188477, Train Acc: 82.87%, Train Discrimination: 0.10379645973443985 Test Loss: 7.601461887359619, Test Acc: 82.66%,Test Discrimination: 0.07924606651067734\nEpoch 5, Train Loss: 4.696783542633057, Train Acc: 82.87%, Train Discrimination: 0.04625870659947395 Test Loss: 3.873684883117676, Test Acc: 82.66%,Test Discrimination: 0.034578826278448105\nEpoch 6, Train Loss: 2.5706000328063965, Train Acc: 82.87%, Train Discrimination: 0.02020670659840107 Test Loss: 2.108959913253784, Test Acc: 82.66%,Test Discrimination: 0.01374199241399765\nEpoch 7, Train Loss: 1.579584002494812, Train Acc: 82.87%, Train Discrimination: 0.008166934363543987 Test Loss: 1.367756724357605, Test Acc: 82.66%,Test Discrimination: 0.005068087484687567\nEpoch 8, Train Loss: 1.169830560684204, Train Acc: 82.87%, Train Discrimination: 0.0031696120277047157 Test Loss: 1.0890002250671387, Test Acc: 82.66%,Test Discrimination: 0.0017613265663385391\nEpoch 9, Train Loss: 1.017871379852295, Train Acc: 82.87%, Train Discrimination: 0.001234313822351396 Test Loss: 0.9949354529380798, Test Acc: 82.66%,Test Discrimination: 0.0005632542306557298\nEpoch 10, Train Loss: 0.9675214290618896, Train Acc: 82.87%, Train Discrimination: 0.0004987393622286618 Test Loss: 0.968485414981842, Test Acc: 82.66%,Test Discrimination: 0.0001494826574344188\nEpoch 11, Train Loss: 0.9534360766410828, Train Acc: 82.87%, Train Discrimination: 0.00021293242753017694 Test Loss: 0.9643883109092712, Test Acc: 82.66%,Test Discrimination: 2.184201730415225e-05\nEpoch 12, Train Loss: 0.9504719376564026, Train Acc: 82.87%, Train Discrimination: 9.624656377127394e-05 Test Loss: 0.9659908413887024, Test Acc: 82.66%,Test Discrimination: 3.8280805370050075e-07\nEpoch 13, Train Loss: 0.9492924213409424, Train Acc: 82.87%, Train Discrimination: 4.5405256969388574e-05 Test Loss: 0.9674115777015686, Test Acc: 82.66%,Test Discrimination: 2.1346231733332388e-05\nEpoch 14, Train Loss: 0.9465421438217163, Train Acc: 82.87%, Train Discrimination: 2.2052401618566364e-05 Test Loss: 0.9668164253234863, Test Acc: 82.66%,Test Discrimination: 6.389908958226442e-05\nEpoch 15, Train Loss: 0.9409127235412598, Train Acc: 82.87%, Train Discrimination: 1.068957590177888e-05 Test Loss: 0.9637989401817322, Test Acc: 82.66%,Test Discrimination: 0.00012362998677417636\nEpoch 16, Train Loss: 0.9319369792938232, Train Acc: 82.87%, Train Discrimination: 5.2250561566324905e-06 Test Loss: 0.9588673114776611, Test Acc: 82.66%,Test Discrimination: 0.00020664601470343769\nEpoch 17, Train Loss: 0.9193623065948486, Train Acc: 82.87%, Train Discrimination: 2.4458565803797683e-06 Test Loss: 0.9535431861877441, Test Acc: 82.66%,Test Discrimination: 0.00033388298470526934\nEpoch 18, Train Loss: 0.9028666615486145, Train Acc: 82.87%, Train Discrimination: 6.912121079949429e-07 Test Loss: 0.9499441981315613, Test Acc: 82.66%,Test Discrimination: 0.0005363122327253222\nEpoch 19, Train Loss: 0.8821762204170227, Train Acc: 82.87%, Train Discrimination: 1.3125004416636443e-09 Test Loss: 0.9531121253967285, Test Acc: 82.66%,Test Discrimination: 0.0008811535662971437\nEpoch 20, Train Loss: 0.8572307825088501, Train Acc: 82.87%, Train Discrimination: 1.345642090200272e-06 Test Loss: 0.973260223865509, Test Acc: 82.66%,Test Discrimination: 0.0015014660311862826\nEpoch 21, Train Loss: 0.8286730647087097, Train Acc: 82.87%, Train Discrimination: 7.08078732714057e-06 Test Loss: 1.0317484140396118, Test Acc: 82.66%,Test Discrimination: 0.002678176388144493\nEpoch 22, Train Loss: 0.7980899810791016, Train Acc: 82.87%, Train Discrimination: 2.1819911125930957e-05 Test Loss: 1.1767431497573853, Test Acc: 82.66%,Test Discrimination: 0.005080036818981171\nEpoch 23, Train Loss: 0.7685965299606323, Train Acc: 82.87%, Train Discrimination: 6.511683750431985e-05 Test Loss: 1.5102810859680176, Test Acc: 82.66%,Test Discrimination: 0.010152535513043404\nEpoch 24, Train Loss: 0.7478821277618408, Train Acc: 82.86%, Train Discrimination: 0.00019554585742298514 Test Loss: 2.153079032897949, Test Acc: 82.56%,Test Discrimination: 0.019485153257846832\nEpoch 25, Train Loss: 0.7506402134895325, Train Acc: 82.85%, Train Discrimination: 0.0004985617124475539 Test Loss: 2.7824699878692627, Test Acc: 80.54%,Test Discrimination: 0.0281366016715765\nEpoch 26, Train Loss: 0.7560930848121643, Train Acc: 80.69%, Train Discrimination: 0.0005774438031949103 Test Loss: 2.837212085723877, Test Acc: 78.68%,Test Discrimination: 0.028529640287160873\nEpoch 27, Train Loss: 0.7321838140487671, Train Acc: 78.70%, Train Discrimination: 0.00014320132322609425 Test Loss: 2.5272648334503174, Test Acc: 78.74%,Test Discrimination: 0.024176504462957382\nEpoch 28, Train Loss: 0.7269140481948853, Train Acc: 78.55%, Train Discrimination: 2.1906656911596656e-05 Test Loss: 2.2568483352661133, Test Acc: 79.79%,Test Discrimination: 0.02083236537873745\nEpoch 29, Train Loss: 0.7244833707809448, Train Acc: 79.85%, Train Discrimination: 8.998852717923e-05 Test Loss: 2.1351840496063232, Test Acc: 81.43%,Test Discrimination: 0.019712287932634354\nEpoch 30, Train Loss: 0.7057511210441589, Train Acc: 81.40%, Train Discrimination: 1.0529709470574744e-05 Test Loss: 2.1184818744659424, Test Acc: 82.34%,Test Discrimination: 0.020081672817468643\nEpoch 31, Train Loss: 0.697006106376648, Train Acc: 82.58%, Train Discrimination: 9.081877942662686e-05 Test Loss: 2.085883617401123, Test Acc: 82.66%,Test Discrimination: 0.02004893496632576\nEpoch 32, Train Loss: 0.7025237083435059, Train Acc: 82.86%, Train Discrimination: 0.0003208376292604953 Test Loss: 1.9577003717422485, Test Acc: 82.66%,Test Discrimination: 0.018389374017715454\nEpoch 33, Train Loss: 0.6999026536941528, Train Acc: 82.86%, Train Discrimination: 0.0003514807904139161 Test Loss: 1.792019248008728, Test Acc: 82.66%,Test Discrimination: 0.016021478921175003\nEpoch 34, Train Loss: 0.6904038786888123, Train Acc: 82.86%, Train Discrimination: 0.0002262700436403975 Test Loss: 1.646807074546814, Test Acc: 82.66%,Test Discrimination: 0.013883988372981548\nEpoch 35, Train Loss: 0.681954026222229, Train Acc: 82.86%, Train Discrimination: 8.955790690379217e-05 Test Loss: 1.5441527366638184, Test Acc: 82.66%,Test Discrimination: 0.012366348877549171\nEpoch 36, Train Loss: 0.6776283383369446, Train Acc: 82.86%, Train Discrimination: 1.2570098078867886e-05 Test Loss: 1.4808372259140015, Test Acc: 82.66%,Test Discrimination: 0.011473124846816063\nEpoch 37, Train Loss: 0.67612624168396, Train Acc: 82.86%, Train Discrimination: 7.96033361893933e-07 Test Loss: 1.4504456520080566, Test Acc: 82.66%,Test Discrimination: 0.011140910908579826\nEpoch 38, Train Loss: 0.6729456782341003, Train Acc: 82.86%, Train Discrimination: 4.136281404498732e-06 Test Loss: 1.4428966045379639, Test Acc: 82.66%,Test Discrimination: 0.011224158108234406\nEpoch 39, Train Loss: 0.6666244268417358, Train Acc: 82.86%, Train Discrimination: 2.770426021925232e-08 Test Loss: 1.454705834388733, Test Acc: 82.66%,Test Discrimination: 0.01165025308728218\nEpoch 40, Train Loss: 0.6600421071052551, Train Acc: 82.86%, Train Discrimination: 1.558014082547743e-05 Test Loss: 1.4815967082977295, Test Acc: 82.66%,Test Discrimination: 0.01232194621115923\nEpoch 41, Train Loss: 0.654700756072998, Train Acc: 82.86%, Train Discrimination: 5.453625271911733e-05 Test Loss: 1.5220767259597778, Test Acc: 82.69%,Test Discrimination: 0.013196742162108421\nEpoch 42, Train Loss: 0.6489155292510986, Train Acc: 82.86%, Train Discrimination: 7.614127389388159e-05 Test Loss: 1.5693572759628296, Test Acc: 82.69%,Test Discrimination: 0.014150256291031837\nEpoch 43, Train Loss: 0.6415050625801086, Train Acc: 82.86%, Train Discrimination: 5.184347173781134e-05 Test Loss: 1.61744225025177, Test Acc: 82.63%,Test Discrimination: 0.015085555613040924\nEpoch 44, Train Loss: 0.6343680024147034, Train Acc: 82.86%, Train Discrimination: 1.1204567272216082e-05 Test Loss: 1.6670290231704712, Test Acc: 82.60%,Test Discrimination: 0.01605314388871193\nEpoch 45, Train Loss: 0.6292930245399475, Train Acc: 82.75%, Train Discrimination: 4.155396027272218e-08 Test Loss: 1.7307188510894775, Test Acc: 82.44%,Test Discrimination: 0.017327889800071716\nEpoch 46, Train Loss: 0.6232834458351135, Train Acc: 82.66%, Train Discrimination: 3.589822483718308e-08 Test Loss: 1.7991687059402466, Test Acc: 82.28%,Test Discrimination: 0.018782418221235275\nEpoch 47, Train Loss: 0.6167179346084595, Train Acc: 82.61%, Train Discrimination: 2.0278188458178192e-05 Test Loss: 1.7985247373580933, Test Acc: 82.34%,Test Discrimination: 0.019137542694807053\nEpoch 48, Train Loss: 0.6112115383148193, Train Acc: 82.65%, Train Discrimination: 5.350239734980278e-05 Test Loss: 1.677141547203064, Test Acc: 82.53%,Test Discrimination: 0.017426714301109314\nEpoch 49, Train Loss: 0.6036722660064697, Train Acc: 82.79%, Train Discrimination: 1.9525708921719342e-05 Test Loss: 1.5230921506881714, Test Acc: 82.63%,Test Discrimination: 0.015103863552212715\nEpoch 50, Train Loss: 0.5979263782501221, Train Acc: 82.78%, Train Discrimination: 7.583054184578941e-08 Test Loss: 1.4365432262420654, Test Acc: 82.63%,Test Discrimination: 0.013927778229117393\nEpoch 51, Train Loss: 0.5918025374412537, Train Acc: 82.80%, Train Discrimination: 1.6969216858342406e-06 Test Loss: 1.415098786354065, Test Acc: 82.63%,Test Discrimination: 0.01388073991984129\nEpoch 52, Train Loss: 0.5860604047775269, Train Acc: 82.85%, Train Discrimination: 3.354820728418417e-05 Test Loss: 1.4109747409820557, Test Acc: 82.60%,Test Discrimination: 0.014095714315772057\nEpoch 53, Train Loss: 0.5798729658126831, Train Acc: 82.83%, Train Discrimination: 4.067596819368191e-05 Test Loss: 1.40736985206604, Test Acc: 82.72%,Test Discrimination: 0.014263376593589783\nEpoch 54, Train Loss: 0.5727940797805786, Train Acc: 82.83%, Train Discrimination: 2.2069218630349496e-06 Test Loss: 1.453865647315979, Test Acc: 82.82%,Test Discrimination: 0.01535029336810112\nEpoch 55, Train Loss: 0.5668334364891052, Train Acc: 82.82%, Train Discrimination: 1.5335253067405574e-07 Test Loss: 1.549080491065979, Test Acc: 82.75%,Test Discrimination: 0.017395274713635445\nEpoch 56, Train Loss: 0.5615146160125732, Train Acc: 82.69%, Train Discrimination: 3.865927646984346e-05 Test Loss: 1.477245569229126, Test Acc: 82.75%,Test Discrimination: 0.016369853168725967\nEpoch 57, Train Loss: 0.555031955242157, Train Acc: 82.66%, Train Discrimination: 1.0313720849808306e-05 Test Loss: 1.3424140214920044, Test Acc: 82.88%,Test Discrimination: 0.01418379507958889\nEpoch 58, Train Loss: 0.5494706034660339, Train Acc: 82.78%, Train Discrimination: 2.1201040567575546e-07 Test Loss: 1.2921582460403442, Test Acc: 82.91%,Test Discrimination: 0.013602794148027897\nEpoch 59, Train Loss: 0.5441387295722961, Train Acc: 82.96%, Train Discrimination: 4.386077125673182e-05 Test Loss: 1.2002079486846924, Test Acc: 82.94%,Test Discrimination: 0.012090937234461308\nEpoch 60, Train Loss: 0.5382333397865295, Train Acc: 82.97%, Train Discrimination: 7.675248525629286e-06 Test Loss: 1.1533699035644531, Test Acc: 83.01%,Test Discrimination: 0.011414957232773304\nEpoch 61, Train Loss: 0.5336291790008545, Train Acc: 82.88%, Train Discrimination: 1.3861312027074746e-06 Test Loss: 1.2151113748550415, Test Acc: 82.98%,Test Discrimination: 0.01291086245328188\nEpoch 62, Train Loss: 0.5291166305541992, Train Acc: 82.90%, Train Discrimination: 6.526482320623472e-05 Test Loss: 1.1653772592544556, Test Acc: 83.04%,Test Discrimination: 0.0120852030813694\nEpoch 63, Train Loss: 0.5240165591239929, Train Acc: 82.76%, Train Discrimination: 4.348344646132318e-06 Test Loss: 1.1964033842086792, Test Acc: 83.29%,Test Discrimination: 0.012983974069356918\nEpoch 64, Train Loss: 0.5189733505249023, Train Acc: 82.95%, Train Discrimination: 4.06902254326269e-05 Test Loss: 1.0838316679000854, Test Acc: 83.35%,Test Discrimination: 0.01095158327370882\nEpoch 65, Train Loss: 0.5137745141983032, Train Acc: 83.04%, Train Discrimination: 4.1675551898379126e-08 Test Loss: 1.0343509912490845, Test Acc: 83.61%,Test Discrimination: 0.010207772254943848\nEpoch 66, Train Loss: 0.5094195604324341, Train Acc: 83.16%, Train Discrimination: 2.6484041882213205e-05 Test Loss: 0.9570425748825073, Test Acc: 83.64%,Test Discrimination: 0.008809830993413925\nEpoch 67, Train Loss: 0.5055661201477051, Train Acc: 83.15%, Train Discrimination: 4.04616366722621e-06 Test Loss: 0.9909486770629883, Test Acc: 83.86%,Test Discrimination: 0.009737719781696796\nEpoch 68, Train Loss: 0.5033428072929382, Train Acc: 83.31%, Train Discrimination: 9.606428648112342e-05 Test Loss: 0.8988428711891174, Test Acc: 83.58%,Test Discrimination: 0.00791251752525568\nEpoch 69, Train Loss: 0.5078108906745911, Train Acc: 83.18%, Train Discrimination: 0.00016399893502239138 Test Loss: 1.1021060943603516, Test Acc: 84.02%,Test Discrimination: 0.012495671398937702\nEpoch 70, Train Loss: 0.525585412979126, Train Acc: 83.61%, Train Discrimination: 0.0007948702550493181 Test Loss: 0.8282182812690735, Test Acc: 83.64%,Test Discrimination: 0.006742666941136122\nEpoch 71, Train Loss: 0.5039606094360352, Train Acc: 83.34%, Train Discrimination: 0.0002280757762491703 Test Loss: 0.8144513368606567, Test Acc: 83.89%,Test Discrimination: 0.006632653996348381\nEpoch 72, Train Loss: 0.48751363158226013, Train Acc: 83.49%, Train Discrimination: 1.1555933269846719e-05 Test Loss: 0.8771834373474121, Test Acc: 84.08%,Test Discrimination: 0.008109557442367077\nEpoch 73, Train Loss: 0.5013831853866577, Train Acc: 83.74%, Train Discrimination: 0.00043187206028960645 Test Loss: 0.7832282781600952, Test Acc: 84.02%,Test Discrimination: 0.006211674772202969\nEpoch 74, Train Loss: 0.481157124042511, Train Acc: 83.67%, Train Discrimination: 1.1442743925726973e-05 Test Loss: 0.7684154510498047, Test Acc: 83.99%,Test Discrimination: 0.005973204970359802\nEpoch 75, Train Loss: 0.48704633116722107, Train Acc: 83.62%, Train Discrimination: 0.00015960162272676826 Test Loss: 0.9038161635398865, Test Acc: 84.11%,Test Discrimination: 0.009107229299843311\nEpoch 76, Train Loss: 0.4827423691749573, Train Acc: 83.86%, Train Discrimination: 0.00025101209757849574 Test Loss: 0.844440221786499, Test Acc: 84.05%,Test Discrimination: 0.007919040508568287\nEpoch 77, Train Loss: 0.47325801849365234, Train Acc: 83.85%, Train Discrimination: 8.512281783623621e-05 Test Loss: 0.7099458575248718, Test Acc: 83.89%,Test Discrimination: 0.005035607144236565\nEpoch 78, Train Loss: 0.4788656234741211, Train Acc: 83.73%, Train Discrimination: 0.00017695823044050485 Test Loss: 0.7229729294776917, Test Acc: 84.05%,Test Discrimination: 0.005457676015794277\nEpoch 79, Train Loss: 0.46542054414749146, Train Acc: 83.84%, Train Discrimination: 2.8157503493275726e-06 Test Loss: 0.7528676986694336, Test Acc: 84.11%,Test Discrimination: 0.006232020445168018\nEpoch 80, Train Loss: 0.4708258807659149, Train Acc: 83.81%, Train Discrimination: 0.0002139747084584087 Test Loss: 0.6953473091125488, Test Acc: 84.05%,Test Discrimination: 0.005041742231696844\nEpoch 81, Train Loss: 0.4600912630558014, Train Acc: 83.81%, Train Discrimination: 9.967582315084655e-11 Test Loss: 0.6754117012023926, Test Acc: 83.89%,Test Discrimination: 0.004670294933021069\nEpoch 82, Train Loss: 0.4633500576019287, Train Acc: 83.73%, Train Discrimination: 9.539071470499039e-05 Test Loss: 0.7612380385398865, Test Acc: 83.92%,Test Discrimination: 0.006725351791828871\nEpoch 83, Train Loss: 0.456024706363678, Train Acc: 83.92%, Train Discrimination: 4.94365522172302e-05 Test Loss: 0.7638309597969055, Test Acc: 83.92%,Test Discrimination: 0.006897918414324522\nEpoch 84, Train Loss: 0.45604079961776733, Train Acc: 83.96%, Train Discrimination: 0.0001200683182105422 Test Loss: 0.6517820954322815, Test Acc: 83.83%,Test Discrimination: 0.0044163865968585014\nEpoch 85, Train Loss: 0.45163944363594055, Train Acc: 83.83%, Train Discrimination: 2.2032376364222728e-05 Test Loss: 0.6195765137672424, Test Acc: 83.77%,Test Discrimination: 0.003758012782782316\nEpoch 86, Train Loss: 0.44985491037368774, Train Acc: 83.77%, Train Discrimination: 2.726267121033743e-05 Test Loss: 0.6547641158103943, Test Acc: 83.89%,Test Discrimination: 0.004662782419472933\nEpoch 87, Train Loss: 0.44728314876556396, Train Acc: 83.82%, Train Discrimination: 6.0248872614465654e-05 Test Loss: 0.6544946432113647, Test Acc: 83.92%,Test Discrimination: 0.004742877092212439\nEpoch 88, Train Loss: 0.4450967013835907, Train Acc: 83.88%, Train Discrimination: 6.504591146949679e-05 Test Loss: 0.6115290522575378, Test Acc: 83.83%,Test Discrimination: 0.0038061051163822412\nEpoch 89, Train Loss: 0.4423922300338745, Train Acc: 83.84%, Train Discrimination: 1.5029850146675017e-05 Test Loss: 0.6300622820854187, Test Acc: 83.89%,Test Discrimination: 0.004305840469896793\nEpoch 90, Train Loss: 0.44006112217903137, Train Acc: 83.86%, Train Discrimination: 8.594597602495924e-06 Test Loss: 0.6947887539863586, Test Acc: 83.95%,Test Discrimination: 0.005898856557905674\nEpoch 91, Train Loss: 0.43969130516052246, Train Acc: 83.94%, Train Discrimination: 7.174794882303104e-05 Test Loss: 0.6260159611701965, Test Acc: 83.80%,Test Discrimination: 0.0043615191243588924\nEpoch 92, Train Loss: 0.43526676297187805, Train Acc: 83.95%, Train Discrimination: 3.628321792348288e-06 Test Loss: 0.5677086710929871, Test Acc: 83.80%,Test Discrimination: 0.0030321876984089613\nEpoch 93, Train Loss: 0.4352673888206482, Train Acc: 83.90%, Train Discrimination: 2.0909761587972753e-05 Test Loss: 0.5836467146873474, Test Acc: 83.80%,Test Discrimination: 0.0034746676683425903\nEpoch 94, Train Loss: 0.43207043409347534, Train Acc: 83.99%, Train Discrimination: 7.627435479662381e-06 Test Loss: 0.6017699837684631, Test Acc: 83.64%,Test Discrimination: 0.003975182306021452\nEpoch 95, Train Loss: 0.43172743916511536, Train Acc: 84.02%, Train Discrimination: 5.6273143854923546e-05 Test Loss: 0.5661624670028687, Test Acc: 83.77%,Test Discrimination: 0.0031671752221882343\nEpoch 96, Train Loss: 0.42878711223602295, Train Acc: 84.00%, Train Discrimination: 2.1105093139794917e-07 Test Loss: 0.5655755996704102, Test Acc: 83.89%,Test Discrimination: 0.0032055818010121584\nEpoch 97, Train Loss: 0.4277358949184418, Train Acc: 84.03%, Train Discrimination: 7.55599603508017e-06 Test Loss: 0.6209938526153564, Test Acc: 83.95%,Test Discrimination: 0.004590899217873812\nEpoch 98, Train Loss: 0.42664745450019836, Train Acc: 84.00%, Train Discrimination: 3.3408530725864694e-05 Test Loss: 0.5862168669700623, Test Acc: 83.89%,Test Discrimination: 0.003801785409450531\nEpoch 99, Train Loss: 0.42424747347831726, Train Acc: 83.99%, Train Discrimination: 1.6426238289568573e-06 Test Loss: 0.5501956939697266, Test Acc: 83.83%,Test Discrimination: 0.0029715769924223423\nEpoch 100, Train Loss: 0.4236089289188385, Train Acc: 84.07%, Train Discrimination: 8.231560059357435e-06 Test Loss: 0.5715388059616089, Test Acc: 83.86%,Test Discrimination: 0.0035419573541730642\nimport plotly.graph_objects as go\n\ndef plot_metric(title, y_label, train_data, test_data):\n    epochs = list(range(1, 101))\n    fig = go.Figure()\n\n    # Adding Train Line with Markers\n    fig.add_trace(go.Scatter(\n        x=epochs, y=train_data, mode='lines+markers',\n        name='Train',\n        line=dict(color='RoyalBlue', width=2),\n        marker=dict(color='RoyalBlue', size=6, line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    # Adding Test Line with Markers\n    fig.add_trace(go.Scatter(\n        x=epochs, y=test_data, mode='lines+markers',\n        name='Test',\n        line=dict(color='Crimson', width=2, dash='dot'),\n        marker=dict(color='Crimson', size=6, line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    # Update Layout\n    fig.update_layout(\n        title={'text': title, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n        xaxis_title='Epoch',\n        yaxis_title=y_label,\n        legend=dict(x=0.1, y=1.1, orientation='h'),\n        font=dict(family=\"Helvetica, Arial, sans-serif\", size=12, color=\"black\"),\n        plot_bgcolor='white',\n        margin=dict(l=40, r=40, t=40, b=30)\n    )\n\n    # Gridlines and Axes styles\n    fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='LightGrey')\n    fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='LightGrey')\n\n    fig.show()\n\n# Example usage\nplot_metric(\"Training and Testing Loss\", \"Loss\", train_losses, test_losses)\nplot_metric(\"Training and Testing Accuracy\", \"Accuracy (%)\", train_accuracies, test_accuracies)\nplot_metric(\"Training and Testing Discrimination\", \"Discrimination\", train_discriminations, test_discriminations)\nplot_metric(\"Training and Testing Fairness\", \"Fairness\", train_fairness, test_fairness)\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "disc_loss_fn.html#bce-loss-fn",
    "href": "disc_loss_fn.html#bce-loss-fn",
    "title": "Discrimination Loss Fn",
    "section": "BCE Loss Fn",
    "text": "BCE Loss Fn\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Custom loss function|\ndef discrimination_loss(output, target, sensitive_features, lambda_val=100, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(output, target)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    #discrimination=torch.abs(prob_priv)\n    prob_unpriv = torch.mean(output[mask_unpriv])\n    prob_priv = torch.mean(output[mask_priv])\n    discrimination = lambda_val*(prob_priv - prob_unpriv) ** k\n\n# Handle cases where one group might be missing\n    #discrimination=torch.abs(prob_priv)\n \n    loss_val=standard_loss\n\n    return loss_val,discrimination.item() \n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\n\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\n\n# Correctly preparing the sensitive features\nthreshold = 0.5  # Adjust the threshold according to your specific case\nsensitive_features = torch.tensor((data[:, 1].numpy() &gt; threshold).astype(float)).float()\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\n# Assuming similar preparation for test data\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = torch.tensor((test_data[:, 1].numpy() &gt; threshold).astype(float)).float()\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ntrain_losses1, train_accuracies1, train_discriminations1,train_fairness1 = [], [], [],[]\ntest_losses1, test_accuracies1, test_discriminations1,test_fairness1 = [], [], [],[]\n# Training loop\nmodel.train()\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model(features)\n    loss, discrimination = discrimination_loss(outputs, targets.squeeze(), sensitive_features)\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluation on test data\n    model.eval()\n    with torch.no_grad():\n        test_outputs = model(test_features)\n        test_loss,test_discrimination = discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n    \n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, Train Discrimination: {discrimination} '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%,Test Discrimination: {test_discrimination}')\n    train_losses1.append(loss.item())\n    train_accuracies1.append(train_accuracy.item() * 100)\n    train_discriminations1.append(discrimination)\n    fairness=1-discrimination\n    train_fairness1.append(fairness)\n    test_fairness1.append(fairness)\n    test_losses1.append(test_loss.item())\n    test_accuracies1.append(test_accuracy.item() * 100)\n    test_discriminations1.append(test_discrimination)\n    model.train()\n\nEpoch 1, Train Loss: 0.6893765330314636, Train Acc: 56.96%, Train Discrimination: 0.028892265632748604 Test Loss: 0.8665273189544678, Test Acc: 82.66%,Test Discrimination: 0.11413952708244324\nEpoch 2, Train Loss: 0.8494628667831421, Train Acc: 82.87%, Train Discrimination: 0.0646490603685379 Test Loss: 0.7340521812438965, Test Acc: 82.66%,Test Discrimination: 0.02978677488863468\nEpoch 3, Train Loss: 0.7180451154708862, Train Acc: 82.87%, Train Discrimination: 0.01662645861506462 Test Loss: 0.49305757880210876, Test Acc: 82.66%,Test Discrimination: 0.0008751878049224615\nEpoch 4, Train Loss: 0.4822317659854889, Train Acc: 82.87%, Train Discrimination: 0.0016992390155792236 Test Loss: 0.40948766469955444, Test Acc: 84.37%,Test Discrimination: 0.5070899724960327\nEpoch 5, Train Loss: 0.4052659273147583, Train Acc: 84.97%, Train Discrimination: 0.41889819502830505 Test Loss: 0.5344403386116028, Test Acc: 71.70%,Test Discrimination: 1.4629510641098022\nEpoch 6, Train Loss: 0.5349717736244202, Train Acc: 72.08%, Train Discrimination: 1.1699885129928589 Test Loss: 0.4432348310947418, Test Acc: 82.91%,Test Discrimination: 1.1179109811782837\nEpoch 7, Train Loss: 0.44207313656806946, Train Acc: 82.79%, Train Discrimination: 0.9448722004890442 Test Loss: 0.38032716512680054, Test Acc: 84.27%,Test Discrimination: 0.44283074140548706\nEpoch 8, Train Loss: 0.3749988079071045, Train Acc: 85.03%, Train Discrimination: 0.4019760489463806 Test Loss: 0.403175413608551, Test Acc: 83.48%,Test Discrimination: 0.15531465411186218\nEpoch 9, Train Loss: 0.3944748342037201, Train Acc: 83.99%, Train Discrimination: 0.1460663229227066 Test Loss: 0.4298821985721588, Test Acc: 83.10%,Test Discrimination: 0.08754545450210571\nEpoch 10, Train Loss: 0.4196593761444092, Train Acc: 83.64%, Train Discrimination: 0.08160459250211716 Test Loss: 0.42858901619911194, Test Acc: 83.16%,Test Discrimination: 0.08645991235971451\nEpoch 11, Train Loss: 0.41827115416526794, Train Acc: 83.73%, Train Discrimination: 0.07904145121574402 Test Loss: 0.405176043510437, Test Acc: 83.67%,Test Discrimination: 0.1285136193037033\nEpoch 12, Train Loss: 0.39585673809051514, Train Acc: 84.05%, Train Discrimination: 0.11706404387950897 Test Loss: 0.3776590824127197, Test Acc: 84.21%,Test Discrimination: 0.23645246028900146\nEpoch 13, Train Loss: 0.3701803684234619, Train Acc: 84.74%, Train Discrimination: 0.21740812063217163 Test Loss: 0.36462756991386414, Test Acc: 84.78%,Test Discrimination: 0.44214367866516113\nEpoch 14, Train Loss: 0.35978808999061584, Train Acc: 85.26%, Train Discrimination: 0.4071773886680603 Test Loss: 0.37136322259902954, Test Acc: 85.06%,Test Discrimination: 0.7065772414207458\nEpoch 15, Train Loss: 0.3690268397331238, Train Acc: 85.07%, Train Discrimination: 0.6450950503349304 Test Loss: 0.3793603777885437, Test Acc: 85.15%,Test Discrimination: 0.8762152194976807\nEpoch 16, Train Loss: 0.3783196806907654, Train Acc: 84.62%, Train Discrimination: 0.7950994968414307 Test Loss: 0.3731483221054077, Test Acc: 85.47%,Test Discrimination: 0.872829020023346\nEpoch 17, Train Loss: 0.37199270725250244, Train Acc: 84.83%, Train Discrimination: 0.7976083159446716 Test Loss: 0.3602275550365448, Test Acc: 85.63%,Test Discrimination: 0.7373597621917725\nEpoch 18, Train Loss: 0.35798850655555725, Train Acc: 85.40%, Train Discrimination: 0.6868329644203186 Test Loss: 0.35391145944595337, Test Acc: 85.15%,Test Discrimination: 0.558799147605896\nEpoch 19, Train Loss: 0.3501380681991577, Train Acc: 85.72%, Train Discrimination: 0.5334701538085938 Test Loss: 0.35717904567718506, Test Acc: 84.84%,Test Discrimination: 0.4125717878341675\nEpoch 20, Train Loss: 0.35183075070381165, Train Acc: 85.71%, Train Discrimination: 0.4035780727863312 Test Loss: 0.36388465762138367, Test Acc: 84.78%,Test Discrimination: 0.32251864671707153\nEpoch 21, Train Loss: 0.35740622878074646, Train Acc: 85.53%, Train Discrimination: 0.32156074047088623 Test Loss: 0.36761748790740967, Test Acc: 84.46%,Test Discrimination: 0.2813608944416046\nEpoch 22, Train Loss: 0.36062532663345337, Train Acc: 85.31%, Train Discrimination: 0.28369131684303284 Test Loss: 0.36592987179756165, Test Acc: 84.46%,Test Discrimination: 0.2781275808811188\nEpoch 23, Train Loss: 0.3590368628501892, Train Acc: 85.31%, Train Discrimination: 0.2813844382762909 Test Loss: 0.3602959215641022, Test Acc: 84.81%,Test Discrimination: 0.3062696158885956\nEpoch 24, Train Loss: 0.35391226410865784, Train Acc: 85.53%, Train Discrimination: 0.3087084889411926 Test Loss: 0.3541712462902069, Test Acc: 84.87%,Test Discrimination: 0.3615400195121765\nEpoch 25, Train Loss: 0.34844857454299927, Train Acc: 85.78%, Train Discrimination: 0.3611733019351959 Test Loss: 0.3506123423576355, Test Acc: 85.12%,Test Discrimination: 0.4371921122074127\nEpoch 26, Train Loss: 0.3456374704837799, Train Acc: 85.89%, Train Discrimination: 0.43151891231536865 Test Loss: 0.3504486382007599, Test Acc: 85.31%,Test Discrimination: 0.5199912190437317\nEpoch 27, Train Loss: 0.34624630212783813, Train Acc: 85.85%, Train Discrimination: 0.5065035820007324 Test Loss: 0.35205116868019104, Test Acc: 85.63%,Test Discrimination: 0.5924646258354187\nEpoch 28, Train Loss: 0.34839606285095215, Train Acc: 85.76%, Train Discrimination: 0.5701208114624023 Test Loss: 0.35275527834892273, Test Acc: 85.85%,Test Discrimination: 0.6370248794555664\nEpoch 29, Train Loss: 0.34930771589279175, Train Acc: 85.81%, Train Discrimination: 0.6082808971405029 Test Loss: 0.3512765169143677, Test Acc: 85.94%,Test Discrimination: 0.6456495523452759\nEpoch 30, Train Loss: 0.34765130281448364, Train Acc: 85.86%, Train Discrimination: 0.6136332750320435 Test Loss: 0.3484106957912445, Test Acc: 85.63%,Test Discrimination: 0.6203233599662781\nEpoch 31, Train Loss: 0.3443194031715393, Train Acc: 85.92%, Train Discrimination: 0.5881646871566772 Test Loss: 0.3460047245025635, Test Acc: 85.44%,Test Discrimination: 0.5660967826843262\nEpoch 32, Train Loss: 0.3411797285079956, Train Acc: 85.98%, Train Discrimination: 0.5363804697990417 Test Loss: 0.34660080075263977, Test Acc: 85.03%,Test Discrimination: 0.48366230726242065\nEpoch 33, Train Loss: 0.34083840250968933, Train Acc: 85.87%, Train Discrimination: 0.45925331115722656 Test Loss: 0.34859907627105713, Test Acc: 85.03%,Test Discrimination: 0.44366443157196045\nEpoch 34, Train Loss: 0.3422602415084839, Train Acc: 85.87%, Train Discrimination: 0.42211058735847473 Test Loss: 0.34842362999916077, Test Acc: 85.12%,Test Discrimination: 0.4580681324005127\nEpoch 35, Train Loss: 0.3420337736606598, Train Acc: 85.88%, Train Discrimination: 0.43511027097702026 Test Loss: 0.34670528769493103, Test Acc: 85.09%,Test Discrimination: 0.509789228439331\nEpoch 36, Train Loss: 0.34059980511665344, Train Acc: 85.91%, Train Discrimination: 0.4828766882419586 Test Loss: 0.34537655115127563, Test Acc: 85.25%,Test Discrimination: 0.5624973773956299\nEpoch 37, Train Loss: 0.3396434485912323, Train Acc: 86.03%, Train Discrimination: 0.5316358804702759 Test Loss: 0.3443288207054138, Test Acc: 85.34%,Test Discrimination: 0.6093973517417908\nEpoch 38, Train Loss: 0.338971883058548, Train Acc: 86.03%, Train Discrimination: 0.5730817317962646 Test Loss: 0.3436129689216614, Test Acc: 85.66%,Test Discrimination: 0.6492713093757629\nEpoch 39, Train Loss: 0.33865389227867126, Train Acc: 86.05%, Train Discrimination: 0.6067491769790649 Test Loss: 0.3431760370731354, Test Acc: 85.63%,Test Discrimination: 0.6723722219467163\nEpoch 40, Train Loss: 0.33838164806365967, Train Acc: 86.11%, Train Discrimination: 0.6271101236343384 Test Loss: 0.3427199125289917, Test Acc: 85.69%,Test Discrimination: 0.6735063195228577\nEpoch 41, Train Loss: 0.33782997727394104, Train Acc: 86.10%, Train Discrimination: 0.6293227076530457 Test Loss: 0.3422622084617615, Test Acc: 85.69%,Test Discrimination: 0.6547510027885437\nEpoch 42, Train Loss: 0.3370249271392822, Train Acc: 86.09%, Train Discrimination: 0.6146485209465027 Test Loss: 0.34194856882095337, Test Acc: 85.63%,Test Discrimination: 0.622936487197876\nEpoch 43, Train Loss: 0.336249977350235, Train Acc: 86.09%, Train Discrimination: 0.5883059501647949 Test Loss: 0.34171062707901, Test Acc: 85.41%,Test Discrimination: 0.587398111820221\nEpoch 44, Train Loss: 0.3356246054172516, Train Acc: 86.13%, Train Discrimination: 0.5576801896095276 Test Loss: 0.34146469831466675, Test Acc: 85.34%,Test Discrimination: 0.5568069815635681\nEpoch 45, Train Loss: 0.33514276146888733, Train Acc: 86.10%, Train Discrimination: 0.530700147151947 Test Loss: 0.3410678207874298, Test Acc: 85.34%,Test Discrimination: 0.538408100605011\nEpoch 46, Train Loss: 0.3346046209335327, Train Acc: 86.10%, Train Discrimination: 0.5130552053451538 Test Loss: 0.34082841873168945, Test Acc: 85.31%,Test Discrimination: 0.5392794609069824\nEpoch 47, Train Loss: 0.33392462134361267, Train Acc: 86.15%, Train Discrimination: 0.5127732157707214 Test Loss: 0.3406904935836792, Test Acc: 85.50%,Test Discrimination: 0.5777758359909058\nEpoch 48, Train Loss: 0.33346039056777954, Train Acc: 86.31%, Train Discrimination: 0.5492798089981079 Test Loss: 0.34042471647262573, Test Acc: 85.38%,Test Discrimination: 0.5883434414863586\nEpoch 49, Train Loss: 0.3329395651817322, Train Acc: 86.36%, Train Discrimination: 0.5596801042556763 Test Loss: 0.3402789533138275, Test Acc: 85.44%,Test Discrimination: 0.564018726348877\nEpoch 50, Train Loss: 0.3322851061820984, Train Acc: 86.31%, Train Discrimination: 0.5371590852737427 Test Loss: 0.34003692865371704, Test Acc: 85.38%,Test Discrimination: 0.5607860684394836\nEpoch 51, Train Loss: 0.33175668120384216, Train Acc: 86.32%, Train Discrimination: 0.5351575613021851 Test Loss: 0.3393281400203705, Test Acc: 85.28%,Test Discrimination: 0.5884456038475037\nEpoch 52, Train Loss: 0.33099010586738586, Train Acc: 86.36%, Train Discrimination: 0.563715934753418 Test Loss: 0.3385639786720276, Test Acc: 85.69%,Test Discrimination: 0.6353873014450073\nEpoch 53, Train Loss: 0.3303709924221039, Train Acc: 86.39%, Train Discrimination: 0.6094982624053955 Test Loss: 0.33810877799987793, Test Acc: 85.75%,Test Discrimination: 0.6539232134819031\nEpoch 54, Train Loss: 0.3297540247440338, Train Acc: 86.38%, Train Discrimination: 0.6278684139251709 Test Loss: 0.3378382623195648, Test Acc: 85.69%,Test Discrimination: 0.6449885368347168\nEpoch 55, Train Loss: 0.3290434181690216, Train Acc: 86.40%, Train Discrimination: 0.6200804114341736 Test Loss: 0.3376803994178772, Test Acc: 85.72%,Test Discrimination: 0.6379352807998657\nEpoch 56, Train Loss: 0.32844457030296326, Train Acc: 86.32%, Train Discrimination: 0.6120428442955017 Test Loss: 0.33716490864753723, Test Acc: 85.66%,Test Discrimination: 0.653712809085846\nEpoch 57, Train Loss: 0.3276909291744232, Train Acc: 86.36%, Train Discrimination: 0.6235107779502869 Test Loss: 0.33645111322402954, Test Acc: 85.69%,Test Discrimination: 0.6918460726737976\nEpoch 58, Train Loss: 0.32676252722740173, Train Acc: 86.43%, Train Discrimination: 0.6560997366905212 Test Loss: 0.3356865644454956, Test Acc: 85.66%,Test Discrimination: 0.7060287594795227\nEpoch 59, Train Loss: 0.3256959319114685, Train Acc: 86.44%, Train Discrimination: 0.666750431060791 Test Loss: 0.33536139130592346, Test Acc: 85.66%,Test Discrimination: 0.6983868479728699\nEpoch 60, Train Loss: 0.3248368799686432, Train Acc: 86.46%, Train Discrimination: 0.660330593585968 Test Loss: 0.33474215865135193, Test Acc: 85.88%,Test Discrimination: 0.7734810709953308\nEpoch 61, Train Loss: 0.3241473436355591, Train Acc: 86.55%, Train Discrimination: 0.7274165153503418 Test Loss: 0.3346967101097107, Test Acc: 85.88%,Test Discrimination: 0.7544864416122437\nEpoch 62, Train Loss: 0.3234710097312927, Train Acc: 86.51%, Train Discrimination: 0.7115174531936646 Test Loss: 0.3344646990299225, Test Acc: 85.85%,Test Discrimination: 0.7700434327125549\nEpoch 63, Train Loss: 0.3228937089443207, Train Acc: 86.51%, Train Discrimination: 0.7257398962974548 Test Loss: 0.3340303599834442, Test Acc: 85.88%,Test Discrimination: 0.836399495601654\nEpoch 64, Train Loss: 0.32239267230033875, Train Acc: 86.66%, Train Discrimination: 0.7845637202262878 Test Loss: 0.3344355523586273, Test Acc: 85.98%,Test Discrimination: 0.7675795555114746\nEpoch 65, Train Loss: 0.32194194197654724, Train Acc: 86.51%, Train Discrimination: 0.7250076532363892 Test Loss: 0.3336296081542969, Test Acc: 86.04%,Test Discrimination: 0.9147170186042786\nEpoch 66, Train Loss: 0.3215138018131256, Train Acc: 86.74%, Train Discrimination: 0.8539789319038391 Test Loss: 0.3339884281158447, Test Acc: 85.91%,Test Discrimination: 0.802850604057312\nEpoch 67, Train Loss: 0.3208818733692169, Train Acc: 86.61%, Train Discrimination: 0.7571476101875305 Test Loss: 0.33318930864334106, Test Acc: 86.07%,Test Discrimination: 0.8864215612411499\nEpoch 68, Train Loss: 0.32016313076019287, Train Acc: 86.69%, Train Discrimination: 0.8304963111877441 Test Loss: 0.33291688561439514, Test Acc: 86.10%,Test Discrimination: 0.9457162618637085\nEpoch 69, Train Loss: 0.31984007358551025, Train Acc: 86.77%, Train Discrimination: 0.8820505738258362 Test Loss: 0.33355897665023804, Test Acc: 85.85%,Test Discrimination: 0.8437219262123108\nEpoch 70, Train Loss: 0.319575697183609, Train Acc: 86.76%, Train Discrimination: 0.7928959131240845 Test Loss: 0.3325559198856354, Test Acc: 86.07%,Test Discrimination: 0.9743655323982239\nEpoch 71, Train Loss: 0.3189741373062134, Train Acc: 86.79%, Train Discrimination: 0.9075414538383484 Test Loss: 0.3324671685695648, Test Acc: 86.10%,Test Discrimination: 0.9538917541503906\nEpoch 72, Train Loss: 0.3184824287891388, Train Acc: 86.84%, Train Discrimination: 0.8906792998313904 Test Loss: 0.3328649699687958, Test Acc: 85.91%,Test Discrimination: 0.8971131443977356\nEpoch 73, Train Loss: 0.31828832626342773, Train Acc: 86.76%, Train Discrimination: 0.8422393202781677 Test Loss: 0.3321385085582733, Test Acc: 85.98%,Test Discrimination: 1.0253230333328247\nEpoch 74, Train Loss: 0.3179641366004944, Train Acc: 86.83%, Train Discrimination: 0.9551102519035339 Test Loss: 0.3323608338832855, Test Acc: 86.17%,Test Discrimination: 0.9427615404129028\nEpoch 75, Train Loss: 0.3174552321434021, Train Acc: 86.81%, Train Discrimination: 0.8845437169075012 Test Loss: 0.33224791288375854, Test Acc: 86.07%,Test Discrimination: 0.9520696401596069\nEpoch 76, Train Loss: 0.31711119413375854, Train Acc: 86.83%, Train Discrimination: 0.8933191299438477 Test Loss: 0.3318634033203125, Test Acc: 85.88%,Test Discrimination: 1.037575364112854\nEpoch 77, Train Loss: 0.3169209361076355, Train Acc: 86.77%, Train Discrimination: 0.9679976105690002 Test Loss: 0.33232301473617554, Test Acc: 86.07%,Test Discrimination: 0.933535099029541\nEpoch 78, Train Loss: 0.31655648350715637, Train Acc: 86.84%, Train Discrimination: 0.8780548572540283 Test Loss: 0.33179783821105957, Test Acc: 85.94%,Test Discrimination: 0.9951685667037964\nEpoch 79, Train Loss: 0.31612369418144226, Train Acc: 86.85%, Train Discrimination: 0.9319747686386108 Test Loss: 0.33166858553886414, Test Acc: 85.94%,Test Discrimination: 1.015641212463379\nEpoch 80, Train Loss: 0.3158622980117798, Train Acc: 86.94%, Train Discrimination: 0.9494182467460632 Test Loss: 0.3321272134780884, Test Acc: 86.01%,Test Discrimination: 0.9333077073097229\nEpoch 81, Train Loss: 0.31563952565193176, Train Acc: 86.97%, Train Discrimination: 0.8777487874031067 Test Loss: 0.3314824104309082, Test Acc: 85.98%,Test Discrimination: 1.0153406858444214\nEpoch 82, Train Loss: 0.31527379155158997, Train Acc: 86.88%, Train Discrimination: 0.9482680559158325 Test Loss: 0.33151355385780334, Test Acc: 86.07%,Test Discrimination: 0.9788159132003784\nEpoch 83, Train Loss: 0.3149093985557556, Train Acc: 86.96%, Train Discrimination: 0.9160862565040588 Test Loss: 0.33172643184661865, Test Acc: 86.01%,Test Discrimination: 0.9364306926727295\nEpoch 84, Train Loss: 0.3147119879722595, Train Acc: 87.07%, Train Discrimination: 0.8783939480781555 Test Loss: 0.33136454224586487, Test Acc: 86.01%,Test Discrimination: 1.008489727973938\nEpoch 85, Train Loss: 0.3145003914833069, Train Acc: 87.01%, Train Discrimination: 0.9396288394927979 Test Loss: 0.3317037522792816, Test Acc: 85.91%,Test Discrimination: 0.9213685989379883\nEpoch 86, Train Loss: 0.31423279643058777, Train Acc: 87.07%, Train Discrimination: 0.8646906614303589 Test Loss: 0.33114081621170044, Test Acc: 86.01%,Test Discrimination: 0.9889554381370544\nEpoch 87, Train Loss: 0.31390252709388733, Train Acc: 87.03%, Train Discrimination: 0.9235911965370178 Test Loss: 0.33105161786079407, Test Acc: 86.07%,Test Discrimination: 0.976599931716919\nEpoch 88, Train Loss: 0.31362399458885193, Train Acc: 87.01%, Train Discrimination: 0.9139345288276672 Test Loss: 0.33122849464416504, Test Acc: 85.88%,Test Discrimination: 0.9288054704666138\nEpoch 89, Train Loss: 0.3134077787399292, Train Acc: 87.01%, Train Discrimination: 0.8731881976127625 Test Loss: 0.33072996139526367, Test Acc: 86.01%,Test Discrimination: 0.9963604807853699\nEpoch 90, Train Loss: 0.3131285607814789, Train Acc: 87.00%, Train Discrimination: 0.9329630136489868 Test Loss: 0.3308558464050293, Test Acc: 86.04%,Test Discrimination: 0.9482054114341736\nEpoch 91, Train Loss: 0.312815397977829, Train Acc: 86.90%, Train Discrimination: 0.892885148525238 Test Loss: 0.3308136761188507, Test Acc: 85.94%,Test Discrimination: 0.9436584711074829\nEpoch 92, Train Loss: 0.3125527501106262, Train Acc: 86.96%, Train Discrimination: 0.8900060057640076 Test Loss: 0.3305400013923645, Test Acc: 85.98%,Test Discrimination: 0.983189582824707\nEpoch 93, Train Loss: 0.31234806776046753, Train Acc: 87.03%, Train Discrimination: 0.925059974193573 Test Loss: 0.33068811893463135, Test Acc: 85.85%,Test Discrimination: 0.9317584037780762\nEpoch 94, Train Loss: 0.31211480498313904, Train Acc: 87.07%, Train Discrimination: 0.8807531595230103 Test Loss: 0.3305257558822632, Test Acc: 85.72%,Test Discrimination: 0.9462864398956299\nEpoch 95, Train Loss: 0.3118909001350403, Train Acc: 87.07%, Train Discrimination: 0.8945513367652893 Test Loss: 0.3304610252380371, Test Acc: 85.72%,Test Discrimination: 0.956537663936615\nEpoch 96, Train Loss: 0.3116954267024994, Train Acc: 87.07%, Train Discrimination: 0.9055699110031128 Test Loss: 0.3307129740715027, Test Acc: 85.79%,Test Discrimination: 0.9164716005325317\nEpoch 97, Train Loss: 0.3114984929561615, Train Acc: 87.11%, Train Discrimination: 0.8730623722076416 Test Loss: 0.3305681347846985, Test Acc: 85.72%,Test Discrimination: 0.9634522795677185\nEpoch 98, Train Loss: 0.3112768530845642, Train Acc: 87.07%, Train Discrimination: 0.9169492721557617 Test Loss: 0.330710232257843, Test Acc: 85.72%,Test Discrimination: 0.9306386113166809\nEpoch 99, Train Loss: 0.3110225200653076, Train Acc: 87.11%, Train Discrimination: 0.8897609114646912 Test Loss: 0.33071762323379517, Test Acc: 85.75%,Test Discrimination: 0.9277322292327881\nEpoch 100, Train Loss: 0.31081321835517883, Train Acc: 87.15%, Train Discrimination: 0.8878940939903259 Test Loss: 0.33061498403549194, Test Acc: 85.69%,Test Discrimination: 0.9497249722480774\n\n\n\nimport plotly.graph_objects as go\n\ndef plot_metric(title, y_label, train_data, test_data):\n    epochs = list(range(1, 101))\n    fig = go.Figure()\n\n    # Adding Train Line with Markers\n    fig.add_trace(go.Scatter(\n        x=epochs, y=train_data, mode='lines+markers',\n        name='Train',\n        line=dict(color='RoyalBlue', width=2),\n        marker=dict(color='RoyalBlue', size=6, line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    # Adding Test Line with Markers\n    fig.add_trace(go.Scatter(\n        x=epochs, y=test_data, mode='lines+markers',\n        name='Test',\n        line=dict(color='Crimson', width=2, dash='dot'),\n        marker=dict(color='Crimson', size=6, line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    # Update Layout\n    fig.update_layout(\n        title={'text': title, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n        xaxis_title='Epoch',\n        yaxis_title=y_label,\n        legend=dict(x=0.1, y=1.1, orientation='h'),\n        font=dict(family=\"Helvetica, Arial, sans-serif\", size=12, color=\"black\"),\n        plot_bgcolor='white',\n        margin=dict(l=40, r=40, t=40, b=30)\n    )\n\n    # Gridlines and Axes styles\n    fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='LightGrey')\n    fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='LightGrey')\n\n    fig.show()\n\n# Example usage\nplot_metric(\"Training and Testing Loss\", \"Loss\", train_losses1, test_losses1)\nplot_metric(\"Training and Testing Accuracy\", \"Accuracy (%)\", train_accuracies1, test_accuracies1)\nplot_metric(\"Training and Testing Discrimination\", \"Discrimination\", train_discriminations1, test_discriminations1)\nplot_metric(\"Training and Testing Fairness\", \"Fairness\", train_fairness1, test_fairness1)\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nplot_metric(\"Disc Aware Model and Unaware Model Fairness\", \"Fairness\", test_fairness, test_fairness1)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "FinalDisc_LossFunction.html",
    "href": "FinalDisc_LossFunction.html",
    "title": "Discrimination Loss Fn",
    "section": "",
    "text": "!pip install aif360\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting aif360\n  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: numpy&gt;=1.16 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from aif360) (1.22.3)\nRequirement already satisfied: scipy&gt;=1.2.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from aif360) (1.10.1)\nRequirement already satisfied: pandas&gt;=0.24.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from aif360) (1.3.5)\nRequirement already satisfied: scikit-learn&gt;=1.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from aif360) (1.3.2)\nRequirement already satisfied: matplotlib in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from aif360) (3.5.2)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from pandas&gt;=0.24.0-&gt;aif360) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from pandas&gt;=0.24.0-&gt;aif360) (2022.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn&gt;=1.0-&gt;aif360) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn&gt;=1.0-&gt;aif360) (3.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib-&gt;aif360) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib-&gt;aif360) (4.33.3)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib-&gt;aif360) (1.4.2)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib-&gt;aif360) (21.3)\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib-&gt;aif360) (9.1.0)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib-&gt;aif360) (3.0.7)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24.0-&gt;aif360) (1.16.0)\nDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\nInstalling collected packages: aif360\nSuccessfully installed aif360-0.6.1\n\n\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\nWARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -orch (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n    tinycss2 (&gt;=1.1.0&lt;1.2) ; extra == 'css'\n             ~~~~~~~~^\nWARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -orch (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -orch (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\nWARNING: Ignoring invalid distribution -rotobuf (c:\\users\\srinivas\\appdata\\roaming\\python\\python38\\site-packages)\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\nWARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\npip install 'aif360[inFairness]'\ndataset_orig_panel19_train = MEPSDataset19()\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\ndataset_orig_panel19_train.feature_names[1]\n\n'RACE'\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data\nX = dataset_orig_panel19_train.features\ny = dataset_orig_panel19_train.labels.ravel()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = X_train\nX_test_scaled = X_test\n\n# Train logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the same scaled training data\ntrain_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n# Calculation of discrimination index without modifying dataset structure\nsens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n\n\n# Define unprivileged and privileged values\nunprivileged_val = 0.0\nprivileged_val = 1.0\n\nC:\\Users\\srinivas\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Custom loss function\ndef discrimination_loss(output, target, sensitive_features, lambda_val=10, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(output, target)\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    prob_unpriv = torch.mean(output[mask_unpriv])\n    prob_priv = torch.mean(output[mask_priv])\n\n    discrimination = lambda_val * (prob_priv - prob_unpriv) ** k\n\n    loss_val=(1 + lambda_val * discrimination) * standard_loss\n    return loss_val,discrimination.item() \n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\n\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\n\n# Correctly preparing the sensitive features\nthreshold = 0.5  # Adjust the threshold according to your specific case\nsensitive_features = torch.tensor((data[:, 1].numpy() &gt; threshold).astype(float)).float()\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\n# Assuming similar preparation for test data\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = torch.tensor((test_data[:, 1].numpy() &gt; threshold).astype(float)).float()\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel2 = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model2.parameters(), lr=0.01)\ntrain_losses, train_accuracies, train_discriminations,train_fairness = [], [], [],[]\ntest_losses, test_accuracies, test_discriminations,test_fairness = [], [], [],[]\n\n# Training loop\nmodel2.train()\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model2(features)\n    loss, discrimination = discrimination_loss(outputs, targets.squeeze(), sensitive_features)\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluation on test data\n    model2.eval()\n    with torch.no_grad():\n        test_outputs = model2(test_features)\n        test_loss,test_discrimination = discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n    \n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, Train Discrimination: {discrimination} '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%,Test Discrimination: {test_discrimination}')\n    train_losses.append(loss.item())\n    train_accuracies.append(train_accuracy.item() * 100)\n    train_discriminations.append(discrimination)\n    fairness=1-discrimination\n    train_fairness.append(fairness)\n    test_fairness.append(fairness)\n    test_losses.append(test_loss.item())\n    test_accuracies.append(test_accuracy.item() * 100)\n    test_discriminations.append(test_discrimination)\n    model2.train()\n\nEpoch 1, Train Loss: 1.2005432844161987, Train Acc: 25.40%, Train Discrimination: 0.014157179743051529 Test Loss: 1.3504408597946167, Test Acc: 68.04%,Test Discrimination: 0.06081617996096611\nEpoch 2, Train Loss: 1.1340562105178833, Train Acc: 69.18%, Train Discrimination: 0.037128642201423645 Test Loss: 1.2218055725097656, Test Acc: 82.66%,Test Discrimination: 0.0197057593613863\nEpoch 3, Train Loss: 1.1161670684814453, Train Acc: 82.87%, Train Discrimination: 0.011497516185045242 Test Loss: 1.1219680309295654, Test Acc: 82.66%,Test Discrimination: 0.005324292927980423\nEpoch 4, Train Loss: 1.074970006942749, Train Acc: 82.87%, Train Discrimination: 0.003039707662537694 Test Loss: 1.0276302099227905, Test Acc: 82.66%,Test Discrimination: 0.0017148805782198906\nEpoch 5, Train Loss: 0.9983471632003784, Train Acc: 82.87%, Train Discrimination: 0.000977194868028164 Test Loss: 0.8889058232307434, Test Acc: 82.66%,Test Discrimination: 0.000632383453194052\nEpoch 6, Train Loss: 0.8670161962509155, Train Acc: 82.87%, Train Discrimination: 0.00035825211671181023 Test Loss: 0.714589536190033, Test Acc: 82.66%,Test Discrimination: 0.00015712952881585807\nEpoch 7, Train Loss: 0.6973254084587097, Train Acc: 82.87%, Train Discrimination: 7.520290091633797e-05 Test Loss: 0.5324214100837708, Test Acc: 82.66%,Test Discrimination: 0.00021039014973212034\nEpoch 8, Train Loss: 0.5200832486152649, Train Acc: 82.87%, Train Discrimination: 0.00022246502339839935 Test Loss: 0.4716324508190155, Test Acc: 82.75%,Test Discrimination: 0.010705008171498775\nEpoch 9, Train Loss: 0.45311859250068665, Train Acc: 83.01%, Train Discrimination: 0.008226670324802399 Test Loss: 0.7152367234230042, Test Acc: 82.60%,Test Discrimination: 0.05112812668085098\nEpoch 10, Train Loss: 0.6443049907684326, Train Acc: 83.39%, Train Discrimination: 0.03706512972712517 Test Loss: 0.6685908436775208, Test Acc: 83.35%,Test Discrimination: 0.046006254851818085\nEpoch 11, Train Loss: 0.6075366735458374, Train Acc: 83.77%, Train Discrimination: 0.03349513188004494 Test Loss: 0.4885546863079071, Test Acc: 82.98%,Test Discrimination: 0.018402203917503357\nEpoch 12, Train Loss: 0.46437230706214905, Train Acc: 83.35%, Train Discrimination: 0.013792199082672596 Test Loss: 0.43772050738334656, Test Acc: 82.66%,Test Discrimination: 0.004386838525533676\nEpoch 13, Train Loss: 0.426496684551239, Train Acc: 82.90%, Train Discrimination: 0.0034285150468349457 Test Loss: 0.45840921998023987, Test Acc: 82.66%,Test Discrimination: 0.0009370779152959585\nEpoch 14, Train Loss: 0.4488235116004944, Train Acc: 82.87%, Train Discrimination: 0.0007487634429708123 Test Loss: 0.48420649766921997, Test Acc: 82.66%,Test Discrimination: 0.00020888439030386508\nEpoch 15, Train Loss: 0.4745861291885376, Train Acc: 82.87%, Train Discrimination: 0.00017393939197063446 Test Loss: 0.4974011182785034, Test Acc: 82.66%,Test Discrimination: 4.43688259110786e-05\nEpoch 16, Train Loss: 0.48769083619117737, Train Acc: 82.87%, Train Discrimination: 4.0095881558954716e-05 Test Loss: 0.49500569701194763, Test Acc: 82.66%,Test Discrimination: 1.2954888006788678e-05\nEpoch 17, Train Loss: 0.4854975640773773, Train Acc: 82.87%, Train Discrimination: 1.2169022738817148e-05 Test Loss: 0.47962817549705505, Test Acc: 82.66%,Test Discrimination: 1.7027890862664208e-05\nEpoch 18, Train Loss: 0.47075119614601135, Train Acc: 82.87%, Train Discrimination: 1.4123769688012544e-05 Test Loss: 0.4566044211387634, Test Acc: 82.66%,Test Discrimination: 7.318196003325284e-05\nEpoch 19, Train Loss: 0.44868138432502747, Train Acc: 82.87%, Train Discrimination: 5.647099533234723e-05 Test Loss: 0.43277832865715027, Test Acc: 82.66%,Test Discrimination: 0.00032303319312632084\nEpoch 20, Train Loss: 0.4259135127067566, Train Acc: 82.87%, Train Discrimination: 0.00024351027968805283 Test Loss: 0.4155779778957367, Test Acc: 82.69%,Test Discrimination: 0.0010984605178236961\nEpoch 21, Train Loss: 0.40939560532569885, Train Acc: 82.89%, Train Discrimination: 0.0008234671549871564 Test Loss: 0.41082775592803955, Test Acc: 82.72%,Test Discrimination: 0.0028028381057083607\nEpoch 22, Train Loss: 0.4043589234352112, Train Acc: 83.07%, Train Discrimination: 0.0021003808360546827 Test Loss: 0.41864466667175293, Test Acc: 83.35%,Test Discrimination: 0.005389069672673941\nEpoch 23, Train Loss: 0.41090667247772217, Train Acc: 83.59%, Train Discrimination: 0.004053126089274883 Test Loss: 0.4310532808303833, Test Acc: 84.14%,Test Discrimination: 0.007870962843298912\nEpoch 24, Train Loss: 0.4218834638595581, Train Acc: 84.18%, Train Discrimination: 0.005946801044046879 Test Loss: 0.4365692138671875, Test Acc: 84.62%,Test Discrimination: 0.008870854042470455\nEpoch 25, Train Loss: 0.42714983224868774, Train Acc: 84.64%, Train Discrimination: 0.006790172308683395 Test Loss: 0.4300886392593384, Test Acc: 84.59%,Test Discrimination: 0.007894868962466717\nEpoch 26, Train Loss: 0.42204299569129944, Train Acc: 84.63%, Train Discrimination: 0.006190710701048374 Test Loss: 0.41647931933403015, Test Acc: 84.43%,Test Discrimination: 0.005689942743629217\nEpoch 27, Train Loss: 0.4105229377746582, Train Acc: 84.36%, Train Discrimination: 0.004645996727049351 Test Loss: 0.40423160791397095, Test Acc: 83.86%,Test Discrimination: 0.003438601503148675\nEpoch 28, Train Loss: 0.39972370862960815, Train Acc: 84.03%, Train Discrimination: 0.002983218291774392 Test Loss: 0.39808389544487, Test Acc: 83.51%,Test Discrimination: 0.001831222209148109\nEpoch 29, Train Loss: 0.39392372965812683, Train Acc: 83.64%, Train Discrimination: 0.0017365551320835948 Test Loss: 0.39776673913002014, Test Acc: 83.35%,Test Discrimination: 0.0009413770749233663\nEpoch 30, Train Loss: 0.3932155966758728, Train Acc: 83.34%, Train Discrimination: 0.0010058180196210742 Test Loss: 0.4004814922809601, Test Acc: 83.16%,Test Discrimination: 0.0005395567859522998\nEpoch 31, Train Loss: 0.39529725909233093, Train Acc: 83.20%, Train Discrimination: 0.0006552800186909735 Test Loss: 0.40331077575683594, Test Acc: 83.04%,Test Discrimination: 0.00040668746805749834\nEpoch 32, Train Loss: 0.39764776825904846, Train Acc: 83.13%, Train Discrimination: 0.0005417151842266321 Test Loss: 0.40429383516311646, Test Acc: 83.07%,Test Discrimination: 0.00043160252971574664\nEpoch 33, Train Loss: 0.39840617775917053, Train Acc: 83.12%, Train Discrimination: 0.000589441682677716 Test Loss: 0.40266871452331543, Test Acc: 83.16%,Test Discrimination: 0.0006084764027036726\nEpoch 34, Train Loss: 0.39677801728248596, Train Acc: 83.19%, Train Discrimination: 0.0007881385972723365 Test Loss: 0.3989834785461426, Test Acc: 83.35%,Test Discrimination: 0.0009820341365411878\nEpoch 35, Train Loss: 0.3932212293148041, Train Acc: 83.35%, Train Discrimination: 0.0011707082157954574 Test Loss: 0.39452505111694336, Test Acc: 83.64%,Test Discrimination: 0.0015984605997800827\nEpoch 36, Train Loss: 0.3890628218650818, Train Acc: 83.65%, Train Discrimination: 0.0017822475638240576 Test Loss: 0.3908197283744812, Test Acc: 83.77%,Test Discrimination: 0.0024994472041726112\nEpoch 37, Train Loss: 0.38574594259262085, Train Acc: 83.96%, Train Discrimination: 0.0026481803506612778 Test Loss: 0.3890173137187958, Test Acc: 84.27%,Test Discrimination: 0.003634978085756302\nEpoch 38, Train Loss: 0.3842768371105194, Train Acc: 84.26%, Train Discrimination: 0.0037144499365240335 Test Loss: 0.38918423652648926, Test Acc: 84.55%,Test Discrimination: 0.004832973703742027\nEpoch 39, Train Loss: 0.3846794664859772, Train Acc: 84.72%, Train Discrimination: 0.004816039465367794 Test Loss: 0.3902970850467682, Test Acc: 84.62%,Test Discrimination: 0.005801618564873934\nEpoch 40, Train Loss: 0.3859330415725708, Train Acc: 84.91%, Train Discrimination: 0.005684469826519489 Test Loss: 0.3907865285873413, Test Acc: 84.78%,Test Discrimination: 0.006219923961907625\nEpoch 41, Train Loss: 0.3865598142147064, Train Acc: 85.02%, Train Discrimination: 0.006049678660929203 Test Loss: 0.38967570662498474, Test Acc: 84.74%,Test Discrimination: 0.005942164920270443\nEpoch 42, Train Loss: 0.385648638010025, Train Acc: 84.98%, Train Discrimination: 0.005799609236419201 Test Loss: 0.3872186243534088, Test Acc: 84.71%,Test Discrimination: 0.005078119691461325\nEpoch 43, Train Loss: 0.38340896368026733, Train Acc: 85.01%, Train Discrimination: 0.005031323991715908 Test Loss: 0.38457104563713074, Test Acc: 84.55%,Test Discrimination: 0.003922297153621912\nEpoch 44, Train Loss: 0.3808959424495697, Train Acc: 85.00%, Train Discrimination: 0.003995964303612709 Test Loss: 0.38285356760025024, Test Acc: 84.55%,Test Discrimination: 0.002789432415738702\nEpoch 45, Train Loss: 0.3791574537754059, Train Acc: 84.83%, Train Discrimination: 0.0029617254622280598 Test Loss: 0.3824731111526489, Test Acc: 84.49%,Test Discrimination: 0.0018836656818166375\nEpoch 46, Train Loss: 0.37859538197517395, Train Acc: 84.80%, Train Discrimination: 0.0021109068766236305 Test Loss: 0.3830282688140869, Test Acc: 84.46%,Test Discrimination: 0.0012709356378763914\nEpoch 47, Train Loss: 0.3788890540599823, Train Acc: 84.64%, Train Discrimination: 0.0015142235206440091 Test Loss: 0.3837045729160309, Test Acc: 84.46%,Test Discrimination: 0.0009231196017935872\nEpoch 48, Train Loss: 0.3793371915817261, Train Acc: 84.55%, Train Discrimination: 0.001160685089416802 Test Loss: 0.38379067182540894, Test Acc: 84.46%,Test Discrimination: 0.0007817264413461089\nEpoch 49, Train Loss: 0.3793098032474518, Train Acc: 84.53%, Train Discrimination: 0.0010072262957692146 Test Loss: 0.38300344347953796, Test Acc: 84.43%,Test Discrimination: 0.0008042200352065265\nEpoch 50, Train Loss: 0.37855198979377747, Train Acc: 84.58%, Train Discrimination: 0.0010171417379751801 Test Loss: 0.3815461993217468, Test Acc: 84.52%,Test Discrimination: 0.0009746323339641094\nEpoch 51, Train Loss: 0.3772430419921875, Train Acc: 84.70%, Train Discrimination: 0.001170840347185731 Test Loss: 0.3799368441104889, Test Acc: 84.40%,Test Discrimination: 0.0012891972437500954\nEpoch 52, Train Loss: 0.3758367896080017, Train Acc: 84.85%, Train Discrimination: 0.0014550797641277313 Test Loss: 0.378708153963089, Test Acc: 84.49%,Test Discrimination: 0.0017290347022935748\nEpoch 53, Train Loss: 0.37478238344192505, Train Acc: 84.89%, Train Discrimination: 0.0018409424228593707 Test Loss: 0.3781189024448395, Test Acc: 84.59%,Test Discrimination: 0.0022333788219839334\nEpoch 54, Train Loss: 0.3742811977863312, Train Acc: 85.01%, Train Discrimination: 0.0022692368365824223 Test Loss: 0.3780350387096405, Test Acc: 84.65%,Test Discrimination: 0.0027022473514080048\nEpoch 55, Train Loss: 0.3741959035396576, Train Acc: 84.97%, Train Discrimination: 0.002650295151397586 Test Loss: 0.37806984782218933, Test Acc: 84.65%,Test Discrimination: 0.003020332893356681\nEpoch 56, Train Loss: 0.3741605877876282, Train Acc: 85.00%, Train Discrimination: 0.0028909845277667046 Test Loss: 0.3778536319732666, Test Acc: 84.65%,Test Discrimination: 0.003107474185526371\nEpoch 57, Train Loss: 0.3738515079021454, Train Acc: 84.99%, Train Discrimination: 0.0029306598007678986 Test Loss: 0.37728315591812134, Test Acc: 84.65%,Test Discrimination: 0.0029558660462498665\nEpoch 58, Train Loss: 0.373189777135849, Train Acc: 85.03%, Train Discrimination: 0.0027682885993272066 Test Loss: 0.37653377652168274, Test Acc: 84.59%,Test Discrimination: 0.0026268502697348595\nEpoch 59, Train Loss: 0.3723528981208801, Train Acc: 84.97%, Train Discrimination: 0.002458025934174657 Test Loss: 0.375884473323822, Test Acc: 84.59%,Test Discrimination: 0.002218634355813265\nEpoch 60, Train Loss: 0.3716050982475281, Train Acc: 84.97%, Train Discrimination: 0.002081372309476137 Test Loss: 0.3754950165748596, Test Acc: 84.62%,Test Discrimination: 0.001821950078010559\nEpoch 61, Train Loss: 0.37110841274261475, Train Acc: 84.92%, Train Discrimination: 0.001714807585813105 Test Loss: 0.37533071637153625, Test Acc: 84.52%,Test Discrimination: 0.0014964420115575194\nEpoch 62, Train Loss: 0.3708418607711792, Train Acc: 84.89%, Train Discrimination: 0.0014091026969254017 Test Loss: 0.37521469593048096, Test Acc: 84.40%,Test Discrimination: 0.0012681250227615237\nEpoch 63, Train Loss: 0.3706526458263397, Train Acc: 84.82%, Train Discrimination: 0.001187182147987187 Test Loss: 0.37496712803840637, Test Acc: 84.49%,Test Discrimination: 0.0011402207892388105\nEpoch 64, Train Loss: 0.3703800141811371, Train Acc: 84.82%, Train Discrimination: 0.0010522634256631136 Test Loss: 0.37451714277267456, Test Acc: 84.55%,Test Discrimination: 0.0011068442836403847\nEpoch 65, Train Loss: 0.36995723843574524, Train Acc: 84.82%, Train Discrimination: 0.0009971718536689878 Test Loss: 0.37393832206726074, Test Acc: 84.59%,Test Discrimination: 0.0011567104374989867\nEpoch 66, Train Loss: 0.36943379044532776, Train Acc: 84.85%, Train Discrimination: 0.0010106867412105203 Test Loss: 0.3733743131160736, Test Acc: 84.62%,Test Discrimination: 0.0012741395039483905\nEpoch 67, Train Loss: 0.368927001953125, Train Acc: 84.93%, Train Discrimination: 0.0010785114718601108 Test Loss: 0.37293753027915955, Test Acc: 84.71%,Test Discrimination: 0.001432808581739664\nEpoch 68, Train Loss: 0.36853721737861633, Train Acc: 84.94%, Train Discrimination: 0.0011808876879513264 Test Loss: 0.3726685345172882, Test Acc: 84.78%,Test Discrimination: 0.00160060147754848\nEpoch 69, Train Loss: 0.36827659606933594, Train Acc: 84.97%, Train Discrimination: 0.001290497020818293 Test Loss: 0.3724863529205322, Test Acc: 84.78%,Test Discrimination: 0.0017364883096888661\nEpoch 70, Train Loss: 0.3680700659751892, Train Acc: 84.97%, Train Discrimination: 0.001377397682517767 Test Loss: 0.37228330969810486, Test Acc: 84.81%,Test Discrimination: 0.0018045335309579968\nEpoch 71, Train Loss: 0.36781787872314453, Train Acc: 84.98%, Train Discrimination: 0.0014152934309095144 Test Loss: 0.3719920516014099, Test Acc: 84.81%,Test Discrimination: 0.0017873478354886174\nEpoch 72, Train Loss: 0.367470920085907, Train Acc: 85.01%, Train Discrimination: 0.0013934531016275287 Test Loss: 0.3716362416744232, Test Acc: 84.81%,Test Discrimination: 0.0016929487464949489\nEpoch 73, Train Loss: 0.3670569658279419, Train Acc: 84.98%, Train Discrimination: 0.0013177713844925165 Test Loss: 0.37129947543144226, Test Acc: 84.74%,Test Discrimination: 0.001551422756165266\nEpoch 74, Train Loss: 0.3666563034057617, Train Acc: 84.97%, Train Discrimination: 0.001209201873280108 Test Loss: 0.37103888392448425, Test Acc: 84.78%,Test Discrimination: 0.001398298074491322\nEpoch 75, Train Loss: 0.36632972955703735, Train Acc: 84.95%, Train Discrimination: 0.0010942852823063731 Test Loss: 0.3708503842353821, Test Acc: 84.68%,Test Discrimination: 0.0012636117171496153\nEpoch 76, Train Loss: 0.3660777509212494, Train Acc: 84.95%, Train Discrimination: 0.0009946973295882344 Test Loss: 0.3706786036491394, Test Acc: 84.65%,Test Discrimination: 0.0011678335722535849\nEpoch 77, Train Loss: 0.3658493757247925, Train Acc: 84.90%, Train Discrimination: 0.0009235434117726982 Test Loss: 0.3704555928707123, Test Acc: 84.68%,Test Discrimination: 0.0011214592959731817\nEpoch 78, Train Loss: 0.3655897080898285, Train Acc: 84.93%, Train Discrimination: 0.0008892670157365501 Test Loss: 0.3701593577861786, Test Acc: 84.71%,Test Discrimination: 0.001124257338233292\nEpoch 79, Train Loss: 0.36527591943740845, Train Acc: 85.01%, Train Discrimination: 0.0008920289692468941 Test Loss: 0.36981409788131714, Test Acc: 84.71%,Test Discrimination: 0.0011673344997689128\nEpoch 80, Train Loss: 0.36492496728897095, Train Acc: 85.02%, Train Discrimination: 0.0009252167074009776 Test Loss: 0.3694836497306824, Test Acc: 84.84%,Test Discrimination: 0.0012361345579847693\nEpoch 81, Train Loss: 0.3645872175693512, Train Acc: 85.00%, Train Discrimination: 0.0009782722918316722 Test Loss: 0.36920905113220215, Test Acc: 84.78%,Test Discrimination: 0.0013086255639791489\nEpoch 82, Train Loss: 0.3642946779727936, Train Acc: 85.04%, Train Discrimination: 0.001033667242154479 Test Loss: 0.36896538734436035, Test Acc: 84.81%,Test Discrimination: 0.001365308417007327\nEpoch 83, Train Loss: 0.3640435039997101, Train Acc: 85.08%, Train Discrimination: 0.0010767201893031597 Test Loss: 0.3687349259853363, Test Acc: 84.74%,Test Discrimination: 0.001389343524351716\nEpoch 84, Train Loss: 0.3637964129447937, Train Acc: 85.09%, Train Discrimination: 0.0010943880770355463 Test Loss: 0.3684976100921631, Test Acc: 84.74%,Test Discrimination: 0.0013772858073934913\nEpoch 85, Train Loss: 0.36352846026420593, Train Acc: 85.07%, Train Discrimination: 0.0010838788002729416 Test Loss: 0.3682655692100525, Test Acc: 84.74%,Test Discrimination: 0.001342118252068758\nEpoch 86, Train Loss: 0.36322951316833496, Train Acc: 85.10%, Train Discrimination: 0.0010551849845796824 Test Loss: 0.3680484890937805, Test Acc: 84.78%,Test Discrimination: 0.001289671054109931\nEpoch 87, Train Loss: 0.36291611194610596, Train Acc: 85.12%, Train Discrimination: 0.001012626220472157 Test Loss: 0.367868572473526, Test Acc: 84.74%,Test Discrimination: 0.0012446248438209295\nEpoch 88, Train Loss: 0.36261463165283203, Train Acc: 85.07%, Train Discrimination: 0.0009748825104907155 Test Loss: 0.3677263557910919, Test Acc: 84.71%,Test Discrimination: 0.0012231699656695127\nEpoch 89, Train Loss: 0.36233192682266235, Train Acc: 85.08%, Train Discrimination: 0.0009541662293486297 Test Loss: 0.36760184168815613, Test Acc: 84.71%,Test Discrimination: 0.0012274817563593388\nEpoch 90, Train Loss: 0.36206239461898804, Train Acc: 85.08%, Train Discrimination: 0.0009516003774479032 Test Loss: 0.3674684762954712, Test Acc: 84.71%,Test Discrimination: 0.001249900320544839\nEpoch 91, Train Loss: 0.3617932200431824, Train Acc: 85.12%, Train Discrimination: 0.0009608706459403038 Test Loss: 0.36729779839515686, Test Acc: 84.71%,Test Discrimination: 0.001275169081054628\nEpoch 92, Train Loss: 0.3615138828754425, Train Acc: 85.11%, Train Discrimination: 0.000970585155300796 Test Loss: 0.36708176136016846, Test Acc: 84.65%,Test Discrimination: 0.0012913604732602835\nEpoch 93, Train Loss: 0.36122554540634155, Train Acc: 85.11%, Train Discrimination: 0.0009717100183479488 Test Loss: 0.36683353781700134, Test Acc: 84.68%,Test Discrimination: 0.0012964284978806973\nEpoch 94, Train Loss: 0.3609369099140167, Train Acc: 85.12%, Train Discrimination: 0.0009649298153817654 Test Loss: 0.3665827810764313, Test Acc: 84.68%,Test Discrimination: 0.0012913164682686329\nEpoch 95, Train Loss: 0.3606562912464142, Train Acc: 85.15%, Train Discrimination: 0.0009503709734417498 Test Loss: 0.3663380444049835, Test Acc: 84.68%,Test Discrimination: 0.0012646605027839541\nEpoch 96, Train Loss: 0.36038607358932495, Train Acc: 85.15%, Train Discrimination: 0.0009205529349856079 Test Loss: 0.3661346137523651, Test Acc: 84.68%,Test Discrimination: 0.0012472628150135279\nEpoch 97, Train Loss: 0.36013227701187134, Train Acc: 85.15%, Train Discrimination: 0.0008976419339887798 Test Loss: 0.36599403619766235, Test Acc: 84.68%,Test Discrimination: 0.0012744254199787974\nEpoch 98, Train Loss: 0.35986828804016113, Train Acc: 85.15%, Train Discrimination: 0.0009072819957509637 Test Loss: 0.3659040331840515, Test Acc: 84.65%,Test Discrimination: 0.0013320177095010877\nEpoch 99, Train Loss: 0.3595961928367615, Train Acc: 85.17%, Train Discrimination: 0.0009394118096679449 Test Loss: 0.3658422827720642, Test Acc: 84.65%,Test Discrimination: 0.0013930415734648705\nEpoch 100, Train Loss: 0.35932740569114685, Train Acc: 85.17%, Train Discrimination: 0.0009756212239153683 Test Loss: 0.36578404903411865, Test Acc: 84.68%,Test Discrimination: 0.0014270032988861203"
  },
  {
    "objectID": "FinalDisc_LossFunction.html#bce-loss-fn",
    "href": "FinalDisc_LossFunction.html#bce-loss-fn",
    "title": "Discrimination Loss Fn",
    "section": "BCE Loss Fn",
    "text": "BCE Loss Fn\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Custom loss function|\ndef discrimination_loss(output, target, sensitive_features, lambda_val=100, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(output, target)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    #discrimination=torch.abs(prob_priv)\n    prob_unpriv = torch.mean(output[mask_unpriv])\n    prob_priv = torch.mean(output[mask_priv])\n    discrimination = lambda_val*(prob_priv - prob_unpriv) ** k\n\n# Handle cases where one group might be missing\n    #discrimination=torch.abs(prob_priv)\n \n    loss_val=standard_loss\n\n    return loss_val,discrimination.item() \n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\n\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\n\n# Correctly preparing the sensitive features\nthreshold = 0.5  # Adjust the threshold according to your specific case\nsensitive_features = torch.tensor((data[:, 1].numpy() &gt; threshold).astype(float)).float()\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\n# Assuming similar preparation for test data\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = torch.tensor((test_data[:, 1].numpy() &gt; threshold).astype(float)).float()\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ntrain_losses1, train_accuracies1, train_discriminations1,train_fairness1 = [], [], [],[]\ntest_losses1, test_accuracies1, test_discriminations1,test_fairness1 = [], [], [],[]\n# Training loop\nmodel.train()\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model(features)\n    loss, discrimination = discrimination_loss(outputs, targets.squeeze(), sensitive_features)\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluation on test data\n    model.eval()\n    with torch.no_grad():\n        test_outputs = model(test_features)\n        test_loss,test_discrimination = discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n    \n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, Train Discrimination: {discrimination} '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%,Test Discrimination: {test_discrimination}')\n    train_losses1.append(loss.item())\n    train_accuracies1.append(train_accuracy.item() * 100)\n    train_discriminations1.append(discrimination)\n    fairness=1-discrimination\n    train_fairness1.append(fairness)\n    test_fairness1.append(fairness)\n    test_losses1.append(test_loss.item())\n    test_accuracies1.append(test_accuracy.item() * 100)\n    test_discriminations1.append(test_discrimination)\n    model.train()\n\nEpoch 1, Train Loss: 0.6134274005889893, Train Acc: 65.26%, Train Discrimination: 0.20013360679149628 Test Loss: 0.5357574224472046, Test Acc: 82.66%,Test Discrimination: 0.0808272510766983\nEpoch 2, Train Loss: 0.5260337591171265, Train Acc: 82.87%, Train Discrimination: 0.03571147844195366 Test Loss: 0.4763486981391907, Test Acc: 79.06%,Test Discrimination: 0.8350032567977905\nEpoch 3, Train Loss: 0.4802321195602417, Train Acc: 78.40%, Train Discrimination: 0.7649175524711609 Test Loss: 0.3763365149497986, Test Acc: 84.81%,Test Discrimination: 0.4385420083999634\nEpoch 4, Train Loss: 0.37259721755981445, Train Acc: 85.47%, Train Discrimination: 0.4454280138015747 Test Loss: 0.42487236857414246, Test Acc: 83.51%,Test Discrimination: 0.12675483524799347\nEpoch 5, Train Loss: 0.41500017046928406, Train Acc: 84.18%, Train Discrimination: 0.14125816524028778 Test Loss: 0.4300435185432434, Test Acc: 83.83%,Test Discrimination: 0.13620014488697052\nEpoch 6, Train Loss: 0.4198794960975647, Train Acc: 84.37%, Train Discrimination: 0.15133412182331085 Test Loss: 0.3860330283641815, Test Acc: 84.59%,Test Discrimination: 0.35307714343070984\nEpoch 7, Train Loss: 0.37966153025627136, Train Acc: 85.39%, Train Discrimination: 0.36894407868385315 Test Loss: 0.3782263398170471, Test Acc: 85.44%,Test Discrimination: 0.8565509915351868\nEpoch 8, Train Loss: 0.37712201476097107, Train Acc: 84.88%, Train Discrimination: 0.8253031373023987 Test Loss: 0.39177942276000977, Test Acc: 84.71%,Test Discrimination: 1.1371583938598633\nEpoch 9, Train Loss: 0.392727792263031, Train Acc: 84.16%, Train Discrimination: 1.0591684579849243 Test Loss: 0.374019593000412, Test Acc: 85.60%,Test Discrimination: 0.901528000831604\nEpoch 10, Train Loss: 0.3726418614387512, Train Acc: 84.86%, Train Discrimination: 0.8526864647865295 Test Loss: 0.36674070358276367, Test Acc: 84.93%,Test Discrimination: 0.524904727935791\nEpoch 11, Train Loss: 0.36137402057647705, Train Acc: 85.45%, Train Discrimination: 0.5123254060745239 Test Loss: 0.37892723083496094, Test Acc: 84.74%,Test Discrimination: 0.3088102638721466\nEpoch 12, Train Loss: 0.3706624507904053, Train Acc: 85.40%, Train Discrimination: 0.3106633424758911 Test Loss: 0.383313924074173, Test Acc: 84.52%,Test Discrimination: 0.2505933940410614\nEpoch 13, Train Loss: 0.37412750720977783, Train Acc: 85.19%, Train Discrimination: 0.25809013843536377 Test Loss: 0.37120336294174194, Test Acc: 84.87%,Test Discrimination: 0.3016584813594818\nEpoch 14, Train Loss: 0.36291953921318054, Train Acc: 85.52%, Train Discrimination: 0.31302934885025024 Test Loss: 0.3574870824813843, Test Acc: 85.06%,Test Discrimination: 0.4648455083370209\nEpoch 15, Train Loss: 0.3513195216655731, Train Acc: 85.57%, Train Discrimination: 0.47291168570518494 Test Loss: 0.35962268710136414, Test Acc: 86.01%,Test Discrimination: 0.7229820489883423\nEpoch 16, Train Loss: 0.35604748129844666, Train Acc: 85.58%, Train Discrimination: 0.7065554857254028 Test Loss: 0.3586658239364624, Test Acc: 86.13%,Test Discrimination: 0.7388442754745483\nEpoch 17, Train Loss: 0.3551360070705414, Train Acc: 85.60%, Train Discrimination: 0.7258914709091187 Test Loss: 0.35162365436553955, Test Acc: 85.38%,Test Discrimination: 0.5901193618774414\nEpoch 18, Train Loss: 0.3467879593372345, Train Acc: 85.92%, Train Discrimination: 0.5994109511375427 Test Loss: 0.35068127512931824, Test Acc: 85.09%,Test Discrimination: 0.42984551191329956\nEpoch 19, Train Loss: 0.34421810507774353, Train Acc: 85.97%, Train Discrimination: 0.4518979787826538 Test Loss: 0.3547644019126892, Test Acc: 85.00%,Test Discrimination: 0.3347969651222229\nEpoch 20, Train Loss: 0.3472128212451935, Train Acc: 85.79%, Train Discrimination: 0.3611827790737152 Test Loss: 0.35629138350486755, Test Acc: 85.03%,Test Discrimination: 0.3151935338973999\nEpoch 21, Train Loss: 0.3484920859336853, Train Acc: 85.72%, Train Discrimination: 0.34112751483917236 Test Loss: 0.3529544770717621, Test Acc: 85.09%,Test Discrimination: 0.3636103570461273\nEpoch 22, Train Loss: 0.3457191586494446, Train Acc: 85.81%, Train Discrimination: 0.38558873534202576 Test Loss: 0.3492819666862488, Test Acc: 85.34%,Test Discrimination: 0.46737316250801086\nEpoch 23, Train Loss: 0.3430730998516083, Train Acc: 86.09%, Train Discrimination: 0.47895610332489014 Test Loss: 0.3489447832107544, Test Acc: 85.82%,Test Discrimination: 0.5837351083755493\nEpoch 24, Train Loss: 0.34372279047966003, Train Acc: 86.27%, Train Discrimination: 0.5795387029647827 Test Loss: 0.34917473793029785, Test Acc: 85.72%,Test Discrimination: 0.6414510607719421\nEpoch 25, Train Loss: 0.34429749846458435, Train Acc: 86.21%, Train Discrimination: 0.6264647841453552 Test Loss: 0.34675297141075134, Test Acc: 85.91%,Test Discrimination: 0.6036818027496338\nEpoch 26, Train Loss: 0.341439425945282, Train Acc: 86.36%, Train Discrimination: 0.5905063152313232 Test Loss: 0.34481555223464966, Test Acc: 85.47%,Test Discrimination: 0.5130152702331543\nEpoch 27, Train Loss: 0.33858758211135864, Train Acc: 86.28%, Train Discrimination: 0.5076196193695068 Test Loss: 0.34553927183151245, Test Acc: 85.41%,Test Discrimination: 0.4365430772304535\nEpoch 28, Train Loss: 0.3384379744529724, Train Acc: 86.01%, Train Discrimination: 0.43631723523139954 Test Loss: 0.346224308013916, Test Acc: 85.34%,Test Discrimination: 0.4126248359680176\nEpoch 29, Train Loss: 0.3386895954608917, Train Acc: 85.94%, Train Discrimination: 0.41201263666152954 Test Loss: 0.3445855975151062, Test Acc: 85.44%,Test Discrimination: 0.450536847114563\nEpoch 30, Train Loss: 0.33712905645370483, Train Acc: 85.98%, Train Discrimination: 0.44369468092918396 Test Loss: 0.34214985370635986, Test Acc: 85.63%,Test Discrimination: 0.543743908405304\nEpoch 31, Train Loss: 0.33511975407600403, Train Acc: 86.11%, Train Discrimination: 0.5244483947753906 Test Loss: 0.3415783643722534, Test Acc: 85.85%,Test Discrimination: 0.6625670194625854\nEpoch 32, Train Loss: 0.3349342942237854, Train Acc: 86.39%, Train Discrimination: 0.6257960200309753 Test Loss: 0.34182965755462646, Test Acc: 86.04%,Test Discrimination: 0.7388002276420593\nEpoch 33, Train Loss: 0.33524447679519653, Train Acc: 86.44%, Train Discrimination: 0.6904962062835693 Test Loss: 0.3407096862792969, Test Acc: 85.82%,Test Discrimination: 0.7204099893569946\nEpoch 34, Train Loss: 0.3336443305015564, Train Acc: 86.47%, Train Discrimination: 0.6745143532752991 Test Loss: 0.3401065468788147, Test Acc: 85.75%,Test Discrimination: 0.6455580592155457\nEpoch 35, Train Loss: 0.3322693705558777, Train Acc: 86.20%, Train Discrimination: 0.6099650859832764 Test Loss: 0.34097856283187866, Test Acc: 85.72%,Test Discrimination: 0.5888910293579102\nEpoch 36, Train Loss: 0.33245590329170227, Train Acc: 86.09%, Train Discrimination: 0.5606443881988525 Test Loss: 0.34085309505462646, Test Acc: 85.79%,Test Discrimination: 0.5950759649276733\nEpoch 37, Train Loss: 0.331985205411911, Train Acc: 86.12%, Train Discrimination: 0.5664157271385193 Test Loss: 0.33939042687416077, Test Acc: 85.66%,Test Discrimination: 0.6699245572090149\nEpoch 38, Train Loss: 0.3305213749408722, Train Acc: 86.27%, Train Discrimination: 0.6325047016143799 Test Loss: 0.33877697587013245, Test Acc: 85.69%,Test Discrimination: 0.7837302684783936\nEpoch 39, Train Loss: 0.3301395773887634, Train Acc: 86.57%, Train Discrimination: 0.732053279876709 Test Loss: 0.33876025676727295, Test Acc: 85.82%,Test Discrimination: 0.8358525037765503\nEpoch 40, Train Loss: 0.330069363117218, Train Acc: 86.58%, Train Discrimination: 0.7775329351425171 Test Loss: 0.3378388285636902, Test Acc: 85.60%,Test Discrimination: 0.7715796232223511\nEpoch 41, Train Loss: 0.3285832107067108, Train Acc: 86.60%, Train Discrimination: 0.7240158319473267 Test Loss: 0.3378903567790985, Test Acc: 85.72%,Test Discrimination: 0.6841261982917786\nEpoch 42, Train Loss: 0.328041672706604, Train Acc: 86.39%, Train Discrimination: 0.6490739583969116 Test Loss: 0.33789023756980896, Test Acc: 85.75%,Test Discrimination: 0.6556568741798401\nEpoch 43, Train Loss: 0.3277396559715271, Train Acc: 86.31%, Train Discrimination: 0.625366747379303 Test Loss: 0.336744099855423, Test Acc: 85.85%,Test Discrimination: 0.6993946433067322\nEpoch 44, Train Loss: 0.3265928030014038, Train Acc: 86.42%, Train Discrimination: 0.6651010513305664 Test Loss: 0.33588287234306335, Test Acc: 85.82%,Test Discrimination: 0.7834543585777283\nEpoch 45, Train Loss: 0.3258735239505768, Train Acc: 86.59%, Train Discrimination: 0.74001544713974 Test Loss: 0.3356870412826538, Test Acc: 85.82%,Test Discrimination: 0.8347284197807312\nEpoch 46, Train Loss: 0.3257128894329071, Train Acc: 86.61%, Train Discrimination: 0.7860814929008484 Test Loss: 0.33504873514175415, Test Acc: 85.79%,Test Discrimination: 0.8032159805297852\nEpoch 47, Train Loss: 0.3248644471168518, Train Acc: 86.65%, Train Discrimination: 0.760148823261261 Test Loss: 0.3348771929740906, Test Acc: 85.75%,Test Discrimination: 0.739197313785553\nEpoch 48, Train Loss: 0.3244439959526062, Train Acc: 86.61%, Train Discrimination: 0.707162618637085 Test Loss: 0.3348325788974762, Test Acc: 85.79%,Test Discrimination: 0.7204441428184509\nEpoch 49, Train Loss: 0.3242218792438507, Train Acc: 86.58%, Train Discrimination: 0.6933344006538391 Test Loss: 0.33412808179855347, Test Acc: 85.69%,Test Discrimination: 0.764785885810852\nEpoch 50, Train Loss: 0.32340049743652344, Train Acc: 86.68%, Train Discrimination: 0.7319745421409607 Test Loss: 0.33382081985473633, Test Acc: 85.88%,Test Discrimination: 0.8386752605438232\nEpoch 51, Train Loss: 0.32294318079948425, Train Acc: 86.66%, Train Discrimination: 0.7953499555587769 Test Loss: 0.33381083607673645, Test Acc: 85.75%,Test Discrimination: 0.8667795062065125\nEpoch 52, Train Loss: 0.32262516021728516, Train Acc: 86.66%, Train Discrimination: 0.8197501301765442 Test Loss: 0.3335072696208954, Test Acc: 85.82%,Test Discrimination: 0.8191007375717163\nEpoch 53, Train Loss: 0.3218657672405243, Train Acc: 86.71%, Train Discrimination: 0.7800191044807434 Test Loss: 0.33373868465423584, Test Acc: 85.82%,Test Discrimination: 0.7730465531349182\nEpoch 54, Train Loss: 0.32167235016822815, Train Acc: 86.62%, Train Discrimination: 0.7415419220924377 Test Loss: 0.3335752487182617, Test Acc: 85.85%,Test Discrimination: 0.7931897640228271\nEpoch 55, Train Loss: 0.32116368412971497, Train Acc: 86.65%, Train Discrimination: 0.7588697075843811 Test Loss: 0.33333808183670044, Test Acc: 85.75%,Test Discrimination: 0.8682560324668884\nEpoch 56, Train Loss: 0.3206660747528076, Train Acc: 86.68%, Train Discrimination: 0.8230858445167542 Test Loss: 0.3332976698875427, Test Acc: 85.63%,Test Discrimination: 0.9091278314590454\nEpoch 57, Train Loss: 0.32037293910980225, Train Acc: 86.80%, Train Discrimination: 0.8586173057556152 Test Loss: 0.3328898251056671, Test Acc: 85.75%,Test Discrimination: 0.875327467918396\nEpoch 58, Train Loss: 0.31973350048065186, Train Acc: 86.68%, Train Discrimination: 0.8303385376930237 Test Loss: 0.3328651785850525, Test Acc: 85.75%,Test Discrimination: 0.8477399945259094\nEpoch 59, Train Loss: 0.3195272982120514, Train Acc: 86.73%, Train Discrimination: 0.8078646063804626 Test Loss: 0.33263829350471497, Test Acc: 85.69%,Test Discrimination: 0.8837615847587585\nEpoch 60, Train Loss: 0.31903254985809326, Train Acc: 86.68%, Train Discrimination: 0.8380828499794006 Test Loss: 0.33269721269607544, Test Acc: 85.60%,Test Discrimination: 0.9491618871688843\nEpoch 61, Train Loss: 0.31875187158584595, Train Acc: 86.81%, Train Discrimination: 0.8926598429679871 Test Loss: 0.33265718817710876, Test Acc: 85.63%,Test Discrimination: 0.9491920471191406\nEpoch 62, Train Loss: 0.3183782398700714, Train Acc: 86.82%, Train Discrimination: 0.8919209241867065 Test Loss: 0.3325957953929901, Test Acc: 85.72%,Test Discrimination: 0.8965989351272583\nEpoch 63, Train Loss: 0.31800737977027893, Train Acc: 86.82%, Train Discrimination: 0.8470218777656555 Test Loss: 0.33266204595565796, Test Acc: 85.72%,Test Discrimination: 0.8899616003036499\nEpoch 64, Train Loss: 0.31767937541007996, Train Acc: 86.84%, Train Discrimination: 0.8414261341094971 Test Loss: 0.3326748311519623, Test Acc: 85.66%,Test Discrimination: 0.9413970708847046\nEpoch 65, Train Loss: 0.3172518014907837, Train Acc: 86.84%, Train Discrimination: 0.8853862285614014 Test Loss: 0.33270540833473206, Test Acc: 85.63%,Test Discrimination: 0.9599632620811462\nEpoch 66, Train Loss: 0.316932737827301, Train Acc: 86.90%, Train Discrimination: 0.903622031211853 Test Loss: 0.33247503638267517, Test Acc: 85.66%,Test Discrimination: 0.9254145622253418\nEpoch 67, Train Loss: 0.31656256318092346, Train Acc: 86.84%, Train Discrimination: 0.8776929378509521 Test Loss: 0.3322347104549408, Test Acc: 85.69%,Test Discrimination: 0.9266924858093262\nEpoch 68, Train Loss: 0.31618842482566833, Train Acc: 86.86%, Train Discrimination: 0.8789283037185669 Test Loss: 0.3320883810520172, Test Acc: 85.69%,Test Discrimination: 0.964611291885376\nEpoch 69, Train Loss: 0.3158738613128662, Train Acc: 86.92%, Train Discrimination: 0.9090332388877869 Test Loss: 0.33181580901145935, Test Acc: 85.69%,Test Discrimination: 0.9556538462638855\nEpoch 70, Train Loss: 0.3155333697795868, Train Acc: 86.93%, Train Discrimination: 0.9014745354652405 Test Loss: 0.3316538631916046, Test Acc: 85.66%,Test Discrimination: 0.9307590126991272\nEpoch 71, Train Loss: 0.3152202367782593, Train Acc: 86.93%, Train Discrimination: 0.8821228742599487 Test Loss: 0.33174580335617065, Test Acc: 85.69%,Test Discrimination: 0.9565079808235168\nEpoch 72, Train Loss: 0.31483298540115356, Train Acc: 86.97%, Train Discrimination: 0.9046382308006287 Test Loss: 0.3319389522075653, Test Acc: 85.72%,Test Discrimination: 0.9841709136962891\nEpoch 73, Train Loss: 0.31455299258232117, Train Acc: 87.00%, Train Discrimination: 0.9280744194984436 Test Loss: 0.33193331956863403, Test Acc: 85.63%,Test Discrimination: 0.9458189010620117\nEpoch 74, Train Loss: 0.31419095396995544, Train Acc: 87.04%, Train Discrimination: 0.8976611495018005 Test Loss: 0.33195894956588745, Test Acc: 85.60%,Test Discrimination: 0.9513502717018127\nEpoch 75, Train Loss: 0.31384193897247314, Train Acc: 87.00%, Train Discrimination: 0.9000087976455688 Test Loss: 0.332019567489624, Test Acc: 85.60%,Test Discrimination: 0.9510520696640015\nEpoch 76, Train Loss: 0.31356143951416016, Train Acc: 87.05%, Train Discrimination: 0.897897481918335 Test Loss: 0.33201152086257935, Test Acc: 85.63%,Test Discrimination: 0.9114383459091187\nEpoch 77, Train Loss: 0.3133009076118469, Train Acc: 87.03%, Train Discrimination: 0.8665303587913513 Test Loss: 0.3321487009525299, Test Acc: 85.47%,Test Discrimination: 0.9976710081100464\nEpoch 78, Train Loss: 0.31315895915031433, Train Acc: 87.09%, Train Discrimination: 0.9332451820373535 Test Loss: 0.3320115804672241, Test Acc: 85.69%,Test Discrimination: 0.9211483597755432\nEpoch 79, Train Loss: 0.3128153085708618, Train Acc: 87.10%, Train Discrimination: 0.8740503191947937 Test Loss: 0.3317583203315735, Test Acc: 85.50%,Test Discrimination: 0.9704768657684326\nEpoch 80, Train Loss: 0.31230083107948303, Train Acc: 87.07%, Train Discrimination: 0.9113655686378479 Test Loss: 0.33165737986564636, Test Acc: 85.60%,Test Discrimination: 0.9650099873542786\nEpoch 81, Train Loss: 0.312034547328949, Train Acc: 87.07%, Train Discrimination: 0.907887876033783 Test Loss: 0.33169323205947876, Test Acc: 85.72%,Test Discrimination: 0.9206826686859131\nEpoch 82, Train Loss: 0.31191548705101013, Train Acc: 87.11%, Train Discrimination: 0.8733884692192078 Test Loss: 0.33173856139183044, Test Acc: 85.47%,Test Discrimination: 1.0089668035507202\nEpoch 83, Train Loss: 0.3116932511329651, Train Acc: 87.16%, Train Discrimination: 0.9426926374435425 Test Loss: 0.33162587881088257, Test Acc: 85.75%,Test Discrimination: 0.916898787021637\nEpoch 84, Train Loss: 0.31130969524383545, Train Acc: 87.08%, Train Discrimination: 0.8692718148231506 Test Loss: 0.3314472734928131, Test Acc: 85.50%,Test Discrimination: 0.9674863219261169\nEpoch 85, Train Loss: 0.3109159469604492, Train Acc: 87.12%, Train Discrimination: 0.9078733921051025 Test Loss: 0.33149778842926025, Test Acc: 85.50%,Test Discrimination: 0.9789692163467407\nEpoch 86, Train Loss: 0.3106570243835449, Train Acc: 87.11%, Train Discrimination: 0.9184695482254028 Test Loss: 0.33172619342803955, Test Acc: 85.60%,Test Discrimination: 0.9028964042663574\nEpoch 87, Train Loss: 0.310575932264328, Train Acc: 87.18%, Train Discrimination: 0.8618366718292236 Test Loss: 0.3316841721534729, Test Acc: 85.60%,Test Discrimination: 1.0036262273788452\nEpoch 88, Train Loss: 0.3103819191455841, Train Acc: 87.22%, Train Discrimination: 0.9453766345977783 Test Loss: 0.33155685663223267, Test Acc: 85.50%,Test Discrimination: 0.9100217223167419\nEpoch 89, Train Loss: 0.30996719002723694, Train Acc: 87.17%, Train Discrimination: 0.868437647819519 Test Loss: 0.3313591778278351, Test Acc: 85.50%,Test Discrimination: 0.9449762105941772\nEpoch 90, Train Loss: 0.30959197878837585, Train Acc: 87.23%, Train Discrimination: 0.8959579467773438 Test Loss: 0.3312327563762665, Test Acc: 85.53%,Test Discrimination: 0.9720578789710999\nEpoch 91, Train Loss: 0.30940482020378113, Train Acc: 87.26%, Train Discrimination: 0.9153538346290588 Test Loss: 0.3311755955219269, Test Acc: 85.47%,Test Discrimination: 0.906120777130127\nEpoch 92, Train Loss: 0.3092677891254425, Train Acc: 87.17%, Train Discrimination: 0.8625436425209045 Test Loss: 0.33131617307662964, Test Acc: 85.47%,Test Discrimination: 0.9848475456237793\nEpoch 93, Train Loss: 0.3090141713619232, Train Acc: 87.31%, Train Discrimination: 0.9256561994552612 Test Loss: 0.3314042389392853, Test Acc: 85.50%,Test Discrimination: 0.8987420201301575\nEpoch 94, Train Loss: 0.30876991152763367, Train Acc: 87.20%, Train Discrimination: 0.8606014847755432 Test Loss: 0.33138471841812134, Test Acc: 85.50%,Test Discrimination: 0.9700459241867065\nEpoch 95, Train Loss: 0.3084326684474945, Train Acc: 87.32%, Train Discrimination: 0.918921709060669 Test Loss: 0.3313281834125519, Test Acc: 85.53%,Test Discrimination: 0.9282905459403992\nEpoch 96, Train Loss: 0.3081008791923523, Train Acc: 87.28%, Train Discrimination: 0.8856826424598694 Test Loss: 0.3313008248806, Test Acc: 85.53%,Test Discrimination: 0.937439501285553\nEpoch 97, Train Loss: 0.30782637000083923, Train Acc: 87.30%, Train Discrimination: 0.8936178088188171 Test Loss: 0.3311961889266968, Test Acc: 85.50%,Test Discrimination: 0.9419636130332947\nEpoch 98, Train Loss: 0.30757206678390503, Train Acc: 87.33%, Train Discrimination: 0.8969781994819641 Test Loss: 0.3311799168586731, Test Acc: 85.50%,Test Discrimination: 0.918307363986969\nEpoch 99, Train Loss: 0.30734243988990784, Train Acc: 87.39%, Train Discrimination: 0.8787064552307129 Test Loss: 0.33124685287475586, Test Acc: 85.60%,Test Discrimination: 0.9741513729095459\nEpoch 100, Train Loss: 0.30719390511512756, Train Acc: 87.35%, Train Discrimination: 0.9256513714790344 Test Loss: 0.33203598856925964, Test Acc: 85.60%,Test Discrimination: 0.8613821268081665\n\n\n\nimport plotly.graph_objects as go\n\n\ndef plot_metric(title, y_label, train_data, test_data, x_title = \"Epoch\", epoch_blue = \"train\", epoch_red = \"Test\"):\n    epochs = list(range(1, 101))\n    fig = go.Figure()\n\n    # Adding Train Line with Markers\n    fig.add_trace(go.Scatter(\n        x=epochs, y=train_data, mode='lines+markers',\n        name=epoch_blue,\n        line=dict(color='RoyalBlue', width=2),\n        marker=dict(color='RoyalBlue', size=6, line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    # Adding Test Line with Markers\n    fig.add_trace(go.Scatter(\n        x=epochs, y=test_data, mode='lines+markers',\n        name=epoch_red,\n        line=dict(color='Crimson', width=2, dash='dot'),\n        marker=dict(color='Crimson', size=6, line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    # Update Layout\n    fig.update_layout(\n        title={'text': title, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n        xaxis_title=x_title,\n        yaxis_title=y_label,\n        legend=dict(x=0.1, y=1.1, orientation='h'),\n        font=dict(family=\"Helvetica, Arial, sans-serif\", size=12, color=\"black\"),\n        plot_bgcolor='white',\n        margin=dict(l=40, r=40, t=40, b=30)\n    )\n\n    # Gridlines and Axes styles\n    fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='LightGrey')\n    fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='LightGrey')\n\n    fig.show()\n\n# Example usage\nplot_metric(\"Training and Testing Loss\", \"Loss\", train_losses1, test_losses1)\nplot_metric(\"Training and Testing Accuracy\", \"Accuracy (%)\", train_accuracies1, test_accuracies1)\nplot_metric(\"Training and Testing Discrimination\", \"Discrimination\", train_discriminations1, test_discriminations1)\nplot_metric(\"Training and Testing Fairness\", \"Fairness\", train_fairness1, test_fairness1)\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\n\nplot_metric(\"DISPARITY IN FAIRNESS\", \"FAIRNESS\", test_fairness, test_fairness1, \"EPOCH\", \"WITH DISCRIMINATION FUNCTION\", \"BCE\")"
  },
  {
    "objectID": "inprocessing2.html",
    "href": "inprocessing2.html",
    "title": "",
    "section": "",
    "text": "%pip install 'aif360[Reductions]'\n\n\n%pip install tensorflow\n\n\nfrom aif360.algorithms.inprocessing import PrejudiceRemover,AdversarialDebiasing\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\nfrom aif360.datasets import MEPSDataset20\nfrom aif360.datasets import MEPSDataset21\nfrom aif360.datasets import GermanDataset\n\n\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.metrics import ClassificationMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n\n#inprocessing technique \n\nfrom aif360.algorithms.inprocessing import MetaFairClassifier\nfrom aif360.algorithms.inprocessing import GerryFairClassifier\n\n\ndataset_orig_panel19 = MEPSDataset19()\n\n\ndataset_orig_panel19_train, dataset_orig_panel19_val = dataset_orig_panel19.split([0.7], shuffle=True)\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\ndef test(dataset, model, thresh, unprivileged_groups, privileged_groups):\n    y_val_pred_prob = model.predict(dataset).scores\n    pos_ind = 0  # Assuming scores are for the favorable outcome\n\n    # Ensure y_val_pred_prob is two-dimensional\n    if y_val_pred_prob.ndim == 1:\n        # If y_val_pred_prob is one-dimensional, use it directly\n        y_val_pred = (y_val_pred_prob &gt; thresh).astype(int)\n    else:\n        # Use pos_ind to index the second dimension\n        y_val_pred = (y_val_pred_prob[:, pos_ind] &gt; thresh).astype(int)\n\n    # Continue with the evaluation using y_val_pred\n    # Calculate accuracy, fairness metrics, etc., based on y_val_pred\n\n    return {\n        'accuracy': None,  # Replace with actual accuracy calculation\n        'fairness_metric': None  # Replace with actual fairness metric calculation\n    }\n\n\nfeatures = dataset_orig_panel19_train.features\nlabel = dataset_orig_panel19_train.labels\nfeature_names = dataset_orig_panel19_train.feature_names\ndataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\ndataset_orig_panel19_train_df['RACE2']=dataset_orig_panel19_train_df['RACE']\ndataset_orig_panel19_train_df.drop('RACE',axis=1,inplace=True)\ndataset_orig_panel19_train_df['label'] = label\n\n\ndataset_orig_panel19_train_df\n\n\n\n\n\n\n\n\nAGE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\nSEX=2\n...\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nRACE2\nlabel\n\n\n\n\n0\n28.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n20.0\n52.40\n62.66\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n72.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n38.0\n-1.00\n-1.00\n-1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n79.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11076\n25.0\n48.86\n58.43\n4.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n11077\n9.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n11078\n16.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n11079\n5.0\n-1.00\n-1.00\n-1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n11080\n69.0\n49.67\n59.15\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n\n\n11081 rows × 139 columns\n\n\n\n\nfrom aif360.datasets import StandardDataset\n\n# Define which column is the label and which one is the protected attribute\nlabel_name = 'label'  # or whatever the name of your label column is\nprotected_attribute_name = 'RACE2'  # or your protected attribute column name\n\n# Create the aif360 dataset\naif360_dataset = StandardDataset(\n    df=dataset_orig_panel19_train_df,\n    label_name=label_name,\n    favorable_classes=[1],  # assuming 1 is the favorable class\n    protected_attribute_names=[protected_attribute_name],\n    privileged_classes=[[1.0]],  # assuming 'White' is the privileged class\n)\n\n\naif360_dataset_train,aif360_dataset_test=aif360_dataset.split([0.7], shuffle=True)\n\n\naif360_dataset_train\n\n               instance weights features                                 \\\n                                                                          \n                                     AGE  PCS42  MCS42 K6SUM42 REGION=1   \ninstance names                                                            \n4466                        1.0      1.0  -1.00  -1.00    -1.0      0.0   \n8666                        1.0     35.0  58.84  51.24     1.0      0.0   \n2903                        1.0     10.0  -1.00  -1.00    -1.0      0.0   \n1551                        1.0     23.0  56.15  57.16     0.0      0.0   \n10019                       1.0     58.0  52.94  37.17     7.0      0.0   \n...                         ...      ...    ...    ...     ...      ...   \n923                         1.0     19.0  46.70  57.24     0.0      0.0   \n1976                        1.0     11.0  -1.00  -1.00    -1.0      0.0   \n4842                        1.0     19.0  56.71  62.39     1.0      1.0   \n7799                        1.0     53.0  51.29  58.72     0.0      0.0   \n5192                        1.0     31.0  55.86  54.79     2.0      0.0   \n\n                                                 ...                    \\\n                                                 ...                     \n               REGION=2 REGION=3 REGION=4 SEX=1  ... POVCAT=1 POVCAT=2   \ninstance names                                   ...                     \n4466                0.0      0.0      1.0   0.0  ...      1.0      0.0   \n8666                0.0      0.0      1.0   0.0  ...      0.0      1.0   \n2903                1.0      0.0      0.0   1.0  ...      1.0      0.0   \n1551                0.0      0.0      1.0   0.0  ...      0.0      0.0   \n10019               0.0      0.0      1.0   1.0  ...      0.0      0.0   \n...                 ...      ...      ...   ...  ...      ...      ...   \n923                 0.0      1.0      0.0   1.0  ...      0.0      0.0   \n1976                0.0      1.0      0.0   0.0  ...      0.0      0.0   \n4842                0.0      0.0      0.0   0.0  ...      0.0      0.0   \n7799                0.0      1.0      0.0   0.0  ...      0.0      0.0   \n5192                0.0      0.0      1.0   1.0  ...      0.0      0.0   \n\n                                                                      \\\n                                                                       \n               POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2 INSCOV=3   \ninstance names                                                         \n4466                0.0      0.0      0.0      0.0      1.0      0.0   \n8666                0.0      0.0      0.0      0.0      1.0      0.0   \n2903                0.0      0.0      0.0      0.0      1.0      0.0   \n1551                0.0      0.0      1.0      1.0      0.0      0.0   \n10019               0.0      1.0      0.0      1.0      0.0      0.0   \n...                 ...      ...      ...      ...      ...      ...   \n923                 1.0      0.0      0.0      0.0      1.0      0.0   \n1976                1.0      0.0      0.0      0.0      1.0      0.0   \n4842                0.0      1.0      0.0      1.0      0.0      0.0   \n7799                0.0      0.0      1.0      1.0      0.0      0.0   \n5192                0.0      0.0      1.0      1.0      0.0      0.0   \n\n                                   labels  \n               protected attribute         \n                             RACE2         \ninstance names                             \n4466                           0.0    0.0  \n8666                           0.0    0.0  \n2903                           0.0    0.0  \n1551                           0.0    0.0  \n10019                          1.0    0.0  \n...                            ...    ...  \n923                            0.0    0.0  \n1976                           0.0    0.0  \n4842                           1.0    0.0  \n7799                           1.0    0.0  \n5192                           1.0    0.0  \n\n[7756 rows x 140 columns]\n\n\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\npr=PrejudiceRemover(sensitive_attr='RACE2', eta=25.0)\ndset_scaled_trn = aif360_dataset_train.copy()\ndset_scaled_trn.features = scaler.fit_transform(dset_scaled_trn.features)\n\npr_fitted = pr.fit(dset_scaled_trn)\nprint(pr_fitted)\naccs = []\nthresholds = np.linspace(0.01, 0.5, 10)\n\ndset_val = aif360_dataset_test.copy()\ndset_val.features = scaler.transform(dset_val.features)\n\n##################### STEP 1 TRAINING WITH INPROCESSING #####################\npr_pred_prob = pr_fitted.predict(dset_val).scores\nprint(pr_pred_prob)\n##################### STEP 2 PICKING THRESHOLD WITH VALIDATION DATA #####################\nfor threshold in thresholds:\n    dset_val_pred = dset_val.copy()\n    dset_val_pred.labels = (pr_pred_prob[:, 0] &gt; threshold).astype(np.float64)\n\n    metric = ClassificationMetric(\n                dset_val, dset_val_pred,\n                unprivileged_groups = unprivileged_groups,\n                privileged_groups=privileged_groups)\n    accs.append((metric.true_positive_rate() + metric.true_negative_rate()) / 2)\n\n\npr_val_best_idx = np.argmax(accs)\nbest_threshold = thresholds[pr_val_best_idx]\n\n##################### STEP 3 TEST DATA #####################\ndset_tst = dataset_orig_panel19_val.copy()\ndset_tst.features = scaler.transform(dset_tst.features)\n\npr_pred_prob = pr_fitted.predict(dset_tst).scores\n\n\ndset_tst_pred = dset_tst.copy()\ndset_tst_pred.labels = (pr_pred_prob[:, 0] &gt; best_threshold).astype(np.float64)\n\nmetric = ClassificationMetric(\n            dset_tst, dset_tst_pred,\n            unprivileged_groups = unprivileged_groups,\n            privileged_groups   = privileged_groups)\ntest_acc = (metric.true_positive_rate() + metric.true_negative_rate()) / 2 ## no built in balanced error rate\ntest_disp_impact = metric.disparate_impact()\n\nprint(\"Testing accuracy with ETA %0.2f = %0.2f\\n Disparate impact %0.2f\" % (25.0, test_acc, test_disp_impact))\n\n\npr_orig_best_ind = np.argmax(val_metrics['bal_acc'])\n\n\nprint(val_metrics['bal_acc'])\nprint(val_metrics['bal_acc'].shape)\n\n\nimport tensorflow as tf\n\ntf.compat.v1.disable_eager_execution()\n\n# Now you can use tf.Session like in TensorFlow 1.x\nsession = tf.compat.v1.Session()\n\ndebiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n                          unprivileged_groups = unprivileged_groups,\n                          scope_name='debiased_classifier',\n                          num_epochs=10,\n                          debias=True,\n                          sess=session)\n\ndebiased_model.fit(dataset_orig_panel19_train)\n\n#fair = get_fair_metrics_and_plot(data_orig_test, debiased_model, plot=False, model_aif=True)\ndata_pred = debiased_model.predict(dataset_orig_panel19_val)\n\nWARNING:tensorflow:From /opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n\n\nWARNING:tensorflow:From /opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n2024-04-07 19:38:47.458787: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n\n\nepoch 0; iter: 0; batch classifier loss: 1.064564; batch adversarial loss: 0.844722\nepoch 1; iter: 0; batch classifier loss: 0.492836; batch adversarial loss: 0.762379\nepoch 2; iter: 0; batch classifier loss: 0.422149; batch adversarial loss: 0.702992\nepoch 3; iter: 0; batch classifier loss: 0.309038; batch adversarial loss: 0.677298\nepoch 4; iter: 0; batch classifier loss: 0.360344; batch adversarial loss: 0.676065\nepoch 5; iter: 0; batch classifier loss: 0.325244; batch adversarial loss: 0.623558\nepoch 6; iter: 0; batch classifier loss: 0.259387; batch adversarial loss: 0.650880\nepoch 7; iter: 0; batch classifier loss: 0.348109; batch adversarial loss: 0.688949\nepoch 8; iter: 0; batch classifier loss: 0.364343; batch adversarial loss: 0.641030\nepoch 9; iter: 0; batch classifier loss: 0.395801; batch adversarial loss: 0.633827\n\n\n\nmetric_transf_panel19_train = BinaryLabelDatasetMetric(\ndata_pred,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_ad=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\ntest_results_ad\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.09253696095777574',\n 'Consistency (Zemel, et al. 2013): [0.93821857]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.09253696095777574',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49708199611485']\n\n\n\nMC = MetaFairClassifier(tau=0, sensitive_attr=\"RACE\", type=\"fdr\")\n\n\nmc_model = MC.fit(dataset_orig_panel19_train)\n\n\ndataset_transf_panel19_train_MC = mc_model.predict(dataset_orig_panel19_train)\n\n\ndataset_transf_panel19_test_MC = mc_model.predict(dataset_orig_panel19_val)\n\n\nclassified_metric_bias_test = BinaryLabelDatasetMetric(dataset_transf_panel19_test_MC,\n                                                   unprivileged_groups=unprivileged_groups,\n                                                   privileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_mc=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\ntest_results_mc\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.09253696095777574',\n 'Consistency (Zemel, et al. 2013): [0.93821857]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.09253696095777574',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49708199611485']\n\n\n\nGC = GerryFairClassifier(C= 100, printflag=True, gamma=.005, fairness_def='FP',\n             max_iters=10, heatmapflag=False)\n\n\n# fit method\ngfc_model = GC.fit(dataset_orig_panel19_train, early_termination=True)\n\n# predict method. If threshold in (0, 1) produces binary predictions\ndataset_yhat = gfc_model.predict(dataset_orig_panel19_train, threshold=False)\n\niteration: 1, error: 0.13437415395722407, fairness violation: 0.005949616964211739, violated group size: 0.2653190145293746\niteration: 2, error: 0.15251331107300786, fairness violation: 0.0029764690924395944, violated group size: 0.2653190145293746\niteration: 3, error: 0.15846945221550404, fairness violation: 0.002005843173648617, violated group size: 0.2653190145293746\niteration: 4, error: 0.16131215594260445, fairness violation: 0.0015132866024116579, violated group size: 0.2653190145293746\niteration: 5, error: 0.16296363144120565, fairness violation: 0.0012300066827493246, violated group size: 0.2653190145293746\niteration: 6, error: 0.16401949282555725, fairness violation: 0.001041153402974436, violated group size: 0.2653190145293746\niteration: 7, error: 0.1646576508051104, fairness violation: 0.0009021189963686753, violated group size: 0.5610504467105857\niteration: 8, error: 0.16509114700839275, fairness violation: 0.0008131607202641582, violated group size: 0.5610504467105857\niteration: 9, error: 0.16532803898565113, fairness violation: 0.0007507787405605569, violated group size: 0.2653190145293746\n\n\n\n# auditing \nfrom aif360.algorithms.inprocessing.gerryfair.clean import array_to_tuple\n\ngerry_metric = BinaryLabelDatasetMetric(dataset_orig_panel19_train)\ngamma_disparity = gerry_metric.rich_subgroup(array_to_tuple(dataset_yhat.labels), 'FP')\nprint(gamma_disparity)\n\n0.0007507787405605569\n\n\n\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn import linear_model\nimport pickle\n\n\n# set to 10 iterations for fast running of notebook - set &gt;= 1000 when running real experiments\n# tests learning with different hypothesis classes\npareto_iters = 10\ndef multiple_classifiers_pareto(dataset, gamma_list=[0.002, 0.005, 0.01], save_results=True, iters=pareto_iters):\n\n    ln_predictor = linear_model.LinearRegression()\n    svm_predictor = svm.LinearSVR()\n    tree_predictor = tree.DecisionTreeRegressor(max_depth=3)\n    kernel_predictor = KernelRidge(alpha=1.0, gamma=1.0, kernel='rbf')\n    predictor_dict = {'Linear': {'predictor': ln_predictor, 'iters': iters},\n                       'SVR': {'predictor': svm_predictor, 'iters': iters},\n                       'Tree': {'predictor': tree_predictor, 'iters': iters},\n                       'Kernel': {'predictor': kernel_predictor, 'iters': iters}}\n    #predictor_dict = {'Linear': {'predictor': ln_predictor, 'iters': iters}}\n\n    results_dict = {}\n\n    for pred in predictor_dict:\n        print('Curr Predictor: {}'.format(pred))\n        predictor = predictor_dict[pred]['predictor']\n        max_iters = predictor_dict[pred]['iters']\n        fair_clf = GerryFairClassifier(C=100, printflag=True, gamma=1, predictor=predictor, max_iters=max_iters)\n        fair_clf.printflag = False\n        fair_clf.max_iters=max_iters\n        errors, fp_violations, fn_violations = fair_clf.pareto(dataset, gamma_list)\n        results_dict[pred] = {'errors': errors, 'fp_violations': fp_violations, 'fn_violations': fn_violations}\n    if save_results:\n        print('path:'+'results_dict_' + str(gamma_list) + '_gammas' + str(gamma_list) + '.pkl', 'wb')\n        pickle.dump(results_dict, open('results_dict_' + str(gamma_list) + '_gammas' + str(gamma_list) + '.pkl', 'wb'))\n\nmultiple_classifiers_pareto(dataset_orig_panel19_train)\n\nCurr Predictor: Linear\npath:results_dict_[0.002, 0.005, 0.01]_gammas[0.002, 0.005, 0.01].pkl wb"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "3.2 First models",
    "section": "",
    "text": "::: {#cell-1 .cell _uuid=‘832049268e597621b8995d308611ce2dac7e471c’ execution_count=243}\n%matplotlib inline\n# data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\nfrom time import time\n\n# Graphs libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nplt.style.use('seaborn-v0_8-whitegrid')\nimport seaborn as sns\n\n\n\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom plotly import tools\n\n# Libraries to study\nfrom aif360.datasets import StandardDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\nfrom aif360.algorithms.preprocessing import LFR, Reweighing\nfrom aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\nfrom aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n\n# ML libraries\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\n\n# Design libraries\nfrom IPython.display import Markdown, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n:::\n\nfrom aif360.datasets import MEPSDataset19\n\ndataset_orig_panel19=MEPSDataset19()\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19.privileged_protected_attributes[sens_ind]]\n\nI’d like to remember that the goal of this Kernel is not to get a performant model, but the main goal is to find out how we can prevent bias on our model. So I will just construct a simple Random Forest model.\n\nSplit into train and test set\n\nnp.random.seed(42)\n\ndataset_orig_panel19_train, dataset_orig_panel19_test = dataset_orig_panel19.split([0.7], shuffle=True)\n\n#  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\ndisplay(Markdown(\"#### Train Dataset shape\"))\nprint(\"Perpetrator Sex :\",dataset_orig_panel19_train.features.shape)\ndisplay(Markdown(\"#### Test Dataset shape\"))\nprint(\"Perpetrator Sex :\",dataset_orig_panel19_test.features.shape)\n\nTrain Dataset shape\n\n\nPerpetrator Sex : (11081, 138)\n\n\nTest Dataset shape\n\n\nPerpetrator Sex : (4749, 138)\n\n\n\n\nTraining the model : classic Random Forest\n\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Train and save the models\nrf_orig_sex = LogisticRegression().fit(dataset_orig_panel19_train.features, \n                     dataset_orig_panel19_train.labels.ravel(), \n                     sample_weight=dataset_orig_panel19_train.instance_weights)\n\n\n\nPredict on test set\n\nX_test_sex = dataset_orig_panel19_test.features\ny_test_sex = dataset_orig_panel19_test.labels.ravel()\n\n::: {#cell-11 .cell _kg_hide-input=‘false’ execution_count=196}\n# algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n# def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n#     # Using loc to add a new row to the DataFrame directly\n#     algo_metrics.loc[name] = [model, fair_metrics, preds, probs]\n#     return algo_metrics\n:::\n\n\nPREETHI _ DONOT DELETE\n\n\nMETRICS\n\n\n\ndef get_model_performance(X_test, y_true, y_pred, probs):\n    accuracy = accuracy_score(y_true, y_pred)\n    matrix = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n\n    return accuracy, matrix, f1, fpr, tpr, roc_auc\n\ndef plot_confusion_matrix(matrix):\n    plt.figure(figsize=(8, 6))\n    \n    sns.heatmap(matrix, annot=True, cmap='OrRd', fmt='g')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\n\ndef plot_roc_curve(fpr, tpr, roc_auc):\n    fig = px.area(\n        x=fpr, y=tpr,\n        title=f'ROC curve ( Area = {roc_auc:.2f})',\n        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n        width=600, height=600\n    )\n\n    fig.add_shape(\n        type='line', line=dict(dash='dash', color='navy', width=2),\n        x0=0, x1=1, y0=0, y1=1\n    )\n\n\n    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n    fig.update_xaxes(constrain='domain')\n\n\n    fig.add_vrect(x0=0, x1=0.5, fillcolor=\"red\", opacity=0.1, line_width=0)  \n    fig.add_vrect(x0=0.5, x1=1, fillcolor=\"green\", opacity=0.1, line_width=0) \n\n    fig.show()\n\n\n\n\n\n\n\n\n\nACCURACY and F1 SCORE\n\ndef model_performance(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n    display(Markdown('#### Accuracy of the model :'))\n    print(accuracy)\n    display(Markdown('#### F1 score of the model :'))\n    print(f1)\n\nmodel_performance(rf_orig_sex, dataset_orig_panel19_test.features, y_test_sex)\n\nAccuracy of the model :\n\n\n0.866919351442409\n\n\nF1 score of the model :\n\n\n0.5182926829268293\n\n\n\n\nROC CURVE\n\ndef ROC_Plot(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n    plot_roc_curve(fpr, tpr, roc_auc)\n    \nROC_Plot(rf_orig_sex, dataset_orig_panel19_test.features, y_test_sex)\n\n                                                \n\n\n\n\nCONFUSION MATRIX\n\ndef Confusion_Matrix(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n    plot_confusion_matrix(matrix)\n    \nConfusion_Matrix(rf_orig_sex, dataset_orig_panel19_test.features, y_test_sex)\n\n    \n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom IPython.display import display, Markdown\n\ndef plot_fair_metrics(fair_metrics):\n    # Enhanced style settings\n    sns.set(style=\"whitegrid\", palette=\"muted\")\n    \n    # Data preparation\n    cols = fair_metrics.columns.values\n    metrics_df = fair_metrics.iloc[1:]  # Exclude the 'objective' row for plotting\n    objective_values = fair_metrics.loc['objective']\n\n    # Plot setup\n    fig, axs = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n    colors = sns.color_palette(\"husl\", n_colors=len(cols))\n\n    # Plot each metric in a subplot\n    for i, col in enumerate(cols):\n        sns.barplot(x=metrics_df.index, y=metrics_df[col], ax=axs[i], palette=[colors[i]])\n        \n        # Add the objective line, ensuring it spans the full width of the plot\n        xlim = axs[i].get_xlim()\n        axs[i].plot(xlim, [objective_values[col]] * 2, 'r--', label='Objective')  # Adjusted to use plot for full width\n        axs[i].set_xlim(xlim)  # Reset the xlim to ensure the plot is not altered by line plotting\n\n        # Add shaded area for bounds\n        bound = [-0.1, 0.1] if i &lt; 3 else [0.8, 1.2] if i == 3 else [0, 0.25]\n        axs[i].add_patch(patches.Rectangle((xlim[0], bound[0]), xlim[1] - xlim[0], bound[1] - bound[0], color='GREEN', alpha=0.2))\n\n        # Set titles and labels\n        axs[i].set_title(col.replace('_', ' ').title(), fontsize=16)\n        axs[i].set_xlabel('')\n        axs[i].set_ylabel('Metric Value' if i == 0 else '')\n\n    # General layout adjustments\n    plt.suptitle('Fairness Metrics Overview', fontsize=24, y=1.05)\n    plt.tight_layout()\n    plt.legend()\n\n    # Display the plot\n    plt.show()\n\n\nimport numpy as np\nimport pandas as pd\nfrom aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n\ndef fair_metrics():\n    dataset = dataset_orig_panel19_test\n    pred = rf_orig_sex.predict(dataset.features)\n    \n    dataset_pred = dataset.copy()\n    dataset_pred.labels = pred\n\n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0, 0, 0, 1, 0]]\n\n    # Initialize DataFrame with objective row ---- Adjusted to initialize correctly\n    fair_metrics = pd.DataFrame(data=obj_fairness, columns=cols, index=['objective'])\n    \n    rows_list = []  # Collect rows in a list for efficiency ---- Added for better performance\n\n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n\n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n        # Create a dictionary for the row ---- Simplified row creation\n        row_dict = {\n            'statistical_parity_difference': metric_pred.mean_difference(),\n            'equal_opportunity_difference': classified_metric.equal_opportunity_difference(),\n            'average_abs_odds_difference': classified_metric.average_abs_odds_difference(),\n            'disparate_impact': metric_pred.disparate_impact(),\n            'theil_index': classified_metric.theil_index()\n        }\n        \n        rows_list.append(row_dict)  # Append dictionary to list ---- Efficient collection of rows\n\n    # Convert list of dicts to DataFrame and concatenate with the objective row ---- Proper use of pd.concat\n    if rows_list:\n        new_rows_df = pd.DataFrame(rows_list, index=dataset_pred.protected_attribute_names)\n        fair_metrics = pd.concat([fair_metrics, new_rows_df])\n\n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)  # Handle infinite values\n\n    print('fair_metrics')\n    print(fair_metrics)\n    print(type(fair_metrics))\n\n    return fair_metrics\n\nfair = fair_metrics()\n\n# Assuming plot_fair_metrics function exists and is correct ---- No changes made here\nplot_fair_metrics(fair)\ndisplay(fair)\n\nfair_metrics\n           statistical_parity_difference  equal_opportunity_difference  \\\nobjective                       0.000000                      0.000000   \nRACE                           -0.115925                     -0.181738   \n\n           average_abs_odds_difference  disparate_impact  theil_index  \nobjective                     0.000000          1.000000     0.000000  \nRACE                          0.110251          0.352207     0.115688  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical_parity_difference\nequal_opportunity_difference\naverage_abs_odds_difference\ndisparate_impact\ntheil_index\n\n\n\n\nobjective\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\nRACE\n-0.115925\n-0.181738\n0.110251\n0.352207\n0.115688\n\n\n\n\n\n\n\n\n\nPREETHI _ UNTIL THIS"
  },
  {
    "objectID": "preprocessing_bias.html",
    "href": "preprocessing_bias.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\nfrom aif360.datasets import MEPSDataset20\nfrom aif360.datasets import MEPSDataset21\nfrom aif360.datasets import GermanDataset\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Bias mitigation techniques\nfrom aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\nfrom aif360.algorithms.preprocessing import LFR\nfrom aif360.algorithms.preprocessing import OptimPreproc\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n  warn_deprecated('vmap', 'torch.vmap')\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ndataset_orig_panel19 = MEPSDataset19()\n\n\ndataset_orig_panel19_train = MEPSDataset19()\n\n\ntype(dataset_orig_panel19_train)\n\naif360.datasets.meps_dataset_panel19_fy2015.MEPSDataset19\n\n\n\ndataset_orig_panel19_train.features\n\narray([[53.  ,  1.  , 25.93, ...,  0.  ,  1.  ,  0.  ],\n       [56.  ,  1.  , 20.42, ...,  0.  ,  1.  ,  0.  ],\n       [23.  ,  1.  , 53.12, ...,  0.  ,  1.  ,  0.  ],\n       ...,\n       [ 2.  ,  1.  , -1.  , ...,  0.  ,  1.  ,  0.  ],\n       [54.  ,  0.  , 43.97, ...,  0.  ,  1.  ,  0.  ],\n       [73.  ,  0.  , 42.68, ...,  0.  ,  1.  ,  0.  ]])\n\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\nsens_attr\n\n'RACE'\n\n\n\nprivileged_groups\n\n[{'RACE': 1.0}]\n\n\n\nunprivileged_groups\n\n[{'RACE': 0.0}]\n\n\n\ndataset_orig_panel19_train\n\n               instance weights features                                    \\\n                                         protected attribute                 \n                                     AGE                RACE  PCS42  MCS42   \ninstance names                                                               \n0                  21854.981705     53.0                 1.0  25.93  58.47   \n1                  18169.604822     56.0                 1.0  20.42  26.57   \n3                  17191.832515     23.0                 1.0  53.12  50.33   \n4                  20261.485463      3.0                 1.0  -1.00  -1.00   \n5                      0.000000     27.0                 0.0  -1.00  -1.00   \n...                         ...      ...                 ...    ...    ...   \n16573               4111.315754     25.0                 0.0  56.71  62.39   \n16574               5415.228173     25.0                 0.0  56.71  62.39   \n16575               3896.116219      2.0                 1.0  -1.00  -1.00   \n16576               4883.851005     54.0                 0.0  43.97  42.45   \n16577               6630.588948     73.0                 0.0  42.68  43.46   \n\n                                                            ...          \\\n                                                            ...           \n               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \ninstance names                                              ...           \n0                  3.0      0.0      1.0      0.0      0.0  ...     1.0   \n1                 17.0      0.0      1.0      0.0      0.0  ...     1.0   \n3                  7.0      0.0      1.0      0.0      0.0  ...     0.0   \n4                 -1.0      0.0      1.0      0.0      0.0  ...     0.0   \n5                 -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n...                ...      ...      ...      ...      ...  ...     ...   \n16573              0.0      0.0      0.0      1.0      0.0  ...     0.0   \n16574              0.0      0.0      0.0      1.0      0.0  ...     1.0   \n16575             -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n16576             24.0      1.0      0.0      0.0      0.0  ...     0.0   \n16577              0.0      1.0      0.0      0.0      0.0  ...     1.0   \n\n                                                                               \\\n                                                                                \n               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \ninstance names                                                                  \n0                   1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n1                   0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n3                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n4                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n5                   0.0      0.0      1.0      0.0      0.0      1.0      0.0   \n...                 ...      ...      ...      ...      ...      ...      ...   \n16573               1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n16574               1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n16575               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n16576               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n16577               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n\n                        labels  \n                                \n               INSCOV=3         \ninstance names                  \n0                   0.0    1.0  \n1                   0.0    1.0  \n3                   0.0    0.0  \n4                   0.0    0.0  \n5                   0.0    0.0  \n...                 ...    ...  \n16573               0.0    0.0  \n16574               0.0    0.0  \n16575               0.0    0.0  \n16576               0.0    0.0  \n16577               0.0    0.0  \n\n[15830 rows x 140 columns]\n\n\n\nmetric_orig_panel19_train = BinaryLabelDatasetMetric(\n        dataset_orig_panel19_train,\n        unprivileged_groups=unprivileged_groups,\n        privileged_groups=privileged_groups)\n\n\nexplainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n\n\ntest_name=['Mean Difference','Consistency','Statistical Parity Difference','Disparate Impact']\ntest_results=[explainer_orig_panel19_train.mean_difference(),\n              explainer_orig_panel19_train.consistency(),\n              explainer_orig_panel19_train.statistical_parity_difference(),\n              explainer_orig_panel19_train.disparate_impact()]\n\n\ntest_results\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n 'Consistency (Zemel, et al. 2013): [0.83665193]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']\n\n\n\nRW = Reweighing(unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\ndataset_transf_panel19_train_rw = RW.fit_transform(dataset_orig_panel19_train)\n\n\nmetric_transf_panel19_train = BinaryLabelDatasetMetric(\ndataset_transf_panel19_train_rw,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_rw=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\ntest_results_rw\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -2.7755575615628914e-17',\n 'Consistency (Zemel, et al. 2013): [0.83665193]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -2.7755575615628914e-17',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999999']\n\n\n\ntest_results\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n 'Consistency (Zemel, et al. 2013): [0.83665193]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']\n\n\n\ntest_results_rw\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -2.7755575615628914e-17',\n 'Consistency (Zemel, et al. 2013): [0.83665193]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -2.7755575615628914e-17',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999999']\n\n\n\nfeatures = dataset_transf_panel19_train_rw.features\nlabel = dataset_transf_panel19_train_rw.labels.ravel()  # Flatten the label array if necessary\nweights = dataset_transf_panel19_train_rw.instance_weights\nfeature_names = dataset_transf_panel19_train_rw.feature_names\ndf_rw = pd.DataFrame(features, columns=feature_names)\ndf_rw['label'] = label\ndf_rw['weights'] = weights\n\n\ndf_rw\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\nweights\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n17459.483776\n\n\n1\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n14515.313940\n\n\n2\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n18465.607681\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n21762.696983\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n3727.042408\n\n\n15826\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n4909.081729\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n4184.786789\n\n\n15828\n54.0\n0.0\n43.97\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n4427.370919\n\n\n15829\n73.0\n0.0\n42.68\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n6010.846084\n\n\n\n\n15830 rows × 140 columns\n\n\n\n\nfeatures = dataset_orig_panel19_train.features\nlabel = dataset_orig_panel19_train.labels\nfeature_names = dataset_orig_panel19_train.feature_names\ndataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\ndataset_orig_panel19_train_df['label'] = label\ndataset_orig_panel19_train_df\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15826\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15828\n54.0\n0.0\n43.97\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15829\n73.0\n0.0\n42.68\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n15830 rows × 139 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\ndf=dataset_orig_panel19_train_df\n# Split the DataFrame into features, labels, and weights\n\nX = df.drop(['label'], axis=1)\ny = df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\n# train_weights = weights[train_indices]\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model.predict_proba(X_train)\ny_test_pred_proba = model.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nTraining Accuracy: 0.8654453569172458, Training Log Loss: 0.3231973192817746\nTesting Accuracy: 0.8597599494630449, Testing Log Loss: 0.33654418113825585\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix\nimport plotly.figure_factory as ff\n\n# Assuming 'df' is your Pandas DataFrame with features and 'label' columns\ndf = dataset_orig_panel19_train_df  # Replace with your actual DataFrame\n\n# Split the DataFrame into features and labels\nX = df.drop(['label'], axis=1)\ny = df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel = LogisticRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model.predict_proba(X_train)\ny_test_pred_proba = model.predict_proba(X_test)\n\n# Calculate accuracy and log loss\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Generate confusion matrix for test data\ncm = confusion_matrix(y_test, y_test_pred)\ncm_figure = ff.create_annotated_heatmap(z=cm, x=['Predicted Negative', 'Predicted Positive'], y=['Actual Negative', 'Actual Positive'], colorscale='Viridis')\n\n# Bar graph for accuracy\naccuracy_fig = go.Figure()\naccuracy_fig.add_trace(go.Bar(x=['Train Accuracy', 'Test Accuracy'],\n                              y=[train_accuracy, test_accuracy],\n                              marker_color=['blue', 'green'],\n                              text=[f\"{train_accuracy:.2%}\", f\"{test_accuracy:.2%}\"],\n                              textposition='auto'))\n\naccuracy_fig.update_layout(title='Train vs Test Accuracy',\n                           xaxis_title='Dataset',\n                           yaxis_title='Accuracy',\n                           template='plotly_dark',\n                           showlegend=False)\n\n# Bar graph for log loss\nloss_fig = go.Figure()\nloss_fig.add_trace(go.Bar(x=['Train Log Loss', 'Test Log Loss'],\n                          y=[train_loss, test_loss],\n                          marker_color=['blue', 'green'],\n                          text=[f\"{train_loss:.4f}\", f\"{test_loss:.4f}\"],\n                          textposition='auto'))\n\nloss_fig.update_layout(title='Train vs Test Log Loss',\n                       xaxis_title='Dataset',\n                       yaxis_title='Log Loss',\n                       template='plotly_dark',\n                       showlegend=False)\n\n# Show plots\naccuracy_fig.show()\nloss_fig.show()\ncm_figure.show()\n\nc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n\n# Split the DataFrame into features, labels, and weights\nX = df_rw.drop(['label', 'weights'], axis=1)\ny = df_rw['label']\nweights = df_rw['weights']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel_rw = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel_rw.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_rw.predict(X_train)\ny_test_pred = model_rw.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_rw.predict_proba(X_train)\ny_test_pred_proba = model_rw.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653, in Index.get_loc(self, key)\n   3652 try:\n-&gt; 3653     return self._engine.get_loc(casted_key)\n   3654 except KeyError as err:\n\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147, in pandas._libs.index.IndexEngine.get_loc()\n\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas\\_libs\\hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas\\_libs\\hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'weights'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 13\n     11 X = df_rw.drop(['label'], axis=1)\n     12 y = df_rw['label']\n---&gt; 13 weights = df['weights']  # Make sure this is the correct column for weights\n     15 # Split the data into training and testing sets\n     16 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761, in DataFrame.__getitem__(self, key)\n   3759 if self.columns.nlevels &gt; 1:\n   3760     return self._getitem_multilevel(key)\n-&gt; 3761 indexer = self.columns.get_loc(key)\n   3762 if is_integer(indexer):\n   3763     indexer = [indexer]\n\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655, in Index.get_loc(self, key)\n   3653     return self._engine.get_loc(casted_key)\n   3654 except KeyError as err:\n-&gt; 3655     raise KeyError(key) from err\n   3656 except TypeError:\n   3657     # If we have a listlike key, _check_indexing_error will raise\n   3658     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3659     #  the TypeError.\n   3660     self._check_indexing_error(key)\n\nKeyError: 'weights'"
  },
  {
    "objectID": "preprocess_disc_bias.html",
    "href": "preprocess_disc_bias.html",
    "title": "DEFINING custom_loss FUNCTION with Standard Lambda = 0.5",
    "section": "",
    "text": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport joblib\nimport re\nimport sys\nsys.path.insert(0, '../')\nimport numpy as np\n\n# Datasets\nfrom aif360.datasets import MEPSDataset19\n# Fairness metrics\nfrom aif360.metrics import BinaryLabelDatasetMetric\n\n# Explainers\nfrom aif360.explainers import MetricTextExplainer\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Bias mitigation techniques\nfrom aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\nfrom aif360.algorithms.preprocessing import LFR\nfrom aif360.algorithms.preprocessing import OptimPreproc\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ndataset_orig_panel19 = MEPSDataset19()\n\n\ndataset_orig_panel19_train = MEPSDataset19()\n\n\ndataset_orig_panel19_train.features\n\narray([[53.  ,  1.  , 25.93, ...,  0.  ,  1.  ,  0.  ],\n       [56.  ,  1.  , 20.42, ...,  0.  ,  1.  ,  0.  ],\n       [23.  ,  1.  , 53.12, ...,  0.  ,  1.  ,  0.  ],\n       ...,\n       [ 2.  ,  1.  , -1.  , ...,  0.  ,  1.  ,  0.  ],\n       [54.  ,  0.  , 43.97, ...,  0.  ,  1.  ,  0.  ],\n       [73.  ,  0.  , 42.68, ...,  0.  ,  1.  ,  0.  ]])\n\n\n\nsens_ind = 0\nsens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\nunprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\nprivileged_groups = [{sens_attr: v} for v in\n                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]\n\n\nsens_attr\n\n'RACE'\n\n\n\nprivileged_groups\n\n[{'RACE': 1.0}]\n\n\n\nunprivileged_groups\n\n[{'RACE': 0.0}]\n\n\n\ndataset_orig_panel19_train\n\n               instance weights features                                    \\\n                                         protected attribute                 \n                                     AGE                RACE  PCS42  MCS42   \ninstance names                                                               \n0                  21854.981705     53.0                 1.0  25.93  58.47   \n1                  18169.604822     56.0                 1.0  20.42  26.57   \n3                  17191.832515     23.0                 1.0  53.12  50.33   \n4                  20261.485463      3.0                 1.0  -1.00  -1.00   \n5                      0.000000     27.0                 0.0  -1.00  -1.00   \n...                         ...      ...                 ...    ...    ...   \n16573               4111.315754     25.0                 0.0  56.71  62.39   \n16574               5415.228173     25.0                 0.0  56.71  62.39   \n16575               3896.116219      2.0                 1.0  -1.00  -1.00   \n16576               4883.851005     54.0                 0.0  43.97  42.45   \n16577               6630.588948     73.0                 0.0  42.68  43.46   \n\n                                                            ...          \\\n                                                            ...           \n               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \ninstance names                                              ...           \n0                  3.0      0.0      1.0      0.0      0.0  ...     1.0   \n1                 17.0      0.0      1.0      0.0      0.0  ...     1.0   \n3                  7.0      0.0      1.0      0.0      0.0  ...     0.0   \n4                 -1.0      0.0      1.0      0.0      0.0  ...     0.0   \n5                 -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n...                ...      ...      ...      ...      ...  ...     ...   \n16573              0.0      0.0      0.0      1.0      0.0  ...     0.0   \n16574              0.0      0.0      0.0      1.0      0.0  ...     1.0   \n16575             -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n16576             24.0      1.0      0.0      0.0      0.0  ...     0.0   \n16577              0.0      1.0      0.0      0.0      0.0  ...     1.0   \n\n                                                                               \\\n                                                                                \n               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \ninstance names                                                                  \n0                   1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n1                   0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n3                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n4                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n5                   0.0      0.0      1.0      0.0      0.0      1.0      0.0   \n...                 ...      ...      ...      ...      ...      ...      ...   \n16573               1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n16574               1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n16575               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n16576               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n16577               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n\n                        labels  \n                                \n               INSCOV=3         \ninstance names                  \n0                   0.0    1.0  \n1                   0.0    1.0  \n3                   0.0    0.0  \n4                   0.0    0.0  \n5                   0.0    0.0  \n...                 ...    ...  \n16573               0.0    0.0  \n16574               0.0    0.0  \n16575               0.0    0.0  \n16576               0.0    0.0  \n16577               0.0    0.0  \n\n[15830 rows x 140 columns]\n\n\n\nmetric_orig_panel19_train = BinaryLabelDatasetMetric(\n        dataset_orig_panel19_train,\n        unprivileged_groups=unprivileged_groups,\n        privileged_groups=privileged_groups)\n\n\nexplainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n\n\ntest_name=['Mean Difference','Consistency','Statistical Parity Difference','Disparate Impact']\ntest_definitions=['difference between mean values of two labels','Individual fairness metric that measures how similar the labels are for similar instances.','Difference in selection rates.','ratio of positive outcomes in the unprivileged group divided by the ratio of positive outcomes in the privileged group.']\ntest_results=[explainer_orig_panel19_train.mean_difference(),explainer_orig_panel19_train.consistency(),explainer_orig_panel19_train.statistical_parity_difference(),explainer_orig_panel19_train.disparate_impact()]\ntest_status=['Bias Detected','Bias Not Detected','Bias Detected','Bias Detected']\ndf=pd.DataFrame({'Test Name':test_name,'Test Definitions':test_definitions,'Test Results':test_results,'Test Status':test_status})\n\n\ntest_results\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n 'Consistency (Zemel, et al. 2013): [0.83665193]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']\n\n\n\nRW = Reweighing(unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\ndataset_transf_panel19_train_rw = RW.fit_transform(dataset_orig_panel19_train)\n\n\n# Calculate Statistical Parity Difference\nstatistical_parity_difference = metric_orig_panel19_train.statistical_parity_difference()\nprint(\"Statistical Parity Difference (SPD):\", statistical_parity_difference)\n\n# Calculate Disparate Impact\ndisparate_impact = metric_orig_panel19_train.disparate_impact()\nprint(\"Disparate Impact (DI):\", disparate_impact)\n\nStatistical Parity Difference (SPD): -0.13507447726478142\nDisparate Impact (DI): 0.49826823461176517\n\n\n\nX_train[1]\n\narray([37.  ,  0.  , 48.09, 36.94,  7.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n        1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n        0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,\n        0.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n        1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n        1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,\n        0.  ,  0.  ,  1.  ])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data\nX = dataset_orig_panel19_train.features\ny = dataset_orig_panel19_train.labels.ravel()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the same scaled training data\ntrain_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n# Calculation of discrimination index without modifying dataset structure\nsens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\ndef calculate_discrimination(X, probabilities, sens_attr_index, unprivileged_val, privileged_val):\n    # Filter by sensitive attribute for unprivileged and privileged groups\n    unpriv_indices = X[:, sens_attr_index] == unprivileged_val\n    priv_indices = X[:, sens_attr_index] == privileged_val\n    \n    # Calculate mean probabilities for both groups\n    mean_prob_unpriv = probabilities[unpriv_indices].mean()\n    mean_prob_priv = probabilities[priv_indices].mean()\n    # Discrimination index\n    discrimination = mean_prob_priv - mean_prob_unpriv\n    return discrimination\n\n# Define unprivileged and privileged values\nunprivileged_val = 0.0\nprivileged_val = 1.0\n\n# Compute discrimination\ndiscrimination_index = calculate_discrimination(X_train, train_probabilities, sens_attr_index, unprivileged_val, privileged_val)\nprint(\"Discrimination Index: {:.4f}\".format(discrimination_index))\n\nDiscrimination Index: 0.1315\n\n\n\nX_train_scaled\n\narray([[-0.76152525, -0.7401269 , -1.16660499, ..., -1.06074916,\n        -0.744093  ,  2.78531021],\n       [ 0.08369436, -0.7401269 ,  0.72413337, ..., -1.06074916,\n        -0.744093  ,  2.78531021],\n       [-0.45012855, -0.7401269 ,  0.94174738, ...,  0.94272994,\n        -0.744093  , -0.35902644],\n       ...,\n       [ 0.79545824,  1.35111965,  0.09517111, ..., -1.06074916,\n         1.34391803, -0.35902644],\n       [-0.09424661, -0.7401269 ,  1.09658071, ...,  0.94272994,\n        -0.744093  , -0.35902644],\n       [-0.62806952, -0.7401269 ,  0.4680036 , ...,  0.94272994,\n        -0.744093  , -0.35902644]])\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)  # Adjust input size to exclude sensitive attribute\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Custom loss function\ndef custom_loss(output, target, sensitive_features, lambda_val=0.1, k=6):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(output, target)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    \n    if torch.sum(mask_unpriv) &gt; 0 and torch.sum(mask_priv) &gt; 0:\n        prob_unpriv = torch.mean(output[mask_unpriv])\n        prob_priv = torch.mean(output[mask_priv])\n        discrimination = (prob_priv - prob_unpriv) ** k\n    else:\n        discrimination = torch.tensor(0.0)  # Handle cases where one group might be missing\n\n    return standard_loss + lambda_val * discrimination\n\n# Assuming your dataset is loaded correctly\ndata = torch.tensor(X_train_scaled).float()  # Make sure this conversion is done correctly\ntargets = torch.tensor(y_train).float().unsqueeze(1)  # Ensure targets are correctly shaped\nsensitive_features = data[:, 1]  # Extract the sensitive features\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)  # Exclude the sensitive attribute from the main features\n\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nmodel.train()\nfor epoch in range(10):\n    optimizer.zero_grad()\n    outputs = model(features)\n    loss = custom_loss(outputs, targets.squeeze(), sensitive_features)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n\nEpoch 1, Loss: 0.6436237692832947\nEpoch 2, Loss: 0.6127090454101562\nEpoch 3, Loss: 0.5840549468994141\nEpoch 4, Loss: 0.557695209980011\nEpoch 5, Loss: 0.5335997343063354\nEpoch 6, Loss: 0.5117872953414917\nEpoch 7, Loss: 0.49214401841163635\nEpoch 8, Loss: 0.47453635931015015\nEpoch 9, Loss: 0.45879927277565\nEpoch 10, Loss: 0.44474470615386963\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Custom loss function\ndef custom_loss(output, target, sensitive_features, lambda_val=0.01, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(output, target)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n    \n    if torch.sum(mask_unpriv) &gt; 0 and torch.sum(mask_priv) &gt; 0:\n        prob_unpriv = torch.mean(output[mask_unpriv])\n        prob_priv = torch.mean(output[mask_priv])\n        discrimination = (prob_priv - prob_unpriv) ** k\n    else:\n        discrimination = torch.tensor(0.0)  # Handle cases where one group might be missing\n\n    return standard_loss + lambda_val * discrimination\n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\n# Data preparation\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\nsensitive_features = data[:, 1]\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\n# Test data (assuming it's prepared similarly)\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = test_data[:, 1]\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nmodel.train()\nfor epoch in range(10):\n    optimizer.zero_grad()\n    outputs = model(features)\n    loss = custom_loss(outputs, targets.squeeze(), sensitive_features)\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluation on test data\n    model.eval()\n    with torch.no_grad():\n        test_outputs = model(test_features)\n        test_loss = custom_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n    \n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%')\n    model.train()\n\nEpoch 1, Train Loss: 0.7673807740211487, Train Acc: 24.21%, Test Loss: 0.7409172654151917, Test Acc: 31.46%\nEpoch 2, Train Loss: 0.7404283881187439, Train Acc: 32.01%, Test Loss: 0.716571569442749, Test Acc: 41.63%\nEpoch 3, Train Loss: 0.7157283425331116, Train Acc: 41.29%, Test Loss: 0.6942327618598938, Test Acc: 53.85%\nEpoch 4, Train Loss: 0.693112313747406, Train Acc: 53.62%, Test Loss: 0.6738004684448242, Test Acc: 67.66%\nEpoch 5, Train Loss: 0.6722973585128784, Train Acc: 68.04%, Test Loss: 0.6548610329627991, Test Acc: 74.76%\nEpoch 6, Train Loss: 0.6529971957206726, Train Acc: 75.09%, Test Loss: 0.637105405330658, Test Acc: 77.51%\nEpoch 7, Train Loss: 0.6348973512649536, Train Acc: 78.02%, Test Loss: 0.6203395128250122, Test Acc: 79.41%\nEpoch 8, Train Loss: 0.6178111433982849, Train Acc: 79.60%, Test Loss: 0.6044129729270935, Test Acc: 80.51%\nEpoch 9, Train Loss: 0.6016006469726562, Train Acc: 80.84%, Test Loss: 0.589247465133667, Test Acc: 81.30%\nEpoch 10, Train Loss: 0.5861577391624451, Train Acc: 81.67%, Test Loss: 0.5747091770172119, Test Acc: 81.87%\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Initialize binary cross-entropy loss\ncriterion = nn.BCELoss()\n\ndef calculate_accuracy(predictions, targets):\n    predicted_classes = (predictions &gt;= 0.5).float()\n    return (predicted_classes == targets).float().mean()\n\n# Data preparation\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\nsensitive_features = data[:, 1]\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n\n# Test data (assuming it's prepared similarly)\ntest_data = torch.tensor(X_test_scaled).float()\ntest_targets = torch.tensor(y_test).float().unsqueeze(1)\ntest_sensitive_features = test_data[:, 1]\ntest_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nmodel.train()\nfor epoch in range(10):\n    optimizer.zero_grad()\n    outputs = model(features)\n    loss = criterion(outputs, targets.squeeze())\n    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluation on test data\n    model.eval()\n    with torch.no_grad():\n        test_outputs = model(test_features)\n        test_loss = criterion(test_outputs, test_targets.squeeze())\n        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n    \n    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, '\n          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%')\n    model.train()\n\nEpoch 1, Train Loss: 0.7674269676208496, Train Acc: 34.36%, Test Loss: 0.7305501103401184, Test Acc: 39.80%\nEpoch 2, Train Loss: 0.7305780053138733, Train Acc: 40.48%, Test Loss: 0.6975762248039246, Test Acc: 49.31%\nEpoch 3, Train Loss: 0.6973879337310791, Train Acc: 49.25%, Test Loss: 0.668036162853241, Test Acc: 66.20%\nEpoch 4, Train Loss: 0.6675889492034912, Train Acc: 66.80%, Test Loss: 0.6415244936943054, Test Acc: 75.43%\nEpoch 5, Train Loss: 0.6408513188362122, Train Acc: 75.71%, Test Loss: 0.61774742603302, Test Acc: 78.68%\nEpoch 6, Train Loss: 0.6167588233947754, Train Acc: 78.40%, Test Loss: 0.5962265729904175, Test Acc: 80.39%\nEpoch 7, Train Loss: 0.5949429273605347, Train Acc: 80.14%, Test Loss: 0.5766510367393494, Test Acc: 81.40%\nEpoch 8, Train Loss: 0.5751211643218994, Train Acc: 81.06%, Test Loss: 0.5587018132209778, Test Acc: 81.84%\nEpoch 9, Train Loss: 0.5569527745246887, Train Acc: 81.71%, Test Loss: 0.5421833395957947, Test Acc: 82.25%\nEpoch 10, Train Loss: 0.5401964783668518, Train Acc: 82.35%, Test Loss: 0.5268954634666443, Test Acc: 82.72%\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n# Helper function to calculate accuracy\ndef binary_accuracy(y_pred, y_true):\n    # Applying threshold to get binary output\n    y_pred_tag = torch.round(y_pred)\n    correct_results_sum = (y_pred_tag == y_true).sum().float()\n    acc = correct_results_sum / y_true.shape[0]\n    acc = torch.round(acc * 100)\n    return acc\n\n# Custom discrimination loss function\ndef discrimination_loss(outputs, targets, sensitive_features, lambda_val=0.5, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(outputs, targets)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n\n    prob_unpriv = torch.mean(outputs[mask_unpriv])\n    prob_priv = torch.mean(outputs[mask_priv])\n    discrimination = (prob_priv - prob_unpriv) ** k\n    return standard_loss + lambda_val * discrimination\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Define a simple binary crossentropy loss model for comparison\nclass SimpleBinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleBinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Prepare datasets\ndataset = TensorDataset(features, targets, sensitive_features)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize model and optimizer\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train_model(model, optimizer, train_loader, val_loader, custom_loss=None):\n    for epoch in range(10):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n        for inputs, labels, sens in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            if custom_loss:\n                loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n            else:\n                loss = nn.BCELoss()(outputs, labels.squeeze())\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            train_acc += binary_accuracy(outputs, labels.squeeze()).item()\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_acc = 0\n        with torch.no_grad():\n            for inputs, labels, sens in val_loader:\n                outputs = model(inputs)\n                if custom_loss:\n                    loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n                else:\n                    loss = nn.BCELoss()(outputs, labels.squeeze())\n                val_loss += loss.item()\n                val_acc += binary_accuracy(outputs, labels.squeeze()).item()\n\n        # Average loss and accuracy\n        train_loss /= len(train_loader)\n        train_acc /= len(train_loader)\n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n\n        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}')\n\n# Train the model with discrimination loss\n\n# Train another model with only binary crossentropy\n\n\n\ntrain_model(model, optimizer, train_loader, val_loader, custom_loss=discrimination_loss)\n\nEpoch 1, Train Loss: nan, Train Acc: 85.77, Val Loss: nan, Val Acc: 85.97\nEpoch 2, Train Loss: nan, Train Acc: 86.90, Val Loss: nan, Val Acc: 86.05\nEpoch 3, Train Loss: nan, Train Acc: 87.13, Val Loss: nan, Val Acc: 86.30\nEpoch 4, Train Loss: nan, Train Acc: 87.40, Val Loss: nan, Val Acc: 86.24\nEpoch 5, Train Loss: nan, Train Acc: 87.93, Val Loss: nan, Val Acc: 86.33\nEpoch 6, Train Loss: nan, Train Acc: 88.23, Val Loss: nan, Val Acc: 86.01\nEpoch 7, Train Loss: nan, Train Acc: 88.21, Val Loss: nan, Val Acc: 86.25\nEpoch 8, Train Loss: nan, Train Acc: 88.37, Val Loss: nan, Val Acc: 86.17\nEpoch 9, Train Loss: nan, Train Acc: 88.96, Val Loss: nan, Val Acc: 85.75\nEpoch 10, Train Loss: nan, Train Acc: 88.86, Val Loss: nan, Val Acc: 85.99\n\n\n\nsimple_model = SimpleBinaryClassifier(features.shape[1])\nsimple_optimizer = optim.Adam(simple_model.parameters(), lr=0.001)\ntrain_model(simple_model, simple_optimizer, train_loader, val_loader)\n\nEpoch 1, Train Loss: 0.3526, Train Acc: 85.74, Val Loss: 0.3346, Val Acc: 86.34\nEpoch 2, Train Loss: 0.3199, Train Acc: 86.70, Val Loss: 0.3338, Val Acc: 85.76\nEpoch 3, Train Loss: 0.3129, Train Acc: 87.31, Val Loss: 0.3343, Val Acc: 86.40\nEpoch 4, Train Loss: 0.3095, Train Acc: 87.41, Val Loss: 0.3304, Val Acc: 86.11\nEpoch 5, Train Loss: 0.3047, Train Acc: 87.72, Val Loss: 0.3305, Val Acc: 86.55\nEpoch 6, Train Loss: 0.2998, Train Acc: 88.05, Val Loss: 0.3306, Val Acc: 86.38\nEpoch 7, Train Loss: 0.2960, Train Acc: 88.30, Val Loss: 0.3326, Val Acc: 86.10\nEpoch 8, Train Loss: 0.2913, Train Acc: 88.62, Val Loss: 0.3351, Val Acc: 85.94\nEpoch 9, Train Loss: 0.2864, Train Acc: 88.95, Val Loss: 0.3380, Val Acc: 86.04\nEpoch 10, Train Loss: 0.2806, Train Acc: 89.21, Val Loss: 0.3388, Val Acc: 85.90\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n# Helper function to calculate accuracy\ndef binary_accuracy(y_pred, y_true):\n    # Applying threshold to get binary output\n    y_pred_tag = torch.round(y_pred)\n    correct_results_sum = (y_pred_tag == y_true).sum().float()\n    acc = correct_results_sum / y_true.shape[0]\n    acc = torch.round(acc * 100)\n    return acc\n\n# Custom discrimination loss function\ndef discrimination_loss(outputs, targets, sensitive_features, lambda_val=0.5, k=2):\n    criterion = nn.BCELoss()\n    standard_loss = criterion(outputs, targets)\n\n    mask_unpriv = (sensitive_features == 0)\n    mask_priv = (sensitive_features == 1)\n\n    prob_unpriv = torch.mean(outputs[mask_unpriv])\n    prob_priv = torch.mean(outputs[mask_priv])\n    discrimination = (prob_priv - prob_unpriv) ** k\n    return standard_loss + lambda_val * discrimination\n\n# Define the model\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\n# Define a simple binary crossentropy loss model for comparison\nclass SimpleBinaryClassifier(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleBinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x.squeeze()\n\ndata = torch.tensor(X_train_scaled).float()\ntargets = torch.tensor(y_train).float().unsqueeze(1)\nsensitive_features = data[:, 1]  # Extract the sensitive feature\nfeatures = torch.cat((data[:, :1], data[:, 2:]), dim=1)  # Exclude the sensitive attribute\n\n# Setup DataLoader\ndataset = TensorDataset(features, targets, sensitive_features)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize model and optimizer\nmodel = BinaryClassifier(features.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train_model(model, optimizer, train_loader, val_loader, custom_loss=None):\n    for epoch in range(10):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n        for inputs, labels, sens in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            if custom_loss:\n                loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n            else:\n                loss = nn.BCELoss()(outputs, labels.squeeze())\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            train_acc += binary_accuracy(outputs, labels.squeeze()).item()\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_acc = 0\n        with torch.no_grad():\n            for inputs, labels, sens in val_loader:\n                outputs = model(inputs)\n                if custom_loss:\n                    loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n                else:\n                    loss = nn.BCELoss()(outputs, labels.squeeze())\n                val_loss += loss.item()\n                val_acc += binary_accuracy(outputs, labels.squeeze()).item()\n\n        # Average loss and accuracy\n        train_loss /= len(train_loader)\n        train_acc /= len(train_loader)\n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n\n        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}')\n\n# Train the model with discrimination loss\ntrain_model(model, optimizer, train_loader, val_loader, custom_loss=discrimination_loss)\n\n# Initialize and train another model with only binary crossentropy\n\n\nEpoch 1, Train Loss: nan, Train Acc: 86.01, Val Loss: nan, Val Acc: 86.56\nEpoch 2, Train Loss: nan, Train Acc: 86.83, Val Loss: nan, Val Acc: 86.97\nEpoch 3, Train Loss: nan, Train Acc: 87.25, Val Loss: nan, Val Acc: 87.54\nEpoch 4, Train Loss: nan, Train Acc: 87.22, Val Loss: nan, Val Acc: 87.04\nEpoch 5, Train Loss: nan, Train Acc: 87.67, Val Loss: nan, Val Acc: 87.53\nEpoch 6, Train Loss: nan, Train Acc: 87.95, Val Loss: nan, Val Acc: 87.24\nEpoch 7, Train Loss: nan, Train Acc: 88.06, Val Loss: nan, Val Acc: 87.11\nEpoch 8, Train Loss: nan, Train Acc: 88.62, Val Loss: nan, Val Acc: 86.70\nEpoch 9, Train Loss: nan, Train Acc: 88.73, Val Loss: nan, Val Acc: 86.92\nEpoch 10, Train Loss: nan, Train Acc: 89.14, Val Loss: nan, Val Acc: 86.97\n\n\n\nsimple_model = SimpleBinaryClassifier(features.shape[1])\nsimple_optimizer = optim.Adam(simple_model.parameters(), lr=0.001)\ntrain_model(simple_model, simple_optimizer, train_loader, val_loader)  # This time without custom loss\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nclass DiscriminationLayer(Layer):\n    def __init__(self, sensitive_index, k=2, **kwargs):\n        super().__init__(**kwargs)\n        self.sensitive_index = sensitive_index\n        self.k = k\n\n    def call(self, inputs):\n        y_pred, features = inputs\n        sensitive_attr = features[:, self.sensitive_index]\n        mask_unpriv = tf.cast(tf.equal(sensitive_attr, 0), dtype=tf.float32)\n        mask_priv = tf.cast(tf.equal(sensitive_attr, 1), dtype=tf.float32)\n        epsilon = 1e-8\n        prob_unpriv = tf.reduce_sum(y_pred * mask_unpriv) / (tf.reduce_sum(mask_unpriv) + epsilon)\n        prob_priv = tf.reduce_sum(y_pred * mask_priv) / (tf.reduce_sum(mask_priv) + epsilon)\n        discrimination = tf.pow((prob_priv - prob_unpriv), self.k)\n        return [y_pred, discrimination]\n\n@tf.function\ndef custom_loss(y_true, y_pred_and_discrimination, lambda_val=0.5):\n    y_pred, discrimination = y_pred_and_discrimination\n    standard_loss = BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n    return standard_loss - lambda_val * discrimination\n\n# Model building\ninput_features = Input(shape=(input_size,))\nsensitive_index = 3  # Update this to your sensitive attribute index\n\nx = Dense(64, activation='relu')(input_features)\npredictions = Dense(1, activation='sigmoid')(x)\ndisc_layer = DiscriminationLayer(sensitive_index=sensitive_index)([predictions, input_features])\n\nmodel = Model(inputs=input_features, outputs=disc_layer)\n\n# Model compilation and training\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, 0.5), metrics=['accuracy'])\nhistory = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\nEpoch 1/10\n\n\n\n---------------------------------------------------------------------------\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\nCell In[60], line 42\n     40 # Model compilation and training\n     41 model.compile(optimizer=Adam(learning_rate=0.001), loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, 0.5), metrics=['accuracy'])\n---&gt; 42 history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\n    120     # To get the full stack trace, call:\n    121     # `keras.config.disable_traceback_filtering()`\n--&gt; 122     raise e.with_traceback(filtered_tb) from None\n    123 finally:\n    124     del filtered_tb\n\nFile c:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:52, in py_func_from_autograph.&lt;locals&gt;.autograph_handler(*args, **kwargs)\n     50 except Exception as e:  # pylint:disable=broad-except\n     51   if hasattr(e, \"ag_error_metadata\"):\n---&gt; 52     raise e.ag_error_metadata.to_exception(e)\n     53   else:\n     54     raise\n\nOperatorNotAllowedInGraphError: in user code:\n\n    File \"C:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_19508\\3984959856.py\", line 26, in custom_loss  *\n        y_pred, discrimination = y_pred_and_discrimination\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n\n\n\n\n\nDEFINING custom_loss FUNCTION with custom Lambda , here 0.01\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\ndef custom_loss(sensitive_attr, lambda_val=0.5):\n    \"\"\"\n    Creates a custom loss function that incorporates discrimination penalty.\n\n    Args:\n    sensitive_attr (int): Index of the sensitive attribute in the input features.\n    lambda_val (float): Regularization strength for the discrimination penalty.\n\n    Returns:\n    loss (function): A loss function that takes (y_true, y_pred).\n    \"\"\"\n    def loss(y_true, y_pred):\n        # Standard binary crossentropy loss\n        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n        \n        # Calculate discrimination index\n        # We assume sensitive attribute is binary and 0 is unprivileged, 1 is privileged\n        mask_unpriv = K.cast(K.equal(sensitive_attr, 0), 'float32')\n        mask_priv = K.cast(K.equal(sensitive_attr, 1), 'float32')\n        \n        # Probabilities of positive class\n        # prob_unpriv = K.mean(y_pred * mask_unpriv) / K.mean(mask_unpriv)\n        # prob_priv = K.mean(y_pred * mask_priv) / K.mean(mask_priv)\n        epsilon = 1e-8\n        prob_unpriv = K.mean(y_pred * mask_unpriv) / (K.mean(mask_unpriv) + epsilon)\n        prob_priv = K.mean(y_pred * mask_priv) / (K.mean(mask_priv) + epsilon)\n\n        # Discrimination as the squared difference in probabilities\n        discrimination = K.square(prob_priv - prob_unpriv)\n        print(discrimination)\n        \n        # Custom loss calculation\n        return standard_loss - lambda_val * discrimination\n    \n    return loss\n\n# Model parameters\ninput_size = X_train_scaled.shape[1]  # Number of features\nsensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n# Define the model architecture\ninputs = Input(shape=(input_size,))\nx = Dense(64, activation='relu')(inputs)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=custom_loss(sensitive_attr=X_train_scaled[:, sensitive_index], lambda_val=0.01),\n              metrics=['accuracy'])\n\n# Train the model\nhistory1 = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\nEpoch 1/100\nTensor(\"compile_loss/loss/Square:0\", shape=(), dtype=float32)\nTensor(\"compile_loss/loss/Square:0\", shape=(), dtype=float32)\n299/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8033 - loss: 0.4365Tensor(\"compile_loss/loss/Square:0\", shape=(), dtype=float32)\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8058 - loss: 0.4327 - val_accuracy: 0.8547 - val_loss: 0.3441\nEpoch 2/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8763 - loss: 0.3119 - val_accuracy: 0.8543 - val_loss: 0.3379\nEpoch 3/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8794 - loss: 0.2985 - val_accuracy: 0.8587 - val_loss: 0.3372\nEpoch 4/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8796 - loss: 0.3065 - val_accuracy: 0.8579 - val_loss: 0.3346\nEpoch 5/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8846 - loss: 0.2883 - val_accuracy: 0.8591 - val_loss: 0.3314\nEpoch 6/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8845 - loss: 0.2882 - val_accuracy: 0.8563 - val_loss: 0.3369\nEpoch 7/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8925 - loss: 0.2760 - val_accuracy: 0.8579 - val_loss: 0.3360\nEpoch 8/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8917 - loss: 0.2739 - val_accuracy: 0.8579 - val_loss: 0.3495\nEpoch 9/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8951 - loss: 0.2717 - val_accuracy: 0.8512 - val_loss: 0.3459\nEpoch 10/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9040 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3561\nEpoch 11/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9010 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3470\nEpoch 12/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9050 - loss: 0.2490 - val_accuracy: 0.8512 - val_loss: 0.3560\nEpoch 13/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9083 - loss: 0.2481 - val_accuracy: 0.8523 - val_loss: 0.3670\nEpoch 14/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9054 - loss: 0.2492 - val_accuracy: 0.8539 - val_loss: 0.3552\nEpoch 15/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9137 - loss: 0.2301 - val_accuracy: 0.8508 - val_loss: 0.3714\nEpoch 16/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9157 - loss: 0.2330 - val_accuracy: 0.8523 - val_loss: 0.3805\nEpoch 17/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9144 - loss: 0.2339 - val_accuracy: 0.8500 - val_loss: 0.3780\nEpoch 18/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9118 - loss: 0.2302 - val_accuracy: 0.8555 - val_loss: 0.3919\nEpoch 19/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9162 - loss: 0.2299 - val_accuracy: 0.8480 - val_loss: 0.3883\nEpoch 20/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9226 - loss: 0.2144 - val_accuracy: 0.8460 - val_loss: 0.4096\nEpoch 21/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9211 - loss: 0.2217 - val_accuracy: 0.8516 - val_loss: 0.4037\nEpoch 22/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9192 - loss: 0.2170 - val_accuracy: 0.8500 - val_loss: 0.4034\nEpoch 23/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9270 - loss: 0.2019 - val_accuracy: 0.8413 - val_loss: 0.4156\nEpoch 24/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9334 - loss: 0.1965 - val_accuracy: 0.8488 - val_loss: 0.4213\nEpoch 25/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9256 - loss: 0.2058 - val_accuracy: 0.8512 - val_loss: 0.4194\nEpoch 26/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9230 - loss: 0.2079 - val_accuracy: 0.8437 - val_loss: 0.4259\nEpoch 27/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9303 - loss: 0.2013 - val_accuracy: 0.8441 - val_loss: 0.4273\nEpoch 28/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9316 - loss: 0.1936 - val_accuracy: 0.8381 - val_loss: 0.4543\nEpoch 29/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9313 - loss: 0.2016 - val_accuracy: 0.8468 - val_loss: 0.4473\nEpoch 30/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9373 - loss: 0.1827 - val_accuracy: 0.8417 - val_loss: 0.4511\nEpoch 31/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9346 - loss: 0.1876 - val_accuracy: 0.8350 - val_loss: 0.4681\nEpoch 32/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9367 - loss: 0.1823 - val_accuracy: 0.8441 - val_loss: 0.4578\nEpoch 33/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9380 - loss: 0.1823 - val_accuracy: 0.8433 - val_loss: 0.4643\nEpoch 34/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9382 - loss: 0.1730 - val_accuracy: 0.8429 - val_loss: 0.4602\nEpoch 35/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9407 - loss: 0.1748 - val_accuracy: 0.8397 - val_loss: 0.4727\nEpoch 36/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.9402 - loss: 0.1778 - val_accuracy: 0.8373 - val_loss: 0.4841\nEpoch 37/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.9409 - loss: 0.1663 - val_accuracy: 0.8350 - val_loss: 0.4929\nEpoch 38/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9420 - loss: 0.1681 - val_accuracy: 0.8421 - val_loss: 0.4988\nEpoch 39/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9475 - loss: 0.1611 - val_accuracy: 0.8429 - val_loss: 0.4904\nEpoch 40/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9457 - loss: 0.1633 - val_accuracy: 0.8425 - val_loss: 0.5039\nEpoch 41/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9436 - loss: 0.1630 - val_accuracy: 0.8413 - val_loss: 0.5223\nEpoch 42/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9502 - loss: 0.1493 - val_accuracy: 0.8452 - val_loss: 0.5219\nEpoch 43/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9460 - loss: 0.1563 - val_accuracy: 0.8405 - val_loss: 0.5146\nEpoch 44/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9419 - loss: 0.1671 - val_accuracy: 0.8373 - val_loss: 0.6083\nEpoch 45/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9563 - loss: 0.1423 - val_accuracy: 0.8441 - val_loss: 0.5172\nEpoch 46/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9518 - loss: 0.1563 - val_accuracy: 0.8393 - val_loss: 0.5386\nEpoch 47/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9515 - loss: 0.1459 - val_accuracy: 0.8401 - val_loss: 0.5497\nEpoch 48/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9506 - loss: 0.1465 - val_accuracy: 0.8267 - val_loss: 0.5693\nEpoch 49/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9482 - loss: 0.1613 - val_accuracy: 0.8362 - val_loss: 0.5661\nEpoch 50/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9562 - loss: 0.1408 - val_accuracy: 0.8377 - val_loss: 0.5786\nEpoch 51/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9569 - loss: 0.1348 - val_accuracy: 0.8287 - val_loss: 0.5823\nEpoch 52/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9506 - loss: 0.1394 - val_accuracy: 0.8389 - val_loss: 0.5750\nEpoch 53/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9569 - loss: 0.1372 - val_accuracy: 0.8362 - val_loss: 0.5819\nEpoch 54/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9597 - loss: 0.1356 - val_accuracy: 0.8298 - val_loss: 0.5924\nEpoch 55/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9589 - loss: 0.1328 - val_accuracy: 0.8310 - val_loss: 0.6046\nEpoch 56/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9569 - loss: 0.1342 - val_accuracy: 0.8200 - val_loss: 0.6179\nEpoch 57/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9584 - loss: 0.1330 - val_accuracy: 0.8350 - val_loss: 0.6040\nEpoch 58/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9620 - loss: 0.1266 - val_accuracy: 0.8223 - val_loss: 0.6398\nEpoch 59/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9587 - loss: 0.1314 - val_accuracy: 0.8322 - val_loss: 0.6161\nEpoch 60/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9629 - loss: 0.1177 - val_accuracy: 0.8366 - val_loss: 0.6166\nEpoch 61/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9633 - loss: 0.1183 - val_accuracy: 0.8227 - val_loss: 0.6447\nEpoch 62/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9637 - loss: 0.1208 - val_accuracy: 0.8326 - val_loss: 0.6244\nEpoch 63/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9604 - loss: 0.1245 - val_accuracy: 0.8346 - val_loss: 0.6591\nEpoch 64/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9629 - loss: 0.1260 - val_accuracy: 0.8366 - val_loss: 0.6509\nEpoch 65/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9611 - loss: 0.1246 - val_accuracy: 0.8373 - val_loss: 0.6708\nEpoch 66/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9631 - loss: 0.1189 - val_accuracy: 0.8310 - val_loss: 0.6936\nEpoch 67/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9592 - loss: 0.1239 - val_accuracy: 0.8216 - val_loss: 0.6786\nEpoch 68/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9643 - loss: 0.1230 - val_accuracy: 0.8326 - val_loss: 0.6815\nEpoch 69/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9636 - loss: 0.1216 - val_accuracy: 0.8267 - val_loss: 0.6902\nEpoch 70/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9636 - loss: 0.1147 - val_accuracy: 0.8251 - val_loss: 0.6926\nEpoch 71/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9649 - loss: 0.1131 - val_accuracy: 0.8342 - val_loss: 0.7052\nEpoch 72/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9671 - loss: 0.1105 - val_accuracy: 0.8251 - val_loss: 0.6964\nEpoch 73/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9681 - loss: 0.1057 - val_accuracy: 0.8251 - val_loss: 0.7397\nEpoch 74/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9627 - loss: 0.1106 - val_accuracy: 0.8227 - val_loss: 0.7392\nEpoch 75/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9667 - loss: 0.1124 - val_accuracy: 0.8310 - val_loss: 0.7185\nEpoch 76/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9668 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7144\nEpoch 77/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9672 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7278\nEpoch 78/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9690 - loss: 0.1136 - val_accuracy: 0.8318 - val_loss: 0.7388\nEpoch 79/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9696 - loss: 0.1021 - val_accuracy: 0.8247 - val_loss: 0.7466\nEpoch 80/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9666 - loss: 0.1124 - val_accuracy: 0.8196 - val_loss: 0.7523\nEpoch 81/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9700 - loss: 0.1022 - val_accuracy: 0.8243 - val_loss: 0.7548\nEpoch 82/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9684 - loss: 0.1008 - val_accuracy: 0.8255 - val_loss: 0.7539\nEpoch 83/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9720 - loss: 0.0999 - val_accuracy: 0.8176 - val_loss: 0.7742\nEpoch 84/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9713 - loss: 0.0964 - val_accuracy: 0.8192 - val_loss: 0.7893\nEpoch 85/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9693 - loss: 0.0950 - val_accuracy: 0.8239 - val_loss: 0.8019\nEpoch 86/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9700 - loss: 0.0959 - val_accuracy: 0.8231 - val_loss: 0.8244\nEpoch 87/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9680 - loss: 0.1088 - val_accuracy: 0.8259 - val_loss: 0.8061\nEpoch 88/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9694 - loss: 0.0984 - val_accuracy: 0.8231 - val_loss: 0.7985\nEpoch 89/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9744 - loss: 0.0878 - val_accuracy: 0.8081 - val_loss: 0.8120\nEpoch 90/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9722 - loss: 0.0955 - val_accuracy: 0.8160 - val_loss: 0.8065\nEpoch 91/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9715 - loss: 0.0998 - val_accuracy: 0.8291 - val_loss: 0.7710\nEpoch 92/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9731 - loss: 0.0992 - val_accuracy: 0.8235 - val_loss: 0.8246\nEpoch 93/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9745 - loss: 0.0861 - val_accuracy: 0.8243 - val_loss: 0.8357\nEpoch 94/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9698 - loss: 0.0975 - val_accuracy: 0.8279 - val_loss: 0.8380\nEpoch 95/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9749 - loss: 0.0870 - val_accuracy: 0.8239 - val_loss: 0.8321\nEpoch 96/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9729 - loss: 0.0938 - val_accuracy: 0.8184 - val_loss: 0.8346\nEpoch 97/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9736 - loss: 0.0948 - val_accuracy: 0.8247 - val_loss: 0.8299\nEpoch 98/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9750 - loss: 0.0909 - val_accuracy: 0.8223 - val_loss: 0.8562\nEpoch 99/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9722 - loss: 0.0915 - val_accuracy: 0.8283 - val_loss: 0.8644\nEpoch 100/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9696 - loss: 0.0965 - val_accuracy: 0.8231 - val_loss: 0.8665\n\n\n\n\nModal with with Standard Loss(binary_crossentropy), No discrimination\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nhistory2 = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\nEpoch 1/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.8989 - loss: 0.2601 - val_accuracy: 0.8551 - val_loss: 0.3496\nEpoch 2/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 782us/step - accuracy: 0.8954 - loss: 0.2608 - val_accuracy: 0.8531 - val_loss: 0.3531\nEpoch 3/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 791us/step - accuracy: 0.9064 - loss: 0.2512 - val_accuracy: 0.8531 - val_loss: 0.3671\nEpoch 4/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 846us/step - accuracy: 0.9090 - loss: 0.2452 - val_accuracy: 0.8468 - val_loss: 0.3754\nEpoch 5/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 786us/step - accuracy: 0.9121 - loss: 0.2368 - val_accuracy: 0.8484 - val_loss: 0.3660\nEpoch 6/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 767us/step - accuracy: 0.9150 - loss: 0.2350 - val_accuracy: 0.8535 - val_loss: 0.3656\nEpoch 7/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 880us/step - accuracy: 0.9145 - loss: 0.2336 - val_accuracy: 0.8468 - val_loss: 0.3764\nEpoch 8/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 729us/step - accuracy: 0.9177 - loss: 0.2211 - val_accuracy: 0.8500 - val_loss: 0.3734\nEpoch 9/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 747us/step - accuracy: 0.9162 - loss: 0.2237 - val_accuracy: 0.8543 - val_loss: 0.3917\nEpoch 10/10\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 799us/step - accuracy: 0.9165 - loss: 0.2274 - val_accuracy: 0.8496 - val_loss: 0.3897\n\n\n\nimport plotly.graph_objects as go\n\ndef plot_accuracy(histories):\n    # Create figure for accuracy\n    fig = go.Figure()\n\n    # Add accuracy traces with specified colors\n    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n    for name, history in histories:\n        train_color, val_color = color_map[name]\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['accuracy']))),\n                                 y=history.history['accuracy'],\n                                 name=f'Training Accuracy - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=train_color)))\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_accuracy']))),\n                                 y=history.history['val_accuracy'],\n                                 name=f'Validation Accuracy - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=val_color)))\n\n    # Update layout for accuracy graph\n    fig.update_layout(title='Training and Validation Accuracy',\n                      xaxis_title='Epochs',\n                      yaxis_title='Accuracy',\n                      legend_title='Metric Type')\n\n    fig.show()\n\ndef plot_loss(histories):\n    # Create figure for loss\n    fig = go.Figure()\n\n    # Add loss traces with specified colors\n    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n    for name, history in histories:\n        train_color, val_color = color_map[name]\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['loss']))),\n                                 y=history.history['loss'],\n                                 name=f'Training Loss - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=train_color)))\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_loss']))),\n                                 y=history.history['val_loss'],\n                                 name=f'Validation Loss - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=val_color)))\n\n    # Update layout for loss graph\n    fig.update_layout(title='Training and Validation Loss',\n                      xaxis_title='Epochs',\n                      yaxis_title='Loss',\n                      legend_title='Metric Type')\n\n    fig.show()\n\n# Assuming you have history1 and history2 as the history objects from your model training\nplot_accuracy([('Custom Loss', history1), ('Standard Loss', history2)])\nplot_loss([('Custom Loss', history1), ('Standard Loss', history2)])\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nDEFINING custom_loss FUNCTION with Standard Lambda = 0.5, but Discrimination!=0\n\ndef custom_loss(lambda_val=0.5):\n    def loss(y_true, y_pred):\n        # Extract predictions and sensitive attributes\n        predictions = y_pred[:, 0]\n        sensitive_attr = y_pred[:, 1]\n\n        # Standard binary crossentropy loss\n        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n\n        # Calculate discrimination based on sensitive attribute\n        threshold = 0\n        mask_unpriv = K.cast(sensitive_attr &lt;= threshold, 'float32')\n        mask_priv = K.cast(sensitive_attr &gt; threshold, 'float32')\n\n        sum_unpriv = K.sum(mask_unpriv)\n        sum_priv = K.sum(mask_priv)\n\n        epsilon = 1e-8\n        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n\n        discrimination = K.square(prob_priv - prob_unpriv)\n\n        # Debug outputs\n        # tf.print(\"Standard Loss:\", standard_loss, \"Discrimination:\", discrimination)\n\n        # Total loss with discrimination penalty\n        return standard_loss + lambda_val * discrimination\n    \n    return loss\n\n# Model compilation\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=custom_loss(lambda_val=0.01),  # Change lambda_val as needed\n              metrics=['accuracy'])\n\n# Training the model\nhistory1 = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 38\n     33 model.compile(optimizer=Adam(learning_rate=0.001),\n     34               loss=custom_loss(lambda_val=0.01),  # Change lambda_val as needed\n     35               metrics=['accuracy'])\n     37 # Training the model\n---&gt; 38 history1 = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\nNameError: name 'X_train_with_sensitive' is not defined\n\n\n\n\n\nDEFINING custom_loss FUNCTION with cust Lambda = 0.01, but Discrimination!=0\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\ndef custom_loss(lambda_val=0.001):\n    def loss(y_true, y_pred):\n        # Extract predictions and sensitive attributes from y_pred\n        predictions = y_pred[:, 0]\n        sensitive_attr = y_pred[:, 1]\n\n        # Debug prints to check outputs\n        # tf.print(\"Predictions sample:\", predictions[:10])\n        # tf.print(\"Sensitive Attr sample:\", sensitive_attr[:10])\n\n        # Standard binary crossentropy loss\n        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n        \n        # Determine thresholds to convert sensitive attributes to binary\n        # Assuming the negative and positive classes are split around zero\n        threshold = 0\n        mask_unpriv = K.cast(sensitive_attr &lt;= threshold, 'float32')\n        mask_priv = K.cast(sensitive_attr &gt; threshold, 'float32')\n\n        epsilon = 1e-8\n        sum_unpriv = K.sum(mask_unpriv)\n        sum_priv = K.sum(mask_priv)\n\n        # # Debug prints for mask sums\n        # tf.print(\"Sum unprivileged:\", sum_unpriv)\n        # tf.print(\"Sum privileged:\", sum_priv)\n\n        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n\n        # Discrimination as the squared difference in probabilities\n        discrimination = K.square(prob_priv - prob_unpriv)\n\n        # # Debug print\n        # tf.print(\"Discrimination:\", discrimination)\n\n        # Total loss with discrimination penalty\n        return standard_loss + lambda_val * discrimination\n    \n    return loss\n\n\n\n# Model parameters\ninput_size = X_train_scaled.shape[1]  # Number of features\nsensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n# Input layers\ninputs = Input(shape=(input_size,))\nsensitive_inputs = Input(shape=(1,))\n\n# Network architecture\nx = Dense(64, activation='relu')(inputs)\noutputs = Dense(1, activation='sigmoid')(x)\ncombined_outputs = tf.keras.layers.concatenate([outputs, sensitive_inputs])\n\nmodel = Model(inputs=[inputs, sensitive_inputs], outputs=combined_outputs)\n\n# Compile the model with the custom loss function\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=custom_loss(lambda_val = 0.01),\n              metrics=['accuracy'])\n\n# Prepare data with sensitive attribute\nX_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n\n# Train the model\nhistory1_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\nEpoch 1/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 600us/step - accuracy: 0.6550 - loss: 0.7064 - val_accuracy: 0.6609 - val_loss: 0.6894\nEpoch 2/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6535 - loss: 0.6854 - val_accuracy: 0.6609 - val_loss: 0.6880\nEpoch 3/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6558 - loss: 0.6812 - val_accuracy: 0.6609 - val_loss: 0.6878\nEpoch 4/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 402us/step - accuracy: 0.6569 - loss: 0.6801 - val_accuracy: 0.6609 - val_loss: 0.6883\nEpoch 5/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 409us/step - accuracy: 0.6568 - loss: 0.6789 - val_accuracy: 0.6609 - val_loss: 0.6881\nEpoch 6/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 391us/step - accuracy: 0.6590 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6878\nEpoch 7/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 748us/step - accuracy: 0.6499 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6891\nEpoch 8/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 413us/step - accuracy: 0.6574 - loss: 0.6754 - val_accuracy: 0.6609 - val_loss: 0.6888\nEpoch 9/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 393us/step - accuracy: 0.6514 - loss: 0.6772 - val_accuracy: 0.6609 - val_loss: 0.6886\nEpoch 10/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step - accuracy: 0.6547 - loss: 0.6752 - val_accuracy: 0.6609 - val_loss: 0.6893\nEpoch 11/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 388us/step - accuracy: 0.6602 - loss: 0.6736 - val_accuracy: 0.6609 - val_loss: 0.6887\nEpoch 12/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step - accuracy: 0.6497 - loss: 0.6741 - val_accuracy: 0.6609 - val_loss: 0.6896\nEpoch 13/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step - accuracy: 0.6501 - loss: 0.6703 - val_accuracy: 0.6609 - val_loss: 0.6900\nEpoch 14/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 389us/step - accuracy: 0.6578 - loss: 0.6716 - val_accuracy: 0.6609 - val_loss: 0.6891\nEpoch 15/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.6528 - loss: 0.6702 - val_accuracy: 0.6609 - val_loss: 0.6884\nEpoch 16/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step - accuracy: 0.6607 - loss: 0.6709 - val_accuracy: 0.6609 - val_loss: 0.6890\nEpoch 17/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6566 - loss: 0.6708 - val_accuracy: 0.6609 - val_loss: 0.6896\nEpoch 18/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6558 - loss: 0.6706 - val_accuracy: 0.6609 - val_loss: 0.6901\nEpoch 19/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6534 - loss: 0.6695 - val_accuracy: 0.6609 - val_loss: 0.6898\nEpoch 20/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 443us/step - accuracy: 0.6589 - loss: 0.6673 - val_accuracy: 0.6609 - val_loss: 0.6903\nEpoch 21/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6594 - loss: 0.6697 - val_accuracy: 0.6609 - val_loss: 0.6895\nEpoch 22/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6541 - loss: 0.6656 - val_accuracy: 0.6609 - val_loss: 0.6897\nEpoch 23/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6504 - loss: 0.6680 - val_accuracy: 0.6609 - val_loss: 0.6898\nEpoch 24/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6491 - loss: 0.6642 - val_accuracy: 0.6609 - val_loss: 0.6893\nEpoch 25/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 489us/step - accuracy: 0.6595 - loss: 0.6663 - val_accuracy: 0.6609 - val_loss: 0.6908\nEpoch 26/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6486 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6899\nEpoch 27/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.6512 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6891\nEpoch 28/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 434us/step - accuracy: 0.6623 - loss: 0.6659 - val_accuracy: 0.6609 - val_loss: 0.6902\nEpoch 29/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6488 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6908\nEpoch 30/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6629 - loss: 0.6647 - val_accuracy: 0.6609 - val_loss: 0.6891\nEpoch 31/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6499 - loss: 0.6653 - val_accuracy: 0.6609 - val_loss: 0.6902\nEpoch 32/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 379us/step - accuracy: 0.6537 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903\nEpoch 33/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6560 - loss: 0.6649 - val_accuracy: 0.6609 - val_loss: 0.6893\nEpoch 34/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 367us/step - accuracy: 0.6515 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6899\nEpoch 35/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6549 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6904\nEpoch 36/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 418us/step - accuracy: 0.6498 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903\nEpoch 37/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.6572 - loss: 0.6641 - val_accuracy: 0.6609 - val_loss: 0.6926\nEpoch 38/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6623 - loss: 0.6660 - val_accuracy: 0.6609 - val_loss: 0.6901\nEpoch 39/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6571 - loss: 0.6645 - val_accuracy: 0.6609 - val_loss: 0.6906\nEpoch 40/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step - accuracy: 0.6571 - loss: 0.6637 - val_accuracy: 0.6609 - val_loss: 0.6909\nEpoch 41/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6551 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6906\nEpoch 42/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 514us/step - accuracy: 0.6476 - loss: 0.6631 - val_accuracy: 0.6609 - val_loss: 0.6917\nEpoch 43/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6529 - loss: 0.6662 - val_accuracy: 0.6609 - val_loss: 0.6911\nEpoch 44/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6571 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6911\nEpoch 45/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.6486 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6914\nEpoch 46/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 434us/step - accuracy: 0.6591 - loss: 0.6633 - val_accuracy: 0.6609 - val_loss: 0.6913\nEpoch 47/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6451 - loss: 0.6622 - val_accuracy: 0.6609 - val_loss: 0.6913\nEpoch 48/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6502 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907\nEpoch 49/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6517 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6918\nEpoch 50/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6593 - loss: 0.6606 - val_accuracy: 0.6609 - val_loss: 0.6911\nEpoch 51/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6534 - loss: 0.6638 - val_accuracy: 0.6609 - val_loss: 0.6910\nEpoch 52/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.6602 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6910\nEpoch 53/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.6466 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6917\nEpoch 54/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.6599 - loss: 0.6630 - val_accuracy: 0.6609 - val_loss: 0.6906\nEpoch 55/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.6499 - loss: 0.6612 - val_accuracy: 0.6609 - val_loss: 0.6906\nEpoch 56/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6580 - loss: 0.6635 - val_accuracy: 0.6609 - val_loss: 0.6913\nEpoch 57/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6571 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900\nEpoch 58/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.6615 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6904\nEpoch 59/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 622us/step - accuracy: 0.6572 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6899\nEpoch 60/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6577 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6912\nEpoch 61/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.6541 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907\nEpoch 62/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6614 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6903\nEpoch 63/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6569 - loss: 0.6602 - val_accuracy: 0.6609 - val_loss: 0.6903\nEpoch 64/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.6489 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6905\nEpoch 65/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 379us/step - accuracy: 0.6461 - loss: 0.6604 - val_accuracy: 0.6609 - val_loss: 0.6912\nEpoch 66/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 379us/step - accuracy: 0.6633 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6904\nEpoch 67/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.6551 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6900\nEpoch 68/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6567 - loss: 0.6610 - val_accuracy: 0.6609 - val_loss: 0.6902\nEpoch 69/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6503 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6915\nEpoch 70/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.6579 - loss: 0.6627 - val_accuracy: 0.6609 - val_loss: 0.6905\nEpoch 71/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.6569 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6915\nEpoch 72/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6494 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6918\nEpoch 73/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.6484 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6916\nEpoch 74/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 416us/step - accuracy: 0.6512 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6909\nEpoch 75/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 551us/step - accuracy: 0.6613 - loss: 0.6588 - val_accuracy: 0.6609 - val_loss: 0.6914\nEpoch 76/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 367us/step - accuracy: 0.6557 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6907\nEpoch 77/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step - accuracy: 0.6592 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900\nEpoch 78/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6553 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6917\nEpoch 79/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6564 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6912\nEpoch 80/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6525 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907\nEpoch 81/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.6482 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6912\nEpoch 82/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.6534 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6900\nEpoch 83/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6580 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907\nEpoch 84/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 370us/step - accuracy: 0.6500 - loss: 0.6596 - val_accuracy: 0.6609 - val_loss: 0.6907\nEpoch 85/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 367us/step - accuracy: 0.6518 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6899\nEpoch 86/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.6579 - loss: 0.6587 - val_accuracy: 0.6609 - val_loss: 0.6908\nEpoch 87/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step - accuracy: 0.6646 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6897\nEpoch 88/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 418us/step - accuracy: 0.6484 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6904\nEpoch 89/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.6500 - loss: 0.6595 - val_accuracy: 0.6609 - val_loss: 0.6917\nEpoch 90/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 532us/step - accuracy: 0.6472 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6915\nEpoch 91/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.6485 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6917\nEpoch 92/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6596 - loss: 0.6621 - val_accuracy: 0.6609 - val_loss: 0.6902\nEpoch 93/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 371us/step - accuracy: 0.6508 - loss: 0.6600 - val_accuracy: 0.6609 - val_loss: 0.6894\nEpoch 94/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.6520 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6909\nEpoch 95/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.6587 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6928\nEpoch 96/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 385us/step - accuracy: 0.6538 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6923\nEpoch 97/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 394us/step - accuracy: 0.6521 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6910\nEpoch 98/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 530us/step - accuracy: 0.6471 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6923\nEpoch 99/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.6484 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6891\nEpoch 100/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.6508 - loss: 0.6590 - val_accuracy: 0.6609 - val_loss: 0.6903\n\n\n\n\nModal with with Standard Loss(binary_crossentropy), No discrimination , but Discrimination!=0 in custom losses\n\n\n# Model parameters\ninput_size = X_train_scaled.shape[1]  # Number of features\nsensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n\n# Input layers\ninputs = Input(shape=(input_size,))\nsensitive_inputs = Input(shape=(1,))\n\n# Network architecture\nx = Dense(64, activation='relu')(inputs)\noutputs = Dense(1, activation='sigmoid')(x)\n\n# No need to concatenate outputs and sensitive inputs for the loss calculation\n# Separate outputs for predictions and sensitive attributes\n# Since we're using binary_crossentropy, we only need the main outputs\nmodel = Model(inputs=[inputs, sensitive_inputs], outputs=outputs)\n\n# Compile the model with binary crossentropy\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Prepare data with sensitive attribute\nX_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n\n# Train the model\nhistory2_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\nEpoch 1/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 1s 598us/step - accuracy: 0.8259 - loss: 0.4073 - val_accuracy: 0.8595 - val_loss: 0.3396\nEpoch 2/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 442us/step - accuracy: 0.8696 - loss: 0.3200 - val_accuracy: 0.8634 - val_loss: 0.3325\nEpoch 3/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 754us/step - accuracy: 0.8746 - loss: 0.3044 - val_accuracy: 0.8630 - val_loss: 0.3334\nEpoch 4/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 399us/step - accuracy: 0.8820 - loss: 0.2964 - val_accuracy: 0.8630 - val_loss: 0.3361\nEpoch 5/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 387us/step - accuracy: 0.8827 - loss: 0.2969 - val_accuracy: 0.8650 - val_loss: 0.3331\nEpoch 6/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 385us/step - accuracy: 0.8857 - loss: 0.2855 - val_accuracy: 0.8646 - val_loss: 0.3304\nEpoch 7/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.8811 - loss: 0.2898 - val_accuracy: 0.8595 - val_loss: 0.3401\nEpoch 8/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.8928 - loss: 0.2674 - val_accuracy: 0.8658 - val_loss: 0.3319\nEpoch 9/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 386us/step - accuracy: 0.8897 - loss: 0.2836 - val_accuracy: 0.8646 - val_loss: 0.3381\nEpoch 10/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.8926 - loss: 0.2663 - val_accuracy: 0.8567 - val_loss: 0.3486\nEpoch 11/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 412us/step - accuracy: 0.8995 - loss: 0.2607 - val_accuracy: 0.8595 - val_loss: 0.3509\nEpoch 12/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 399us/step - accuracy: 0.9013 - loss: 0.2601 - val_accuracy: 0.8598 - val_loss: 0.3511\nEpoch 13/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.9017 - loss: 0.2531 - val_accuracy: 0.8595 - val_loss: 0.3629\nEpoch 14/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 366us/step - accuracy: 0.9109 - loss: 0.2382 - val_accuracy: 0.8606 - val_loss: 0.3490\nEpoch 15/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.9070 - loss: 0.2503 - val_accuracy: 0.8535 - val_loss: 0.3574\nEpoch 16/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 444us/step - accuracy: 0.9119 - loss: 0.2331 - val_accuracy: 0.8551 - val_loss: 0.3676\nEpoch 17/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 356us/step - accuracy: 0.9143 - loss: 0.2275 - val_accuracy: 0.8535 - val_loss: 0.3737\nEpoch 18/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9121 - loss: 0.2327 - val_accuracy: 0.8575 - val_loss: 0.3660\nEpoch 19/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 563us/step - accuracy: 0.9233 - loss: 0.2215 - val_accuracy: 0.8539 - val_loss: 0.3894\nEpoch 20/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.9159 - loss: 0.2244 - val_accuracy: 0.8523 - val_loss: 0.3763\nEpoch 21/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9230 - loss: 0.2136 - val_accuracy: 0.8555 - val_loss: 0.3882\nEpoch 22/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 385us/step - accuracy: 0.9242 - loss: 0.2074 - val_accuracy: 0.8535 - val_loss: 0.3934\nEpoch 23/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9253 - loss: 0.2120 - val_accuracy: 0.8496 - val_loss: 0.3942\nEpoch 24/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 354us/step - accuracy: 0.9266 - loss: 0.2041 - val_accuracy: 0.8464 - val_loss: 0.4131\nEpoch 25/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.9202 - loss: 0.2132 - val_accuracy: 0.8500 - val_loss: 0.4122\nEpoch 26/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step - accuracy: 0.9325 - loss: 0.1921 - val_accuracy: 0.8535 - val_loss: 0.4094\nEpoch 27/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9351 - loss: 0.1893 - val_accuracy: 0.8504 - val_loss: 0.4226\nEpoch 28/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 351us/step - accuracy: 0.9320 - loss: 0.1931 - val_accuracy: 0.8480 - val_loss: 0.4264\nEpoch 29/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 358us/step - accuracy: 0.9325 - loss: 0.1899 - val_accuracy: 0.8496 - val_loss: 0.4329\nEpoch 30/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 352us/step - accuracy: 0.9317 - loss: 0.1922 - val_accuracy: 0.8500 - val_loss: 0.4303\nEpoch 31/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 352us/step - accuracy: 0.9371 - loss: 0.1796 - val_accuracy: 0.8437 - val_loss: 0.4379\nEpoch 32/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 347us/step - accuracy: 0.9337 - loss: 0.1851 - val_accuracy: 0.8460 - val_loss: 0.4467\nEpoch 33/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 362us/step - accuracy: 0.9348 - loss: 0.1881 - val_accuracy: 0.8445 - val_loss: 0.4406\nEpoch 34/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step - accuracy: 0.9399 - loss: 0.1758 - val_accuracy: 0.8441 - val_loss: 0.4533\nEpoch 35/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 343us/step - accuracy: 0.9370 - loss: 0.1763 - val_accuracy: 0.8429 - val_loss: 0.4685\nEpoch 36/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.9420 - loss: 0.1644 - val_accuracy: 0.8437 - val_loss: 0.4601\nEpoch 37/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.9382 - loss: 0.1685 - val_accuracy: 0.8421 - val_loss: 0.4721\nEpoch 38/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 373us/step - accuracy: 0.9397 - loss: 0.1732 - val_accuracy: 0.8401 - val_loss: 0.4915\nEpoch 39/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 529us/step - accuracy: 0.9463 - loss: 0.1582 - val_accuracy: 0.8354 - val_loss: 0.5038\nEpoch 40/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 361us/step - accuracy: 0.9438 - loss: 0.1608 - val_accuracy: 0.8429 - val_loss: 0.4831\nEpoch 41/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.9403 - loss: 0.1676 - val_accuracy: 0.8452 - val_loss: 0.4940\nEpoch 42/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 390us/step - accuracy: 0.9445 - loss: 0.1581 - val_accuracy: 0.8393 - val_loss: 0.4956\nEpoch 43/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step - accuracy: 0.9461 - loss: 0.1537 - val_accuracy: 0.8350 - val_loss: 0.5094\nEpoch 44/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 361us/step - accuracy: 0.9465 - loss: 0.1573 - val_accuracy: 0.8350 - val_loss: 0.5115\nEpoch 45/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 395us/step - accuracy: 0.9467 - loss: 0.1530 - val_accuracy: 0.8409 - val_loss: 0.5131\nEpoch 46/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 362us/step - accuracy: 0.9473 - loss: 0.1580 - val_accuracy: 0.8393 - val_loss: 0.5254\nEpoch 47/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 513us/step - accuracy: 0.9492 - loss: 0.1451 - val_accuracy: 0.8377 - val_loss: 0.5261\nEpoch 48/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 393us/step - accuracy: 0.9517 - loss: 0.1462 - val_accuracy: 0.8247 - val_loss: 0.5360\nEpoch 49/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9528 - loss: 0.1422 - val_accuracy: 0.8287 - val_loss: 0.5420\nEpoch 50/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 390us/step - accuracy: 0.9515 - loss: 0.1378 - val_accuracy: 0.8216 - val_loss: 0.5578\nEpoch 51/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step - accuracy: 0.9508 - loss: 0.1454 - val_accuracy: 0.8373 - val_loss: 0.5463\nEpoch 52/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 387us/step - accuracy: 0.9487 - loss: 0.1405 - val_accuracy: 0.8385 - val_loss: 0.5607\nEpoch 53/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 384us/step - accuracy: 0.9545 - loss: 0.1364 - val_accuracy: 0.8310 - val_loss: 0.5591\nEpoch 54/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 584us/step - accuracy: 0.9557 - loss: 0.1387 - val_accuracy: 0.8366 - val_loss: 0.5878\nEpoch 55/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.9549 - loss: 0.1338 - val_accuracy: 0.8334 - val_loss: 0.5751\nEpoch 56/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 421us/step - accuracy: 0.9518 - loss: 0.1390 - val_accuracy: 0.8389 - val_loss: 0.5626\nEpoch 57/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9583 - loss: 0.1279 - val_accuracy: 0.8389 - val_loss: 0.5940\nEpoch 58/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 345us/step - accuracy: 0.9558 - loss: 0.1300 - val_accuracy: 0.8314 - val_loss: 0.5842\nEpoch 59/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 349us/step - accuracy: 0.9592 - loss: 0.1307 - val_accuracy: 0.8366 - val_loss: 0.5996\nEpoch 60/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 351us/step - accuracy: 0.9568 - loss: 0.1323 - val_accuracy: 0.8326 - val_loss: 0.5967\nEpoch 61/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 347us/step - accuracy: 0.9587 - loss: 0.1232 - val_accuracy: 0.8283 - val_loss: 0.5802\nEpoch 62/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step - accuracy: 0.9572 - loss: 0.1475 - val_accuracy: 0.8267 - val_loss: 0.6030\nEpoch 63/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 348us/step - accuracy: 0.9628 - loss: 0.1194 - val_accuracy: 0.8279 - val_loss: 0.6128\nEpoch 64/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.9615 - loss: 0.1162 - val_accuracy: 0.8302 - val_loss: 0.6099\nEpoch 65/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step - accuracy: 0.9601 - loss: 0.1192 - val_accuracy: 0.8314 - val_loss: 0.6176\nEpoch 66/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 390us/step - accuracy: 0.9592 - loss: 0.1237 - val_accuracy: 0.8298 - val_loss: 0.6224\nEpoch 67/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 355us/step - accuracy: 0.9606 - loss: 0.1188 - val_accuracy: 0.8223 - val_loss: 0.6446\nEpoch 68/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.9616 - loss: 0.1147 - val_accuracy: 0.8322 - val_loss: 0.6292\nEpoch 69/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 548us/step - accuracy: 0.9624 - loss: 0.1157 - val_accuracy: 0.8330 - val_loss: 0.6557\nEpoch 70/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.9615 - loss: 0.1165 - val_accuracy: 0.8176 - val_loss: 0.6655\nEpoch 71/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 404us/step - accuracy: 0.9618 - loss: 0.1145 - val_accuracy: 0.8298 - val_loss: 0.6618\nEpoch 72/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 406us/step - accuracy: 0.9617 - loss: 0.1177 - val_accuracy: 0.8018 - val_loss: 0.6880\nEpoch 73/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step - accuracy: 0.9627 - loss: 0.1134 - val_accuracy: 0.8330 - val_loss: 0.6874\nEpoch 74/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step - accuracy: 0.9636 - loss: 0.1144 - val_accuracy: 0.8279 - val_loss: 0.6749\nEpoch 75/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 348us/step - accuracy: 0.9638 - loss: 0.1070 - val_accuracy: 0.8239 - val_loss: 0.6767\nEpoch 76/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9642 - loss: 0.1097 - val_accuracy: 0.8330 - val_loss: 0.6812\nEpoch 77/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 381us/step - accuracy: 0.9632 - loss: 0.1096 - val_accuracy: 0.8310 - val_loss: 0.6936\nEpoch 78/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9667 - loss: 0.1049 - val_accuracy: 0.8255 - val_loss: 0.7077\nEpoch 79/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 376us/step - accuracy: 0.9657 - loss: 0.1040 - val_accuracy: 0.8251 - val_loss: 0.6987\nEpoch 80/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 347us/step - accuracy: 0.9640 - loss: 0.1099 - val_accuracy: 0.8220 - val_loss: 0.7047\nEpoch 81/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 343us/step - accuracy: 0.9661 - loss: 0.1059 - val_accuracy: 0.8243 - val_loss: 0.7189\nEpoch 82/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 493us/step - accuracy: 0.9657 - loss: 0.1057 - val_accuracy: 0.8330 - val_loss: 0.7181\nEpoch 83/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step - accuracy: 0.9708 - loss: 0.0954 - val_accuracy: 0.8295 - val_loss: 0.7182\nEpoch 84/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 389us/step - accuracy: 0.9676 - loss: 0.1044 - val_accuracy: 0.8247 - val_loss: 0.7164\nEpoch 85/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.9695 - loss: 0.1019 - val_accuracy: 0.8306 - val_loss: 0.7415\nEpoch 86/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 336us/step - accuracy: 0.9633 - loss: 0.1033 - val_accuracy: 0.8302 - val_loss: 0.7433\nEpoch 87/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 342us/step - accuracy: 0.9687 - loss: 0.1010 - val_accuracy: 0.8302 - val_loss: 0.7664\nEpoch 88/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 372us/step - accuracy: 0.9683 - loss: 0.0958 - val_accuracy: 0.8279 - val_loss: 0.7715\nEpoch 89/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 342us/step - accuracy: 0.9671 - loss: 0.1022 - val_accuracy: 0.8298 - val_loss: 0.7661\nEpoch 90/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 355us/step - accuracy: 0.9731 - loss: 0.0872 - val_accuracy: 0.8397 - val_loss: 0.7873\nEpoch 91/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.9685 - loss: 0.0973 - val_accuracy: 0.8220 - val_loss: 0.7802\nEpoch 92/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step - accuracy: 0.9667 - loss: 0.1003 - val_accuracy: 0.8259 - val_loss: 0.7762\nEpoch 93/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.9679 - loss: 0.0941 - val_accuracy: 0.8259 - val_loss: 0.7889\nEpoch 94/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 391us/step - accuracy: 0.9719 - loss: 0.0876 - val_accuracy: 0.8298 - val_loss: 0.7972\nEpoch 95/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 386us/step - accuracy: 0.9744 - loss: 0.0854 - val_accuracy: 0.8156 - val_loss: 0.8149\nEpoch 96/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 374us/step - accuracy: 0.9703 - loss: 0.0935 - val_accuracy: 0.8302 - val_loss: 0.8087\nEpoch 97/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 345us/step - accuracy: 0.9698 - loss: 0.0894 - val_accuracy: 0.8295 - val_loss: 0.8223\nEpoch 98/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 489us/step - accuracy: 0.9695 - loss: 0.0954 - val_accuracy: 0.8318 - val_loss: 0.8068\nEpoch 99/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step - accuracy: 0.9722 - loss: 0.0856 - val_accuracy: 0.8338 - val_loss: 0.8178\nEpoch 100/100\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step - accuracy: 0.9726 - loss: 0.0874 - val_accuracy: 0.8200 - val_loss: 0.8233\n\n\n\nimport matplotlib.pyplot as plt\n\n# Assuming history1_nd.history['accuracy'] and history2_nd.history['accuracy'] are available\n# These lists should contain the accuracy for each epoch\nepochs = range(1, len(history1_nd.history['accuracy']) + 1)\n\nplt.figure(figsize=(10, 5))\n\n# Plotting accuracy for history1_nd\nplt.scatter(epochs, history1_nd.history['accuracy'], color='blue', label='Model 1 Accuracy')\n\n# Plotting accuracy for history2_nd\nplt.scatter(epochs, history2_nd.history['accuracy'], color='red', label='Model 2 Accuracy')\n\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Comparison of Model Accuracies Across Epochs')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef custom_loss(lambda_val=0.5):\n    def loss(y_true, y_pred):\n        # Extract predictions and sensitive attributes\n        predictions = y_pred[:, 0]\n        sensitive_attr = y_pred[:, 1]\n\n        # Standard binary crossentropy loss\n        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n\n        # Calculate discrimination based on sensitive attribute\n        threshold = 0\n        mask_unpriv = K.cast(sensitive_attr &lt;= threshold, 'float32')\n        mask_priv = K.cast(sensitive_attr &gt; threshold, 'float32')\n\n        sum_unpriv = K.sum(mask_unpriv)\n        sum_priv = K.sum(mask_priv)\n\n        epsilon = 1e-8\n        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n\n        discrimination = K.square(prob_priv - prob_unpriv)\n\n        # Debug outputs\n        # tf.print(\"Standard Loss:\", standard_loss, \"Discrimination:\", discrimination)\n\n        # Total loss with discrimination penalty\n        return standard_loss + lambda_val * discrimination\n    \n    return loss\n\n# Model compilation\nmodel.compile(optimizer=Adam(learning_rate=0.01),\n              loss=custom_loss(lambda_val=0.01),  # Change lambda_val as needed\n              metrics=['accuracy'])\n\n# Training the model\nhistory1 = model.fit(X_train_with_sensitive, y_train, epochs=5, batch_size=32, validation_split=0.2)\n\nEpoch 1/5\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 622us/step - accuracy: 0.6523 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\nEpoch 2/5\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 440us/step - accuracy: 0.6535 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\nEpoch 3/5\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 382us/step - accuracy: 0.6649 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\nEpoch 4/5\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 425us/step - accuracy: 0.6511 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\nEpoch 5/5\n317/317 ━━━━━━━━━━━━━━━━━━━━ 0s 416us/step - accuracy: 0.6526 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\n\n\n\nimport plotly.graph_objects as go\n\n# Assuming history1_nd and history2_nd have 'accuracy' data from your model training\n# Extracting the accuracy data from each history object\naccuracy1 = history1_nd.history['accuracy']\naccuracy2 = history2_nd.history['accuracy']\n\n# Assuming the number of epochs is determined by the length of the accuracy data\nepochs = list(range(1, len(accuracy1) + 1))\n\n# Create a scatter plot using Plotly\nfig = go.Figure()\n\n# Adding scatter plot for Model 1\nfig.add_trace(go.Scatter(\n    x=epochs,\n    y=accuracy1,\n    mode='markers',  # This specifies to use markers (points)\n    name='Model 1 Accuracy',  # Name of the series\n    marker=dict(color='blue', size=8)  # Marker settings\n))\n\n# Adding scatter plot for Model 2\nfig.add_trace(go.Scatter(\n    x=epochs,\n    y=accuracy2,\n    mode='markers',\n    name='Model 2 Accuracy',\n    marker=dict(color='red', size=8)\n))\n\n# Update the layout to add titles and labels\nfig.update_layout(\n    title='Comparison of Model Accuracies Across Epochs',\n    xaxis_title='Epochs',\n    yaxis_title='Accuracy',\n    template='plotly_white'\n)\n\n# Show the figure\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nGRAPH\n\nimport plotly.graph_objects as go\n\ndef plot_accuracy(histories):\n    # Create figure for accuracy\n    fig = go.Figure()\n\n    # Add accuracy traces with specified colors\n    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n    for name, history in histories:\n        train_color, val_color = color_map[name]\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['accuracy']))),\n                                 y=history.history['accuracy'],\n                                 name=f'Training Accuracy - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=train_color)))\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_accuracy']))),\n                                 y=history.history['val_accuracy'],\n                                 name=f'Validation Accuracy - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=val_color)))\n\n    # Update layout for accuracy graph\n    fig.update_layout(title='Training and Validation Accuracy',\n                      xaxis_title='Epochs',\n                      yaxis_title='Accuracy',\n                      legend_title='Metric Type')\n\n    fig.show()\n\ndef plot_loss(histories):\n    # Create figure for loss\n    fig = go.Figure()\n\n    # Add loss traces with specified colors\n    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n    for name, history in histories:\n        train_color, val_color = color_map[name]\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['loss']))),\n                                 y=history.history['loss'],\n                                 name=f'Training Loss - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=train_color)))\n        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_loss']))),\n                                 y=history.history['val_loss'],\n                                 name=f'Validation Loss - {name}',\n                                 mode='lines+markers',\n                                 line=dict(color=val_color)))\n\n    # Update layout for loss graph\n    fig.update_layout(title='Training and Validation Loss',\n                      xaxis_title='Epochs',\n                      yaxis_title='Loss',\n                      legend_title='Metric Type')\n\n    fig.show()\n\n# Assuming you have history1 and history2 as the history objects from your model training\nplot_accuracy([('Custom Loss', history1_nd), ('Standard Loss', history2)])\nplot_loss([('Custom Loss', history1_nd), ('Standard Loss', history2)])\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nlfr_model = LFR(unprivileged_groups=unprivileged_groups, \n                privileged_groups=privileged_groups)\n\n# Fit the model and transform the dataset\nlfr_model.fit(dataset_orig_panel19_train)\ndataset_transf_panel19_train_lfr = lfr_model.transform(dataset_orig_panel19_train, threshold = 0.22)\ndataset_transf_panel19_train_lfr = dataset_orig_panel19_train.align_datasets(dataset_transf_panel19_train_lfr)\n\n\nimport numpy as np\n\ndef get_distortion_meps(vold, vnew):\n    # Initialize distortion score\n    distortion_score = 0.0\n\n    # Define weights for different categories of attributes\n    sensitive_weight = 3.0\n    health_status_weight = 2.0\n    socio_economic_weight = 1.0\n    behavior_weight = 1.5\n\n    # Sensitive attributes\n    for attr in ['AGE', 'RACE', 'SEX=1', 'SEX=2']:\n        if vold[attr] != vnew[attr]:\n            distortion_score += sensitive_weight\n\n    # Health status indicators\n    health_attrs = ['PCS42', 'MCS42', 'K6SUM42', 'HIBPDX', 'DIABDX', 'CHDDX', 'ANGIDX', 'MIDX', 'OHRTDX', 'STRKDX', 'EMPHDX', 'CANCERDX', 'JTPAIN', 'ARTHDX', 'ASTHDX', 'ADHDADDX']\n    for attr in health_attrs:\n        # Assuming health status attributes are numerical and a difference in value indicates a change in health status\n        distortion_score += health_status_weight * abs(vold.get(attr, 0) - vnew.get(attr, 0))\n\n    # Socioeconomic and environmental factors\n    socio_attrs = ['REGION=1', 'REGION=2', 'REGION=3', 'REGION=4', 'MARRY', 'FTSTU', 'EMPST', 'POVCAT', 'INSCOV']\n    for attr in socio_attrs:\n        if vold[attr] != vnew[attr]:\n            distortion_score += socio_economic_weight\n\n    # Health-related behaviors\n    behavior_attrs = ['ADSMOK42', 'WLKLIM', 'ACTLIM', 'SOCLIM', 'COGLIM']\n    for attr in behavior_attrs:\n        if vold[attr] != vnew[attr]:\n            distortion_score += behavior_weight\n\n    return distortion_score\n\n\nDI = DisparateImpactRemover()\ndataset_transf_panel19_train_di = DI.fit_transform(dataset_orig_panel19_train)\n\n\nmetric_transf_panel19_train = BinaryLabelDatasetMetric(\ndataset_transf_panel19_train_rw,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_rw=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\nmetric_transf_panel19_train = BinaryLabelDatasetMetric(\ndataset_transf_panel19_train_di,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_di=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n\nmetric_transf_panel19_train = BinaryLabelDatasetMetric(\ndataset_transf_panel19_train_lfr,\nunprivileged_groups=unprivileged_groups,\nprivileged_groups=privileged_groups)\nexplainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\ntest_results_lfr=[explainer_transf_panel19_train.mean_difference()\n               ,explainer_transf_panel19_train.consistency()\n               ,explainer_transf_panel19_train.statistical_parity_difference()\n               ,explainer_transf_panel19_train.disparate_impact()]\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide\n  return metric_fun(privileged=False) / metric_fun(privileged=True)\n\n\n\ntest_results\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n 'Consistency (Zemel, et al. 2013): [0.83660139]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']\n\n\n\ntest_results_rw\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -2.7755575615628914e-17',\n 'Consistency (Zemel, et al. 2013): [0.83660139]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -2.7755575615628914e-17',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999999']\n\n\n\ntest_results_lfr\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.0',\n 'Consistency (Zemel, et al. 2013): [1.]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.0',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): nan']\n\n\n\ntest_results_di\n\n['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n 'Consistency (Zemel, et al. 2013): [0.83689198]',\n 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']\n\n\n\nfeatures = dataset_transf_panel19_train_rw.features\nlabel = dataset_transf_panel19_train_rw.labels.ravel()  # Flatten the label array if necessary\nweights = dataset_transf_panel19_train_rw.instance_weights\nfeature_names = dataset_transf_panel19_train_rw.feature_names\ndf_rw = pd.DataFrame(features, columns=feature_names)\ndf_rw['label'] = label\ndf_rw['weights'] = weights\n\n\ndf_rw\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\nweights\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n17459.483776\n\n\n1\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n14515.313940\n\n\n2\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n18465.607681\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n21762.696983\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n3727.042408\n\n\n15826\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n4909.081729\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n4184.786789\n\n\n15828\n54.0\n0.0\n43.97\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n4427.370919\n\n\n15829\n73.0\n0.0\n42.68\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n6010.846084\n\n\n\n\n15830 rows × 140 columns\n\n\n\n\nfeatures = dataset_orig_panel19_train.features\nlabel = dataset_orig_panel19_train.labels\nfeature_names = dataset_orig_panel19_train.feature_names\ndataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\ndataset_orig_panel19_train_df['label'] = label\ndataset_orig_panel19_train_df\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n56.0\n1.0\n20.42\n26.57\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n23.0\n1.0\n53.12\n50.33\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15826\n25.0\n0.0\n56.71\n62.39\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15828\n54.0\n0.0\n43.97\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15829\n73.0\n0.0\n42.68\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n15830 rows × 139 columns\n\n\n\n\n# # Assuming `dataset_transf_panel19_train_lfr` is your LFR transformed dataset\n# features_lfr = dataset_transf_panel19_train_lfr.features\n# label_lfr = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary\n# feature_names_lfr = dataset_transf_panel19_train_lfr.feature_names\n\n# # Create a DataFrame\n# df_lfr = pd.DataFrame(features_lfr, columns=feature_names_lfr)\n# df_lfr['label'] = label_lfr\n\n\nfeatures = dataset_transf_panel19_train_lfr.features\nlabel = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary\nweights = dataset_transf_panel19_train_lfr.instance_weights\nfeature_names = dataset_transf_panel19_train_lfr.feature_names\ndf_lfr = pd.DataFrame(features, columns=feature_names)\ndf_lfr['label'] = label\ndf_lfr['weights'] = weights\n\n\ndf_lfr\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\nweights\n\n\n\n\n0\n0.364698\n0.611075\n0.319138\n0.516439\n0.683071\n0.513830\n0.556535\n0.363196\n0.402933\n0.589060\n...\n0.238986\n0.553776\n0.493602\n0.513294\n0.386629\n0.380919\n0.498550\n0.363095\n0.0\n21854.981705\n\n\n1\n0.379547\n0.597793\n0.328517\n0.502386\n0.682940\n0.523566\n0.577716\n0.373460\n0.395062\n0.600292\n...\n0.237634\n0.551191\n0.504359\n0.533097\n0.374361\n0.356593\n0.499183\n0.369585\n0.0\n18169.604822\n\n\n2\n0.345017\n0.593873\n0.327056\n0.509281\n0.682566\n0.524971\n0.551491\n0.349275\n0.416220\n0.575195\n...\n0.246142\n0.570321\n0.511401\n0.519411\n0.420631\n0.397081\n0.509235\n0.382303\n0.0\n17191.832515\n\n\n3\n0.370890\n0.582912\n0.338159\n0.494210\n0.676912\n0.535713\n0.577500\n0.369280\n0.388361\n0.583862\n...\n0.233116\n0.577892\n0.512044\n0.551296\n0.382591\n0.373165\n0.506936\n0.374916\n0.0\n20261.485463\n\n\n4\n0.377148\n0.587833\n0.331081\n0.493269\n0.679347\n0.528798\n0.582084\n0.376288\n0.383771\n0.602294\n...\n0.238654\n0.558482\n0.514984\n0.543083\n0.382213\n0.364254\n0.495283\n0.377611\n0.0\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n0.343260\n0.597189\n0.324804\n0.512213\n0.683004\n0.522468\n0.548023\n0.347933\n0.417919\n0.574523\n...\n0.246629\n0.568838\n0.508985\n0.514854\n0.421901\n0.399881\n0.508553\n0.380749\n0.0\n4111.315754\n\n\n15826\n0.343186\n0.596636\n0.325113\n0.511802\n0.682861\n0.522864\n0.548279\n0.347965\n0.417634\n0.574357\n...\n0.246577\n0.569381\n0.509405\n0.515495\n0.422105\n0.400035\n0.508671\n0.381059\n0.0\n5415.228173\n\n\n15827\n0.364434\n0.584266\n0.332311\n0.494401\n0.677981\n0.531335\n0.573192\n0.367677\n0.389164\n0.590592\n...\n0.240138\n0.570064\n0.517921\n0.541949\n0.397951\n0.380818\n0.500083\n0.381961\n0.0\n3896.116219\n\n\n15828\n0.364365\n0.591464\n0.330698\n0.502169\n0.683230\n0.527371\n0.569395\n0.362334\n0.406439\n0.589670\n...\n0.242613\n0.560312\n0.511955\n0.530772\n0.397606\n0.370904\n0.505687\n0.379358\n0.0\n4883.851005\n\n\n15829\n0.369746\n0.591171\n0.330650\n0.499689\n0.681767\n0.527452\n0.573792\n0.367704\n0.397916\n0.593893\n...\n0.240415\n0.559683\n0.511546\n0.535024\n0.389960\n0.368115\n0.501997\n0.377222\n0.0\n6630.588948\n\n\n\n\n15830 rows × 140 columns\n\n\n\n\n# Assuming `dataset_transf_panel19_train_di` is your DI transformed dataset\nfeatures_di = dataset_transf_panel19_train_di.features\nlabel_di = dataset_transf_panel19_train_di.labels.ravel()  # Flatten the label array if necessary\nfeature_names_di = dataset_transf_panel19_train_di.feature_names\n\n# Create a DataFrame\ndf_di = pd.DataFrame(features_di, columns=feature_names_di)\ndf_di['label'] = label_di\ndf_di\n\n\n\n\n\n\n\n\nAGE\nRACE\nPCS42\nMCS42\nK6SUM42\nREGION=1\nREGION=2\nREGION=3\nREGION=4\nSEX=1\n...\nEMPST=4\nPOVCAT=1\nPOVCAT=2\nPOVCAT=3\nPOVCAT=4\nPOVCAT=5\nINSCOV=1\nINSCOV=2\nINSCOV=3\nlabel\n\n\n\n\n0\n53.0\n1.0\n25.93\n58.47\n3.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n56.0\n1.0\n20.42\n26.53\n17.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n23.0\n1.0\n52.92\n50.28\n7.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n3.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4\n27.0\n0.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15825\n25.0\n0.0\n56.68\n62.11\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15826\n25.0\n0.0\n56.68\n62.11\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n15827\n2.0\n1.0\n-1.00\n-1.00\n-1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15828\n54.0\n0.0\n43.43\n42.45\n24.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n15829\n73.0\n0.0\n41.83\n43.46\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n15830 rows × 139 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\ndf=dataset_orig_panel19_train_df\n# Split the DataFrame into features, labels, and weights\n\nX = df.drop(['label'], axis=1)\ny = df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model.predict_proba(X_train)\ny_test_pred_proba = model.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8669456727732154, Training Log Loss: 0.3230202782236161\nTesting Accuracy: 0.8619709412507897, Testing Log Loss: 0.335720614452724\n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/1117411220.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\n\n\n\ndf.head(1).to_csv('meps.csv')\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n\n# Split the DataFrame into features, labels, and weights\nX = df_rw.drop(['label', 'weights'], axis=1)\ny = df_rw['label']\nweights = df_rw['weights']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel_rw = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel_rw.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_rw.predict(X_train)\ny_test_pred = model_rw.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_rw.predict_proba(X_train)\ny_test_pred_proba = model_rw.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8651295009475679, Training Log Loss: 0.33949214668723315\nTesting Accuracy: 0.8562855337965888, Testing Log Loss: 0.3484870792605314\n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/2225796879.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n\n# Split the DataFrame into features, labels, and weights\nX = df_di.drop(['label'], axis=1)\ny = df_di['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel_di = LogisticRegression()\n\n# Train the model using the sample_weight parameter\n# We need to extract the corresponding weights for the training samples\ntrain_indices = X_train.index\ntrain_weights = weights[train_indices]\n\nmodel_di.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_di.predict(X_train)\ny_test_pred = model_di.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_di.predict_proba(X_train)\ny_test_pred_proba = model_di.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\nTraining Accuracy: 0.8646557169930512, Training Log Loss: 0.33943553174854924\nTesting Accuracy: 0.8550221099178774, Testing Log Loss: 0.3481697691277553\n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/789110131.py:62: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  plt.show()\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Assuming 'df_lfr' is your Pandas DataFrame with features and 'label' columns, \n# resulting from the LFR transformation\n\n# Split the DataFrame into features and labels\nX = df_lfr.drop(['label'], axis=1)\ny = df_lfr['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Initialize the model\nmodel_lfr = LogisticRegression()\n\n# Train the model (no sample weights are needed as the data is already transformed by LFR)\nmodel_lfr.fit(X_train, y_train, sample_weight=train_weights)\n\n# Predictions\ny_train_pred = model_lfr.predict(X_train)\ny_test_pred = model_lfr.predict(X_test)\n\n# Probability predictions for log loss calculation\ny_train_pred_proba = model_lfr.predict_proba(X_train)\ny_test_pred_proba = model_lfr.predict_proba(X_test)\n\n# Calculate accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Calculate log loss\ntrain_loss = log_loss(y_train, y_train_pred_proba)\ntest_loss = log_loss(y_test, y_test_pred_proba)\n\n# Print errors and losses\nprint(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\nprint(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.ylabel('Accuracy')\nplt.title('Train vs Test Accuracy')\n\n# Log Loss plot\nplt.subplot(1, 2, 2)\nplt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\nplt.ylabel('Log Loss')\nplt.title('Train vs Test Log Loss')\n\nplt.tight_layout()\nplt.show()\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[50], line 21\n     18 model_lfr = LogisticRegression()\n     20 # Train the model (no sample weights are needed as the data is already transformed by LFR)\n---&gt; 21 model_lfr.fit(X_train, y_train, sample_weight=train_weights)\n     23 # Predictions\n     24 y_train_pred = model_lfr.predict(X_train)\n\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1467     estimator._validate_params()\n   1469 with config_context(\n   1470     skip_parameter_validation=(\n   1471         prefer_skip_nested_validation or global_skip_validation\n   1472     )\n   1473 ):\n-&gt; 1474     return fit_method(estimator, *args, **kwargs)\n\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1246, in LogisticRegression.fit(self, X, y, sample_weight)\n   1244 classes_ = self.classes_\n   1245 if n_classes &lt; 2:\n-&gt; 1246     raise ValueError(\n   1247         \"This solver needs samples of at least 2 classes\"\n   1248         \" in the data, but the data contains only one\"\n   1249         \" class: %r\"\n   1250         % classes_[0]\n   1251     )\n   1253 if len(self.classes_) == 2:\n   1254     n_classes = 1\n\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0"
  }
]