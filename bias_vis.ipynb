{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import numpy as np\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "from aif360.datasets import GermanDataset\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from aif360.algorithms.preprocessing import OptimPreproc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19 = MEPSDataset19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19_train = MEPSDataset19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_orig_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Prepare data\n",
    "X = dataset_orig_panel19_train.features\n",
    "y = dataset_orig_panel19_train.labels.ravel()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities on the same scaled training data\n",
    "train_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Calculation of discrimination index without modifying dataset structure\n",
    "sens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "def calculate_discrimination(X, probabilities, sens_attr_index, unprivileged_val, privileged_val):\n",
    "    # Filter by sensitive attribute for unprivileged and privileged groups\n",
    "    unpriv_indices = X[:, sens_attr_index] == unprivileged_val\n",
    "    priv_indices = X[:, sens_attr_index] == privileged_val\n",
    "    \n",
    "    # Calculate mean probabilities for both groups\n",
    "    mean_prob_unpriv = probabilities[unpriv_indices].mean()\n",
    "    mean_prob_priv = probabilities[priv_indices].mean()\n",
    "    \n",
    "    # Discrimination index\n",
    "    discrimination = mean_prob_priv - mean_prob_unpriv\n",
    "    return discrimination\n",
    "\n",
    "# Define unprivileged and privileged values\n",
    "unprivileged_val = 0.0\n",
    "privileged_val = 1.0\n",
    "\n",
    "# Compute discrimination\n",
    "discrimination_index = calculate_discrimination(X_train, train_probabilities, sens_attr_index, unprivileged_val, privileged_val)\n",
    "print(\"Discrimination Index: {:.4f}\".format(discrimination_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def custom_loss(lambda_val=0.001):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Extract predictions and sensitive attributes from y_pred\n",
    "        predictions = y_pred[:, 0]\n",
    "        sensitive_attr = y_pred[:, 1]\n",
    "\n",
    "        # Debug prints to check outputs\n",
    "        # tf.print(\"Predictions sample:\", predictions[:10])\n",
    "        # tf.print(\"Sensitive Attr sample:\", sensitive_attr[:10])\n",
    "\n",
    "        # Standard binary crossentropy loss\n",
    "        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n",
    "        \n",
    "        # Determine thresholds to convert sensitive attributes to binary\n",
    "        # Assuming the negative and positive classes are split around zero\n",
    "        threshold = 0\n",
    "        mask_unpriv = K.cast(sensitive_attr <= threshold, 'float32')\n",
    "        mask_priv = K.cast(sensitive_attr > threshold, 'float32')\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        sum_unpriv = K.sum(mask_unpriv)\n",
    "        sum_priv = K.sum(mask_priv)\n",
    "\n",
    "        # # Debug prints for mask sums\n",
    "        # tf.print(\"Sum unprivileged:\", sum_unpriv)\n",
    "        # tf.print(\"Sum privileged:\", sum_priv)\n",
    "\n",
    "        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n",
    "        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n",
    "\n",
    "        # Discrimination as the squared difference in probabilities\n",
    "        discrimination = K.square(prob_priv - prob_unpriv)\n",
    "\n",
    "        # # Debug print\n",
    "        # tf.print(\"Discrimination:\", discrimination)\n",
    "\n",
    "        # Total loss with discrimination penalty\n",
    "        return standard_loss + lambda_val * discrimination\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features\n",
    "sensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "# Input layers\n",
    "inputs = Input(shape=(input_size,))\n",
    "sensitive_inputs = Input(shape=(1,))\n",
    "\n",
    "# Network architecture\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "combined_outputs = tf.keras.layers.concatenate([outputs, sensitive_inputs])\n",
    "\n",
    "model = Model(inputs=[inputs, sensitive_inputs], outputs=combined_outputs)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=custom_loss(lambda_val = 0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Prepare data with sensitive attribute\n",
    "X_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n",
    "\n",
    "# Train the model\n",
    "history1_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features\n",
    "sensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "# Input layers\n",
    "inputs = Input(shape=(input_size,))\n",
    "sensitive_inputs = Input(shape=(1,))\n",
    "\n",
    "# Network architecture\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# No need to concatenate outputs and sensitive inputs for the loss calculation\n",
    "# Separate outputs for predictions and sensitive attributes\n",
    "# Since we're using binary_crossentropy, we only need the main outputs\n",
    "model = Model(inputs=[inputs, sensitive_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model with binary crossentropy\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Prepare data with sensitive attribute\n",
    "X_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n",
    "\n",
    "# Train the model\n",
    "history2_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
