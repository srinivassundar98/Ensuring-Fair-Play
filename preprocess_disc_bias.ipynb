{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import numpy as np\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing,DisparateImpactRemover\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from aif360.algorithms.preprocessing import OptimPreproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19 = MEPSDataset19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_panel19_train = MEPSDataset19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53.  ,  1.  , 25.93, ...,  0.  ,  1.  ,  0.  ],\n",
       "       [56.  ,  1.  , 20.42, ...,  0.  ,  1.  ,  0.  ],\n",
       "       [23.  ,  1.  , 53.12, ...,  0.  ,  1.  ,  0.  ],\n",
       "       ...,\n",
       "       [ 2.  ,  1.  , -1.  , ...,  0.  ,  1.  ,  0.  ],\n",
       "       [54.  ,  0.  , 43.97, ...,  0.  ,  1.  ,  0.  ],\n",
       "       [73.  ,  0.  , 42.68, ...,  0.  ,  1.  ,  0.  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig_panel19_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RACE'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RACE': 1.0}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RACE': 0.0}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprivileged_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                                    \\\n",
       "                                         protected attribute                 \n",
       "                                     AGE                RACE  PCS42  MCS42   \n",
       "instance names                                                               \n",
       "0                  21854.981705     53.0                 1.0  25.93  58.47   \n",
       "1                  18169.604822     56.0                 1.0  20.42  26.57   \n",
       "3                  17191.832515     23.0                 1.0  53.12  50.33   \n",
       "4                  20261.485463      3.0                 1.0  -1.00  -1.00   \n",
       "5                      0.000000     27.0                 0.0  -1.00  -1.00   \n",
       "...                         ...      ...                 ...    ...    ...   \n",
       "16573               4111.315754     25.0                 0.0  56.71  62.39   \n",
       "16574               5415.228173     25.0                 0.0  56.71  62.39   \n",
       "16575               3896.116219      2.0                 1.0  -1.00  -1.00   \n",
       "16576               4883.851005     54.0                 0.0  43.97  42.45   \n",
       "16577               6630.588948     73.0                 0.0  42.68  43.46   \n",
       "\n",
       "                                                            ...          \\\n",
       "                                                            ...           \n",
       "               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \n",
       "instance names                                              ...           \n",
       "0                  3.0      0.0      1.0      0.0      0.0  ...     1.0   \n",
       "1                 17.0      0.0      1.0      0.0      0.0  ...     1.0   \n",
       "3                  7.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "4                 -1.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "5                 -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "...                ...      ...      ...      ...      ...  ...     ...   \n",
       "16573              0.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "16574              0.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "16575             -1.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "16576             24.0      1.0      0.0      0.0      0.0  ...     0.0   \n",
       "16577              0.0      1.0      0.0      0.0      0.0  ...     1.0   \n",
       "\n",
       "                                                                               \\\n",
       "                                                                                \n",
       "               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \n",
       "instance names                                                                  \n",
       "0                   1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "1                   0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "3                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n",
       "4                   0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n",
       "5                   0.0      0.0      1.0      0.0      0.0      1.0      0.0   \n",
       "...                 ...      ...      ...      ...      ...      ...      ...   \n",
       "16573               1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "16574               1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "16575               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "16576               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "16577               0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "                        labels  \n",
       "                                \n",
       "               INSCOV=3         \n",
       "instance names                  \n",
       "0                   0.0    1.0  \n",
       "1                   0.0    1.0  \n",
       "3                   0.0    0.0  \n",
       "4                   0.0    0.0  \n",
       "5                   0.0    0.0  \n",
       "...                 ...    ...  \n",
       "16573               0.0    0.0  \n",
       "16574               0.0    0.0  \n",
       "16575               0.0    0.0  \n",
       "16576               0.0    0.0  \n",
       "16577               0.0    0.0  \n",
       "\n",
       "[15830 rows x 140 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig_panel19_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_orig_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name=['Mean Difference','Consistency','Statistical Parity Difference','Disparate Impact']\n",
    "test_definitions=['difference between mean values of two labels','Individual fairness metric that measures how similar the labels are for similar instances.','Difference in selection rates.','ratio of positive outcomes in the unprivileged group divided by the ratio of positive outcomes in the privileged group.']\n",
    "test_results=[explainer_orig_panel19_train.mean_difference(),explainer_orig_panel19_train.consistency(),explainer_orig_panel19_train.statistical_parity_difference(),explainer_orig_panel19_train.disparate_impact()]\n",
    "test_status=['Bias Detected','Bias Not Detected','Bias Detected','Bias Detected']\n",
    "df=pd.DataFrame({'Test Name':test_name,'Test Definitions':test_definitions,'Test Results':test_results,'Test Status':test_status})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n",
       " 'Consistency (Zemel, et al. 2013): [0.83665193]',\n",
       " 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n",
       " 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\n",
    "dataset_transf_panel19_train_rw = RW.fit_transform(dataset_orig_panel19_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.13507447726478142\n",
      "Disparate Impact (DI): 0.49826823461176517\n"
     ]
    }
   ],
   "source": [
    "# Calculate Statistical Parity Difference\n",
    "statistical_parity_difference = metric_orig_panel19_train.statistical_parity_difference()\n",
    "print(\"Statistical Parity Difference (SPD):\", statistical_parity_difference)\n",
    "\n",
    "# Calculate Disparate Impact\n",
    "disparate_impact = metric_orig_panel19_train.disparate_impact()\n",
    "print(\"Disparate Impact (DI):\", disparate_impact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37.  ,  0.  , 48.09, 36.94,  7.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "        1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,\n",
       "        0.  ,  0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n",
       "        1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "        0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  1.  ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrimination Index: 0.1315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "X = dataset_orig_panel19_train.features\n",
    "y = dataset_orig_panel19_train.labels.ravel()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities on the same scaled training data\n",
    "train_probabilities = model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Calculation of discrimination index without modifying dataset structure\n",
    "sens_attr_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "def calculate_discrimination(X, probabilities, sens_attr_index, unprivileged_val, privileged_val):\n",
    "    # Filter by sensitive attribute for unprivileged and privileged groups\n",
    "    unpriv_indices = X[:, sens_attr_index] == unprivileged_val\n",
    "    priv_indices = X[:, sens_attr_index] == privileged_val\n",
    "    \n",
    "    # Calculate mean probabilities for both groups\n",
    "    mean_prob_unpriv = probabilities[unpriv_indices].mean()\n",
    "    mean_prob_priv = probabilities[priv_indices].mean()\n",
    "    # Discrimination index\n",
    "    discrimination = mean_prob_priv - mean_prob_unpriv\n",
    "    return discrimination\n",
    "\n",
    "# Define unprivileged and privileged values\n",
    "unprivileged_val = 0.0\n",
    "privileged_val = 1.0\n",
    "\n",
    "# Compute discrimination\n",
    "discrimination_index = calculate_discrimination(X_train, train_probabilities, sens_attr_index, unprivileged_val, privileged_val)\n",
    "print(\"Discrimination Index: {:.4f}\".format(discrimination_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING custom_loss FUNCTION with Standard Lambda = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76152525, -0.7401269 , -1.16660499, ..., -1.06074916,\n",
       "        -0.744093  ,  2.78531021],\n",
       "       [ 0.08369436, -0.7401269 ,  0.72413337, ..., -1.06074916,\n",
       "        -0.744093  ,  2.78531021],\n",
       "       [-0.45012855, -0.7401269 ,  0.94174738, ...,  0.94272994,\n",
       "        -0.744093  , -0.35902644],\n",
       "       ...,\n",
       "       [ 0.79545824,  1.35111965,  0.09517111, ..., -1.06074916,\n",
       "         1.34391803, -0.35902644],\n",
       "       [-0.09424661, -0.7401269 ,  1.09658071, ...,  0.94272994,\n",
       "        -0.744093  , -0.35902644],\n",
       "       [-0.62806952, -0.7401269 ,  0.4680036 , ...,  0.94272994,\n",
       "        -0.744093  , -0.35902644]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6436237692832947\n",
      "Epoch 2, Loss: 0.6127090454101562\n",
      "Epoch 3, Loss: 0.5840549468994141\n",
      "Epoch 4, Loss: 0.557695209980011\n",
      "Epoch 5, Loss: 0.5335997343063354\n",
      "Epoch 6, Loss: 0.5117872953414917\n",
      "Epoch 7, Loss: 0.49214401841163635\n",
      "Epoch 8, Loss: 0.47453635931015015\n",
      "Epoch 9, Loss: 0.45879927277565\n",
      "Epoch 10, Loss: 0.44474470615386963\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Adjust input size to exclude sensitive attribute\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(output, target, sensitive_features, lambda_val=0.1, k=6):\n",
    "    criterion = nn.BCELoss()\n",
    "    standard_loss = criterion(output, target)\n",
    "\n",
    "    mask_unpriv = (sensitive_features == 0)\n",
    "    mask_priv = (sensitive_features == 1)\n",
    "    \n",
    "    if torch.sum(mask_unpriv) > 0 and torch.sum(mask_priv) > 0:\n",
    "        prob_unpriv = torch.mean(output[mask_unpriv])\n",
    "        prob_priv = torch.mean(output[mask_priv])\n",
    "        discrimination = (prob_priv - prob_unpriv) ** k\n",
    "    else:\n",
    "        discrimination = torch.tensor(0.0)  # Handle cases where one group might be missing\n",
    "\n",
    "    return standard_loss + lambda_val * discrimination\n",
    "\n",
    "# Assuming your dataset is loaded correctly\n",
    "data = torch.tensor(X_train_scaled).float()  # Make sure this conversion is done correctly\n",
    "targets = torch.tensor(y_train).float().unsqueeze(1)  # Ensure targets are correctly shaped\n",
    "sensitive_features = data[:, 1]  # Extract the sensitive features\n",
    "features = torch.cat((data[:, :1], data[:, 2:]), dim=1)  # Exclude the sensitive attribute from the main features\n",
    "\n",
    "model = BinaryClassifier(features.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = custom_loss(outputs, targets.squeeze(), sensitive_features)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7673807740211487, Train Acc: 24.21%, Test Loss: 0.7409172654151917, Test Acc: 31.46%\n",
      "Epoch 2, Train Loss: 0.7404283881187439, Train Acc: 32.01%, Test Loss: 0.716571569442749, Test Acc: 41.63%\n",
      "Epoch 3, Train Loss: 0.7157283425331116, Train Acc: 41.29%, Test Loss: 0.6942327618598938, Test Acc: 53.85%\n",
      "Epoch 4, Train Loss: 0.693112313747406, Train Acc: 53.62%, Test Loss: 0.6738004684448242, Test Acc: 67.66%\n",
      "Epoch 5, Train Loss: 0.6722973585128784, Train Acc: 68.04%, Test Loss: 0.6548610329627991, Test Acc: 74.76%\n",
      "Epoch 6, Train Loss: 0.6529971957206726, Train Acc: 75.09%, Test Loss: 0.637105405330658, Test Acc: 77.51%\n",
      "Epoch 7, Train Loss: 0.6348973512649536, Train Acc: 78.02%, Test Loss: 0.6203395128250122, Test Acc: 79.41%\n",
      "Epoch 8, Train Loss: 0.6178111433982849, Train Acc: 79.60%, Test Loss: 0.6044129729270935, Test Acc: 80.51%\n",
      "Epoch 9, Train Loss: 0.6016006469726562, Train Acc: 80.84%, Test Loss: 0.589247465133667, Test Acc: 81.30%\n",
      "Epoch 10, Train Loss: 0.5861577391624451, Train Acc: 81.67%, Test Loss: 0.5747091770172119, Test Acc: 81.87%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(output, target, sensitive_features, lambda_val=0.01, k=2):\n",
    "    criterion = nn.BCELoss()\n",
    "    standard_loss = criterion(output, target)\n",
    "\n",
    "    mask_unpriv = (sensitive_features == 0)\n",
    "    mask_priv = (sensitive_features == 1)\n",
    "    \n",
    "    if torch.sum(mask_unpriv) > 0 and torch.sum(mask_priv) > 0:\n",
    "        prob_unpriv = torch.mean(output[mask_unpriv])\n",
    "        prob_priv = torch.mean(output[mask_priv])\n",
    "        discrimination = (prob_priv - prob_unpriv) ** k\n",
    "    else:\n",
    "        discrimination = torch.tensor(0.0)  # Handle cases where one group might be missing\n",
    "\n",
    "    return standard_loss + lambda_val * discrimination\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    predicted_classes = (predictions >= 0.5).float()\n",
    "    return (predicted_classes == targets).float().mean()\n",
    "\n",
    "# Data preparation\n",
    "data = torch.tensor(X_train_scaled).float()\n",
    "targets = torch.tensor(y_train).float().unsqueeze(1)\n",
    "sensitive_features = data[:, 1]\n",
    "features = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n",
    "\n",
    "# Test data (assuming it's prepared similarly)\n",
    "test_data = torch.tensor(X_test_scaled).float()\n",
    "test_targets = torch.tensor(y_test).float().unsqueeze(1)\n",
    "test_sensitive_features = test_data[:, 1]\n",
    "test_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n",
    "\n",
    "model = BinaryClassifier(features.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = custom_loss(outputs, targets.squeeze(), sensitive_features)\n",
    "    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_features)\n",
    "        test_loss = custom_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)\n",
    "        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, '\n",
    "          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%')\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7674269676208496, Train Acc: 34.36%, Test Loss: 0.7305501103401184, Test Acc: 39.80%\n",
      "Epoch 2, Train Loss: 0.7305780053138733, Train Acc: 40.48%, Test Loss: 0.6975762248039246, Test Acc: 49.31%\n",
      "Epoch 3, Train Loss: 0.6973879337310791, Train Acc: 49.25%, Test Loss: 0.668036162853241, Test Acc: 66.20%\n",
      "Epoch 4, Train Loss: 0.6675889492034912, Train Acc: 66.80%, Test Loss: 0.6415244936943054, Test Acc: 75.43%\n",
      "Epoch 5, Train Loss: 0.6408513188362122, Train Acc: 75.71%, Test Loss: 0.61774742603302, Test Acc: 78.68%\n",
      "Epoch 6, Train Loss: 0.6167588233947754, Train Acc: 78.40%, Test Loss: 0.5962265729904175, Test Acc: 80.39%\n",
      "Epoch 7, Train Loss: 0.5949429273605347, Train Acc: 80.14%, Test Loss: 0.5766510367393494, Test Acc: 81.40%\n",
      "Epoch 8, Train Loss: 0.5751211643218994, Train Acc: 81.06%, Test Loss: 0.5587018132209778, Test Acc: 81.84%\n",
      "Epoch 9, Train Loss: 0.5569527745246887, Train Acc: 81.71%, Test Loss: 0.5421833395957947, Test Acc: 82.25%\n",
      "Epoch 10, Train Loss: 0.5401964783668518, Train Acc: 82.35%, Test Loss: 0.5268954634666443, Test Acc: 82.72%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Initialize binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    predicted_classes = (predictions >= 0.5).float()\n",
    "    return (predicted_classes == targets).float().mean()\n",
    "\n",
    "# Data preparation\n",
    "data = torch.tensor(X_train_scaled).float()\n",
    "targets = torch.tensor(y_train).float().unsqueeze(1)\n",
    "sensitive_features = data[:, 1]\n",
    "features = torch.cat((data[:, :1], data[:, 2:]), dim=1)\n",
    "\n",
    "# Test data (assuming it's prepared similarly)\n",
    "test_data = torch.tensor(X_test_scaled).float()\n",
    "test_targets = torch.tensor(y_test).float().unsqueeze(1)\n",
    "test_sensitive_features = test_data[:, 1]\n",
    "test_features = torch.cat((test_data[:, :1], test_data[:, 2:]), dim=1)\n",
    "\n",
    "model = BinaryClassifier(features.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, targets.squeeze())\n",
    "    train_accuracy = calculate_accuracy(outputs, targets.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_features)\n",
    "        test_loss = criterion(test_outputs, test_targets.squeeze())\n",
    "        test_accuracy = calculate_accuracy(test_outputs, test_targets.squeeze())\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Train Acc: {train_accuracy.item()*100:.2f}%, '\n",
    "          f'Test Loss: {test_loss.item()}, Test Acc: {test_accuracy.item()*100:.2f}%')\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Helper function to calculate accuracy\n",
    "def binary_accuracy(y_pred, y_true):\n",
    "    # Applying threshold to get binary output\n",
    "    y_pred_tag = torch.round(y_pred)\n",
    "    correct_results_sum = (y_pred_tag == y_true).sum().float()\n",
    "    acc = correct_results_sum / y_true.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "# Custom discrimination loss function\n",
    "def discrimination_loss(outputs, targets, sensitive_features, lambda_val=0.5, k=2):\n",
    "    criterion = nn.BCELoss()\n",
    "    standard_loss = criterion(outputs, targets)\n",
    "\n",
    "    mask_unpriv = (sensitive_features == 0)\n",
    "    mask_priv = (sensitive_features == 1)\n",
    "\n",
    "    prob_unpriv = torch.mean(outputs[mask_unpriv])\n",
    "    prob_priv = torch.mean(outputs[mask_priv])\n",
    "    discrimination = (prob_priv - prob_unpriv) ** k\n",
    "    return standard_loss + lambda_val * discrimination\n",
    "\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Define a simple binary crossentropy loss model for comparison\n",
    "class SimpleBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleBinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Prepare datasets\n",
    "dataset = TensorDataset(features, targets, sensitive_features)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = BinaryClassifier(features.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, optimizer, train_loader, val_loader, custom_loss=None):\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for inputs, labels, sens in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if custom_loss:\n",
    "                loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n",
    "            else:\n",
    "                loss = nn.BCELoss()(outputs, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += binary_accuracy(outputs, labels.squeeze()).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, sens in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                if custom_loss:\n",
    "                    loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n",
    "                else:\n",
    "                    loss = nn.BCELoss()(outputs, labels.squeeze())\n",
    "                val_loss += loss.item()\n",
    "                val_acc += binary_accuracy(outputs, labels.squeeze()).item()\n",
    "\n",
    "        # Average loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}')\n",
    "\n",
    "# Train the model with discrimination loss\n",
    "\n",
    "# Train another model with only binary crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: nan, Train Acc: 85.77, Val Loss: nan, Val Acc: 85.97\n",
      "Epoch 2, Train Loss: nan, Train Acc: 86.90, Val Loss: nan, Val Acc: 86.05\n",
      "Epoch 3, Train Loss: nan, Train Acc: 87.13, Val Loss: nan, Val Acc: 86.30\n",
      "Epoch 4, Train Loss: nan, Train Acc: 87.40, Val Loss: nan, Val Acc: 86.24\n",
      "Epoch 5, Train Loss: nan, Train Acc: 87.93, Val Loss: nan, Val Acc: 86.33\n",
      "Epoch 6, Train Loss: nan, Train Acc: 88.23, Val Loss: nan, Val Acc: 86.01\n",
      "Epoch 7, Train Loss: nan, Train Acc: 88.21, Val Loss: nan, Val Acc: 86.25\n",
      "Epoch 8, Train Loss: nan, Train Acc: 88.37, Val Loss: nan, Val Acc: 86.17\n",
      "Epoch 9, Train Loss: nan, Train Acc: 88.96, Val Loss: nan, Val Acc: 85.75\n",
      "Epoch 10, Train Loss: nan, Train Acc: 88.86, Val Loss: nan, Val Acc: 85.99\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, train_loader, val_loader, custom_loss=discrimination_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.3526, Train Acc: 85.74, Val Loss: 0.3346, Val Acc: 86.34\n",
      "Epoch 2, Train Loss: 0.3199, Train Acc: 86.70, Val Loss: 0.3338, Val Acc: 85.76\n",
      "Epoch 3, Train Loss: 0.3129, Train Acc: 87.31, Val Loss: 0.3343, Val Acc: 86.40\n",
      "Epoch 4, Train Loss: 0.3095, Train Acc: 87.41, Val Loss: 0.3304, Val Acc: 86.11\n",
      "Epoch 5, Train Loss: 0.3047, Train Acc: 87.72, Val Loss: 0.3305, Val Acc: 86.55\n",
      "Epoch 6, Train Loss: 0.2998, Train Acc: 88.05, Val Loss: 0.3306, Val Acc: 86.38\n",
      "Epoch 7, Train Loss: 0.2960, Train Acc: 88.30, Val Loss: 0.3326, Val Acc: 86.10\n",
      "Epoch 8, Train Loss: 0.2913, Train Acc: 88.62, Val Loss: 0.3351, Val Acc: 85.94\n",
      "Epoch 9, Train Loss: 0.2864, Train Acc: 88.95, Val Loss: 0.3380, Val Acc: 86.04\n",
      "Epoch 10, Train Loss: 0.2806, Train Acc: 89.21, Val Loss: 0.3388, Val Acc: 85.90\n"
     ]
    }
   ],
   "source": [
    "simple_model = SimpleBinaryClassifier(features.shape[1])\n",
    "simple_optimizer = optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "train_model(simple_model, simple_optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: nan, Train Acc: 86.01, Val Loss: nan, Val Acc: 86.56\n",
      "Epoch 2, Train Loss: nan, Train Acc: 86.83, Val Loss: nan, Val Acc: 86.97\n",
      "Epoch 3, Train Loss: nan, Train Acc: 87.25, Val Loss: nan, Val Acc: 87.54\n",
      "Epoch 4, Train Loss: nan, Train Acc: 87.22, Val Loss: nan, Val Acc: 87.04\n",
      "Epoch 5, Train Loss: nan, Train Acc: 87.67, Val Loss: nan, Val Acc: 87.53\n",
      "Epoch 6, Train Loss: nan, Train Acc: 87.95, Val Loss: nan, Val Acc: 87.24\n",
      "Epoch 7, Train Loss: nan, Train Acc: 88.06, Val Loss: nan, Val Acc: 87.11\n",
      "Epoch 8, Train Loss: nan, Train Acc: 88.62, Val Loss: nan, Val Acc: 86.70\n",
      "Epoch 9, Train Loss: nan, Train Acc: 88.73, Val Loss: nan, Val Acc: 86.92\n",
      "Epoch 10, Train Loss: nan, Train Acc: 89.14, Val Loss: nan, Val Acc: 86.97\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Helper function to calculate accuracy\n",
    "def binary_accuracy(y_pred, y_true):\n",
    "    # Applying threshold to get binary output\n",
    "    y_pred_tag = torch.round(y_pred)\n",
    "    correct_results_sum = (y_pred_tag == y_true).sum().float()\n",
    "    acc = correct_results_sum / y_true.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "# Custom discrimination loss function\n",
    "def discrimination_loss(outputs, targets, sensitive_features, lambda_val=0.5, k=2):\n",
    "    criterion = nn.BCELoss()\n",
    "    standard_loss = criterion(outputs, targets)\n",
    "\n",
    "    mask_unpriv = (sensitive_features == 0)\n",
    "    mask_priv = (sensitive_features == 1)\n",
    "\n",
    "    prob_unpriv = torch.mean(outputs[mask_unpriv])\n",
    "    prob_priv = torch.mean(outputs[mask_priv])\n",
    "    discrimination = (prob_priv - prob_unpriv) ** k\n",
    "    return standard_loss + lambda_val * discrimination\n",
    "\n",
    "# Define the model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Define a simple binary crossentropy loss model for comparison\n",
    "class SimpleBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleBinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "data = torch.tensor(X_train_scaled).float()\n",
    "targets = torch.tensor(y_train).float().unsqueeze(1)\n",
    "sensitive_features = data[:, 1]  # Extract the sensitive feature\n",
    "features = torch.cat((data[:, :1], data[:, 2:]), dim=1)  # Exclude the sensitive attribute\n",
    "\n",
    "# Setup DataLoader\n",
    "dataset = TensorDataset(features, targets, sensitive_features)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = BinaryClassifier(features.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, optimizer, train_loader, val_loader, custom_loss=None):\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for inputs, labels, sens in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if custom_loss:\n",
    "                loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n",
    "            else:\n",
    "                loss = nn.BCELoss()(outputs, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += binary_accuracy(outputs, labels.squeeze()).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, sens in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                if custom_loss:\n",
    "                    loss = custom_loss(outputs, labels.squeeze(), sens.squeeze())\n",
    "                else:\n",
    "                    loss = nn.BCELoss()(outputs, labels.squeeze())\n",
    "                val_loss += loss.item()\n",
    "                val_acc += binary_accuracy(outputs, labels.squeeze()).item()\n",
    "\n",
    "        # Average loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}')\n",
    "\n",
    "# Train the model with discrimination loss\n",
    "train_model(model, optimizer, train_loader, val_loader, custom_loss=discrimination_loss)\n",
    "\n",
    "# Initialize and train another model with only binary crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleBinaryClassifier(features.shape[1])\n",
    "simple_optimizer = optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "train_model(simple_model, simple_optimizer, train_loader, val_loader)  # This time without custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_19508\\3984959856.py\", line 26, in custom_loss  *\n        y_pred, discrimination = y_pred_and_discrimination\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Model compilation and training\u001b[39;00m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: custom_loss(y_true, y_pred, \u001b[38;5;241m0.5\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\srinivas\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"C:\\Users\\srinivas\\AppData\\Local\\Temp\\ipykernel_19508\\3984959856.py\", line 26, in custom_loss  *\n        y_pred, discrimination = y_pred_and_discrimination\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class DiscriminationLayer(Layer):\n",
    "    def __init__(self, sensitive_index, k=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sensitive_index = sensitive_index\n",
    "        self.k = k\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y_pred, features = inputs\n",
    "        sensitive_attr = features[:, self.sensitive_index]\n",
    "        mask_unpriv = tf.cast(tf.equal(sensitive_attr, 0), dtype=tf.float32)\n",
    "        mask_priv = tf.cast(tf.equal(sensitive_attr, 1), dtype=tf.float32)\n",
    "        epsilon = 1e-8\n",
    "        prob_unpriv = tf.reduce_sum(y_pred * mask_unpriv) / (tf.reduce_sum(mask_unpriv) + epsilon)\n",
    "        prob_priv = tf.reduce_sum(y_pred * mask_priv) / (tf.reduce_sum(mask_priv) + epsilon)\n",
    "        discrimination = tf.pow((prob_priv - prob_unpriv), self.k)\n",
    "        return [y_pred, discrimination]\n",
    "\n",
    "@tf.function\n",
    "def custom_loss(y_true, y_pred_and_discrimination, lambda_val=0.5):\n",
    "    y_pred, discrimination = y_pred_and_discrimination\n",
    "    standard_loss = BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "    return standard_loss - lambda_val * discrimination\n",
    "\n",
    "# Model building\n",
    "input_features = Input(shape=(input_size,))\n",
    "sensitive_index = 3  # Update this to your sensitive attribute index\n",
    "\n",
    "x = Dense(64, activation='relu')(input_features)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "disc_layer = DiscriminationLayer(sensitive_index=sensitive_index)([predictions, input_features])\n",
    "\n",
    "model = Model(inputs=input_features, outputs=disc_layer)\n",
    "\n",
    "# Model compilation and training\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, 0.5), metrics=['accuracy'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING custom_loss FUNCTION with custom Lambda , here 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Tensor(\"compile_loss/loss/Square:0\", shape=(), dtype=float32)\n",
      "Tensor(\"compile_loss/loss/Square:0\", shape=(), dtype=float32)\n",
      "\u001b[1m299/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8033 - loss: 0.4365Tensor(\"compile_loss/loss/Square:0\", shape=(), dtype=float32)\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8058 - loss: 0.4327 - val_accuracy: 0.8547 - val_loss: 0.3441\n",
      "Epoch 2/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8763 - loss: 0.3119 - val_accuracy: 0.8543 - val_loss: 0.3379\n",
      "Epoch 3/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8794 - loss: 0.2985 - val_accuracy: 0.8587 - val_loss: 0.3372\n",
      "Epoch 4/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8796 - loss: 0.3065 - val_accuracy: 0.8579 - val_loss: 0.3346\n",
      "Epoch 5/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8846 - loss: 0.2883 - val_accuracy: 0.8591 - val_loss: 0.3314\n",
      "Epoch 6/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8845 - loss: 0.2882 - val_accuracy: 0.8563 - val_loss: 0.3369\n",
      "Epoch 7/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8925 - loss: 0.2760 - val_accuracy: 0.8579 - val_loss: 0.3360\n",
      "Epoch 8/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8917 - loss: 0.2739 - val_accuracy: 0.8579 - val_loss: 0.3495\n",
      "Epoch 9/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8951 - loss: 0.2717 - val_accuracy: 0.8512 - val_loss: 0.3459\n",
      "Epoch 10/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9040 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3561\n",
      "Epoch 11/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9010 - loss: 0.2572 - val_accuracy: 0.8539 - val_loss: 0.3470\n",
      "Epoch 12/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9050 - loss: 0.2490 - val_accuracy: 0.8512 - val_loss: 0.3560\n",
      "Epoch 13/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9083 - loss: 0.2481 - val_accuracy: 0.8523 - val_loss: 0.3670\n",
      "Epoch 14/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9054 - loss: 0.2492 - val_accuracy: 0.8539 - val_loss: 0.3552\n",
      "Epoch 15/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9137 - loss: 0.2301 - val_accuracy: 0.8508 - val_loss: 0.3714\n",
      "Epoch 16/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9157 - loss: 0.2330 - val_accuracy: 0.8523 - val_loss: 0.3805\n",
      "Epoch 17/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9144 - loss: 0.2339 - val_accuracy: 0.8500 - val_loss: 0.3780\n",
      "Epoch 18/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9118 - loss: 0.2302 - val_accuracy: 0.8555 - val_loss: 0.3919\n",
      "Epoch 19/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9162 - loss: 0.2299 - val_accuracy: 0.8480 - val_loss: 0.3883\n",
      "Epoch 20/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9226 - loss: 0.2144 - val_accuracy: 0.8460 - val_loss: 0.4096\n",
      "Epoch 21/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9211 - loss: 0.2217 - val_accuracy: 0.8516 - val_loss: 0.4037\n",
      "Epoch 22/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9192 - loss: 0.2170 - val_accuracy: 0.8500 - val_loss: 0.4034\n",
      "Epoch 23/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9270 - loss: 0.2019 - val_accuracy: 0.8413 - val_loss: 0.4156\n",
      "Epoch 24/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9334 - loss: 0.1965 - val_accuracy: 0.8488 - val_loss: 0.4213\n",
      "Epoch 25/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9256 - loss: 0.2058 - val_accuracy: 0.8512 - val_loss: 0.4194\n",
      "Epoch 26/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9230 - loss: 0.2079 - val_accuracy: 0.8437 - val_loss: 0.4259\n",
      "Epoch 27/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9303 - loss: 0.2013 - val_accuracy: 0.8441 - val_loss: 0.4273\n",
      "Epoch 28/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9316 - loss: 0.1936 - val_accuracy: 0.8381 - val_loss: 0.4543\n",
      "Epoch 29/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9313 - loss: 0.2016 - val_accuracy: 0.8468 - val_loss: 0.4473\n",
      "Epoch 30/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9373 - loss: 0.1827 - val_accuracy: 0.8417 - val_loss: 0.4511\n",
      "Epoch 31/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9346 - loss: 0.1876 - val_accuracy: 0.8350 - val_loss: 0.4681\n",
      "Epoch 32/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9367 - loss: 0.1823 - val_accuracy: 0.8441 - val_loss: 0.4578\n",
      "Epoch 33/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9380 - loss: 0.1823 - val_accuracy: 0.8433 - val_loss: 0.4643\n",
      "Epoch 34/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.1730 - val_accuracy: 0.8429 - val_loss: 0.4602\n",
      "Epoch 35/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9407 - loss: 0.1748 - val_accuracy: 0.8397 - val_loss: 0.4727\n",
      "Epoch 36/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9402 - loss: 0.1778 - val_accuracy: 0.8373 - val_loss: 0.4841\n",
      "Epoch 37/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9409 - loss: 0.1663 - val_accuracy: 0.8350 - val_loss: 0.4929\n",
      "Epoch 38/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9420 - loss: 0.1681 - val_accuracy: 0.8421 - val_loss: 0.4988\n",
      "Epoch 39/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9475 - loss: 0.1611 - val_accuracy: 0.8429 - val_loss: 0.4904\n",
      "Epoch 40/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9457 - loss: 0.1633 - val_accuracy: 0.8425 - val_loss: 0.5039\n",
      "Epoch 41/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9436 - loss: 0.1630 - val_accuracy: 0.8413 - val_loss: 0.5223\n",
      "Epoch 42/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9502 - loss: 0.1493 - val_accuracy: 0.8452 - val_loss: 0.5219\n",
      "Epoch 43/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9460 - loss: 0.1563 - val_accuracy: 0.8405 - val_loss: 0.5146\n",
      "Epoch 44/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9419 - loss: 0.1671 - val_accuracy: 0.8373 - val_loss: 0.6083\n",
      "Epoch 45/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9563 - loss: 0.1423 - val_accuracy: 0.8441 - val_loss: 0.5172\n",
      "Epoch 46/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9518 - loss: 0.1563 - val_accuracy: 0.8393 - val_loss: 0.5386\n",
      "Epoch 47/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9515 - loss: 0.1459 - val_accuracy: 0.8401 - val_loss: 0.5497\n",
      "Epoch 48/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9506 - loss: 0.1465 - val_accuracy: 0.8267 - val_loss: 0.5693\n",
      "Epoch 49/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9482 - loss: 0.1613 - val_accuracy: 0.8362 - val_loss: 0.5661\n",
      "Epoch 50/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9562 - loss: 0.1408 - val_accuracy: 0.8377 - val_loss: 0.5786\n",
      "Epoch 51/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9569 - loss: 0.1348 - val_accuracy: 0.8287 - val_loss: 0.5823\n",
      "Epoch 52/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9506 - loss: 0.1394 - val_accuracy: 0.8389 - val_loss: 0.5750\n",
      "Epoch 53/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9569 - loss: 0.1372 - val_accuracy: 0.8362 - val_loss: 0.5819\n",
      "Epoch 54/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9597 - loss: 0.1356 - val_accuracy: 0.8298 - val_loss: 0.5924\n",
      "Epoch 55/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9589 - loss: 0.1328 - val_accuracy: 0.8310 - val_loss: 0.6046\n",
      "Epoch 56/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9569 - loss: 0.1342 - val_accuracy: 0.8200 - val_loss: 0.6179\n",
      "Epoch 57/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9584 - loss: 0.1330 - val_accuracy: 0.8350 - val_loss: 0.6040\n",
      "Epoch 58/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9620 - loss: 0.1266 - val_accuracy: 0.8223 - val_loss: 0.6398\n",
      "Epoch 59/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9587 - loss: 0.1314 - val_accuracy: 0.8322 - val_loss: 0.6161\n",
      "Epoch 60/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9629 - loss: 0.1177 - val_accuracy: 0.8366 - val_loss: 0.6166\n",
      "Epoch 61/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9633 - loss: 0.1183 - val_accuracy: 0.8227 - val_loss: 0.6447\n",
      "Epoch 62/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9637 - loss: 0.1208 - val_accuracy: 0.8326 - val_loss: 0.6244\n",
      "Epoch 63/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9604 - loss: 0.1245 - val_accuracy: 0.8346 - val_loss: 0.6591\n",
      "Epoch 64/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9629 - loss: 0.1260 - val_accuracy: 0.8366 - val_loss: 0.6509\n",
      "Epoch 65/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9611 - loss: 0.1246 - val_accuracy: 0.8373 - val_loss: 0.6708\n",
      "Epoch 66/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9631 - loss: 0.1189 - val_accuracy: 0.8310 - val_loss: 0.6936\n",
      "Epoch 67/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9592 - loss: 0.1239 - val_accuracy: 0.8216 - val_loss: 0.6786\n",
      "Epoch 68/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9643 - loss: 0.1230 - val_accuracy: 0.8326 - val_loss: 0.6815\n",
      "Epoch 69/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9636 - loss: 0.1216 - val_accuracy: 0.8267 - val_loss: 0.6902\n",
      "Epoch 70/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9636 - loss: 0.1147 - val_accuracy: 0.8251 - val_loss: 0.6926\n",
      "Epoch 71/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9649 - loss: 0.1131 - val_accuracy: 0.8342 - val_loss: 0.7052\n",
      "Epoch 72/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9671 - loss: 0.1105 - val_accuracy: 0.8251 - val_loss: 0.6964\n",
      "Epoch 73/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9681 - loss: 0.1057 - val_accuracy: 0.8251 - val_loss: 0.7397\n",
      "Epoch 74/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9627 - loss: 0.1106 - val_accuracy: 0.8227 - val_loss: 0.7392\n",
      "Epoch 75/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9667 - loss: 0.1124 - val_accuracy: 0.8310 - val_loss: 0.7185\n",
      "Epoch 76/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9668 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7144\n",
      "Epoch 77/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9672 - loss: 0.1099 - val_accuracy: 0.8306 - val_loss: 0.7278\n",
      "Epoch 78/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9690 - loss: 0.1136 - val_accuracy: 0.8318 - val_loss: 0.7388\n",
      "Epoch 79/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9696 - loss: 0.1021 - val_accuracy: 0.8247 - val_loss: 0.7466\n",
      "Epoch 80/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9666 - loss: 0.1124 - val_accuracy: 0.8196 - val_loss: 0.7523\n",
      "Epoch 81/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9700 - loss: 0.1022 - val_accuracy: 0.8243 - val_loss: 0.7548\n",
      "Epoch 82/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9684 - loss: 0.1008 - val_accuracy: 0.8255 - val_loss: 0.7539\n",
      "Epoch 83/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9720 - loss: 0.0999 - val_accuracy: 0.8176 - val_loss: 0.7742\n",
      "Epoch 84/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9713 - loss: 0.0964 - val_accuracy: 0.8192 - val_loss: 0.7893\n",
      "Epoch 85/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9693 - loss: 0.0950 - val_accuracy: 0.8239 - val_loss: 0.8019\n",
      "Epoch 86/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9700 - loss: 0.0959 - val_accuracy: 0.8231 - val_loss: 0.8244\n",
      "Epoch 87/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9680 - loss: 0.1088 - val_accuracy: 0.8259 - val_loss: 0.8061\n",
      "Epoch 88/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9694 - loss: 0.0984 - val_accuracy: 0.8231 - val_loss: 0.7985\n",
      "Epoch 89/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9744 - loss: 0.0878 - val_accuracy: 0.8081 - val_loss: 0.8120\n",
      "Epoch 90/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9722 - loss: 0.0955 - val_accuracy: 0.8160 - val_loss: 0.8065\n",
      "Epoch 91/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9715 - loss: 0.0998 - val_accuracy: 0.8291 - val_loss: 0.7710\n",
      "Epoch 92/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9731 - loss: 0.0992 - val_accuracy: 0.8235 - val_loss: 0.8246\n",
      "Epoch 93/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9745 - loss: 0.0861 - val_accuracy: 0.8243 - val_loss: 0.8357\n",
      "Epoch 94/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9698 - loss: 0.0975 - val_accuracy: 0.8279 - val_loss: 0.8380\n",
      "Epoch 95/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9749 - loss: 0.0870 - val_accuracy: 0.8239 - val_loss: 0.8321\n",
      "Epoch 96/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9729 - loss: 0.0938 - val_accuracy: 0.8184 - val_loss: 0.8346\n",
      "Epoch 97/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9736 - loss: 0.0948 - val_accuracy: 0.8247 - val_loss: 0.8299\n",
      "Epoch 98/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9750 - loss: 0.0909 - val_accuracy: 0.8223 - val_loss: 0.8562\n",
      "Epoch 99/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9722 - loss: 0.0915 - val_accuracy: 0.8283 - val_loss: 0.8644\n",
      "Epoch 100/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9696 - loss: 0.0965 - val_accuracy: 0.8231 - val_loss: 0.8665\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def custom_loss(sensitive_attr, lambda_val=0.5):\n",
    "    \"\"\"\n",
    "    Creates a custom loss function that incorporates discrimination penalty.\n",
    "\n",
    "    Args:\n",
    "    sensitive_attr (int): Index of the sensitive attribute in the input features.\n",
    "    lambda_val (float): Regularization strength for the discrimination penalty.\n",
    "\n",
    "    Returns:\n",
    "    loss (function): A loss function that takes (y_true, y_pred).\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # Standard binary crossentropy loss\n",
    "        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "        \n",
    "        # Calculate discrimination index\n",
    "        # We assume sensitive attribute is binary and 0 is unprivileged, 1 is privileged\n",
    "        mask_unpriv = K.cast(K.equal(sensitive_attr, 0), 'float32')\n",
    "        mask_priv = K.cast(K.equal(sensitive_attr, 1), 'float32')\n",
    "        \n",
    "        # Probabilities of positive class\n",
    "        # prob_unpriv = K.mean(y_pred * mask_unpriv) / K.mean(mask_unpriv)\n",
    "        # prob_priv = K.mean(y_pred * mask_priv) / K.mean(mask_priv)\n",
    "        epsilon = 1e-8\n",
    "        prob_unpriv = K.mean(y_pred * mask_unpriv) / (K.mean(mask_unpriv) + epsilon)\n",
    "        prob_priv = K.mean(y_pred * mask_priv) / (K.mean(mask_priv) + epsilon)\n",
    "\n",
    "        # Discrimination as the squared difference in probabilities\n",
    "        discrimination = K.square(prob_priv - prob_unpriv)\n",
    "        print(discrimination)\n",
    "        \n",
    "        # Custom loss calculation\n",
    "        return standard_loss - lambda_val * discrimination\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features\n",
    "sensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "# Define the model architecture\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=custom_loss(sensitive_attr=X_train_scaled[:, sensitive_index], lambda_val=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history1 = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modal with with Standard Loss(binary_crossentropy), No discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8989 - loss: 0.2601 - val_accuracy: 0.8551 - val_loss: 0.3496\n",
      "Epoch 2/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.8954 - loss: 0.2608 - val_accuracy: 0.8531 - val_loss: 0.3531\n",
      "Epoch 3/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - accuracy: 0.9064 - loss: 0.2512 - val_accuracy: 0.8531 - val_loss: 0.3671\n",
      "Epoch 4/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - accuracy: 0.9090 - loss: 0.2452 - val_accuracy: 0.8468 - val_loss: 0.3754\n",
      "Epoch 5/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - accuracy: 0.9121 - loss: 0.2368 - val_accuracy: 0.8484 - val_loss: 0.3660\n",
      "Epoch 6/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - accuracy: 0.9150 - loss: 0.2350 - val_accuracy: 0.8535 - val_loss: 0.3656\n",
      "Epoch 7/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.9145 - loss: 0.2336 - val_accuracy: 0.8468 - val_loss: 0.3764\n",
      "Epoch 8/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - accuracy: 0.9177 - loss: 0.2211 - val_accuracy: 0.8500 - val_loss: 0.3734\n",
      "Epoch 9/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.9162 - loss: 0.2237 - val_accuracy: 0.8543 - val_loss: 0.3917\n",
      "Epoch 10/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - accuracy: 0.9165 - loss: 0.2274 - val_accuracy: 0.8496 - val_loss: 0.3897\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history2 = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Training Accuracy - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.844931423664093,
          0.8705952167510986,
          0.8758266568183899,
          0.8792814016342163,
          0.8821439146995544,
          0.8866844177246094,
          0.888362467288971,
          0.890533983707428,
          0.8928042650222778,
          0.8985292911529541,
          0.8976408839225769,
          0.9010956287384033,
          0.9039581418037415,
          0.90504390001297,
          0.9097818732261658,
          0.9113611578941345,
          0.912743091583252,
          0.9122495055198669,
          0.9147171974182129,
          0.9190602898597717,
          0.9192577004432678,
          0.9198499917984009,
          0.9209357500076294,
          0.9252788424491882,
          0.9251801371574402,
          0.9268581867218018,
          0.92745041847229,
          0.9275491237640381,
          0.9312999844551086,
          0.9338663220405579,
          0.9326818585395813,
          0.9353469610214233,
          0.9357417821884155,
          0.936728835105896,
          0.9369262456893921,
          0.9389004111289978,
          0.9381107687950134,
          0.9397887587547302,
          0.9434409141540527,
          0.9426512718200684,
          0.9444279670715332,
          0.9450202584266663,
          0.9456124901771545,
          0.9449215531349182,
          0.9500542879104614,
          0.9494620561599731,
          0.949165940284729,
          0.9493633508682251,
          0.9503504037857056,
          0.9523245692253113,
          0.9497581720352173,
          0.9519297480583191,
          0.9530155062675476,
          0.9541999697685242,
          0.9567663669586182,
          0.9543973803520203,
          0.9546934962272644,
          0.9574573040008545,
          0.9582469463348389,
          0.9595301747322083,
          0.9595301747322083,
          0.9618003964424133,
          0.9597275853157043,
          0.9609120488166809,
          0.9607146382331848,
          0.9597275853157043,
          0.9607146382331848,
          0.9617016911506653,
          0.9652551412582397,
          0.9626888036727905,
          0.9645642042160034,
          0.9630836248397827,
          0.9640706777572632,
          0.963774561882019,
          0.9664396643638611,
          0.965847373008728,
          0.9668344855308533,
          0.9673280119895935,
          0.9652551412582397,
          0.9673280119895935,
          0.9669331908226013,
          0.9664396643638611,
          0.9696969985961914,
          0.9682163596153259,
          0.96574866771698,
          0.9675254225730896,
          0.9699931144714355,
          0.9693021178245544,
          0.9707827568054199,
          0.9716711044311523,
          0.9712762832641602,
          0.9704866409301758,
          0.970881462097168,
          0.9678215384483337,
          0.9718685150146484,
          0.9724607467651367,
          0.9722633361816406,
          0.973447859287262,
          0.9722633361816406,
          0.9714736938476562
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Validation Accuracy - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.8547177314758301,
          0.8543229103088379,
          0.8586655855178833,
          0.8578760623931885,
          0.8590604066848755,
          0.8562968969345093,
          0.8578760623931885,
          0.8578760623931885,
          0.8511646389961243,
          0.8539281487464905,
          0.8539281487464905,
          0.8511646389961243,
          0.8523489832878113,
          0.8539281487464905,
          0.8507698178291321,
          0.8523489832878113,
          0.8499802350997925,
          0.8555073142051697,
          0.8480063080787659,
          0.8460323810577393,
          0.8515594005584717,
          0.8499802350997925,
          0.8412948846817017,
          0.8487958908081055,
          0.8511646389961243,
          0.8436636328697205,
          0.8440584540367126,
          0.838136613368988,
          0.8468219637870789,
          0.8416897058486938,
          0.8349782824516296,
          0.8440584540367126,
          0.843268871307373,
          0.8428740501403809,
          0.8397157788276672,
          0.8373470306396484,
          0.8349782824516296,
          0.8420844674110413,
          0.8428740501403809,
          0.8424792885780334,
          0.8412948846817017,
          0.8452427983283997,
          0.8405053019523621,
          0.8373470306396484,
          0.8440584540367126,
          0.839320957660675,
          0.8401105403900146,
          0.8266876935958862,
          0.8361626267433167,
          0.8377417922019958,
          0.8286616802215576,
          0.8389261960983276,
          0.8361626267433167,
          0.8298460245132446,
          0.8310304284095764,
          0.8199763298034668,
          0.8349782824516296,
          0.8223450183868408,
          0.8322147727012634,
          0.8365574479103088,
          0.822739839553833,
          0.8326095342636108,
          0.8345835208892822,
          0.8365574479103088,
          0.8373470306396484,
          0.8310304284095764,
          0.821555495262146,
          0.8326095342636108,
          0.8266876935958862,
          0.8251085877418518,
          0.83418869972229,
          0.8251085877418518,
          0.8251085877418518,
          0.822739839553833,
          0.8310304284095764,
          0.8306356072425842,
          0.8306356072425842,
          0.8318199515342712,
          0.8247137665748596,
          0.8195815086364746,
          0.8243190050125122,
          0.8255033493041992,
          0.817607581615448,
          0.8191867470741272,
          0.82392418384552,
          0.8231346011161804,
          0.8258981704711914,
          0.8231346011161804,
          0.8081326484680176,
          0.8160284161567688,
          0.829056441783905,
          0.8235294222831726,
          0.8243190050125122,
          0.827872097492218,
          0.82392418384552,
          0.8183971643447876,
          0.8247137665748596,
          0.8223450183868408,
          0.8282668590545654,
          0.8231346011161804
         ]
        },
        {
         "line": {
          "color": "skyblue"
         },
         "mode": "lines+markers",
         "name": "Training Accuracy - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.9725594520568848,
          0.973447859287262,
          0.9722633361816406,
          0.97354656457901,
          0.9736452698707581,
          0.9762116074562073,
          0.9750271439552307,
          0.9738426804542542,
          0.975619375705719,
          0.9753232598304749,
          0.9750271439552307,
          0.9724607467651367,
          0.9758167862892151,
          0.9748297333717346,
          0.9751258492469788,
          0.9763103127479553,
          0.9765077233314514,
          0.9764090180397034,
          0.9767051339149475,
          0.9758167862892151,
          0.9753232598304749,
          0.9771987199783325,
          0.9764090180397034,
          0.978284478187561,
          0.9793702363967896,
          0.9778896570205688,
          0.9772974252700806,
          0.9776922464370728,
          0.9791728258132935,
          0.9790741205215454,
          0.9793702363967896,
          0.9775935411453247,
          0.978185772895813,
          0.9790741205215454,
          0.9772974252700806,
          0.9791728258132935,
          0.9795676469802856,
          0.978185772895813,
          0.978185772895813,
          0.9764090180397034,
          0.9791728258132935,
          0.978284478187561,
          0.9804560542106628,
          0.9810482859611511,
          0.9813444018363953,
          0.9797650575637817,
          0.9789754152297974,
          0.9797650575637817,
          0.9799624681472778,
          0.9818379282951355,
          0.9810482859611511,
          0.9798637628555298,
          0.9795676469802856,
          0.9801598787307739,
          0.980752170085907,
          0.978284478187561,
          0.9800611734390259,
          0.9818379282951355,
          0.9815418124198914,
          0.9820353388786316,
          0.9804560542106628,
          0.9803573489189148,
          0.9809495806694031,
          0.9787780046463013,
          0.9806534647941589,
          0.9798637628555298,
          0.980752170085907,
          0.9814431071281433,
          0.9824301600456238,
          0.9810482859611511,
          0.9780870676040649,
          0.9811469912528992,
          0.9814431071281433,
          0.9804560542106628,
          0.9801598787307739,
          0.9827262759208679,
          0.9824301600456238,
          0.980258584022522,
          0.9834172129631042,
          0.9833185076713562,
          0.9809495806694031,
          0.9819366335868835,
          0.9838120341300964,
          0.982923686504364,
          0.9790741205215454,
          0.9814431071281433,
          0.9849965572357178,
          0.9827262759208679,
          0.9812456965446472,
          0.9833185076713562,
          0.9815418124198914,
          0.9817392230033875,
          0.9818379282951355,
          0.9831210970878601,
          0.9827262759208679,
          0.9818379282951355,
          0.9823314547538757,
          0.9803573489189148,
          0.9837133288383484,
          0.9837133288383484
         ]
        },
        {
         "line": {
          "color": "pink"
         },
         "mode": "lines+markers",
         "name": "Validation Accuracy - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.8235294222831726,
          0.811290979385376,
          0.8128700852394104,
          0.8195815086364746,
          0.8255033493041992,
          0.8219502568244934,
          0.818791925907135,
          0.8235294222831726,
          0.8258981704711914,
          0.8255033493041992,
          0.8310304284095764,
          0.8199763298034668,
          0.8191867470741272,
          0.8266876935958862,
          0.8172128200531006,
          0.8262929320335388,
          0.822739839553833,
          0.8223450183868408,
          0.8002368807792664,
          0.807343065738678,
          0.8223450183868408,
          0.8168179988861084,
          0.8258981704711914,
          0.8172128200531006,
          0.8211606740951538,
          0.818791925907135,
          0.8219502568244934,
          0.8199763298034668,
          0.8168179988861084,
          0.8223450183868408,
          0.8211606740951538,
          0.8231346011161804,
          0.8235294222831726,
          0.8258981704711914,
          0.8282668590545654,
          0.8120805621147156,
          0.8243190050125122,
          0.8140544891357422,
          0.822739839553833,
          0.817607581615448,
          0.82392418384552,
          0.8183971643447876,
          0.8211606740951538,
          0.8183971643447876,
          0.8255033493041992,
          0.8207659125328064,
          0.8152388334274292,
          0.8097118139266968,
          0.8160284161567688,
          0.8183971643447876,
          0.8235294222831726,
          0.8156336545944214,
          0.8219502568244934,
          0.822739839553833,
          0.8195815086364746,
          0.81365966796875,
          0.81365966796875,
          0.82392418384552,
          0.8199763298034668,
          0.811290979385376,
          0.8160284161567688,
          0.8148440718650818,
          0.807343065738678,
          0.8168179988861084,
          0.8262929320335388,
          0.8219502568244934,
          0.8199763298034668,
          0.8274772763252258,
          0.818791925907135,
          0.8148440718650818,
          0.8152388334274292,
          0.8172128200531006,
          0.817607581615448,
          0.8211606740951538,
          0.8140544891357422,
          0.8168179988861084,
          0.812475323677063,
          0.8030003905296326,
          0.81365966796875,
          0.8258981704711914,
          0.8160284161567688,
          0.8172128200531006,
          0.8180023431777954,
          0.8251085877418518,
          0.8180023431777954,
          0.82392418384552,
          0.817607581615448,
          0.8203710913658142,
          0.8203710913658142,
          0.8140544891357422,
          0.8089222311973572,
          0.8183971643447876,
          0.817607581615448,
          0.8081326484680176,
          0.812475323677063,
          0.8191867470741272,
          0.8294512629508972,
          0.8199763298034668,
          0.822739839553833,
          0.8207659125328064
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Metric Type"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Accuracy"
        },
        "xaxis": {
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Training Loss - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.3730280101299286,
          0.31841549277305603,
          0.30785539746284485,
          0.3006380498409271,
          0.2935963273048401,
          0.28825896978378296,
          0.28253450989723206,
          0.2770962417125702,
          0.2724069356918335,
          0.26683756709098816,
          0.2638258635997772,
          0.25872236490249634,
          0.25369539856910706,
          0.2483646124601364,
          0.24184536933898926,
          0.23836995661258698,
          0.23467066884040833,
          0.23111945390701294,
          0.22880591452121735,
          0.2216365486383438,
          0.22062933444976807,
          0.2166154384613037,
          0.21415309607982635,
          0.2070891410112381,
          0.20715555548667908,
          0.20137542486190796,
          0.201788991689682,
          0.19577285647392273,
          0.19593630731105804,
          0.1902398020029068,
          0.1908109337091446,
          0.1844511330127716,
          0.18369266390800476,
          0.1811671406030655,
          0.17800241708755493,
          0.1756376326084137,
          0.17387844622135162,
          0.17215101420879364,
          0.16862960159778595,
          0.16872096061706543,
          0.16551893949508667,
          0.1598307192325592,
          0.15995921194553375,
          0.16069939732551575,
          0.16107240319252014,
          0.15603066980838776,
          0.15090356767177582,
          0.15219642221927643,
          0.1509147435426712,
          0.1484566032886505,
          0.14543354511260986,
          0.14445582032203674,
          0.14159461855888367,
          0.13997498154640198,
          0.1375800371170044,
          0.13776345551013947,
          0.13823167979717255,
          0.13168898224830627,
          0.13347959518432617,
          0.12815003097057343,
          0.12858635187149048,
          0.12988778948783875,
          0.13076157867908478,
          0.12443265318870544,
          0.12592792510986328,
          0.12501470744609833,
          0.12383727729320526,
          0.12237795442342758,
          0.11614277213811874,
          0.11903081834316254,
          0.11632892489433289,
          0.11642896384000778,
          0.11442980170249939,
          0.11329676955938339,
          0.11226562410593033,
          0.11300978064537048,
          0.10825206339359283,
          0.10947448015213013,
          0.11000173538923264,
          0.10730288922786713,
          0.10806697607040405,
          0.10666906833648682,
          0.1047794371843338,
          0.10263171792030334,
          0.10877548903226852,
          0.10450927168130875,
          0.10242952406406403,
          0.0992858037352562,
          0.09953854233026505,
          0.09926764667034149,
          0.09952782094478607,
          0.10071301460266113,
          0.09490866214036942,
          0.09904690086841583,
          0.0942201316356659,
          0.0936383306980133,
          0.0945807695388794,
          0.09268172085285187,
          0.09102882444858551,
          0.09171217679977417
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Validation Loss - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.3440718650817871,
          0.337867796421051,
          0.3371795117855072,
          0.3346497118473053,
          0.33140820264816284,
          0.3368520140647888,
          0.33604562282562256,
          0.34948626160621643,
          0.3458535373210907,
          0.356126993894577,
          0.3469521701335907,
          0.35603007674217224,
          0.3670363128185272,
          0.3552267849445343,
          0.3713993728160858,
          0.3804858922958374,
          0.3779650926589966,
          0.3919149339199066,
          0.38825780153274536,
          0.409618079662323,
          0.4036833345890045,
          0.4033789038658142,
          0.41556617617607117,
          0.42125052213668823,
          0.41939476132392883,
          0.42593222856521606,
          0.42728304862976074,
          0.4542901813983917,
          0.44728726148605347,
          0.4510708153247833,
          0.46811941266059875,
          0.45782777667045593,
          0.4643261730670929,
          0.4601687788963318,
          0.47268086671829224,
          0.4840947985649109,
          0.49290651082992554,
          0.4988240599632263,
          0.4904173016548157,
          0.5039085149765015,
          0.5222796201705933,
          0.5218754410743713,
          0.5145660042762756,
          0.6082951426506042,
          0.5172246694564819,
          0.538607120513916,
          0.5497127771377563,
          0.5693004131317139,
          0.5661447644233704,
          0.5785986185073853,
          0.5823439359664917,
          0.5749739408493042,
          0.5819164514541626,
          0.5924142599105835,
          0.6045822501182556,
          0.6179047226905823,
          0.6040216088294983,
          0.639769971370697,
          0.6160659193992615,
          0.6165969967842102,
          0.6446908116340637,
          0.6244200468063354,
          0.6590907573699951,
          0.6508577466011047,
          0.6708232760429382,
          0.6936483383178711,
          0.6786296367645264,
          0.6814994812011719,
          0.6901657581329346,
          0.692570149898529,
          0.7051593661308289,
          0.6963772773742676,
          0.739655077457428,
          0.7392184138298035,
          0.718515932559967,
          0.7144261598587036,
          0.7277598977088928,
          0.7388298511505127,
          0.7465894818305969,
          0.7522907853126526,
          0.7548390030860901,
          0.7539017796516418,
          0.7741509079933167,
          0.7892889976501465,
          0.8018888831138611,
          0.8243963122367859,
          0.8061336874961853,
          0.7985100150108337,
          0.812017023563385,
          0.8065258264541626,
          0.7709778547286987,
          0.8245807886123657,
          0.8356582522392273,
          0.8379912376403809,
          0.8320791721343994,
          0.8346257209777832,
          0.8299115896224976,
          0.8561972379684448,
          0.8644313812255859,
          0.8665235042572021
         ]
        },
        {
         "line": {
          "color": "skyblue"
         },
         "mode": "lines+markers",
         "name": "Training Loss - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.09543223679065704,
          0.09182160347700119,
          0.0900220051407814,
          0.08879698812961578,
          0.08986964076757431,
          0.08724035322666168,
          0.08643440902233124,
          0.08802527189254761,
          0.08853942900896072,
          0.08449998497962952,
          0.08555829524993896,
          0.08594276756048203,
          0.08278840780258179,
          0.08511899411678314,
          0.08155360817909241,
          0.08235284686088562,
          0.08442006260156631,
          0.08102738112211227,
          0.0809662714600563,
          0.08172435313463211,
          0.0846736803650856,
          0.07849559932947159,
          0.0785846933722496,
          0.07861988246440887,
          0.07466839253902435,
          0.07577426731586456,
          0.0763082355260849,
          0.07767843455076218,
          0.07283613830804825,
          0.075092613697052,
          0.07471475750207901,
          0.0755617544054985,
          0.07505960017442703,
          0.07766332477331161,
          0.08175951987504959,
          0.07199006527662277,
          0.07249775528907776,
          0.07225887477397919,
          0.07647831737995148,
          0.07417327910661697,
          0.06920836120843887,
          0.07417222857475281,
          0.07027623802423477,
          0.06787420064210892,
          0.0689709410071373,
          0.06861794739961624,
          0.0688970610499382,
          0.06867989152669907,
          0.07003194838762283,
          0.06754738837480545,
          0.06816647946834564,
          0.06855186074972153,
          0.06857269257307053,
          0.0686245784163475,
          0.0654592216014862,
          0.07060776650905609,
          0.06701037287712097,
          0.06284628808498383,
          0.06829078495502472,
          0.06421447545289993,
          0.06794974207878113,
          0.06691566854715347,
          0.06557796895503998,
          0.06672631204128265,
          0.06467962265014648,
          0.06414549797773361,
          0.06396738439798355,
          0.06339122354984283,
          0.05985506623983383,
          0.06366030871868134,
          0.0723239853978157,
          0.06168491393327713,
          0.060490045696496964,
          0.06419075280427933,
          0.06425772607326508,
          0.06194134056568146,
          0.05752413719892502,
          0.06127335503697395,
          0.06130347400903702,
          0.05661775544285774,
          0.06355533748865128,
          0.060322441160678864,
          0.061388831585645676,
          0.05947943031787872,
          0.06683260202407837,
          0.06269068270921707,
          0.055253371596336365,
          0.05973416939377785,
          0.06196645647287369,
          0.05717857927083969,
          0.05848564952611923,
          0.05987707898020744,
          0.05695025250315666,
          0.05767184868454933,
          0.05573294311761856,
          0.058733489364385605,
          0.05952535197138786,
          0.07302850484848022,
          0.05212077125906944,
          0.055759280920028687
         ]
        },
        {
         "line": {
          "color": "pink"
         },
         "mode": "lines+markers",
         "name": "Validation Loss - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.9014393091201782,
          0.8920360803604126,
          0.9225435256958008,
          0.9113712906837463,
          0.9174912571907043,
          0.9254968166351318,
          0.9323633909225464,
          0.9409313797950745,
          0.9621099829673767,
          0.9268715381622314,
          0.9553086757659912,
          0.9496228098869324,
          0.9680402278900146,
          0.9541136026382446,
          0.9629325270652771,
          0.9720340967178345,
          0.9913313984870911,
          0.9962310194969177,
          0.9980430006980896,
          1.0187643766403198,
          0.9940939545631409,
          0.9792071580886841,
          0.9993575811386108,
          0.9987980723381042,
          1.0031522512435913,
          0.9933614730834961,
          1.0090159177780151,
          1.0361982583999634,
          1.0220048427581787,
          1.0267202854156494,
          1.0103586912155151,
          1.0236660242080688,
          1.0505908727645874,
          1.111683964729309,
          1.0331676006317139,
          1.042925477027893,
          1.0422013998031616,
          1.0546516180038452,
          1.0590941905975342,
          1.0613738298416138,
          1.059041976928711,
          1.0931107997894287,
          1.0734825134277344,
          1.090742588043213,
          1.0919227600097656,
          1.090902328491211,
          1.09434175491333,
          1.0986813306808472,
          1.0884751081466675,
          1.105668544769287,
          1.1138616800308228,
          1.1191250085830688,
          1.1208232641220093,
          1.1155362129211426,
          1.126753330230713,
          1.15491783618927,
          1.1268501281738281,
          1.1449100971221924,
          1.1369400024414062,
          1.1562870740890503,
          1.1932629346847534,
          1.1791143417358398,
          1.1899003982543945,
          1.162628173828125,
          1.1849414110183716,
          1.1913154125213623,
          1.1892720460891724,
          1.195395827293396,
          1.184670329093933,
          1.3601082563400269,
          1.1878337860107422,
          1.228316068649292,
          1.2222431898117065,
          1.2231597900390625,
          1.2250300645828247,
          1.2159154415130615,
          1.2150053977966309,
          1.2636111974716187,
          1.235137939453125,
          1.259643316268921,
          1.2527755498886108,
          1.2552157640457153,
          1.2871230840682983,
          1.2866848707199097,
          1.2813091278076172,
          1.2669628858566284,
          1.2794628143310547,
          1.2859307527542114,
          1.2943974733352661,
          1.2822041511535645,
          1.2737176418304443,
          1.2872686386108398,
          1.299058437347412,
          1.3374534845352173,
          1.2934008836746216,
          1.321269154548645,
          1.3261926174163818,
          1.2895008325576782,
          1.311854362487793,
          1.3082466125488281
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Metric Type"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Loss"
        },
        "xaxis": {
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_accuracy(histories):\n",
    "    # Create figure for accuracy\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add accuracy traces with specified colors\n",
    "    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n",
    "    for name, history in histories:\n",
    "        train_color, val_color = color_map[name]\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['accuracy']))),\n",
    "                                 y=history.history['accuracy'],\n",
    "                                 name=f'Training Accuracy - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=train_color)))\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_accuracy']))),\n",
    "                                 y=history.history['val_accuracy'],\n",
    "                                 name=f'Validation Accuracy - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=val_color)))\n",
    "\n",
    "    # Update layout for accuracy graph\n",
    "    fig.update_layout(title='Training and Validation Accuracy',\n",
    "                      xaxis_title='Epochs',\n",
    "                      yaxis_title='Accuracy',\n",
    "                      legend_title='Metric Type')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_loss(histories):\n",
    "    # Create figure for loss\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add loss traces with specified colors\n",
    "    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n",
    "    for name, history in histories:\n",
    "        train_color, val_color = color_map[name]\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['loss']))),\n",
    "                                 y=history.history['loss'],\n",
    "                                 name=f'Training Loss - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=train_color)))\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_loss']))),\n",
    "                                 y=history.history['val_loss'],\n",
    "                                 name=f'Validation Loss - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=val_color)))\n",
    "\n",
    "    # Update layout for loss graph\n",
    "    fig.update_layout(title='Training and Validation Loss',\n",
    "                      xaxis_title='Epochs',\n",
    "                      yaxis_title='Loss',\n",
    "                      legend_title='Metric Type')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Assuming you have history1 and history2 as the history objects from your model training\n",
    "plot_accuracy([('Custom Loss', history1), ('Standard Loss', history2)])\n",
    "plot_loss([('Custom Loss', history1), ('Standard Loss', history2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING custom_loss FUNCTION with Standard Lambda = 0.5, but Discrimination!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_with_sensitive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[0;32m     34\u001b[0m               loss\u001b[38;5;241m=\u001b[39mcustom_loss(lambda_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),  \u001b[38;5;66;03m# Change lambda_val as needed\u001b[39;00m\n\u001b[0;32m     35\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m history1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_with_sensitive, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_with_sensitive' is not defined"
     ]
    }
   ],
   "source": [
    "def custom_loss(lambda_val=0.5):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Extract predictions and sensitive attributes\n",
    "        predictions = y_pred[:, 0]\n",
    "        sensitive_attr = y_pred[:, 1]\n",
    "\n",
    "        # Standard binary crossentropy loss\n",
    "        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n",
    "\n",
    "        # Calculate discrimination based on sensitive attribute\n",
    "        threshold = 0\n",
    "        mask_unpriv = K.cast(sensitive_attr <= threshold, 'float32')\n",
    "        mask_priv = K.cast(sensitive_attr > threshold, 'float32')\n",
    "\n",
    "        sum_unpriv = K.sum(mask_unpriv)\n",
    "        sum_priv = K.sum(mask_priv)\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n",
    "        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n",
    "\n",
    "        discrimination = K.square(prob_priv - prob_unpriv)\n",
    "\n",
    "        # Debug outputs\n",
    "        # tf.print(\"Standard Loss:\", standard_loss, \"Discrimination:\", discrimination)\n",
    "\n",
    "        # Total loss with discrimination penalty\n",
    "        return standard_loss + lambda_val * discrimination\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=custom_loss(lambda_val=0.01),  # Change lambda_val as needed\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history1 = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING custom_loss FUNCTION with cust Lambda = 0.01, but Discrimination!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600us/step - accuracy: 0.6550 - loss: 0.7064 - val_accuracy: 0.6609 - val_loss: 0.6894\n",
      "Epoch 2/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.6535 - loss: 0.6854 - val_accuracy: 0.6609 - val_loss: 0.6880\n",
      "Epoch 3/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - accuracy: 0.6558 - loss: 0.6812 - val_accuracy: 0.6609 - val_loss: 0.6878\n",
      "Epoch 4/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402us/step - accuracy: 0.6569 - loss: 0.6801 - val_accuracy: 0.6609 - val_loss: 0.6883\n",
      "Epoch 5/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - accuracy: 0.6568 - loss: 0.6789 - val_accuracy: 0.6609 - val_loss: 0.6881\n",
      "Epoch 6/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.6590 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6878\n",
      "Epoch 7/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 0.6499 - loss: 0.6781 - val_accuracy: 0.6609 - val_loss: 0.6891\n",
      "Epoch 8/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413us/step - accuracy: 0.6574 - loss: 0.6754 - val_accuracy: 0.6609 - val_loss: 0.6888\n",
      "Epoch 9/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - accuracy: 0.6514 - loss: 0.6772 - val_accuracy: 0.6609 - val_loss: 0.6886\n",
      "Epoch 10/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - accuracy: 0.6547 - loss: 0.6752 - val_accuracy: 0.6609 - val_loss: 0.6893\n",
      "Epoch 11/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - accuracy: 0.6602 - loss: 0.6736 - val_accuracy: 0.6609 - val_loss: 0.6887\n",
      "Epoch 12/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - accuracy: 0.6497 - loss: 0.6741 - val_accuracy: 0.6609 - val_loss: 0.6896\n",
      "Epoch 13/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - accuracy: 0.6501 - loss: 0.6703 - val_accuracy: 0.6609 - val_loss: 0.6900\n",
      "Epoch 14/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.6578 - loss: 0.6716 - val_accuracy: 0.6609 - val_loss: 0.6891\n",
      "Epoch 15/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.6528 - loss: 0.6702 - val_accuracy: 0.6609 - val_loss: 0.6884\n",
      "Epoch 16/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - accuracy: 0.6607 - loss: 0.6709 - val_accuracy: 0.6609 - val_loss: 0.6890\n",
      "Epoch 17/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.6566 - loss: 0.6708 - val_accuracy: 0.6609 - val_loss: 0.6896\n",
      "Epoch 18/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.6558 - loss: 0.6706 - val_accuracy: 0.6609 - val_loss: 0.6901\n",
      "Epoch 19/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.6534 - loss: 0.6695 - val_accuracy: 0.6609 - val_loss: 0.6898\n",
      "Epoch 20/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443us/step - accuracy: 0.6589 - loss: 0.6673 - val_accuracy: 0.6609 - val_loss: 0.6903\n",
      "Epoch 21/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.6594 - loss: 0.6697 - val_accuracy: 0.6609 - val_loss: 0.6895\n",
      "Epoch 22/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - accuracy: 0.6541 - loss: 0.6656 - val_accuracy: 0.6609 - val_loss: 0.6897\n",
      "Epoch 23/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6504 - loss: 0.6680 - val_accuracy: 0.6609 - val_loss: 0.6898\n",
      "Epoch 24/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - accuracy: 0.6491 - loss: 0.6642 - val_accuracy: 0.6609 - val_loss: 0.6893\n",
      "Epoch 25/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.6595 - loss: 0.6663 - val_accuracy: 0.6609 - val_loss: 0.6908\n",
      "Epoch 26/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - accuracy: 0.6486 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6899\n",
      "Epoch 27/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.6512 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6891\n",
      "Epoch 28/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434us/step - accuracy: 0.6623 - loss: 0.6659 - val_accuracy: 0.6609 - val_loss: 0.6902\n",
      "Epoch 29/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.6488 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6908\n",
      "Epoch 30/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - accuracy: 0.6629 - loss: 0.6647 - val_accuracy: 0.6609 - val_loss: 0.6891\n",
      "Epoch 31/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - accuracy: 0.6499 - loss: 0.6653 - val_accuracy: 0.6609 - val_loss: 0.6902\n",
      "Epoch 32/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - accuracy: 0.6537 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903\n",
      "Epoch 33/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6560 - loss: 0.6649 - val_accuracy: 0.6609 - val_loss: 0.6893\n",
      "Epoch 34/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - accuracy: 0.6515 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6899\n",
      "Epoch 35/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.6549 - loss: 0.6640 - val_accuracy: 0.6609 - val_loss: 0.6904\n",
      "Epoch 36/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - accuracy: 0.6498 - loss: 0.6655 - val_accuracy: 0.6609 - val_loss: 0.6903\n",
      "Epoch 37/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - accuracy: 0.6572 - loss: 0.6641 - val_accuracy: 0.6609 - val_loss: 0.6926\n",
      "Epoch 38/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6623 - loss: 0.6660 - val_accuracy: 0.6609 - val_loss: 0.6901\n",
      "Epoch 39/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6571 - loss: 0.6645 - val_accuracy: 0.6609 - val_loss: 0.6906\n",
      "Epoch 40/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - accuracy: 0.6571 - loss: 0.6637 - val_accuracy: 0.6609 - val_loss: 0.6909\n",
      "Epoch 41/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - accuracy: 0.6551 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6906\n",
      "Epoch 42/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.6476 - loss: 0.6631 - val_accuracy: 0.6609 - val_loss: 0.6917\n",
      "Epoch 43/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - accuracy: 0.6529 - loss: 0.6662 - val_accuracy: 0.6609 - val_loss: 0.6911\n",
      "Epoch 44/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.6571 - loss: 0.6650 - val_accuracy: 0.6609 - val_loss: 0.6911\n",
      "Epoch 45/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - accuracy: 0.6486 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6914\n",
      "Epoch 46/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434us/step - accuracy: 0.6591 - loss: 0.6633 - val_accuracy: 0.6609 - val_loss: 0.6913\n",
      "Epoch 47/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6451 - loss: 0.6622 - val_accuracy: 0.6609 - val_loss: 0.6913\n",
      "Epoch 48/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6502 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907\n",
      "Epoch 49/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.6517 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6918\n",
      "Epoch 50/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.6593 - loss: 0.6606 - val_accuracy: 0.6609 - val_loss: 0.6911\n",
      "Epoch 51/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.6534 - loss: 0.6638 - val_accuracy: 0.6609 - val_loss: 0.6910\n",
      "Epoch 52/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - accuracy: 0.6602 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6910\n",
      "Epoch 53/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.6466 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6917\n",
      "Epoch 54/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.6599 - loss: 0.6630 - val_accuracy: 0.6609 - val_loss: 0.6906\n",
      "Epoch 55/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.6499 - loss: 0.6612 - val_accuracy: 0.6609 - val_loss: 0.6906\n",
      "Epoch 56/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6580 - loss: 0.6635 - val_accuracy: 0.6609 - val_loss: 0.6913\n",
      "Epoch 57/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.6571 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900\n",
      "Epoch 58/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.6615 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6904\n",
      "Epoch 59/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - accuracy: 0.6572 - loss: 0.6629 - val_accuracy: 0.6609 - val_loss: 0.6899\n",
      "Epoch 60/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - accuracy: 0.6577 - loss: 0.6608 - val_accuracy: 0.6609 - val_loss: 0.6912\n",
      "Epoch 61/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.6541 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907\n",
      "Epoch 62/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6614 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6903\n",
      "Epoch 63/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6569 - loss: 0.6602 - val_accuracy: 0.6609 - val_loss: 0.6903\n",
      "Epoch 64/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.6489 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6905\n",
      "Epoch 65/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - accuracy: 0.6461 - loss: 0.6604 - val_accuracy: 0.6609 - val_loss: 0.6912\n",
      "Epoch 66/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - accuracy: 0.6633 - loss: 0.6618 - val_accuracy: 0.6609 - val_loss: 0.6904\n",
      "Epoch 67/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.6551 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6900\n",
      "Epoch 68/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.6567 - loss: 0.6610 - val_accuracy: 0.6609 - val_loss: 0.6902\n",
      "Epoch 69/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6503 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6915\n",
      "Epoch 70/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - accuracy: 0.6579 - loss: 0.6627 - val_accuracy: 0.6609 - val_loss: 0.6905\n",
      "Epoch 71/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.6569 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6915\n",
      "Epoch 72/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - accuracy: 0.6494 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6918\n",
      "Epoch 73/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - accuracy: 0.6484 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6916\n",
      "Epoch 74/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - accuracy: 0.6512 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6909\n",
      "Epoch 75/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.6613 - loss: 0.6588 - val_accuracy: 0.6609 - val_loss: 0.6914\n",
      "Epoch 76/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - accuracy: 0.6557 - loss: 0.6599 - val_accuracy: 0.6609 - val_loss: 0.6907\n",
      "Epoch 77/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - accuracy: 0.6592 - loss: 0.6619 - val_accuracy: 0.6609 - val_loss: 0.6900\n",
      "Epoch 78/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6553 - loss: 0.6620 - val_accuracy: 0.6609 - val_loss: 0.6917\n",
      "Epoch 79/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.6564 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6912\n",
      "Epoch 80/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6525 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6907\n",
      "Epoch 81/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - accuracy: 0.6482 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6912\n",
      "Epoch 82/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.6534 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6900\n",
      "Epoch 83/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - accuracy: 0.6580 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6907\n",
      "Epoch 84/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - accuracy: 0.6500 - loss: 0.6596 - val_accuracy: 0.6609 - val_loss: 0.6907\n",
      "Epoch 85/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - accuracy: 0.6518 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6899\n",
      "Epoch 86/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.6579 - loss: 0.6587 - val_accuracy: 0.6609 - val_loss: 0.6908\n",
      "Epoch 87/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - accuracy: 0.6646 - loss: 0.6615 - val_accuracy: 0.6609 - val_loss: 0.6897\n",
      "Epoch 88/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - accuracy: 0.6484 - loss: 0.6625 - val_accuracy: 0.6609 - val_loss: 0.6904\n",
      "Epoch 89/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.6500 - loss: 0.6595 - val_accuracy: 0.6609 - val_loss: 0.6917\n",
      "Epoch 90/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - accuracy: 0.6472 - loss: 0.6614 - val_accuracy: 0.6609 - val_loss: 0.6915\n",
      "Epoch 91/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.6485 - loss: 0.6624 - val_accuracy: 0.6609 - val_loss: 0.6917\n",
      "Epoch 92/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6596 - loss: 0.6621 - val_accuracy: 0.6609 - val_loss: 0.6902\n",
      "Epoch 93/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - accuracy: 0.6508 - loss: 0.6600 - val_accuracy: 0.6609 - val_loss: 0.6894\n",
      "Epoch 94/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.6520 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6909\n",
      "Epoch 95/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.6587 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6928\n",
      "Epoch 96/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - accuracy: 0.6538 - loss: 0.6607 - val_accuracy: 0.6609 - val_loss: 0.6923\n",
      "Epoch 97/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - accuracy: 0.6521 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6910\n",
      "Epoch 98/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.6471 - loss: 0.6617 - val_accuracy: 0.6609 - val_loss: 0.6923\n",
      "Epoch 99/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.6484 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6891\n",
      "Epoch 100/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.6508 - loss: 0.6590 - val_accuracy: 0.6609 - val_loss: 0.6903\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def custom_loss(lambda_val=0.001):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Extract predictions and sensitive attributes from y_pred\n",
    "        predictions = y_pred[:, 0]\n",
    "        sensitive_attr = y_pred[:, 1]\n",
    "\n",
    "        # Debug prints to check outputs\n",
    "        # tf.print(\"Predictions sample:\", predictions[:10])\n",
    "        # tf.print(\"Sensitive Attr sample:\", sensitive_attr[:10])\n",
    "\n",
    "        # Standard binary crossentropy loss\n",
    "        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n",
    "        \n",
    "        # Determine thresholds to convert sensitive attributes to binary\n",
    "        # Assuming the negative and positive classes are split around zero\n",
    "        threshold = 0\n",
    "        mask_unpriv = K.cast(sensitive_attr <= threshold, 'float32')\n",
    "        mask_priv = K.cast(sensitive_attr > threshold, 'float32')\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        sum_unpriv = K.sum(mask_unpriv)\n",
    "        sum_priv = K.sum(mask_priv)\n",
    "\n",
    "        # # Debug prints for mask sums\n",
    "        # tf.print(\"Sum unprivileged:\", sum_unpriv)\n",
    "        # tf.print(\"Sum privileged:\", sum_priv)\n",
    "\n",
    "        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n",
    "        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n",
    "\n",
    "        # Discrimination as the squared difference in probabilities\n",
    "        discrimination = K.square(prob_priv - prob_unpriv)\n",
    "\n",
    "        # # Debug print\n",
    "        # tf.print(\"Discrimination:\", discrimination)\n",
    "\n",
    "        # Total loss with discrimination penalty\n",
    "        return standard_loss + lambda_val * discrimination\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features\n",
    "sensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "# Input layers\n",
    "inputs = Input(shape=(input_size,))\n",
    "sensitive_inputs = Input(shape=(1,))\n",
    "\n",
    "# Network architecture\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "combined_outputs = tf.keras.layers.concatenate([outputs, sensitive_inputs])\n",
    "\n",
    "model = Model(inputs=[inputs, sensitive_inputs], outputs=combined_outputs)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=custom_loss(lambda_val = 0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Prepare data with sensitive attribute\n",
    "X_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n",
    "\n",
    "# Train the model\n",
    "history1_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modal with with Standard Loss(binary_crossentropy), No discrimination , but Discrimination!=0 in custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - accuracy: 0.8259 - loss: 0.4073 - val_accuracy: 0.8595 - val_loss: 0.3396\n",
      "Epoch 2/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442us/step - accuracy: 0.8696 - loss: 0.3200 - val_accuracy: 0.8634 - val_loss: 0.3325\n",
      "Epoch 3/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - accuracy: 0.8746 - loss: 0.3044 - val_accuracy: 0.8630 - val_loss: 0.3334\n",
      "Epoch 4/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399us/step - accuracy: 0.8820 - loss: 0.2964 - val_accuracy: 0.8630 - val_loss: 0.3361\n",
      "Epoch 5/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - accuracy: 0.8827 - loss: 0.2969 - val_accuracy: 0.8650 - val_loss: 0.3331\n",
      "Epoch 6/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - accuracy: 0.8857 - loss: 0.2855 - val_accuracy: 0.8646 - val_loss: 0.3304\n",
      "Epoch 7/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.8811 - loss: 0.2898 - val_accuracy: 0.8595 - val_loss: 0.3401\n",
      "Epoch 8/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.8928 - loss: 0.2674 - val_accuracy: 0.8658 - val_loss: 0.3319\n",
      "Epoch 9/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - accuracy: 0.8897 - loss: 0.2836 - val_accuracy: 0.8646 - val_loss: 0.3381\n",
      "Epoch 10/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.8926 - loss: 0.2663 - val_accuracy: 0.8567 - val_loss: 0.3486\n",
      "Epoch 11/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - accuracy: 0.8995 - loss: 0.2607 - val_accuracy: 0.8595 - val_loss: 0.3509\n",
      "Epoch 12/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399us/step - accuracy: 0.9013 - loss: 0.2601 - val_accuracy: 0.8598 - val_loss: 0.3511\n",
      "Epoch 13/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9017 - loss: 0.2531 - val_accuracy: 0.8595 - val_loss: 0.3629\n",
      "Epoch 14/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - accuracy: 0.9109 - loss: 0.2382 - val_accuracy: 0.8606 - val_loss: 0.3490\n",
      "Epoch 15/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9070 - loss: 0.2503 - val_accuracy: 0.8535 - val_loss: 0.3574\n",
      "Epoch 16/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - accuracy: 0.9119 - loss: 0.2331 - val_accuracy: 0.8551 - val_loss: 0.3676\n",
      "Epoch 17/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9143 - loss: 0.2275 - val_accuracy: 0.8535 - val_loss: 0.3737\n",
      "Epoch 18/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9121 - loss: 0.2327 - val_accuracy: 0.8575 - val_loss: 0.3660\n",
      "Epoch 19/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - accuracy: 0.9233 - loss: 0.2215 - val_accuracy: 0.8539 - val_loss: 0.3894\n",
      "Epoch 20/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - accuracy: 0.9159 - loss: 0.2244 - val_accuracy: 0.8523 - val_loss: 0.3763\n",
      "Epoch 21/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9230 - loss: 0.2136 - val_accuracy: 0.8555 - val_loss: 0.3882\n",
      "Epoch 22/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - accuracy: 0.9242 - loss: 0.2074 - val_accuracy: 0.8535 - val_loss: 0.3934\n",
      "Epoch 23/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9253 - loss: 0.2120 - val_accuracy: 0.8496 - val_loss: 0.3942\n",
      "Epoch 24/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9266 - loss: 0.2041 - val_accuracy: 0.8464 - val_loss: 0.4131\n",
      "Epoch 25/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.9202 - loss: 0.2132 - val_accuracy: 0.8500 - val_loss: 0.4122\n",
      "Epoch 26/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - accuracy: 0.9325 - loss: 0.1921 - val_accuracy: 0.8535 - val_loss: 0.4094\n",
      "Epoch 27/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9351 - loss: 0.1893 - val_accuracy: 0.8504 - val_loss: 0.4226\n",
      "Epoch 28/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.9320 - loss: 0.1931 - val_accuracy: 0.8480 - val_loss: 0.4264\n",
      "Epoch 29/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - accuracy: 0.9325 - loss: 0.1899 - val_accuracy: 0.8496 - val_loss: 0.4329\n",
      "Epoch 30/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9317 - loss: 0.1922 - val_accuracy: 0.8500 - val_loss: 0.4303\n",
      "Epoch 31/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9371 - loss: 0.1796 - val_accuracy: 0.8437 - val_loss: 0.4379\n",
      "Epoch 32/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - accuracy: 0.9337 - loss: 0.1851 - val_accuracy: 0.8460 - val_loss: 0.4467\n",
      "Epoch 33/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - accuracy: 0.9348 - loss: 0.1881 - val_accuracy: 0.8445 - val_loss: 0.4406\n",
      "Epoch 34/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.9399 - loss: 0.1758 - val_accuracy: 0.8441 - val_loss: 0.4533\n",
      "Epoch 35/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - accuracy: 0.9370 - loss: 0.1763 - val_accuracy: 0.8429 - val_loss: 0.4685\n",
      "Epoch 36/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.9420 - loss: 0.1644 - val_accuracy: 0.8437 - val_loss: 0.4601\n",
      "Epoch 37/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.9382 - loss: 0.1685 - val_accuracy: 0.8421 - val_loss: 0.4721\n",
      "Epoch 38/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.9397 - loss: 0.1732 - val_accuracy: 0.8401 - val_loss: 0.4915\n",
      "Epoch 39/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 0.9463 - loss: 0.1582 - val_accuracy: 0.8354 - val_loss: 0.5038\n",
      "Epoch 40/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9438 - loss: 0.1608 - val_accuracy: 0.8429 - val_loss: 0.4831\n",
      "Epoch 41/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9403 - loss: 0.1676 - val_accuracy: 0.8452 - val_loss: 0.4940\n",
      "Epoch 42/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - accuracy: 0.9445 - loss: 0.1581 - val_accuracy: 0.8393 - val_loss: 0.4956\n",
      "Epoch 43/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - accuracy: 0.9461 - loss: 0.1537 - val_accuracy: 0.8350 - val_loss: 0.5094\n",
      "Epoch 44/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9465 - loss: 0.1573 - val_accuracy: 0.8350 - val_loss: 0.5115\n",
      "Epoch 45/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - accuracy: 0.9467 - loss: 0.1530 - val_accuracy: 0.8409 - val_loss: 0.5131\n",
      "Epoch 46/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - accuracy: 0.9473 - loss: 0.1580 - val_accuracy: 0.8393 - val_loss: 0.5254\n",
      "Epoch 47/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9492 - loss: 0.1451 - val_accuracy: 0.8377 - val_loss: 0.5261\n",
      "Epoch 48/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - accuracy: 0.9517 - loss: 0.1462 - val_accuracy: 0.8247 - val_loss: 0.5360\n",
      "Epoch 49/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.9528 - loss: 0.1422 - val_accuracy: 0.8287 - val_loss: 0.5420\n",
      "Epoch 50/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - accuracy: 0.9515 - loss: 0.1378 - val_accuracy: 0.8216 - val_loss: 0.5578\n",
      "Epoch 51/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - accuracy: 0.9508 - loss: 0.1454 - val_accuracy: 0.8373 - val_loss: 0.5463\n",
      "Epoch 52/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - accuracy: 0.9487 - loss: 0.1405 - val_accuracy: 0.8385 - val_loss: 0.5607\n",
      "Epoch 53/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - accuracy: 0.9545 - loss: 0.1364 - val_accuracy: 0.8310 - val_loss: 0.5591\n",
      "Epoch 54/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - accuracy: 0.9557 - loss: 0.1387 - val_accuracy: 0.8366 - val_loss: 0.5878\n",
      "Epoch 55/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.9549 - loss: 0.1338 - val_accuracy: 0.8334 - val_loss: 0.5751\n",
      "Epoch 56/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step - accuracy: 0.9518 - loss: 0.1390 - val_accuracy: 0.8389 - val_loss: 0.5626\n",
      "Epoch 57/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9583 - loss: 0.1279 - val_accuracy: 0.8389 - val_loss: 0.5940\n",
      "Epoch 58/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - accuracy: 0.9558 - loss: 0.1300 - val_accuracy: 0.8314 - val_loss: 0.5842\n",
      "Epoch 59/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - accuracy: 0.9592 - loss: 0.1307 - val_accuracy: 0.8366 - val_loss: 0.5996\n",
      "Epoch 60/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.9568 - loss: 0.1323 - val_accuracy: 0.8326 - val_loss: 0.5967\n",
      "Epoch 61/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - accuracy: 0.9587 - loss: 0.1232 - val_accuracy: 0.8283 - val_loss: 0.5802\n",
      "Epoch 62/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - accuracy: 0.9572 - loss: 0.1475 - val_accuracy: 0.8267 - val_loss: 0.6030\n",
      "Epoch 63/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9628 - loss: 0.1194 - val_accuracy: 0.8279 - val_loss: 0.6128\n",
      "Epoch 64/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9615 - loss: 0.1162 - val_accuracy: 0.8302 - val_loss: 0.6099\n",
      "Epoch 65/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - accuracy: 0.9601 - loss: 0.1192 - val_accuracy: 0.8314 - val_loss: 0.6176\n",
      "Epoch 66/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - accuracy: 0.9592 - loss: 0.1237 - val_accuracy: 0.8298 - val_loss: 0.6224\n",
      "Epoch 67/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.9606 - loss: 0.1188 - val_accuracy: 0.8223 - val_loss: 0.6446\n",
      "Epoch 68/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.9616 - loss: 0.1147 - val_accuracy: 0.8322 - val_loss: 0.6292\n",
      "Epoch 69/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - accuracy: 0.9624 - loss: 0.1157 - val_accuracy: 0.8330 - val_loss: 0.6557\n",
      "Epoch 70/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.9615 - loss: 0.1165 - val_accuracy: 0.8176 - val_loss: 0.6655\n",
      "Epoch 71/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - accuracy: 0.9618 - loss: 0.1145 - val_accuracy: 0.8298 - val_loss: 0.6618\n",
      "Epoch 72/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step - accuracy: 0.9617 - loss: 0.1177 - val_accuracy: 0.8018 - val_loss: 0.6880\n",
      "Epoch 73/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9627 - loss: 0.1134 - val_accuracy: 0.8330 - val_loss: 0.6874\n",
      "Epoch 74/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.9636 - loss: 0.1144 - val_accuracy: 0.8279 - val_loss: 0.6749\n",
      "Epoch 75/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9638 - loss: 0.1070 - val_accuracy: 0.8239 - val_loss: 0.6767\n",
      "Epoch 76/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.9642 - loss: 0.1097 - val_accuracy: 0.8330 - val_loss: 0.6812\n",
      "Epoch 77/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.9632 - loss: 0.1096 - val_accuracy: 0.8310 - val_loss: 0.6936\n",
      "Epoch 78/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.9667 - loss: 0.1049 - val_accuracy: 0.8255 - val_loss: 0.7077\n",
      "Epoch 79/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.9657 - loss: 0.1040 - val_accuracy: 0.8251 - val_loss: 0.6987\n",
      "Epoch 80/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - accuracy: 0.9640 - loss: 0.1099 - val_accuracy: 0.8220 - val_loss: 0.7047\n",
      "Epoch 81/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - accuracy: 0.9661 - loss: 0.1059 - val_accuracy: 0.8243 - val_loss: 0.7189\n",
      "Epoch 82/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - accuracy: 0.9657 - loss: 0.1057 - val_accuracy: 0.8330 - val_loss: 0.7181\n",
      "Epoch 83/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9708 - loss: 0.0954 - val_accuracy: 0.8295 - val_loss: 0.7182\n",
      "Epoch 84/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.9676 - loss: 0.1044 - val_accuracy: 0.8247 - val_loss: 0.7164\n",
      "Epoch 85/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - accuracy: 0.9695 - loss: 0.1019 - val_accuracy: 0.8306 - val_loss: 0.7415\n",
      "Epoch 86/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - accuracy: 0.9633 - loss: 0.1033 - val_accuracy: 0.8302 - val_loss: 0.7433\n",
      "Epoch 87/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - accuracy: 0.9687 - loss: 0.1010 - val_accuracy: 0.8302 - val_loss: 0.7664\n",
      "Epoch 88/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - accuracy: 0.9683 - loss: 0.0958 - val_accuracy: 0.8279 - val_loss: 0.7715\n",
      "Epoch 89/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - accuracy: 0.9671 - loss: 0.1022 - val_accuracy: 0.8298 - val_loss: 0.7661\n",
      "Epoch 90/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.9731 - loss: 0.0872 - val_accuracy: 0.8397 - val_loss: 0.7873\n",
      "Epoch 91/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - accuracy: 0.9685 - loss: 0.0973 - val_accuracy: 0.8220 - val_loss: 0.7802\n",
      "Epoch 92/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - accuracy: 0.9667 - loss: 0.1003 - val_accuracy: 0.8259 - val_loss: 0.7762\n",
      "Epoch 93/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.9679 - loss: 0.0941 - val_accuracy: 0.8259 - val_loss: 0.7889\n",
      "Epoch 94/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.9719 - loss: 0.0876 - val_accuracy: 0.8298 - val_loss: 0.7972\n",
      "Epoch 95/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - accuracy: 0.9744 - loss: 0.0854 - val_accuracy: 0.8156 - val_loss: 0.8149\n",
      "Epoch 96/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - accuracy: 0.9703 - loss: 0.0935 - val_accuracy: 0.8302 - val_loss: 0.8087\n",
      "Epoch 97/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - accuracy: 0.9698 - loss: 0.0894 - val_accuracy: 0.8295 - val_loss: 0.8223\n",
      "Epoch 98/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.9695 - loss: 0.0954 - val_accuracy: 0.8318 - val_loss: 0.8068\n",
      "Epoch 99/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - accuracy: 0.9722 - loss: 0.0856 - val_accuracy: 0.8338 - val_loss: 0.8178\n",
      "Epoch 100/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.9726 - loss: 0.0874 - val_accuracy: 0.8200 - val_loss: 0.8233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features\n",
    "sensitive_index = dataset_orig_panel19_train.feature_names.index('RACE')\n",
    "\n",
    "# Input layers\n",
    "inputs = Input(shape=(input_size,))\n",
    "sensitive_inputs = Input(shape=(1,))\n",
    "\n",
    "# Network architecture\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# No need to concatenate outputs and sensitive inputs for the loss calculation\n",
    "# Separate outputs for predictions and sensitive attributes\n",
    "# Since we're using binary_crossentropy, we only need the main outputs\n",
    "model = Model(inputs=[inputs, sensitive_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model with binary crossentropy\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Prepare data with sensitive attribute\n",
    "X_train_with_sensitive = [X_train_scaled, X_train_scaled[:, sensitive_index]]\n",
    "\n",
    "# Train the model\n",
    "history2_nd = model.fit(X_train_with_sensitive, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB/GklEQVR4nO3deVxUVf8H8M+AMAMoKrIviuKClqJp4oa7Upq5lqIpmmmapEapaSpqGU9WLqlp9Yj5FIqpaJlmIkrmrqmVu5CmooBLCoICzpzfH/ObiWEWZoaBGeDzfr3mBXPuveeeMxyG+XLO/V6JEEKAiIiIiIiISsXO2g0gIiIiIiKqDBhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCGFwREekgkUgwb948azej1L755hsEBwfDwcEBtWrVsnZztFy9ehUSiQRff/21ycempKRAIpEgJSXF4u2qjL7++mtIJBJcvXrV2k0hKwkMDMQLL7xg7WYQVWoMrohIp7S0NLz++uto0KABZDIZXF1d0bFjRyxbtgyPHj2ydvPICBcuXMDo0aMRFBSEr776Cl9++aXefefNmweJRAI7Oztcv35da3t2djacnJwgkUgQFRVVls0uU59//jkkEglCQ0Ot3ZQqZ/r06ZBIJBg6dKi1m1JmAgMDIZFIdD6ee+45azePiMpBNWs3gIhsz44dO/DSSy9BKpVi1KhRePrpp1FQUIADBw5g2rRpOHv2rMEP6pXBo0ePUK1axX6LTElJgUKhwLJly9CwYUOjjpFKpdiwYQOmT5+uUZ6YmFgWTSx38fHxCAwMxLFjx5Cammr061IZjBw5EsOGDYNUKi33cwshsGHDBgQGBmL79u3IyclBjRo1yr0d5aFly5Z4++23tcp9fX2t0BoiKm8V+5MDEVnclStXMGzYMNSrVw979+6Fj4+PetukSZOQmpqKHTt2WLGFZUehUKCgoAAymQwymczazSm1rKwsADBpOWCfPn10Blfr169H3759sWXLFks2sVxduXIFhw4dQmJiIl5//XXEx8cjJibG2s3SKTc3Fy4uLhat097eHvb29hat01gpKSm4ceMG9u7di/DwcCQmJiIyMtIidZfFa1Uafn5+eOWVV6zdDCKyEi4LJCINixYtwsOHD7FmzRqNwEqlYcOGmDJlivr5kydP8P777yMoKAhSqRSBgYGYNWsW8vPzNY5TrfVPSUlBmzZt4OTkhObNm6uvl0lMTETz5s0hk8nQunVrnDp1SuP40aNHo3r16vjrr78QHh4OFxcX+Pr6YsGCBRBCaOz7ySefoEOHDqhTpw6cnJzQunVrbN68WasvqiVu8fHxeOqppyCVSrFr1y71tqLXXOXk5GDq1KkIDAyEVCqFp6cnevXqhZMnT2rUuWnTJrRu3RpOTk5wd3fHK6+8gvT0dJ19SU9Px4ABA1C9enV4eHjgnXfegVwu1/OT0fT555+r2+zr64tJkybh/v37Gq+3KnDw8PAw+hqy4cOH4/Tp07hw4YK6LCMjA3v37sXw4cN1HpOVlYWxY8fCy8sLMpkMISEhWLdundZ+9+/fx+jRo1GzZk3UqlULkZGRGm0u6sKFCxgyZAjc3Nwgk8nQpk0b/PDDDyW235D4+HjUrl0bffv2xZAhQxAfH69zv/v37+Ott95S/6z9/f0xatQo3LlzR73P48ePMW/ePDRu3BgymQw+Pj4YNGgQ0tLSAOi/HkzXNWaq8ZCWloY+ffqgRo0aGDFiBADg119/xUsvvYS6detCKpUiICAAb731ls6luRcuXMDLL78MDw8PODk5oUmTJnjvvffU2/Vdc/XTTz8hLCwMLi4uqFGjBvr27YuzZ89q7JORkYExY8bA398fUqkUPj4+6N+/v9HXb8XHx6NZs2bo1q0bevbsqfe1T09Px9ixY+Hr6wupVIr69etj4sSJKCgo0OjDL7/8gjfeeAOenp7w9/dXH1/S7wUAXL58GYMHD4a3tzdkMhn8/f0xbNgwPHjwQL1PUlISOnXqhFq1aqF69epo0qQJZs2aZVRfjWHK+1lubi7efvttBAQEQCqVokmTJvjkk0+09gOAb7/9Fm3btoWzszNq166Nzp07Y/fu3Vr7HThwAG3btoVMJkODBg3wv//9T2N7YWEh5s+fj0aNGkEmk6FOnTro1KkTkpKSLPYaEFVWnLkiIg3bt29HgwYN0KFDB6P2f+2117Bu3ToMGTIEb7/9No4ePYrY2FicP38eW7du1dg3NTUVw4cPx+uvv45XXnkFn3zyCfr164fVq1dj1qxZeOONNwAAsbGxePnll3Hx4kXY2f37PyC5XI7nnnsO7dq1w6JFi7Br1y7ExMTgyZMnWLBggXq/ZcuW4cUXX8SIESNQUFCAhIQEvPTSS/jxxx/Rt29fjTbt3bsX3333HaKiouDu7o7AwECd/ZwwYQI2b96MqKgoNGvWDHfv3sWBAwdw/vx5PPPMMwCUH/zGjBmDZ599FrGxscjMzMSyZctw8OBBnDp1SmMGSS6XIzw8HKGhofjkk0+wZ88efPrppwgKCsLEiRMNvubz5s3D/Pnz0bNnT0ycOBEXL17EqlWrcPz4cRw8eBAODg5YunQp/ve//2Hr1q1YtWoVqlevjhYtWpT48+zcuTP8/f2xfv169Wu6ceNGVK9eXeu1A5TLJ7t27YrU1FRERUWhfv362LRpE0aPHo379++rA3EhBPr3748DBw5gwoQJaNq0KbZu3apz9uLs2bPo2LEj/Pz88O6778LFxQXfffcdBgwYgC1btmDgwIEl9kOX+Ph4DBo0CI6OjoiIiFC/Zs8++6x6n4cPHyIsLAznz5/Hq6++imeeeQZ37tzBDz/8gBs3bsDd3R1yuRwvvPACkpOTMWzYMEyZMgU5OTlISkrCmTNnEBQUZHLbnjx5gvDwcHTq1AmffPIJnJ2dASiD9by8PEycOBF16tTBsWPHsHz5cty4cQObNm1SH//HH38gLCwMDg4OGD9+PAIDA5GWlobt27dj4cKFes/7zTffIDIyEuHh4fjoo4+Ql5eHVatWoVOnTjh16pT692Hw4ME4e/Ys3nzzTQQGBiIrKwtJSUm4du2a3t8Zlfz8fGzZskW9VC4iIgJjxoxBRkYGvL291fvdvHkTbdu2xf379zF+/HgEBwcjPT0dmzdvRl5eHhwdHdX7vvHGG/Dw8MDcuXORm5sLwLjfi4KCAoSHhyM/Px9vvvkmvL29kZ6ejh9//BH3799HzZo1cfbsWbzwwgto0aIFFixYAKlUitTUVBw8eNCon2VhYaFGIK7i4uICJycn9XNj3s+EEHjxxRexb98+jB07Fi1btsTPP/+MadOmIT09HUuWLFHXN3/+fMybNw8dOnTAggUL4OjoiKNHj2Lv3r3o3bu3er/U1FQMGTIEY8eORWRkJOLi4jB69Gi0bt0aTz31lPq1jI2NxWuvvYa2bdsiOzsbJ06cwMmTJ9GrVy+jXgeiKksQEf2/Bw8eCACif//+Ru1/+vRpAUC89tprGuXvvPOOACD27t2rLqtXr54AIA4dOqQu+/nnnwUA4eTkJP7++291+RdffCEAiH379qnLIiMjBQDx5ptvqssUCoXo27evcHR0FLdv31aX5+XlabSnoKBAPP3006J79+4a5QCEnZ2dOHv2rFbfAIiYmBj185o1a4pJkybpfS0KCgqEp6enePrpp8WjR4/U5T/++KMAIObOnavVlwULFmjU0apVK9G6dWu95xBCiKysLOHo6Ch69+4t5HK5unzFihUCgIiLi1OXxcTECAAar40+Rfd95513RMOGDdXbnn32WTFmzBghhPJ1Kfo6LF26VAAQ3377rcZr0b59e1G9enWRnZ0thBBi27ZtAoBYtGiRer8nT56IsLAwAUCsXbtWXd6jRw/RvHlz8fjxY3WZQqEQHTp0EI0aNVKX7du3T2uc6HPixAkBQCQlJanr8/f3F1OmTNHYb+7cuQKASExM1KpDoVAIIYSIi4sTAMTixYv17qOvbVeuXNHqr2o8vPvuu1r1FR/LQggRGxsrJBKJxu9M586dRY0aNTTKirZHCCHWrl0rAIgrV64IIYTIyckRtWrVEuPGjdM4JiMjQ9SsWVNd/s8//wgA4uOPP9ZqizE2b94sAIjLly8LIYTIzs4WMplMLFmyRGO/UaNGCTs7O3H8+HGtOlT9UPWhU6dO4smTJ+rtxv5enDp1SgAQmzZt0tveJUuWGP17U5zqfU7XIzY2Vr2fse9nqt+bDz74QOM8Q4YMERKJRKSmpgohhLh8+bKws7MTAwcO1Oi/qt7i7du/f7+6LCsrS0ilUvH222+ry0JCQkTfvn1N7j8RCcFlgUSklp2dDQBGX2i+c+dOAEB0dLRGueo/1MWvzWrWrBnat2+vfq7K2Na9e3fUrVtXq/yvv/7SOmfRTHWqZX0FBQXYs2ePurzof4f/+ecfPHjwAGFhYVpL+ACgS5cuaNasWQk9VV63dPToUdy8eVPn9hMnTiArKwtvvPGGxvVaffv2RXBwsM7r1CZMmKDxPCwsTGefi9qzZw8KCgowdepUjVm9cePGwdXV1SLXww0fPhypqak4fvy4+qu+JYE7d+6Et7c3IiIi1GUODg6YPHkyHj58iF9++UW9X7Vq1TRm5ezt7fHmm29q1Hfv3j3s3bsXL7/8MnJycnDnzh3cuXMHd+/eRXh4OC5fvqy1zNIY8fHx8PLyQrdu3QBAnbUuISFBYynmli1bEBISonN2TCKRqPdxd3fXanvRfcyha8ay6FjOzc3FnTt30KFDBwgh1Etnb9++jf379+PVV1/V+D0qqT1JSUm4f/8+IiIi1K/znTt3YG9vj9DQUOzbt0/dBkdHR6SkpOCff/4xuV/x8fFo06aNOnmIaulh0aWBCoUC27ZtQ79+/dCmTRutOor3Y9y4cRrXjxn7e1GzZk0AwM8//4y8vDyd7VXNMH///fdQKBQm9zc0NBRJSUlaj6K/IyolvZ/t3LkT9vb2mDx5ssZxb7/9NoQQ+OmnnwAA27Ztg0KhwNy5czX6r6q3qGbNmiEsLEz93MPDA02aNNF476lVqxbOnj2Ly5cvm9x/oqqOwRURqbm6ugJQXl9kjL///ht2dnZaGde8vb1Rq1Yt/P333xrlxT/4qT7oBAQE6Cwv/kHOzs4ODRo00Chr3LgxAGhc+/Hjjz+iXbt2kMlkcHNzg4eHB1atWqVxTYVK/fr1S+omAOW1aGfOnEFAQADatm2LefPmaXwYUfW1SZMmWscGBwdrvRYymQweHh4aZbVr1y7xw6u+8zg6OqJBgwZa5zFHq1atEBwcjPXr1yM+Ph7e3t7o3r273vY0atRI6wNd06ZNNdr7999/w8fHB9WrV9fYr3g/UlNTIYTAnDlz4OHhofFQXUOmStRhLLlcjoSEBHTr1g1XrlxBamoqUlNTERoaiszMTCQnJ6v3TUtLw9NPP22wvrS0NDRp0sSi2SSrVaumce2QyrVr1zB69Gi4ubmpr83r0qULAKjHs2ocltTu4lQfnLt37671Wu/evVv9OkulUnz00Uf46aef4OXlhc6dO2PRokXIyMgo8Rz379/Hzp070aVLF/Xrnpqaio4dO+LEiRO4dOkSAGWAmJ2dbXQfiv/eGvt7Ub9+fURHR+O///0v3N3dER4ejpUrV2q8NwwdOhQdO3bEa6+9Bi8vLwwbNgzfffed0YGWu7s7evbsqfWoV6+exn7GvJ/9/fff8PX11fqHV/Hfr7S0NNjZ2Rn1j6Li78OA9nvPggULcP/+fTRu3BjNmzfHtGnT8Mcff5RYNxHxmisiKsLV1RW+vr44c+aMSccZ+996fZnK9JULHRdsl+TXX3/Fiy++iM6dO+Pzzz+Hj48PHBwcsHbtWqxfv15r/6IzA4a8/PLLCAsLw9atW7F79258/PHH+Oijj5CYmIjnn3/e5HZaK2ubsYYPH45Vq1ahRo0aGDp0qFbwVFZUH2DfeecdhIeH69zH1PTpe/fuxa1bt5CQkICEhASt7fHx8RrXpFiCvt8JfQlLpFKp1mssl8vRq1cv3Lt3DzNmzEBwcDBcXFyQnp6O0aNHmzWrUpTq+G+++Ubj2ieVosHj1KlT0a9fP2zbtg0///wz5syZg9jYWOzduxetWrXSe45NmzYhPz8fn376KT799FOt7fHx8Zg/f77JbTf291aXTz/9FKNHj8b333+P3bt3Y/LkyYiNjcWRI0fg7+8PJycn7N+/H/v27cOOHTuwa9cubNy4Ed27d8fu3btt/ne3JMa833bu3BlpaWnq1+i///0vlixZgtWrV+O1114rr6YSVUicuSIiDS+88ALS0tJw+PDhEvetV68eFAqF1tKRzMxM3L9/X+s/taWlUCi0ls2p/vOtuqh+y5YtkMlk+Pnnn/Hqq6/i+eefR8+ePS1yfh8fH7zxxhvYtm0brly5gjp16qiTBaj6evHiRa3jLl68aLHXQt95CgoKcOXKFYudZ/jw4bh16xYuXbqkd0mgqj2XL1/W+qCvyjaoak+9evVw69YtPHz4UGO/4v1Q/SffwcFB53//e/bsafL9keLj4+Hp6YlNmzZpPSIiIrB161Z19r2goKAS/7kQFBSEixcvorCwUO8+tWvXBgCtTHWmzCz++eefuHTpEj799FPMmDED/fv3R8+ePbXul6R6zUz9p4gq8Yanp6fO17lr165a+7/99tvYvXs3zpw5g4KCAp0BU1Hx8fF4+umndb72PXv2VP/Dw8PDA66urib3QcXU34vmzZtj9uzZ2L9/P3799Vekp6dj9erV6u12dnbo0aMHFi9ejHPnzmHhwoXYu3eveqmkJRjzflavXj3cvHlTazVB8d+voKAgKBQKnDt3zmLtc3Nzw5gxY7BhwwZcv34dLVq0MCrjKFFVx+CKiDRMnz4dLi4ueO2115CZmam1PS0tDcuWLQOgvCcSACxdulRjn8WLFwOAzuxypbVixQr190IIrFixAg4ODujRowcA5X9lJRKJxgzB1atXsW3bNrPPKZfLtZYUenp6wtfXV51yvk2bNvD09MTq1as10tD/9NNPOH/+vMVei549e8LR0RGfffaZxn+a16xZgwcPHljsPEFBQVi6dCliY2PRtm1bvfv16dMHGRkZ2Lhxo7rsyZMnWL58OapXr65ewtanTx88efIEq1atUu8nl8uxfPlyjfo8PT3RtWtXfPHFF7h165bW+W7fvm1SPx49eoTExES88MILGDJkiNYjKioKOTk56jTvgwcPxu+//66V6RL49z/7gwcPxp07dzTGYvF96tWrB3t7e+zfv19j++eff25021UzDEV/zkII9e+fioeHBzp37oy4uDhcu3ZNZ3t0CQ8Ph6urKz788EOdgaLqtc7Ly8Pjx481tgUFBaFGjRpat1wo6vr169i/fz9efvllna/9mDFjkJqaiqNHj8LOzg4DBgzA9u3bceLECa26SprFNvb3Ijs7G0+ePNE4tnnz5rCzs1P35d69e1r1t2zZEgAM9tccJb2f9enTB3K5XGusLVmyBBKJRD1rPmDAANjZ2WHBggVa/+gwZwXA3bt3NZ5Xr14dDRs2tHj/iSojLgskIg1BQUFYv349hg4diqZNm2LUqFF4+umnUVBQgEOHDqnTbANASEgIIiMj8eWXX+L+/fvo0qULjh07hnXr1mHAgAHq5AGWIpPJsGvXLkRGRiI0NBQ//fQTduzYgVmzZqmvX+rbty8WL16M5557DsOHD0dWVhZWrlyJhg0bmn3NQE5ODvz9/TFkyBCEhISgevXq2LNnD44fP67+z72DgwM++ugjjBkzBl26dEFERIQ6FXtgYCDeeusti7wGHh4emDlzJubPn4/nnnsOL774Ii5evIjPP/8czz77rEVvXlr0fmb6jB8/Hl988QVGjx6N3377DYGBgdi8eTMOHjyIpUuXqmeZ+vXrh44dO+Ldd9/F1atX0axZMyQmJuq8Dm7lypXo1KkTmjdvjnHjxqFBgwbIzMzE4cOHcePGDfz+++9G9+GHH35ATk4OXnzxRZ3b27VrBw8PD8THx2Po0KGYNm0aNm/ejJdeegmvvvoqWrdujXv37uGHH37A6tWrERISglGjRuF///sfoqOjcezYMYSFhSE3Nxd79uzBG2+8gf79+6NmzZp46aWXsHz5ckgkEgQFBeHHH3806Xqx4OBgBAUF4Z133kF6ejpcXV2xZcsWndflffbZZ+jUqROeeeYZjB8/HvXr18fVq1exY8cOnD59Wmf9rq6uWLVqFUaOHIlnnnkGw4YNg4eHB65du4YdO3agY8eOWLFiBS5duoQePXrg5ZdfRrNmzVCtWjVs3boVmZmZGDZsmN72r1+/Xp1KXJc+ffqgWrVqiI+PR2hoKD788EPs3r0bXbp0wfjx49G0aVPcunULmzZtwoEDBwzeDNvY34u9e/ciKioKL730Eho3bownT57gm2++gb29PQYPHgxAeb3R/v370bdvX9SrVw9ZWVn4/PPP4e/vj06dOultg0p6ejq+/fZbrfLq1atjwIAB6ufGvJ/169cP3bp1w3vvvYerV68iJCQEu3fvxvfff4+pU6eqZx8bNmyI9957D++//z7CwsIwaNAgSKVSHD9+HL6+voiNjS2x3UU1a9YMXbt2RevWreHm5oYTJ06ob0VBRCUo9/yERFQhXLp0SYwbN04EBgYKR0dHUaNGDdGxY0exfPlyjRTZhYWFYv78+aJ+/frCwcFBBAQEiJkzZ2rsI4QyBbCu1L4oltpbiH/TVRdN/RwZGSlcXFxEWlqa6N27t3B2dhZeXl4iJiZGK/XwmjVrRKNGjYRUKhXBwcFi7dq16lTjJZ276DZVKvb8/Hwxbdo0ERISImrUqCFcXFxESEiI+Pzzz7WO27hxo2jVqpWQSqXCzc1NjBgxQty4cUNjH1VfitPVRn1WrFghgoODhYODg/Dy8hITJ04U//zzj876TE3Fboiu1ywzM1OMGTNGuLu7C0dHR9G8eXONVOMqd+/eFSNHjhSurq6iZs2aYuTIkerU2MX3T0tLE6NGjRLe3t7CwcFB+Pn5iRdeeEFs3rxZvY8xqdj79esnZDKZyM3N1bvP6NGjhYODg7hz5466nVFRUcLPz084OjoKf39/ERkZqd4uhDJF+nvvvace997e3mLIkCEiLS1Nvc/t27fF4MGDhbOzs6hdu7Z4/fXXxZkzZ3SmYtc1HoQQ4ty5c6Jnz56ievXqwt3dXYwbN078/vvvOl+zM2fOiIEDB4patWoJmUwmmjRpIubMmaPeXjwVe9HXMTw8XNSsWVPIZDIRFBQkRo8eLU6cOCGEEOLOnTti0qRJIjg4WLi4uIiaNWuK0NBQ8d133+l9TYUQonnz5qJu3boG9+natavw9PQUhYWFQggh/v77bzFq1Cjh4eEhpFKpaNCggZg0aZLIz8/X6IOudO1ClPx78ddff4lXX31VBAUFCZlMJtzc3ES3bt3Enj171PskJyeL/v37C19fX+Ho6Ch8fX1FRESEuHTpksG+CGE4FXu9evXU+5nyfpaTkyPeeust4evrKxwcHESjRo3Exx9/rJFiXSUuLk79/lO7dm3RpUsX9e0HVO3T9T7cpUsX0aVLF/XzDz74QLRt21bUqlVLODk5ieDgYLFw4UJRUFBQ4mtAVNVJhDBjvpiIqJyNHj0amzdv1rpmh4ioouH7GVHlxWuuiIiIiIiILIDBFRERERERkQUwuCIiIiIiIrIAXnNFRERERERkAZy5IiIiIiIisgAGV0RERERERBbAmwjroFAocPPmTdSoUQMSicTazSEiIiIiIisRQiAnJwe+vr6wszM8N8XgSoebN28iICDA2s0gIiIiIiIbcf36dfj7+xvch8GVDjVq1ACgfAFdXV3L/HyFhYXYvXs3evfuDQcHhzI/H1UOHDdkLo4dMgfHDZmD44bMZUtjJzs7GwEBAeoYwRAGVzqolgK6urqWW3Dl7OwMV1dXqw8eqjg4bshcHDtkDo4bMgfHDZnLFseOMZcLMaEFERERERGRBTC4IiIiIiIisgAGV0RERERERBbAa67MJITAkydPIJfLS11XYWEhqlWrhsePH1ukPqoajBk39vb2qFatGm8pQERERFQOGFyZoaCgALdu3UJeXp5F6hNCwNvbG9evX+eHYDKasePG2dkZPj4+cHR0LMfWEREREVU9DK5MpFAocOXKFdjb28PX1xeOjo6lDogUCgUePnyI6tWrl3hjMiKVksaNEAIFBQW4ffs2rly5gkaNGnF8EREREZUhBlcmKigogEKhQEBAAJydnS1Sp0KhQEFBAWQyGT/8ktGMGTdOTk5wcHDA33//rd6XiIiIiMoGP8mbiUEQVRQcq0RERETlg5+6iIiIiIiILIDBFRERERER2Q65HDhwQPn9gQPK5xUEgyuymJSUFEgkEty/f9/oYwIDA7F06dIyaxMRERERlUAuB1JSgA0blF+LBjOGtpXFORMTgcBAoG9f5fO+fZXPExMtc94yxuCqihg9ejQkEgkmTJigtW3SpEmQSCQYPXp0+TesBGfPnsXgwYMRGBgIiURiciAWHBwMqVSKjIyMsmkgERERUUVQUjDTrRswfLjyqyqYMbStNPTVO306MGQIcOOG5v7p6cryChBgMbiykrL6J4AhAQEBSEhIwKNHj9Rljx8/xvr161G3bt2yb4AZ8vLy0KBBA/znP/+Bt7e3ScceOHAAjx49wpAhQ7Bu3boyaqHxCgsLrd0EIiIishXl+WHQnGBm8GDlw1CgY04fEhN1n/PGDeDjjwEhtI9RlU2davNLBBlcWUHx8d2jhx1atHAt82D8mWeeQUBAABKLnCgxMRF169ZFq1atNPbNz8/H5MmT4enpCZlMhk6dOuH48eMa++zcuRONGzeGk5MTunXrhqtXr2qd88CBAwgLC4OTkxMCAgIwefJk5ObmGt3mZ599Fh9//DGGDRsGqVRqUn/XrFmD4cOHY+TIkYiLi9PafuPGDURERMDNzQ0uLi5o06YNjh49qt6+fft2PPvss5DJZHB3d8fAgQPV2yQSCbZt26ZRX61atfD1118DAK5evQqJRIKNGzeiS5cukMlkiI+Px927dxEREQE/Pz84OzujefPm2LBhg0Y9CoUCixYtQsOGDSGVSlG3bl0sXLgQANC9e3dERUVp7H/79m04OjoiOTnZpNeHiIioSimLYMbcOkuaETJ3mZ6ubaUJZnRRbRs/vuRZreLtKSgApkwxXL+h816/Dvz6q+nHliPe56qcqcZ38TF165YEL78MbN4MDBpUdud/9dVXsXbtWowYMQIAEBcXhzFjxiAlJUVjv+nTp2PLli1Yt24d6tWrh0WLFiE8PBypqalwc3PD9evXMWjQIEyaNAnjx4/HiRMn8Pbbb2vUkZaWhueeew4ffPAB4uLicPv2bURFRSEqKgpr164tu04CyMnJwaZNm3D06FEEBwfjwYMH+PXXXxEWFgYAePjwIbp06QI/Pz/88MMP8Pb2xsmTJ6FQKAAAO3bswMCBA/Hee+/hf//7HwoKCrBz506T2/Huu+/i008/RatWrSCTyfD48WO0bt0aM2bMgKurK3bs2IGRI0ciKCgIbdu2BQDMnDkTX331FZYsWYJOnTrh1q1buHDhAgDgtddeQ1RUFD799FM4ODgAAOLj4+Hn54fu3btb4qUjIiIqH3K58oPyrVuAjw8QFgbY25fNuRITlR/qiwYY/v7AsmXmf/Aypk5dffz+e90fBlUzQu+8owxGdNUL6D+nrm1+fsDjx+YFM4YIAdy9q12u6sPmzbrb4+4O3LlTunPfulW648uaIC0PHjwQAMSDBw+0tj169EicO3dOPHr0yOR6nzwRwt9fCOWI1H5IJAoREKDcz9IiIyNF//79RVZWlpBKpeLq1avi6tWrQiaTidu3b4v+/fuLyMhIIYQQDx8+FA4ODiI+Pl59fEFBgfD19RWLFi0SQggxc+ZM0axZM41zzJgxQwAQ//zzjxBCiLFjx4rx48dr7PPrr78KOzs79etXr149sWTJEqP6YMq+X375pWjZsqX6+ZQpU9T9E0KIL774QtSoUUPcvXtX5/Ht27cXI0aM0Fs/ALF161aNspo1a4q1a9cKIYS4cuWKACCWLl1aYlv79u0r3n77bSGEENnZ2UIqlYqvvvpK576PHj0StWvXFhs3bhRyuVz8888/okWLFmLevHl66y/NmKXKqaCgQGzbtk0UFBRYuylUgXDcVGFPngixb58Q69crv5rwQUXvuNmyRftDkb+/srykc5rani1bhJBIdH3wUj5U5zSl/8bUqauPfn5C1Kmj/8Og/g+J5m2zxkMiUfaxlO0qcHJSjh0nJ81t+/YZPf4sxVBsUBxnrsrRr79qz8gWJYREPdvZtWvZtMHDwwN9+/bF119/DSEE+vbtC3d3d4190tLSUFhYiI4dO6rLHBwc0LZtW5w/fx4AcP78eYSGhmoc1759e43nv//+O/744w/Ex8ery4QQUCgUuHLlCpo2bWrp7qnFxcXhlVdeUT9/5ZVX0KVLFyxfvhw1atTA6dOn0apVK7i5uek8/vTp0xg3blyp29GmTRuN53K5HB9++CG+++47pKeno6CgAPn5+XB2dgagfF3z8/PRo0cPnfXJZDL1MschQ4bg999/x5kzZ/DDDz+Uuq1ERERaymrGpyxmbgYN0p4p6tBB/zI0IQCJRHkdzwsvAIcOac+i6eq/odkgVZ3jxwP37unuozmMWaZnK/TNapWWRKL8Wf//KiRbxeCqHBk7i1nWs52vvvqq+rqdlStXltl5Hj58iNdffx2TJ0/W2laWCTTOnTuHI0eO4NixY5gxY4a6XC6XIyEhAePGjYOTk5PBOkraLpFIIIq9melKWOHi4qLx/OOPP8ayZcuwdOlSNG/eHC4uLpg6dSoKCgqMOi+gXBrYsmVL3LhxA/Hx8ejWrRvq1atX4nFEREQmKSkI2rwZ6N/ftKV9crnhYAdQXgdUnCrBgi6GgrKSlqGpruPx9wdu3/633N8fiIgAPvnE9ACprIKLqkQi0f186dKyWzpqIUxoUY58fCy7n7mee+45FBQUoLCwEOHh4Vrbg4KC4OjoiIMHD6rLCgsLcfz4cTRr1gwA0LRpUxw7dkzjuCNHjmg8f+aZZ3Du3Dk0bNhQ6+Ho6FgGPVNas2YNOnfujN9//x2nT59WP6Kjo7FmzRoAQIsWLXD69Gncu3dPZx0tWrQwmCDCw8MDt4pEwZcvX0ZeXl6JbTt48CD69++PV155BSEhIWjQoAEuXbqk3t6oUSM4OTkZPHfz5s3Rpk0b/Pe//8XmzZsxZsyYEs9LRESViLlJFEw5zpggqKSEBrpuBFvSMh59Spq5EUIZlBWv29jre4oGVoDhZA+Vkb5gRte2smyDRAJMm6acHSzK37/sExNYCIOrchQWphwb+saoRCIQEFD2s5329vY4f/48zp07B3sd0b+LiwsmTpyIadOmYdeuXTh37hzGjRuHvLw8jB07FgAwYcIEXL58GdOmTcPFixexfv16daY8lRkzZuDQoUOIiorC6dOncfnyZXz//fda2e4MKSgoUAdHBQUFSE9Px+nTp5Gamqpz/8LCQnzzzTeIiIjA008/rfF47bXXcPToUZw9exYRERHw9vbGgAEDcPDgQfz111/YsmULDh8+DACIiYnBhg0bEBMTg/Pnz+PPP//ERx99pD5P9+7dsWLFCpw6dQonTpzAhAkT1AkmDGnUqBGSkpJw6NAhnD9/Hq+//joyMzPV22UyGWbMmIHp06fjf//7H9LS0nDkyBF1UKjy2muv4aOPPoIQQiOLIRERlQFr3D9FH3OzzJl6XEpKSdcyKGdn9KXpnj5d941gv//eAi8CWURJwcyWLcqHrm116pQ+6PLw0K5382Zg0SLg6lVgxw5l+Y4dwJUrFSKwAsCEFrqUVUILIf69/rH4NX4SiUJIJIoSr6k0lyqhhT5FE1oIoeznm2++Kdzd3YVUKhUdO3YUx44d0zhm+/btomHDhkIqlYqwsDARFxenkdBCCCGOHTsmevXqJapXry5cXFxEixYtxMKFC9XbS0pSoUoMUfzRpUsXnftv3rxZ2NnZiYyMDJ3bmzZtKt566y0hhBBXr14VgwcPFq6ursLZ2Vm0adNGHD16VL3vli1bRMuWLYWjo6Nwd3cXgwYNUm9LT08XvXv3Fi4uLqJRo0Zi586dOhNanDp1SuP8d+/eFf379xfVq1cXnp6eYvbs2WLUqFEaPxu5XC4++OADUa9ePeHg4CDq1q0rPvzwQ416cnJyhLOzsxg7dqyQy+V6Xz8hmNCCtDExAZmjyo6bkhIvGFJS4gVLJ2aYNk13W6dNM/04NzeLJDfQSEpga4kXqspDlWCi+M84IMD8BCL6P9AqvxpKaCGRKM+dn29w/NvSe44pCS1QDu2pcMoyuBJCX/IYudi0yfCHZCKVK1euCDs7O5GSksLgikxmS3+wqOKokuPGmIxw+j6UlhSUmRq0lZRy2EYfOjO+2dtX3kDL2OBCX7CjCoT1BS3FvzdmW0ljtTS/H/oCNkPBlzEZGoVtvecwW6CNGzRI8/pPLy8FQkKyUbu2q7WbRjausLAQd+/exezZs9GuXTuEhIRYu0lERLbF3HsnmZNlbvx43ZnrDCVCUCVeMLRdV6IIudy8a5VskWqpokSi+/UtKx4emtdWFX9uCokEcHMDnJy0f/5Llyq/HzJEu4+qpXRffqk/GUi7drrHlapec7apltRZMh118Q+0xX/fNm8uuT2VEIMrK7G3/3d8KxRAdrZVm0MVxMGDB9GtWzc0btwY3333nbWbQ0RkW0pKG64v8NJ1nDFZ5nRlhFMlQtB3DAAsXmx60Kbn1iEV1tSpyg/fRfsYEAAMG6YMPAHtoET13NSgTJXCOzVVM916hw5AUJAyqDVUnzkBEmBccKEr2CkpaDF3W1ko+oHW1H5UUgyuiCqQrl27Qvz/G7xCoUA2o3IiqqhKmmEytF3Xtu+/N+/eSfpmmYzNMmcOQwkx9AVterLblgs3N83z+/sDjx7pvo+Tsfr3V77ulpi5KSkoA5THOjpqBwLLlhmeYdI3boyZDSpNcGEoaDF3mzXYWnvKAYMrIiIiMn85nTlKmmEytB0w/aaugO7ZJEOzTPSv775TjgVdway5s0iq8WXJmRtDQZm+ZWiDBpU8wxQba/7vRhUMLqo6BldERERVnTHL6Yrer6hzZ/OuYzJ2hknf9UiGbiJLJTM3EOraVfvnrS8oKT6LVLw+wLgbwZozO2PuTFFJxzFAIhMwuCIiIrJFZTGTVJrldHfvKr/27au8x4051zEZM8Nk6HqkysLeXnnBtbl9Kr5Mz5hlcbqWtxm7nE7fuDMUlKhmkYoucSyPZAbmBkIMoMhCGFwRERHZGnMTMwCWD3ZUy+acnP7dZu51TMbMMFnzBr36FM8qZ+w1R/qu44mOVr4+5mbL07VMz5hlcfqWt5mznE6lpFmk/fuVWbt27DB+xpOoAmNwRUREZEsSE81LzKDveqTSBju6VJXrmPRlmTN0zZGxiRD0BTSGgjZDy/QA85e3lVVWN3t7oFMnYOdO5VcGVlQFMLgiIiKyBl0zTIDheysBugMXQ9cjVaZgpyzpC5L0ZZkrbSIEfQFNSUFbSdcrcVkckVXZWbsBVHmkpKRAIpHg/v37Rh8TGBiIparUqkRElY1cDqSkKGcwUlL+XfKWmAgEBgLdugHDhyu/BgYCCxead6PYynY9kiqQMIVEorwWzN9fszwgAJg2Tbm9eL2qsmnTlEski/L3VwZPhpbFDRoEXL0K7NsHrF+v/Hrlyr/HqAKWiAjds026tquCNnPaQ0RWx+Cqihg9ejQkEgkmTJigtW3SpEmQSCQYPXp0+TesBF999RXCwsJQu3Zt1K5dGz179sSxY8eMOvbRo0dwc3ODu7s78vPzy7ilRETF6Augpk9XzkwUD6LS04GYGGu01DaoAp3o6H+fF9+u6/uiz7/8Unews2iR4YBl0SLDQZIhJQVQ5igpaCMim2X14GrlypUIDAyETCZDaGiowQ/OhYWFWLBgAYKCgiCTyRASEoJdu3Zp7DNv3jxIJBKNR3BwcFl3w3T6/ptZhgICApCQkIBHjx6pyx4/foz169ejbt26ZX5+c6SkpCAiIgL79u3D4cOHERAQgN69eyPdiOsEtmzZgqeeegrBwcHYtm1b2TfWACEEnjx5YtU2EFEJLPm+rLpuqngApVqiV9mz4Rmib4apaKCjLxDaskX5MDSroy/YKe0sU3mztfYQkVGsGlxt3LgR0dHRiImJwcmTJxESEoLw8HBkZWXp3H/27Nn44osvsHz5cpw7dw4TJkzAwIEDcerUKY39nnrqKdy6dUv9OKC6N4etKPbfTLsePeDaooWyvAw988wzCAgIQGKR8yQmJqJu3bpo1aqVxr75+fmYPHkyPD09IZPJ0KlTJxw/flxjn507d6Jx48ZwcnJCt27dcPXqVa1zHjhwAGFhYXByckJAQAAmT56M3Nxco9scHx+PN954Ay1btkRwcDD++9//QqFQIDk5ucRj16xZg1deeQWvvPIK1qxZo7X97NmzeOGFF+Dq6ooaNWogLCwMaWlp6u1xcXF46qmnIJVK4ePjg6ioKADA1atXIZFIcPr0afW+9+/fh0QiQUpKCoB/l0j+9NNPaN26NaRSKQ4cOIC0tDT0798fXl5eqF69Op599lns2bNHo135+fmYMWMGAgICIJVK0bBhQ6xZswZCCDRs2BCfFLt3yOnTpyGRSJCammrsy0pUuRkKkkxdpqd6vzQl8JLL9V83ZYvMWU5nSt26nuubYVIFOoYCodLM6jBgIaKyJqyobdu2YtKkSerncrlc+Pr6itjYWJ37+/j4iBUrVmiUDRo0SIwYMUL9PCYmRoSEhJSqXQ8ePBAAxIMHD7S2PXr0SJw7d048evTIvMq3bBFCIhFC+WdX/VBIJEIhkSi3l4HIyEjRv39/sXjxYtGjRw91eY8ePcSSJUtE//79RWRkpLp88uTJwtfXV+zcuVOcPXtWREZGitq1a4u7d+8KIYS4du2akEqlIjo6Wly4cEF8++23wsvLSwAQ//zzjxBCiNTUVOHi4iKWLFkiLl26JA4ePChatWolRo8erT5PvXr1xJIlS4zuR3Z2tpDJZGL79u0G90tNTRVSqVTcu3dP3L17V8hkMnH16lX19hs3bgg3NzcxaNAgcfz4cXHx4kURFxcnLly4IIQQ4vPPPxcymUwsXbpUXLx4URw7dkzdzitXrggA4tSpU+r6/vnnHwFA7Nu3TwghxL59+wQA0aJFC7F7926Rmpoq7t69K06fPi1Wr14t/vzzT3Hp0iUxe/ZsIZPJxN9//62u6+WXXxYBAQEiMTFRpKWliT179oiEhAQhhBALFy4UzZo1E0Iof1/++ecf8eabb4rOnTvrfS1KPWap0ikoKBDbtm0TBQUF1m6K5W3ZIoS/v+Z7rL+/slzftmnTdL4vC4lE+Zg2TX+dQgjx5IkQ+/YJsX698uuePdp1mfPQ1SZL16nq45Yt2v148kTjNS1wclKOGycnIQIC/n3d9NWp63ULCCizv3Nkmyr1+w2VKVsaO4Zig+Ksli2woKAAv/32G2bOnKkus7OzQ8+ePXH48GGdx+Tn50Mmk2mUOTk5ac1MXb58Gb6+vpDJZGjfvj1iY2MNLnvLz8/XuCYnOzsbgHIZYmFhoca+hYWFEEJAoVBAoVAY11kVuRyS//9vZvH/AUqEgJBIIKZOhejXz+L/TRNCQAiB4cOHY+bMmbhy5QoA4ODBg1i/fj327dun7ldubi5WrVqFuLg4hIeHAwC++OILJCUl4b///S/eeecdfP755wgKCsLH/5+BqlGjRvjjjz+waNEi9Wvz4YcfYvjw4Zg8eTIAICgoCEuXLkW3bt2wcuVK9c9SdV5jTJ8+Hb6+vujevbvBY9asWYPnnnsONWvWBAD07t0bcXFxiPn/6xlWrFiBmjVrYv369XBwcAAANGzYEACgUCjwwQcfIDo6Gm+++aa6ztatW2v83It/X7RM9XzevHno0aOHuo5atWqhefPm6ufz58/H1q1b8f3332PSpEm4dOkSvvvuO/z888/o2bMnAGXSD1Xdo0aNwty5c3HkyBE8++yzKCwsxIYNG9Svuy4KhQJCCBQWFsKe/6UlQP2+Vvz9zebI5cDhw0BGBuDtDbRvb/i9cft2YORI5cf4ovdkuncPeOWVf58X3Xb3LrBiBVDsb4uGFSu0j7t3T3mu48eVy9GKLlWuXVtzX3PMmgWsW6dZr7+/MiPgZ58pnxfPJKd6ri/L3OTJ2m319wf+8x+gXz/lTW07dvx3m0KhfPTrB/Tpg8JDh4CcHBT++CPQocO/90eaMUN/nQsW6P4Z2vrYI4upMO83ZHNsaeyY0garBVd37tyBXC6Hl5eXRrmXlxcuXLig85jw8HAsXrwYnTt3RlBQEJKTk5GYmAh5keUZoaGh+Prrr9GkSRPcunUL8+fPR1hYGM6cOYMaNWrorDc2Nhbz58/XKt+9ezecnZ01yqpVqwZvb288fPgQBQUFJvW52oEDqG4gC5RECOD6deT+/DOedOpkUt0lKSwsxJMnTyCVStG7d298+eWXEEKgd+/ecHR0xJMnT1BYWIjs7GycOXMGhYWFaNGihTrQBIBWrVrhjz/+QHZ2Nv7880+0atVKY3tISAgAICcnB3Z2djh16hTOnj2L9evXq/dRBVJ//vknmjRpAoVCgcePH2vUo8+SJUuQkJCA7du3o6CgQO/rL5fLsW7dOsTGxqrrHTRoEObMmYMpU6bAzs4OJ06cQGhoKB49eqRxDRoA3L59Gzdv3kS7du10tuvhw4cAgNzcXPX2nJwcAEBeXh6ys7ORl5cHAGjSpIlGHQ8fPsRHH32E3bt3IyMjA3K5HI8ePcLly5eRnZ2Nw4cPw97eXuu1ValevTp69+6NL774AsHBwdi1axfy8/MRHh6u9zUsKCjAo0ePsH//fl73RRqSkpKs3QTjODsrb0L688+G97O3Vy4TK2/t2pVNvcWWAFvkfPqO3bnT6CqScnL+/VnY2+tvZ9E6jf0ZUqVVYd5vyObYwthRfa4zRoW6z9WyZcswbtw4BAcHQyKRICgoCGPGjEFcXJx6n+eff179fYsWLRAaGop69erhu+++w9ixY3XWO3PmTESrshNBOXOlSpzg6uqqse/jx49x/fp1VK9eXWsWrUQPHhi1m/ODB0Cx85aWg4MDqlWrBldXV4wbN049m7R8+XK4urqiWrVqcHBwgKurK6pXrw4AqFGjhkb/i+5T9HsVp///T63quEePHmH8+PEasz8qdevWhaOjI+zs7CCTybRe5+I+/fRTLFu2DLt370abNm0M7rtz507cvHkTr776qka5XC7H8ePH0atXL9SoUUOr/SqS//8vr7Ozs87tqrKi21Uzn6oyVVDu7e2tUceMGTOwZ88eLFq0CA0bNoSTkxNefvllSCQSuLq6ws3NTX0O1Yxaca+//joiIyOxfPlyxMfH4+WXX4a3t7fe1+Px48dwcnJC586dTR+zVCkVFhYiKSkJvXr10jvOyoW+mamiM1BFqWZgvvkG6NNH81i5HHjxxfLvQ2nom2H65hvlrI8hhmb1TJ3xM5LNjBuqUDhuyFy2NHaMmQRQsVpw5e7uDnt7e2RmZmqUZ2Zm6v2g6OHhgW3btuHx48e4e/cufH198e6776JBgwZ6z1OrVi00btzY4MX+UqkUUqlUq9zBwUHrhymXyyGRSGBnZwc7OxPzgRTPbqSHnZ8fYGrdJVBlTrSzs0OfPn0wYcIESCQSPP/887Czs9PY3qhRIzg6OuLw4cOoX78+AOUAP3HiBKZOnQo7Ozs0a9YMP/zwg8ZroMr0qHptnnnmGZw/fx6NGzcusW2GXstFixZh4cKF+Pnnn9G2bdsS+7p27VoMGzYM7733nkb5woULsXbtWoSHhyMkJATr1q2DXC7X+hnXrFkTgYGB2Ldvn8aSPhXVbGtmZqa63X/88YdG31XlxcfJoUOHMHr0aAz+/5t9Pnz4EFevXkXXrl1hZ2eHkJAQKBQK/Prrr+plgcW98MILcHFxwRdffIHk5GSkpKQYfP1UP19d45mqNouNCV03wy36QV/XtsRE3TdfXbxYmYpb338JJRJg/Hjlsruix7q5AcVmoW2SKoB65x1lcoyifQgI+PfmsyVxcFAm3TB1mwXwvYTMwXFD5rKFsWPK+a0WXDk6OqJ169ZITk7GgAEDAECdBU6VlU0fmUwGPz8/FBYWYsuWLXj55Zf17vvw4UOkpaVh5MiRlmy+ecLClB8e0tN1ZpESEgkk/v7K/cqQvb09zp8/r/6+OBcXF0ycOBHTpk2Dm5sb6tati0WLFiEvL089+zdhwgR8+umnmDZtGl577TX89ttv+PrrrzXqmTFjBtq1a4eoqCi89tprcHFxwblz55CUlIQVqmsYSvDRRx9h7ty5WL9+PQIDA5GRkQFAuTxONcNW1O3bt7F9+3b88MMPePrppzW2jRo1CgMHDsS9e/cQFRWF5cuXY9iwYZg5cyZq1qyJI0eOoG3btmjSpAnmzZuHCRMmwNPTE88//zxycnJw8OBBvPnmm3ByckK7du3wn//8B/Xr10dWVhZmz55tVH8aNWqExMRE9OvXDxKJBHPmzNG4ViowMBCRkZF49dVX8dlnnyEkJAR///03srKy1OPc3t4eo0ePxqxZsxAUFIT27dsbdW6iMqEvSFq2TPm9rm0REcqlZMXfB9PTAQPv5wCUx9y9q11+75557S9rbm6abfP3/zeAio3VH5QSEVHFVIaJNUqUkJAgpFKp+Prrr8W5c+fE+PHjRa1atURGRoYQQoiRI0eKd999V73/kSNHxJYtW0RaWprYv3+/6N69u6hfv746O50QQrz99tsiJSVFXLlyRRw8eFD07NlTuLu7i6ysLKPbVS7ZAotlVyqvbIH6FM8W+OjRI/Hmm28Kd3d3IZVKRceOHcWxY8c0jtm+fbto2LChkEqlIiwsTMTFxWlkCxRCiGPHjolevXqJ6tWrCxcXF9GiRQuxcOFC9faSsgXWq1dPANB6xMTE6Nz/k08+EbVq1dKZWSY/P1/UqlVLLFu2TAghxO+//y569+4tnJ2dRY0aNURYWJhIS0tT77969WrRpEkT4eDgIHx8fMSbb76p3nbu3DnRvn174eTkJFq2bCl2796tM1tg0ddCCGWmwW7dugknJycREBAgVqxYIbp06SKmTJmi8dq/9dZbwsfHRzg6OoqGDRuKuLg4jXrS0tIEADF//nwhl8v1vn6q+pgtkIoyKwOTrkxyerKflkmWu/J4lEW79+zRnYGvArKlzF1UcXDckLlsaeyYki3QqsGVEEIsX75c1K1bVzg6Ooq2bduKI0eOqLd16dJF4wN/SkqKaNq0qZBKpaJOnTpi5MiRIj09XaO+oUOHqj+U+vn5iaFDh4rU1FST2lSmwZUQOtMBy/38hHzTJvPrpCpl//79wsHBQVy8eJHBFZlM7x+sElJxawQNfn5C1Klj/YDI1KDJ1LTh+tKNG3POgIAKHUwVZ0sfdKji4Lghc9nS2KkQqdhVoqKi9C4DVN2QVaVLly44d+6cwfoSEhIs1bSyM2gQ0L+/ejmIwssL2SEhcK1d29otIxuXn5+P27dvY968eRgyZAg8PT2t3SSqaORyQHX7igMHgM6dDV8DZWgJn63StxQP0N3HkpbptWunfVxAADBs2L+Z8oq+PqrrqpYu5TI/IqIqxurBVZWluks8oLyPiAlZSKjq2rBhA8aOHYuWLVtqXeNGpFZSEom7d5XJFPr2BerU0R9A3bgB/P+97KyieDY9Y333nbK/uq5lKvKPLa1tRd+Xiyr2D7ESA6+iARsREVUpDK6IKpDRo0dj9OjRAJQJYExJDUqVjKlZ+IoGUEVvcGvtAKo4ieTfrIFvvaXdj0ePlLNSuoIu1bFdu+qfMdIXQJXEnMCLiIiqHAZXREQVjalL+GwtgFLRd58n1azPwIHaQcv33wNDhhg+trwDG3MDNiIiqnQYXJlJmLNUhcgKOFatzNA9oMw5LjFRGVzYcgBVNPDRFwTpus9T8eV0uoKWQYOAzZu5FI+IiGwSgysTqW4ilpeXB6eiS2uIbFTe/9+M1do34KvUTF2it2yZMggw9wa7thIwSyTK5BHFb+hb2gQSJeFSPCIislEMrkxkb2+PWrVqISsrCwDg7OwMieo/sWZSKBQoKCjA48ePYWdnZ4lmUhVQ0rgRQiAvLw9ZWVmoVauWzhtGkwlKc41TUenpypknfTM3pbnBblnSNwP15ZeGAx1zEkgYg0vxiIjIBjG4MoO3tzcAqAOs0hJC4NGjR3Bycip1oEZVh7HjplatWuoxS2ay5DVOqn11bTfmuLJUmiV8+gIdBkFERFSFMLgyg0QigY+PDzw9PVFYWFjq+goLC7F//3507tyZS7fIaMaMGwcHB85YmULX7JQqgYItX+NkiuKBuLEBlLlL+IiIiKoQBlelYG9vb5EPrvb29njy5AlkMhmDKzIax40B5iSR0DU75ecHPH5sO9c4GaukGai7d//dZmwAxRkoIiKiEjG4IqLKxZwkEvpmp9LTy7ftpjB3CV9sLLB/v/LG5Tt2AJ07M4AiIiKyEAZXRGQ95qYp10dfmnJDSSQq0uxUSTfYNXYGqlMnYOdO5Vcu7SMiIrIYBldEZB0lzTCZSi5X1qcrSDKURKKsZ6eKzzCZe5wxN9jlDBQREZFVMbgiovJX0gzT5s0l38eo+KyXXK4ZqJUnU5boBQQAw4YpswwClr3BLhEREVkVgysiKlvFg6AOHQzPMEkkwPjxhme1dM16ubmVfV+KKm2WvXbtyuYGu0RERGQ1DK6IqOzoCoLc3YE7d/QfI4RmNjuVotdN6bq31L17lmlzcRKJMnBzcjL/GiddM0yDBpXdDXaJiIjIKhhcEVHZ0Lf0z1BgZYiqnsWLyy/5hGp26ssvyyYQYgBFRERUqTC4IiLLM5RcwhJ1W5oxs1MAAyEiIiIyiMEVEf3LUqnRf/3VesklAGWgVHSZoDFJJEqanSIiIiIqAYMrIlIy5+a7qsCj+DZr33z3u++UbTM1iQTA2SkiIiIyG4MrIjLv5ruqwAvQnbTCGB4ewO3bmnU+eqScddK3pNDeHlAodG9X3WS3a1fdM04lJZEgIiIiKgUGV0RVXWluvjt4sO46S0paoQqCUlOBQ4c0A53vv1cGdPruHRUdrVzeZ+gmu4aCJSaRICIiojJiZ+0GEJGVmXt9lLHJKlRBT/HnS5cCjo7KQCci4t/ZpkGDlDcR9vPTPM7fX1m+aJHh7arlfURERETljDNXRJWRLV0f5e6uvfSv6DVOupS0fI/L+4iIiMgGMbgiqmwMJaYAzL8+ylxLlihnmUwNgkpavsflfURERGRjGFwRVSaGElOYe31Uafn5MQgiIiKiKoHXXBFVRHI5cOCA8vsDB5TPjUlMUZLi10eVhkSivL9UWJjl6iQiIiKyYQyuiGyVXA6kpChToKekKJ8DytmpwECgb1/l8759lc8XLiz9jXuLLxEMCACmTVMGSvoSUxT/vujzkjL3EREREVUiXBZIZIv0XTcVEaFMQy4E4OT077b0dCAmpvTn1Xd9lKGb7wIl35iXiIiIqApgcEVka/RdN3Xjhu77TQHGL/srib7ro0rKzsfMfUREREQMroisqnha9A4d9F83VZZUN/U1dH2Uoex8zNxHRERExOCKqMzpu+eUrqV/7u6Wyd4nkWgGaEWf69oG8PooIiIiolJiQguisqRKPtGtGzB8uPJrYCAwfbpy6V/xBBSWCKzmz1cu7yvK3x/YskX50LVt82ZeH0VERERUSpy5Iior5lw7VRqqpX3vvad88PooIiIionLF4IrIWPqW9+nbtyyvnTIm9TmvjyIiIiIqV1wWSGQMfcv7EhN17//rr6W/51RxqntNTZvGpX1ERERENogzV0Ql0be8Lz1dWb55s/ZSu/T00p/XwwO4ffvf50XvHRUbC+zfD2RnAzt2AJ07c2kfERERkZUxuCIyxNDyPiGUM0njx+vO+mcu1bVTqanAoUO6lyHa2wOdOgE7dyq/MrAiIiIisjoGV0SGlLS8Twjg7l3tcmOz/hlKi+7oyGujiIiIiCoQXnNFZMitW6WvQ1fyCV47RURERFTpcOaKyBAfn9LX4e5u+NoppkUnIiIiqhQYXBEVVTzdeocOymAoPd38tOpLlihnqPRdO8Wlf0RERESVAoMrqnr03a8qMVE7MYW/PxARAXzyifb1Ucby82MARURERFQFMLiiqqWkAEpXuvVPPgHeeQfYsEH7uEePgHv3dAddqqx/YWFl0xciIiIisikMrqhy0jU79f33uu9XdeMG8PHHuutRpVtPSADS0rRTo6vqNJT1j9dQEREREVUJDK6o8tE1O+XnBzx+bN6yPiGA69eVgVXx5X2DBimz++maDVMlrSAiIiKiKsHqqdhXrlyJwMBAyGQyhIaG4tixY3r3LSwsxIIFCxAUFASZTIaQkBDs2rWrVHVSJZOYqJxJKn5vqvR03fejMoW+tOyDBgFXrwL79gHr1yu/XrnCwIqIiIioirFqcLVx40ZER0cjJiYGJ0+eREhICMLDw5GVlaVz/9mzZ+OLL77A8uXLce7cOUyYMAEDBw7EqVOnzK6TKii5HEhJUV4HlZKifC6XK2eQzM3qVxJDadlVWf8iIpRfuRSQiIiIqMqxanC1ePFijBs3DmPGjEGzZs2wevVqODs7Iy4uTuf+33zzDWbNmoU+ffqgQYMGmDhxIvr06YNPP/3U7DqpAkpMBAIDgW7dgOHDlV8DA4GFC7VnrCxBIgECApiYgoiIiIgMsto1VwUFBfjtt98wc+ZMdZmdnR169uyJw4cP6zwmPz8fMplMo8zJyQkHDhwwu05Vvfn5+ern2dnZAJTLEAsLC03vnIlU5yiPc1V427cDI0cqZ6ecnP4tv3cP+M9/NMvMYSgxhUKhfNgIjhsyF8cOmYPjhszBcUPmsqWxY0obrBZc3blzB3K5HF5eXhrlXl5euHDhgs5jwsPDsXjxYnTu3BlBQUFITk5GYmIi5HK52XUCQGxsLObPn69Vvnv3bjg7O5vaNbMlJSWV27kqLHt75XVN1rBzp3XOWwKOGzIXxw6Zg+OGzMFxQ+ayhbGTl5dn9L4VKlvgsmXLMG7cOAQHB0MikSAoKAhjxowp9ZK/mTNnIjo6Wv08OzsbAQEB6N27N1xdXUvb7BIVFhYiKSkJvXr1goODQ5mfr8I6cADo29f84yUSoHZt5exWevq/5f7+ylmvfv2U120dPgxkZADe3kD79jZ7/RTHDZmLY4fMwXFD5uC4IXPZ0thRrWozhtWCK3d3d9jb2yMzM1OjPDMzE97e3jqP8fDwwLZt2/D48WPcvXsXvr6+ePfdd9GgQQOz6wQAqVQKqVSqVe7g4FCuP8zyPl+Fk5GhvGmvMfQt7/vmG6B/f+17YKkCKAcH5TVcFQjHDZmLY4fMwXFD5uC4IXPZwtgx5fxWS2jh6OiI1q1bIzk5WV2mUCiQnJyM9u3bGzxWJpPBz88PT548wZYtW9C/f/9S10k2qHhGQE9P446bP195X6ui/P2V96MaNIiZ/YiIiIioTFh1WWB0dDQiIyPRpk0btG3bFkuXLkVubi7GjBkDABg1ahT8/PwQGxsLADh69CjS09PRsmVLpKenY968eVAoFJg+fbrRdVIFoe9GwHXqKJNX6Eq3LpEog6j33lM+9M1OERERERGVAasGV0OHDsXt27cxd+5cZGRkoGXLlti1a5c6IcW1a9dgZ/fv5Nrjx48xe/Zs/PXXX6hevTr69OmDb775BrVq1TK6TqoAVDcCLh5A3bz5b5mhrH6qIKpr17JuKRERERGRmtUTWkRFRSEqKkrntpSUFI3nXbp0wblz50pVJ9kYuVxzhqlDB/03AhZCGUS5uSmTUhSd1fL3VwZWgwaVW9OJiIiIiIqyenBFVUDxAEq1RE/X0j93d+DOHf11CQHcvQvs2aOsg8v+iIiIiMhGMLiisqUrgPL3VyaT+OQT7RkqQ4FVUVlZyjqIiIiIiGwEgysqO/qunbpxA/j449LV7eNTuuOJiIiIiCyMwRWVDblc/7VTpaHKCBgWZtl6iYiIiIhKyWr3uaJK7tdfNZcCWoKujIBERERERDaCM1dkGcWTVqSnl75ODw/g9u1/nzMjIBERERHZMAZXZDxTs/6ZS7X0LzUVOHSIGQGJiIiIqEJgcEXGKausf4ZuBuzoyBsBExEREVGFwWuuSJNcDqSkABs2KL/K5f9m/St+DZUq619JSStUAVPR5xIJMG0a4Oenuc3fH9i8mUv/iIiIiKjC4cwV/UvX7JSfH/D4cemy/rm76792KjZW91JDIiIiIqIKhsEVKem7J5UlElMsWaIM0nQFUPb2XPpHRERERJUCgysqu3tSqfj5MYAiIiIiokqPwRWVzT2pAN7wl4iIiIiqFCa0IOVyvdLSlbQC4A1/iYiIiKjKYHBVFRXPCOjpaV49zPpHRERERKTGZYFVjb6MgHXqAPfu6b7uSiIB3NwAJyft+1wx6x8REREREQAGV1WLvoyAN2/+W6bvpr5ffgn0768/gGLWPyIiIiKq4hhcVRWGMgIKYdzsFMAAioiIiIhIDwZXVUVJGQGFAO7eBfbsUc5CcXkfEREREZFJGFxVRnK59vI9YzMCZmUBERFl2z4iIiIiokqIwVVloythhb8/MG6cccf7+JRNu4iIiIiIKjkGV5WJvoQV6elATEzJGQF5w18iIiIiIrPxPleVhTEJK1R4w18iIiIiIovjzFVFVfy6KrncuIQV8+cDX31lOCMgERERERGZjMFVRaTruio3N+OObdQIuHqVN/wlIiIiIrIwBlcVjb7rqu7dM+54Hx/e8JeIiIiIqAwwuKpIDF1XVRImrCAiIiIiKlNMaFGRlHQjYH2YsIKIiIiIqMwxuKpIjL0RcPHrr/z9gc2bmbCCiIiIiKgMcVlgRWLsDX6/+045Q8WEFURERERE5YbBVUUSFqachUpPN3wj4K5dGUwREREREZUzLgusSOztgWXLlN/zRsBERERERDaFwVVFM2iQ8vopPz/Ncl5XRURERERkVVwWWBENGgT0788bARMRERER2RAGVxUVbwRMRERERGRTuCyQiIiIiIjIAhhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCrB5crVy5EoGBgZDJZAgNDcWxY8cM7r906VI0adIETk5OCAgIwFtvvYXHjx+rt8+bNw8SiUTjERwcXNbdKBtyOZCSAmzYoPwql1u7RUREREREpEc1a55848aNiI6OxurVqxEaGoqlS5ciPDwcFy9ehKenp9b+69evx7vvvou4uDh06NABly5dwujRoyGRSLB48WL1fk899RT27Nmjfl6tmlW7aZ7ERGDKFODGjX/L/P2BZcuAQYOs1y4iIiIiItLJqjNXixcvxrhx4zBmzBg0a9YMq1evhrOzM+Li4nTuf+jQIXTs2BHDhw9HYGAgevfujYiICK3ZrmrVqsHb21v9cHd3L4/uWE5iIjBkiGZgBQDp6cryxETrtIuIiIiIiPSy2pROQUEBfvvtN8ycOVNdZmdnh549e+Lw4cM6j+nQoQO+/fZbHDt2DG3btsVff/2FnTt3YuTIkRr7Xb58Gb6+vpDJZGjfvj1iY2NRt25dvW3Jz89Hfn6++nl2djYAoLCwEIWFhaXpplFU5ygsLFQu/ZsxA5DJdO8skQDvvgv06QPY25d528h2aYwbIhNw7JA5OG7IHBw3ZC5bGjumtEEihBBl2Ba9bt68CT8/Pxw6dAjt27dXl0+fPh2//PILjh49qvO4zz77DO+88w6EEHjy5AkmTJiAVatWqbf/9NNPePjwIZo0aYJbt25h/vz5SE9Px5kzZ1CjRg2ddc6bNw/z58/XKl+/fj2cnZ1L2VMiIiIiIqqo8vLyMHz4cDx48ACurq4G961QFyOlpKTgww8/xOeff47Q0FCkpqZiypQpeP/99zFnzhwAwPPPP6/ev0WLFggNDUW9evXw3XffYezYsTrrnTlzJqKjo9XPs7OzERAQgN69e5f4AlpCYWEhkpKS0KtXLzh8/z2gp50a1qxRLhGkKktj3Dg4WLs5VIFw7JA5OG7IHBw3ZC5bGjuqVW3GsFpw5e7uDnt7e2RmZmqUZ2ZmwtvbW+cxc+bMwciRI/Haa68BAJo3b47c3FyMHz8e7733HuzstC8hq1WrFho3bozU1FS9bZFKpZBKpVrlDg4O5frDdHBwgIOPD/DoUck7+/gAfJMilP84pcqDY4fMwXFD5uC4IXPZwtgx5fxWS2jh6OiI1q1bIzk5WV2mUCiQnJyssUywqLy8PK0Ayv7/rzvSt7rx4cOHSEtLg4+Pj4VaXsbCwpRZASUS3dslEiAgQLkfERERERHZDKtmC4yOjsZXX32FdevW4fz585g4cSJyc3MxZswYAMCoUaM0El7069cPq1atQkJCAq5cuYKkpCTMmTMH/fr1UwdZ77zzDn755RdcvXoVhw4dwsCBA2Fvb4+IiAir9NFk9vbKdOuAdoCler50KZNZEBERERHZGKteczV06FDcvn0bc+fORUZGBlq2bIldu3bBy8sLAHDt2jWNmarZs2dDIpFg9uzZSE9Ph4eHB/r164eFCxeq97lx4wYiIiJw9+5deHh4oFOnTjhy5Ag8PDzKvX9mGzQI2LxZ932uli7lfa6IiIiIiGyQ1RNaREVFISoqSue2lJQUjefVqlVDTEwMYmJi9NaXkJBgyeZZz6BBQP/+wK+/ArduKa+xCgvjjBURERERkY2yenBFBtjbA127WrsVRERERERkBKtec0VERERERFRZMLgiIiIiIiKyAAZXREREREREFsDgioiIiIiIyAIYXBEREREREVmAycFVYGAgFixYgGvXrpVFe4iIiIiIiCokk4OrqVOnIjExEQ0aNECvXr2QkJCA/Pz8smgbERERERFRhWFWcHX69GkcO3YMTZs2xZtvvgkfHx9ERUXh5MmTZdFGIiIiIiIim2f2NVfPPPMMPvvsM9y8eRMxMTH473//i2effRYtW7ZEXFwchBCWbCcREREREZFNq2bugYWFhdi6dSvWrl2LpKQktGvXDmPHjsWNGzcwa9Ys7NmzB+vXr7dkW4mIiIiIiGyWycHVyZMnsXbtWmzYsAF2dnYYNWoUlixZguDgYPU+AwcOxLPPPmvRhhIREREREdkyk4OrZ599Fr169cKqVaswYMAAODg4aO1Tv359DBs2zCINJCIiIiIiqghMDq7++usv1KtXz+A+Li4uWLt2rdmNIiIiIiIiqmhMTmiRlZWFo0ePapUfPXoUJ06csEijiIiIiIiIKhqTg6tJkybh+vXrWuXp6emYNGmSRRpFRERERERU0ZgcXJ07dw7PPPOMVnmrVq1w7tw5izSKiIiIiIioojE5uJJKpcjMzNQqv3XrFqpVMzuzOxERERERUYVmcnDVu3dvzJw5Ew8ePFCX3b9/H7NmzUKvXr0s2jgiIiIiIqKKwuSppk8++QSdO3dGvXr10KpVKwDA6dOn4eXlhW+++cbiDSQiIiIiIqoITA6u/Pz88McffyA+Ph6///47nJycMGbMGEREROi85xUREREREVFVYNZFUi4uLhg/fryl20JERERERFRhmZ2B4ty5c7h27RoKCgo0yl988cVSN4qIiIiIiKiiMTm4+uuvvzBw4ED8+eefkEgkEEIAACQSCQBALpdbtoVEREREREQVgMnZAqdMmYL69esjKysLzs7OOHv2LPbv3482bdogJSWlDJpIRERERERk+0yeuTp8+DD27t0Ld3d32NnZwc7ODp06dUJsbCwmT56MU6dOlUU7iYiIiIiIbJrJM1dyuRw1atQAALi7u+PmzZsAgHr16uHixYuWbR0REREREVEFYfLM1dNPP43ff/8d9evXR2hoKBYtWgRHR0d8+eWXaNCgQVm0kYiIiIiIyOaZHFzNnj0bubm5AIAFCxbghRdeQFhYGOrUqYONGzdavIFEREREREQVgcnBVXh4uPr7hg0b4sKFC7h37x5q166tzhhIRERERERU1Zh0zVVhYSGqVauGM2fOaJS7ubkxsCIiIiIioirNpODKwcEBdevW5b2siIiIiIiIijE5W+B7772HWbNm4d69e2XRHiIiIiIiogrJ5GuuVqxYgdTUVPj6+qJevXpwcXHR2H7y5EmLNY6IiIiIiKiiMDm4GjBgQBk0g4iIiIiIqGIzObiKiYkpi3YQERERERFVaCZfc0VERERERETaTJ65srOzM5h2nZkEiYiIiIioKjI5uNq6davG88LCQpw6dQrr1q3D/PnzLdYwIiIiIiKiisTk4Kp///5aZUOGDMFTTz2FjRs3YuzYsRZpGBERERERUUVisWuu2rVrh+TkZEtVR0REREREVKFYJLh69OgRPvvsM/j5+VmiOiIiIiIiogrH5GWBtWvX1khoIYRATk4OnJ2d8e2331q0cURERERERBWFycHVkiVLNIIrOzs7eHh4IDQ0FLVr17Zo44iIiIiIiCoKk4Or0aNHl0EziIiIiIiIKjaTr7lau3YtNm3apFW+adMmrFu3zuQGrFy5EoGBgZDJZAgNDcWxY8cM7r906VI0adIETk5OCAgIwFtvvYXHjx+Xqk4iIiIiIqLSMjm4io2Nhbu7u1a5p6cnPvzwQ5Pq2rhxI6KjoxETE4OTJ08iJCQE4eHhyMrK0rn/+vXr8e677yImJgbnz5/HmjVrsHHjRsyaNcvsOomIiIiIiCzB5GWB165dQ/369bXK69Wrh2vXrplU1+LFizFu3DiMGTMGALB69Wrs2LEDcXFxePfdd7X2P3ToEDp27Ijhw4cDAAIDAxEREYGjR4+aXScA5OfnIz8/X/08OzsbgPIGyYWFhSb1yRyqc5THuajy4Lghc3HskDk4bsgcHDdkLlsaO6a0weTgytPTE3/88QcCAwM1yn///XfUqVPH6HoKCgrw22+/YebMmeoyOzs79OzZE4cPH9Z5TIcOHfDtt9/i2LFjaNu2Lf766y/s3LkTI0eONLtOQDkbN3/+fK3y3bt3w9nZ2eg+lVZSUlK5nYsqD44bMhfHDpmD44bMwXFD5rKFsZOXl2f0viYHVxEREZg8eTJq1KiBzp07AwB++eUXTJkyBcOGDTO6njt37kAul8PLy0uj3MvLCxcuXNB5zPDhw3Hnzh106tQJQgg8efIEEyZMUC8LNKdOAJg5cyaio6PVz7OzsxEQEIDevXvD1dXV6D6Zq7CwEElJSejVqxccHBzK/HxUOXDckLk4dsgcHDdkDo4bMpctjR3VqjZjmBxcvf/++7h69Sp69OiBatWUhysUCowaNcrka65MlZKSgg8//BCff/45QkNDkZqaiilTpuD999/HnDlzzK5XKpVCKpVqlTs4OJTrD7O8z0eVA8cNmYtjh8zBcUPm4Lghc9nC2DHl/CYHV46Ojti4cSM++OADnD59Gk5OTmjevDnq1atnUj3u7u6wt7dHZmamRnlmZia8vb11HjNnzhyMHDkSr732GgCgefPmyM3Nxfjx4/Hee++ZVScREREREZElmJwtUKVRo0Z46aWX8MILL5gcWAHKIK1169ZITk5WlykUCiQnJ6N9+/Y6j8nLy4OdnWaT7e3tAQBCCLPqJCIiIiIisgSTg6vBgwfjo48+0ipftGgRXnrpJZPqio6OxldffYV169bh/PnzmDhxInJzc9WZ/kaNGqWRnKJfv35YtWoVEhIScOXKFSQlJWHOnDno16+fOsgqqU4iIiIiIqKyYPKywP3792PevHla5c8//zw+/fRTk+oaOnQobt++jblz5yIjIwMtW7bErl271Akprl27pjFTNXv2bEgkEsyePRvp6enw8PBAv379sHDhQqPrJCIiIiIiKgsmB1cPHz6Eo6OjVrmDg4NJmTRUoqKiEBUVpXNbSkqKxvNq1aohJiYGMTExZtdJRERERERUFkxeFti8eXNs3LhRqzwhIQHNmjWzSKOIiIiIiIgqGpNnrubMmYNBgwYhLS0N3bt3BwAkJydj/fr12Lx5s8UbSEREREREVBGYHFz169cP27Ztw4cffojNmzfDyckJISEh2Lt3L9zc3MqijURERERERDbP5OAKAPr27Yu+ffsCUN6xeMOGDXjnnXfw22+/QS6XW7SBREREREREFYHZ97nav38/IiMj4evri08//RTdu3fHkSNHLNk2IiIiIiKiCsOkmauMjAx8/fXXWLNmDbKzs/Hyyy8jPz8f27ZtYzILIiIiIiKq0oyeuerXrx+aNGmCP/74A0uXLsXNmzexfPnysmwbERERERFRhWH0zNVPP/2EyZMnY+LEiWjUqFFZtomIiIiIiKjCMXrm6sCBA8jJyUHr1q0RGhqKFStW4M6dO2XZNiIiIiIiogrD6OCqXbt2+Oqrr3Dr1i28/vrrSEhIgK+vLxQKBZKSkpCTk1OW7SQiIiIiIrJpJmcLdHFxwauvvooDBw7gzz//xNtvv43//Oc/8PT0xIsvvlgWbSQiIiIiIrJ5ZqdiB4AmTZpg0aJFuHHjBjZs2GCpNhEREREREVU4pQquVOzt7TFgwAD88MMPlqiOiIiIiIiowrFIcEVERERERFTVMbgiIiIiIiKyAAZXREREREREFsDgioiIiIiIyAIYXBEREREREVkAgysiIiIiIiILYHBFRERERERkAQyuiIiIiIiILIDBFRERERERkQUwuCIiIiIiIrIABldEREREREQWwOCKiIiIiIjIAhhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCGFwRERERERFZAIMrIiIiIiIiC2BwRUREREREZAEMroiIiIiIiCyAwRUREREREZEFMLgiIiIiIiKyAAZXREREREREFsDgioiIiIiIyAIYXBEREREREVkAgysiIiIiIiILYHBFRERERERkATYRXK1cuRKBgYGQyWQIDQ3FsWPH9O7btWtXSCQSrUffvn3V+4wePVpr+3PPPVceXSEiIiIioiqqmrUbsHHjRkRHR2P16tUIDQ3F0qVLER4ejosXL8LT01Nr/8TERBQUFKif3717FyEhIXjppZc09nvuueewdu1a9XOpVFp2nSAiIiIioirP6jNXixcvxrhx4zBmzBg0a9YMq1evhrOzM+Li4nTu7+bmBm9vb/UjKSkJzs7OWsGVVCrV2K927drl0R0iIiIiIqqirDpzVVBQgN9++w0zZ85Ul9nZ2aFnz544fPiwUXWsWbMGw4YNg4uLi0Z5SkoKPD09Ubt2bXTv3h0ffPAB6tSpo7OO/Px85Ofnq59nZ2cDAAoLC1FYWGhqt0ymOkd5nIsqD44bMhfHDpmD44bMwXFD5rKlsWNKGyRCCFGGbTHo5s2b8PPzw6FDh9C+fXt1+fTp0/HLL7/g6NGjBo8/duwYQkNDcfToUbRt21ZdnpCQAGdnZ9SvXx9paWmYNWsWqlevjsOHD8Pe3l6rnnnz5mH+/Pla5evXr4ezs3MpekhERERERBVZXl4ehg8fjgcPHsDV1dXgvla/5qo01qxZg+bNm2sEVgAwbNgw9ffNmzdHixYtEBQUhJSUFPTo0UOrnpkzZyI6Olr9PDs7GwEBAejdu3eJL6AlFBYWIikpCb169YKDg0OZn48qB44bMhfHDpmD44bMwXFD5rKlsaNa1WYMqwZX7u7usLe3R2ZmpkZ5ZmYmvL29DR6bm5uLhIQELFiwoMTzNGjQAO7u7khNTdUZXEmlUp0JLxwcHMr1h1ne56PKgeOGzMWxQ+bguCFzcNyQuWxh7JhyfqsmtHB0dETr1q2RnJysLlMoFEhOTtZYJqjLpk2bkJ+fj1deeaXE89y4cQN3796Fj49PqdtMRERERESki9WzBUZHR+Orr77CunXrcP78eUycOBG5ubkYM2YMAGDUqFEaCS9U1qxZgwEDBmglqXj48CGmTZuGI0eO4OrVq0hOTkb//v3RsGFDhIeHl0ufiIiIiIio6rH6NVdDhw7F7du3MXfuXGRkZKBly5bYtWsXvLy8AADXrl2DnZ1mDHjx4kUcOHAAu3fv1qrP3t4ef/zxB9atW4f79+/D19cXvXv3xvvvv897XRERERERUZmxenAFAFFRUYiKitK5LSUlRausSZMm0Jfk0MnJCT///LMlm0dERERERFQiqy8LJCIiIiIiqgwYXBEREREREVkAgysiIiIiIiILYHBFRERERERkAQyuiIiIiIiILIDBFRERERERkQUwuCIiIiIiIrIABldEREREREQWwOCKiIiIiIjIAhhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCGFwRERERERFZAIMrIiIiIiIiC2BwRUREREREZAEMroiIiIiIiCyAwRUREREREZEFMLgiIiIiIiKyAAZXREREREREFsDgioiIiIiIyAIYXBEREREREVkAgysiIiIiIiILYHBFRERERERkAQyuiIiIiIiILIDBFRERERERkQUwuCIiIiIiIrIABldEREREREQWwOCKiIiIiIjIAhhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCGFwRERERERFZAIMrIiIiIiIiC2BwRUREREREZAE2EVytXLkSgYGBkMlkCA0NxbFjx/Tu27VrV0gkEq1H37591fsIITB37lz4+PjAyckJPXv2xOXLl8ujK0REREREVEVZPbjauHEjoqOjERMTg5MnTyIkJATh4eHIysrSuX9iYiJu3bqlfpw5cwb29vZ46aWX1PssWrQIn332GVavXo2jR4/CxcUF4eHhePz4cXl1i4iIiIiIqhirB1eLFy/GuHHjMGbMGDRr1gyrV6+Gs7Mz4uLidO7v5uYGb29v9SMpKQnOzs7q4EoIgaVLl2L27Nno378/WrRogf/973+4efMmtm3bVo49IyIiIiKiqqSaNU9eUFCA3377DTNnzlSX2dnZoWfPnjh8+LBRdaxZswbDhg2Di4sLAODKlSvIyMhAz5491fvUrFkToaGhOHz4MIYNG6ZVR35+PvLz89XPs7OzAQCFhYUoLCw0q2+mUJ2jPM5FlQfHDZmLY4fMwXFD5uC4IXPZ0tgxpQ1WDa7u3LkDuVwOLy8vjXIvLy9cuHChxOOPHTuGM2fOYM2aNeqyjIwMdR3F61RtKy42Nhbz58/XKt+9ezecnZ1LbIelJCUlldu5qPLguCFzceyQOThuyBwcN2QuWxg7eXl5Ru9r1eCqtNasWYPmzZujbdu2papn5syZiI6OVj/Pzs5GQEAAevfuDVdX19I2s0SFhYVISkpCr1694ODgUObno8qB44bMxbFD5uC4IXNw3JC5bGnsqFa1GcOqwZW7uzvs7e2RmZmpUZ6ZmQlvb2+Dx+bm5iIhIQELFizQKFcdl5mZCR8fH406W7ZsqbMuqVQKqVSqVe7g4FCuP8zyPh9VDhw3ZC6OHTIHxw2Zg+OGzGULY8eU81s1oYWjoyNat26N5ORkdZlCoUBycjLat29v8NhNmzYhPz8fr7zyikZ5/fr14e3trVFndnY2jh49WmKdRERERERE5rL6ssDo6GhERkaiTZs2aNu2LZYuXYrc3FyMGTMGADBq1Cj4+fkhNjZW47g1a9ZgwIABqFOnjka5RCLB1KlT8cEHH6BRo0aoX78+5syZA19fXwwYMKC8ukVERERERFWM1YOroUOH4vbt25g7dy4yMjLQsmVL7Nq1S52Q4tq1a7Cz05xgu3jxIg4cOIDdu3frrHP69OnIzc3F+PHjcf/+fXTq1Am7du2CTCYr8/4QEREREVHVZPXgCgCioqIQFRWlc1tKSopWWZMmTSCE0FufRCLBggULtK7HIiIiIiIiKitWv4kwERERERFRZcDgioiIiIiIyAIYXBEREREREVkAgysiIiIiIiILYHBFRERERERkAQyuiIiIiIiILIDBFRERERERkQUwuCIiIiIiIrIABldEREREREQWwOCKiIiIiIjIAhhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCGFwRERERERFZAIMrIiIiIiIiC2BwRUREREREZAEMroiIiIiIiCyAwRUREREREZEFMLgiIiIiIiKyAAZXREREREREFsDgioiIiIiIyAIYXBEREREREVkAgysiIiIiIiILYHBFRERERERkAQyuiIiIiIiILIDBFRERERERkQUwuCIiIiIiIrIABldEREREREQWwOCKiIiIiIjIAhhcERERERERWQCDKyIiIiIiIgtgcEVERERERGQBDK6IiIiIiIgsoJq1G0D6yeXAr78Ct24BPj5AWBhgb1+6bWVVry1ts7X2lNW2AweU3x84AHTuXLX6b2vtqWj91zd2KlMfK/o2W2tPeY8bW+w/+8i/VfwZ28bfKpsnSMuDBw8EAPHgwYNyOV9BQYHYtm2bKCgoUJdt2SKEv78QwL8Pf39lubnbyqpeW9pWFfqo2ubkpBw3Tk4FVar/VaGPZd1/XWOnsvWxIm+z1f6X17ix1f6zj5YbN1Wh/1Whj9b4W2UtpsQGKIf2VDjWDq62bBFCItEccIDuMmO3SSRCTJtm+XptaVtV6GPR50XfdKpK/6tCH8uj/8XHTmXsY0XdZsv9L49xY8v9Zx8tM26qQv+rQh/Lo/+6xo61AixTYgNec2Vj5HJgyhTlsCpOV5mx24QAFi+2fL22tK0q9NHQtqrQ/6rQR0PbqkL/q0IfDW1j/yt//6tCHw1tqwr9rwp9NLStLPs/darys7ItY3BlY379Fbhxo2zqtvXBaAlVoY+GVIX+V4U+GlIV+l8V+mgI+2/tFpS9qtBHQ6pC/6tCHw0pi/4LAVy/rvysbMsYXNmYW7es3QIiIiIiIttk65+VGVzZGB8fa7eAiIiIiMg22fpnZasHVytXrkRgYCBkMhlCQ0Nx7Ngxg/vfv38fkyZNgo+PD6RSKRo3boydO3eqt8+bNw8SiUTjERwcXNbdsJiwMMDfH5BILF+3vX3Z1GtLqkIfDakK/a8KfTSkKvS/KvTREPa/8ve/KvTRkKrQ/6rQR0PKov8SCRAQoPysbMusGlxt3LgR0dHRiImJwcmTJxESEoLw8HBkZWXp3L+goAC9evXC1atXsXnzZly8eBFfffUV/Pz8NPZ76qmncOvWLfXjgCpRfgVgbw8sW6b8vvigLPrc1G0SCRAdbfl6bWlbVeijoW1Vof9VoY+GtlWF/leFPhraxv5X/v5XhT4a2lYV+l8V+mhoW1n2f+nSCnC/q3LIXqhX27ZtxaRJk9TP5XK58PX1FbGxsTr3X7VqlWjQoIHG/aCKi4mJESEhIaVql7VTsQuhO/9/QID+ewMYs62s6rWlbVWhj7ru/1CV+l8V+ljW/dc1dipbHyvyNlvtf3mNG1vtP/touXFTFfpfFfpojb9V1mJKbCARQghrBHUFBQVwdnbG5s2bMWDAAHV5ZGQk7t+/j++//17rmD59+sDNzQ3Ozs74/vvv4eHhgeHDh2PGjBmw//8wdt68efj4449Rs2ZNyGQytG/fHrGxsahbt67etuTn5yM/P1/9PDs7GwEBAbhz5w5cXV0t12k9CgsLkZSUhF69esHBwUFdLpcDhw8DGRmAtzfQvr3mnavN2VZW9drSNltrT1ltO3SoEDk5SahRoxc6dHCoUv23tfZUtP7rGzuVqY8VfZuttae8x40t9p995N8q/oxt42+VNWRnZ8Pd3R0PHjwoMTawWnB18+ZN+Pn54dChQ2jfvr26fPr06fjll19w9OhRrWOCg4Nx9epVjBgxAm+88QZSU1PxxhtvYPLkyYiJiQEA/PTTT3j48CGaNGmCW7duYf78+UhPT8eZM2dQo0YNnW2ZN28e5s+fr1W+fv16ODs7W6jHRERERERU0eTl5WH48OGVL7hq3LgxHj9+jCtXrqhnqhYvXoyPP/4Yt/TkZbx//z7q1auHxYsXY+zYsTr3sdWZKyJDOG7IXBw7ZA6OGzIHxw2Zy5bGjikzV9XKqU1a3N3dYW9vj8zMTI3yzMxMeHt76zzGx8cHDg4O6sAKAJo2bYqMjAwUFBTA0dFR65hatWqhcePGSE1N1dsWqVQKqVSqVe7g4FCuP8zyPh9VDhw3ZC6OHTIHxw2Zg+OGzGULY8eU81stW6CjoyNat26N5ORkdZlCoUBycrLGTFZRHTt2RGpqKhQKhbrs0qVL8PHx0RlYAcDDhw+RlpYGH1tPik9ERERERBWaVVOxR0dH46uvvsK6detw/vx5TJw4Ebm5uRgzZgwAYNSoUZg5c6Z6/4kTJ+LevXuYMmUKLl26hB07duDDDz/EpEmT1Pu88847+OWXX3D16lUcOnQIAwcOhL29PSIiIsq9f0REREREVHVYbVkgAAwdOhS3b9/G3LlzkZGRgZYtW2LXrl3w8vICAFy7dg12dv/GfwEBAfj555/x1ltvoUWLFvDz88OUKVMwY8YM9T43btxAREQE7t69Cw8PD3Tq1AlHjhyBh4dHufePiIiIiIiqDqsGVwAQFRWFqKgondtSUlK0ytq3b48jR47orS8hIcFSTSMiIiIiIjKaVZcFEhERERERVRYMroiIiIiIiCyAwRUREREREZEFMLgiIiIiIiKyAKsntLBFQggAyrsxl4fCwkLk5eUhOzvb6jdJo4qD44bMxbFD5uC4IXNw3JC5bGnsqGICVYxgCIMrHXJycgAoU78TERERERHl5OSgZs2aBveRCGNCsCpGoVDg5s2bqFGjBiQSSZmfLzs7GwEBAbh+/TpcXV3L/HxUOXDckLk4dsgcHDdkDo4bMpctjR0hBHJycuDr66txD15dOHOlg52dHfz9/cv9vK6urlYfPFTxcNyQuTh2yBwcN2QOjhsyl62MnZJmrFSY0IKIiIiIiMgCGFwRERERERFZAIMrGyCVShETEwOpVGrtplAFwnFD5uLYIXNw3JA5OG7IXBV17DChBRERERERkQVw5oqIiIiIiMgCGFwRERERERFZAIMrIiIiIiIiC2BwRUREREREZAEMrmzAypUrERgYCJlMhtDQUBw7dszaTSIbEhsbi2effRY1atSAp6cnBgwYgIsXL2rs8/jxY0yaNAl16tRB9erVMXjwYGRmZlqpxWSL/vOf/0AikWDq1KnqMo4b0iU9PR2vvPIK6tSpAycnJzRv3hwnTpxQbxdCYO7cufDx8YGTkxN69uyJy5cvW7HFZAvkcjnmzJmD+vXrw8nJCUFBQXj//fdRNG8axw7t378f/fr1g6+vLyQSCbZt26ax3Zgxcu/ePYwYMQKurq6oVasWxo4di4cPH5ZjLwxjcGVlGzduRHR0NGJiYnDy5EmEhIQgPDwcWVlZ1m4a2YhffvkFkyZNwpEjR5CUlITCwkL07t0bubm56n3eeustbN++HZs2bcIvv/yCmzdvYtCgQVZsNdmS48eP44svvkCLFi00yjluqLh//vkHHTt2hIODA3766SecO3cOn376KWrXrq3eZ9GiRfjss8+wevVqHD16FC4uLggPD8fjx4+t2HKyto8++girVq3CihUrcP78eXz00UdYtGgRli9frt6HY4dyc3MREhKClStX6txuzBgZMWIEzp49i6SkJPz444/Yv38/xo8fX15dKJkgq2rbtq2YNGmS+rlcLhe+vr4iNjbWiq0iW5aVlSUAiF9++UUIIcT9+/eFg4OD2LRpk3qf8+fPCwDi8OHD1mom2YicnBzRqFEjkZSUJLp06SKmTJkihOC4Id1mzJghOnXqpHe7QqEQ3t7e4uOPP1aX3b9/X0ilUrFhw4byaCLZqL59+4pXX31Vo2zQoEFixIgRQgiOHdIGQGzdulX93Jgxcu7cOQFAHD9+XL3PTz/9JCQSiUhPTy+3thvCmSsrKigowG+//YaePXuqy+zs7NCzZ08cPnzYii0jW/bgwQMAgJubGwDgt99+Q2FhocY4Cg4ORt26dTmOCJMmTULfvn01xgfAcUO6/fDDD2jTpg1eeukleHp6olWrVvjqq6/U269cuYKMjAyNcVOzZk2EhoZy3FRxHTp0QHJyMi5dugQA+P3333HgwAE8//zzADh2qGTGjJHDhw+jVq1aaNOmjXqfnj17ws7ODkePHi33NutSzdoNqMru3LkDuVwOLy8vjXIvLy9cuHDBSq0iW6ZQKDB16lR07NgRTz/9NAAgIyMDjo6OqFWrlsa+Xl5eyMjIsEIryVYkJCTg5MmTOH78uNY2jhvS5a+//sKqVasQHR2NWbNm4fjx45g8eTIcHR0RGRmpHhu6/m5x3FRt7777LrKzsxEcHAx7e3vI5XIsXLgQI0aMAACOHSqRMWMkIyMDnp6eGturVasGNzc3mxlHDK6IKpBJkybhzJkzOHDggLWbQjbu+vXrmDJlCpKSkiCTyazdHKogFAoF2rRpgw8//BAA0KpVK5w5cwarV69GZGSklVtHtuy7775DfHw81q9fj6eeegqnT5/G1KlT4evry7FDVQqXBVqRu7s77O3ttbJzZWZmwtvb20qtIlsVFRWFH3/8Efv27YO/v7+63NvbGwUFBbh//77G/hxHVdtvv/2GrKwsPPPMM6hWrRqqVauGX375BZ999hmqVasGLy8vjhvS4uPjg2bNmmmUNW3aFNeuXQMA9djg3y0qbtq0aXj33XcxbNgwNG/eHCNHjsRbb72F2NhYABw7VDJjxoi3t7dW0rcnT57g3r17NjOOGFxZkaOjI1q3bo3k5GR1mUKhQHJyMtq3b2/FlpEtEUIgKioKW7duxd69e1G/fn2N7a1bt4aDg4PGOLp48SKuXbvGcVSF9ejRA3/++SdOnz6tfrRp0wYjRoxQf89xQ8V17NhR61YPly5dQr169QAA9evXh7e3t8a4yc7OxtGjRzluqri8vDzY2Wl+rLS3t4dCoQDAsUMlM2aMtG/fHvfv38dvv/2m3mfv3r1QKBQIDQ0t9zbrZO2MGlVdQkKCkEql4uuvvxbnzp0T48ePF7Vq1RIZGRnWbhrZiIkTJ4qaNWuKlJQUcevWLfUjLy9Pvc+ECRNE3bp1xd69e8WJEydE+/btRfv27a3YarJFRbMFCsFxQ9qOHTsmqlWrJhYuXCguX74s4uPjhbOzs/j222/V+/znP/8RtWrVEt9//734448/RP/+/UX9+vXFo0ePrNhysrbIyEjh5+cnfvzxR3HlyhWRmJgo3N3dxfTp09X7cOxQTk6OOHXqlDh16pQAIBYvXixOnTol/v77byGEcWPkueeeE61atRJHjx4VBw4cEI0aNRIRERHW6pIWBlc2YPny5aJu3brC0dFRtG3bVhw5csTaTSIbAkDnY+3atep9Hj16JN544w1Ru3Zt4ezsLAYOHChu3bplvUaTTSoeXHHckC7bt28XTz/9tJBKpSI4OFh8+eWXGtsVCoWYM2eO8PLyElKpVPTo0UNcvHjRSq0lW5GdnS2mTJki6tatK2QymWjQoIF47733RH5+vnofjh3at2+fzs80kZGRQgjjxsjdu3dFRESEqF69unB1dRVjxowROTk5VuiNbhIhitw6m4iIiIiIiMzCa66IiIiIiIgsgMEVERERERGRBTC4IiIiIiIisgAGV0RERERERBbA4IqIiIiIiMgCGFwRERERERFZAIMrIiIiIiIiC2BwRUREREREZAEMroiIiEpJIpFg27Zt1m4GERFZGYMrIiKq0EaPHg2JRKL1eO6556zdNCIiqmKqWbsBREREpfXcc89h7dq1GmVSqdRKrSEioqqKM1dERFThSaVSeHt7azxq164NQLlkb9WqVXj++efh5OSEBg0aYPPmzRrH//nnn+jevTucnJxQp04djB8/Hg8fPtTYJy4uDk899RSkUil8fHwQFRWlsf3OnTsYOHAgnJ2d0ahRI/zwww/qbf/88w9GjBgBDw8PODk5oVGjRlrBIBERVXwMroiIqNKbM2cOBg8ejN9//x0jRozAsGHDcP78eQBAbm4uwsPDUbt2bRw/fhybNm3Cnj17NIKnVatWYdKkSRg/fjz+/PNP/PDDD2jYsKHGOebPn4+XX34Zf/zxB/r06YMRI0bg3r176vOfO3cOP/30E86fP49Vq1bB3d29/F4AIiIqFxIhhLB2I4iIiMw1evRofPvtt5DJZBrls2bNwqxZsyCRSDBhwgSsWrVKva1du3Z45pln8Pnnn+Orr77CjBkzcP36dbi4uAAAdu7ciX79+uHmzZvw8vKCn58fxowZgw8++EBnGyQSCWbPno33338fgDJgq169On766Sc899xzePHFF+Hu7o64uLgyehWIiMgW8JorIiKq8Lp166YRPAGAm5ub+vv27dtrbGvfvj1Onz4NADh//jxCQkLUgRUAdOzYEQqFAhcvXoREIsHNmzfRo0cPg21o0aKF+nsXFxe4uroiKysLADBx4kQMHjwYJ0+eRO/evTFgwAB06NDBrL4SEZHtYnBFREQVnouLi9YyPUtxcnIyaj8HBweN5xKJBAqFAgDw/PPP4++//8bOnTuRlJSEHj16YNKkSfjkk08s3l4iIrIeXnNFRESV3pEjR7SeN23aFADQtGlT/P7778jNzVVvP3jwIOzs7NCkSRPUqFEDgYGBSE5OLlUbPDw8EBkZiW+//RZLly7Fl19+War6iIjI9nDmioiIKrz8/HxkZGRolFWrVk2dNGLTpk1o06YNOnXqhPj4eBw7dgxr1qwBAIwYMQIxMTGIjIzEvHnzcPv2bbz55psYOXIkvLy8AADz5s3DhAkT4Onpieeffx45OTk4ePAg3nzzTaPaN3fuXLRu3RpPPfUU8vPz8eOPP6qDOyIiqjwYXBERUYW3a9cu+Pj4aJQ1adIEFy5cAKDM5JeQkIA33ngDPj4+2LBhA5o1awYAcHZ2xs8//4wpU6bg2WefhbOzMwYPHozFixer64qMjMTjx4+xZMkSvPPOO3B3d8eQIUOMbp+joyNmzpyJq1evwsnJCWFhYUhISLBAz4mIyJYwWyAREVVqEokEW7duxYABA6zdFCIiquR4zRUREREREZEFMLgiIiIiIiKyAF5zRURElRpXvxMRUXnhzBUREREREZEFMLgiIiIiIiKyAAZXREREREREFsDgioiIiIiIyAIYXBEREREREVkAgysiIiIiIiILYHBFRERERERkAQyuiIiIiIiILOD/ACQDCMNVeecuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming history1_nd.history['accuracy'] and history2_nd.history['accuracy'] are available\n",
    "# These lists should contain the accuracy for each epoch\n",
    "epochs = range(1, len(history1_nd.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plotting accuracy for history1_nd\n",
    "plt.scatter(epochs, history1_nd.history['accuracy'], color='blue', label='Model 1 Accuracy')\n",
    "\n",
    "# Plotting accuracy for history2_nd\n",
    "plt.scatter(epochs, history2_nd.history['accuracy'], color='red', label='Model 2 Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Model Accuracies Across Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - accuracy: 0.6523 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\n",
      "Epoch 2/5\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440us/step - accuracy: 0.6535 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\n",
      "Epoch 3/5\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.6649 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\n",
      "Epoch 4/5\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - accuracy: 0.6511 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\n",
      "Epoch 5/5\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - accuracy: 0.6526 - loss: 0.6931 - val_accuracy: 0.6609 - val_loss: 0.6930\n"
     ]
    }
   ],
   "source": [
    "def custom_loss(lambda_val=0.5):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Extract predictions and sensitive attributes\n",
    "        predictions = y_pred[:, 0]\n",
    "        sensitive_attr = y_pred[:, 1]\n",
    "\n",
    "        # Standard binary crossentropy loss\n",
    "        standard_loss = BinaryCrossentropy(from_logits=True)(y_true, predictions)\n",
    "\n",
    "        # Calculate discrimination based on sensitive attribute\n",
    "        threshold = 0\n",
    "        mask_unpriv = K.cast(sensitive_attr <= threshold, 'float32')\n",
    "        mask_priv = K.cast(sensitive_attr > threshold, 'float32')\n",
    "\n",
    "        sum_unpriv = K.sum(mask_unpriv)\n",
    "        sum_priv = K.sum(mask_priv)\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        prob_unpriv = K.sum(predictions * mask_unpriv) / (sum_unpriv + epsilon)\n",
    "        prob_priv = K.sum(predictions * mask_priv) / (sum_priv + epsilon)\n",
    "\n",
    "        discrimination = K.square(prob_priv - prob_unpriv)\n",
    "\n",
    "        # Debug outputs\n",
    "        # tf.print(\"Standard Loss:\", standard_loss, \"Discrimination:\", discrimination)\n",
    "\n",
    "        # Total loss with discrimination penalty\n",
    "        return standard_loss + lambda_val * discrimination\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss=custom_loss(lambda_val=0.01),  # Change lambda_val as needed\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history1 = model.fit(X_train_with_sensitive, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "size": 8
         },
         "mode": "markers",
         "name": "Model 1 Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100
         ],
         "y": [
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849
         ]
        },
        {
         "marker": {
          "color": "red",
          "size": 8
         },
         "mode": "markers",
         "name": "Model 2 Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100
         ],
         "y": [
          0.8491758108139038,
          0.8678314089775085,
          0.8719770908355713,
          0.8762214779853821,
          0.881255567073822,
          0.883229672908783,
          0.8846116065979004,
          0.888362467288971,
          0.8894482254981995,
          0.8926068544387817,
          0.8976408839225769,
          0.9001085758209229,
          0.9019840359687805,
          0.902971088886261,
          0.9093870520591736,
          0.9079064130783081,
          0.9119533896446228,
          0.9132366180419922,
          0.9162965416908264,
          0.9170861840248108,
          0.919553816318512,
          0.9192577004432678,
          0.9236995577812195,
          0.9232059717178345,
          0.9234034419059753,
          0.9277465343475342,
          0.9301154613494873,
          0.9289309978485107,
          0.9288322925567627,
          0.9318922162055969,
          0.9313986897468567,
          0.9330766797065735,
          0.934557318687439,
          0.9355443716049194,
          0.936827540397644,
          0.9359391927719116,
          0.9364327192306519,
          0.9380120635032654,
          0.93929523229599,
          0.9404796957969666,
          0.9407758116722107,
          0.9424538612365723,
          0.9438357353210449,
          0.9438357353210449,
          0.9444279670715332,
          0.9461060166358948,
          0.9447241425514221,
          0.9479814171791077,
          0.9481788277626038,
          0.9488698244094849,
          0.9484750032424927,
          0.9471917748451233,
          0.9509426355361938,
          0.9519297480583191,
          0.9543973803520203,
          0.9533116221427917,
          0.9535090327262878,
          0.9543973803520203,
          0.9561741352081299,
          0.9555819034576416,
          0.9550883173942566,
          0.9558780193328857,
          0.9570624828338623,
          0.9574573040008545,
          0.9577534198760986,
          0.9592340588569641,
          0.9583456516265869,
          0.959036648273468,
          0.9621952176094055,
          0.9601224064826965,
          0.9608133435249329,
          0.9613068699836731,
          0.9593327641487122,
          0.9623926281929016,
          0.9619978070259094,
          0.9631823301315308,
          0.9620965123176575,
          0.9651564359664917,
          0.9632810354232788,
          0.9628862142562866,
          0.966340959072113,
          0.9650577306747437,
          0.965847373008728,
          0.9642680883407593,
          0.9674267172813416,
          0.9647616147994995,
          0.9650577306747437,
          0.9670318961143494,
          0.9659460783004761,
          0.9685124754905701,
          0.9674267172813416,
          0.9669331908226013,
          0.9675254225730896,
          0.9693021178245544,
          0.9699931144714355,
          0.9700918197631836,
          0.968315064907074,
          0.9677228331565857,
          0.9702892303466797,
          0.9706840515136719
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparison of Model Accuracies Across Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming history1_nd and history2_nd have 'accuracy' data from your model training\n",
    "# Extracting the accuracy data from each history object\n",
    "accuracy1 = history1_nd.history['accuracy']\n",
    "accuracy2 = history2_nd.history['accuracy']\n",
    "\n",
    "# Assuming the number of epochs is determined by the length of the accuracy data\n",
    "epochs = list(range(1, len(accuracy1) + 1))\n",
    "\n",
    "# Create a scatter plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adding scatter plot for Model 1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs,\n",
    "    y=accuracy1,\n",
    "    mode='markers',  # This specifies to use markers (points)\n",
    "    name='Model 1 Accuracy',  # Name of the series\n",
    "    marker=dict(color='blue', size=8)  # Marker settings\n",
    "))\n",
    "\n",
    "# Adding scatter plot for Model 2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=epochs,\n",
    "    y=accuracy2,\n",
    "    mode='markers',\n",
    "    name='Model 2 Accuracy',\n",
    "    marker=dict(color='red', size=8)\n",
    "))\n",
    "\n",
    "# Update the layout to add titles and labels\n",
    "fig.update_layout(\n",
    "    title='Comparison of Model Accuracies Across Epochs',\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='Accuracy',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Training Accuracy - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849,
          0.6549205183982849
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Validation Accuracy - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821,
          0.660876452922821
         ]
        },
        {
         "line": {
          "color": "skyblue"
         },
         "mode": "lines+markers",
         "name": "Training Accuracy - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.8513473272323608,
          0.8699042797088623,
          0.8751357197761536,
          0.8794788122177124,
          0.8815516829490662,
          0.8851051330566406,
          0.8892508149147034,
          0.8913236856460571,
          0.8933964967727661,
          0.8953706622123718,
          0.8993189334869385,
          0.9008982181549072,
          0.9044516682624817,
          0.9071167707443237,
          0.9075115919113159,
          0.910275399684906,
          0.912446916103363,
          0.912841796875,
          0.9162965416908264,
          0.9173822999000549,
          0.9205409288406372,
          0.9235021471977234,
          0.9253775477409363,
          0.9245879054069519,
          0.9259697794914246,
          0.9275491237640381,
          0.9301154613494873,
          0.9304116368293762,
          0.9328792691230774,
          0.9333727955818176,
          0.936728835105896,
          0.9369262456893921,
          0.9375185370445251,
          0.9383081793785095,
          0.9383081793785095,
          0.9403809905052185,
          0.9376172423362732,
          0.9416642189025879,
          0.9416642189025879,
          0.9442305564880371,
          0.9437370300292969,
          0.9441318511962891,
          0.9459086060523987,
          0.9483762979507446,
          0.9464021325111389,
          0.9486724138259888,
          0.9477840065956116,
          0.9486724138259888,
          0.9497581720352173,
          0.9471917748451233,
          0.9501529932022095,
          0.9505478143692017,
          0.9530155062675476,
          0.9533116221427917,
          0.9525219798088074,
          0.9551870226860046,
          0.9554831981658936,
          0.9537064433097839,
          0.9569637775421143,
          0.9552857279777527,
          0.958444356918335,
          0.958641767501831,
          0.958543062210083,
          0.9588391780853271,
          0.9566676616668701,
          0.9579508304595947,
          0.9601224064826965,
          0.9597275853157043,
          0.9606159329414368,
          0.9618991017341614,
          0.9617016911506653,
          0.961010754108429,
          0.9632810354232788,
          0.963774561882019,
          0.9618003964424133,
          0.9631823301315308,
          0.9647616147994995,
          0.9666370749473572,
          0.9642680883407593,
          0.963675856590271,
          0.9659460783004761,
          0.965847373008728,
          0.9675254225730896,
          0.9668344855308533,
          0.9686111807823181,
          0.9674267172813416,
          0.9689072966575623,
          0.9698944091796875,
          0.9687098860740662,
          0.9689072966575623,
          0.9700918197631836,
          0.9702892303466797,
          0.9703879356384277,
          0.9680189490318298,
          0.9712762832641602,
          0.9686111807823181,
          0.9703879356384277,
          0.9698944091796875,
          0.9717698097229004,
          0.9712762832641602
         ]
        },
        {
         "line": {
          "color": "pink"
         },
         "mode": "lines+markers",
         "name": "Validation Accuracy - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.8555073142051697,
          0.8586655855178833,
          0.8562968969345093,
          0.8574812412261963,
          0.8527438044548035,
          0.8562968969345093,
          0.8547177314758301,
          0.8566916584968567,
          0.8515594005584717,
          0.8519542217254639,
          0.8574812412261963,
          0.8559020757675171,
          0.8539281487464905,
          0.8484011292457581,
          0.8420844674110413,
          0.8480063080787659,
          0.8539281487464905,
          0.8503750562667847,
          0.8491907119750977,
          0.8476115465164185,
          0.8468219637870789,
          0.8468219637870789,
          0.8444532155990601,
          0.8416897058486938,
          0.8448479771614075,
          0.8424792885780334,
          0.8353731036186218,
          0.8491907119750977,
          0.8401105403900146,
          0.8389261960983276,
          0.8389261960983276,
          0.8373470306396484,
          0.8468219637870789,
          0.8373470306396484,
          0.8420844674110413,
          0.8349782824516296,
          0.8448479771614075,
          0.8369522094726562,
          0.8389261960983276,
          0.8357678651809692,
          0.8318199515342712,
          0.8349782824516296,
          0.833004355430603,
          0.8274772763252258,
          0.8282668590545654,
          0.8361626267433167,
          0.8377417922019958,
          0.8345835208892822,
          0.843268871307373,
          0.8322147727012634,
          0.8270825147628784,
          0.8310304284095764,
          0.8266876935958862,
          0.8270825147628784,
          0.8302408456802368,
          0.8243190050125122,
          0.8361626267433167,
          0.8243190050125122,
          0.8318199515342712,
          0.8255033493041992,
          0.829056441783905,
          0.8306356072425842,
          0.8243190050125122,
          0.8251085877418518,
          0.8298460245132446,
          0.8258981704711914,
          0.8251085877418518,
          0.8286616802215576,
          0.8270825147628784,
          0.8310304284095764,
          0.82392418384552,
          0.829056441783905,
          0.821555495262146,
          0.8148440718650818,
          0.8286616802215576,
          0.822739839553833,
          0.8266876935958862,
          0.8274772763252258,
          0.8326095342636108,
          0.8199763298034668,
          0.8345835208892822,
          0.8168179988861084,
          0.8270825147628784,
          0.817607581615448,
          0.8247137665748596,
          0.8247137665748596,
          0.8219502568244934,
          0.833004355430603,
          0.821555495262146,
          0.8262929320335388,
          0.8223450183868408,
          0.8195815086364746,
          0.8207659125328064,
          0.8203710913658142,
          0.8266876935958862,
          0.821555495262146,
          0.8251085877418518,
          0.8251085877418518,
          0.81365966796875,
          0.8302408456802368
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Metric Type"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Accuracy"
        },
        "xaxis": {
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Training Loss - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.6950972676277161,
          0.6844764947891235,
          0.6826090216636658,
          0.6810823082923889,
          0.679863691329956,
          0.6786313056945801,
          0.677652895450592,
          0.6765182614326477,
          0.6755315065383911,
          0.6747356653213501,
          0.6738979816436768,
          0.6730599403381348,
          0.672105610370636,
          0.6715282797813416,
          0.6707706451416016,
          0.6698925495147705,
          0.6693372130393982,
          0.6689808964729309,
          0.6684167385101318,
          0.6677364706993103,
          0.6673206686973572,
          0.6668700575828552,
          0.6664494872093201,
          0.6662013530731201,
          0.6659903526306152,
          0.6660062670707703,
          0.6653286218643188,
          0.6649823784828186,
          0.6647774577140808,
          0.664544939994812,
          0.6642488241195679,
          0.6639556884765625,
          0.6637604236602783,
          0.6638883948326111,
          0.6634254455566406,
          0.6634060144424438,
          0.662886381149292,
          0.6630544066429138,
          0.6630906462669373,
          0.6624481081962585,
          0.6623424291610718,
          0.6620890498161316,
          0.662244439125061,
          0.662007749080658,
          0.6619920134544373,
          0.6617197394371033,
          0.6614500880241394,
          0.6614280343055725,
          0.6615513563156128,
          0.6614885330200195,
          0.6614998579025269,
          0.6611070036888123,
          0.6607981324195862,
          0.6608192920684814,
          0.6608333587646484,
          0.660934329032898,
          0.6607328653335571,
          0.6608352065086365,
          0.6608881950378418,
          0.6605066657066345,
          0.6604874730110168,
          0.6603888273239136,
          0.6603173613548279,
          0.6603347659111023,
          0.6601858735084534,
          0.6606388688087463,
          0.6611783504486084,
          0.6608231067657471,
          0.6603829860687256,
          0.6602805256843567,
          0.6599730253219604,
          0.6599904298782349,
          0.6600081324577332,
          0.6598430275917053,
          0.6598672866821289,
          0.6598447561264038,
          0.6596840620040894,
          0.6595067381858826,
          0.6596215963363647,
          0.6595869660377502,
          0.6598114967346191,
          0.6603427529335022,
          0.6595942974090576,
          0.6595351099967957,
          0.6595739722251892,
          0.6596202254295349,
          0.6595994830131531,
          0.6593611836433411,
          0.6594353914260864,
          0.6593270897865295,
          0.6597219705581665,
          0.6599931120872498,
          0.6598567366600037,
          0.6595718860626221,
          0.6595244407653809,
          0.659641683101654,
          0.6593872904777527,
          0.6595094799995422,
          0.6593612432479858,
          0.6593157649040222
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Validation Loss - Custom Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.6884137392044067,
          0.6880496740341187,
          0.6882661581039429,
          0.6876260638237,
          0.6877718567848206,
          0.6876440644264221,
          0.6875598430633545,
          0.6880201697349548,
          0.6876917481422424,
          0.6878007650375366,
          0.6871289610862732,
          0.6876068115234375,
          0.6871694922447205,
          0.6874826550483704,
          0.686934232711792,
          0.6875285506248474,
          0.6880151629447937,
          0.6886991858482361,
          0.6876727938652039,
          0.6890107989311218,
          0.6874855756759644,
          0.6879310607910156,
          0.6877992749214172,
          0.687883734703064,
          0.6862456798553467,
          0.6868147253990173,
          0.6875001192092896,
          0.687511682510376,
          0.6882526278495789,
          0.6875656843185425,
          0.6877110600471497,
          0.6874529123306274,
          0.6872164607048035,
          0.6877992749214172,
          0.6886271834373474,
          0.687675952911377,
          0.6879667639732361,
          0.689515233039856,
          0.6884847283363342,
          0.6889680624008179,
          0.6890370845794678,
          0.688967227935791,
          0.6893386840820312,
          0.6878992915153503,
          0.6873947381973267,
          0.6878381371498108,
          0.6881501078605652,
          0.688200831413269,
          0.6876681447029114,
          0.6888827681541443,
          0.689543604850769,
          0.6883373856544495,
          0.688271164894104,
          0.6885732412338257,
          0.6880509853363037,
          0.6888812184333801,
          0.688474178314209,
          0.6885151863098145,
          0.6892544627189636,
          0.6887335181236267,
          0.689531147480011,
          0.6895062327384949,
          0.689099133014679,
          0.6892309188842773,
          0.689382791519165,
          0.6898793578147888,
          0.6900604367256165,
          0.68929523229599,
          0.6893160939216614,
          0.689993143081665,
          0.6894953846931458,
          0.6899245977401733,
          0.6885322332382202,
          0.689306378364563,
          0.6890236735343933,
          0.6909748911857605,
          0.6904558539390564,
          0.6902341246604919,
          0.6894909143447876,
          0.69135981798172,
          0.6904706358909607,
          0.6910791397094727,
          0.6908952593803406,
          0.6902796626091003,
          0.6894152760505676,
          0.6899452209472656,
          0.689446210861206,
          0.6899687051773071,
          0.6894552707672119,
          0.690026581287384,
          0.6906648874282837,
          0.6897381544113159,
          0.6905561089515686,
          0.6913752555847168,
          0.6886337399482727,
          0.6899645328521729,
          0.6895496845245361,
          0.6899464726448059,
          0.6887801885604858,
          0.6890056133270264
         ]
        },
        {
         "line": {
          "color": "skyblue"
         },
         "mode": "lines+markers",
         "name": "Training Loss - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.3583318293094635,
          0.3188842236995697,
          0.3083662986755371,
          0.30080896615982056,
          0.2956360876560211,
          0.2909322679042816,
          0.28460532426834106,
          0.27760595083236694,
          0.2728486955165863,
          0.2661466598510742,
          0.26149117946624756,
          0.257763147354126,
          0.25178131461143494,
          0.24799834191799164,
          0.2443271279335022,
          0.23766924440860748,
          0.23207193613052368,
          0.229426309466362,
          0.2252708077430725,
          0.22125251591205597,
          0.21752706170082092,
          0.2130574882030487,
          0.20924584567546844,
          0.20512919127941132,
          0.20434628427028656,
          0.199198380112648,
          0.19594420492649078,
          0.1937466859817505,
          0.19095055758953094,
          0.1884155124425888,
          0.1826074719429016,
          0.18049940466880798,
          0.17932304739952087,
          0.17695526778697968,
          0.17499685287475586,
          0.1710653007030487,
          0.16991129517555237,
          0.16714631021022797,
          0.16717904806137085,
          0.16433990001678467,
          0.16115467250347137,
          0.1625315248966217,
          0.1570727378129959,
          0.15649370849132538,
          0.1538958102464676,
          0.15253537893295288,
          0.15014469623565674,
          0.1495685875415802,
          0.14980947971343994,
          0.14971406757831573,
          0.14437760412693024,
          0.14221921563148499,
          0.14123712480068207,
          0.14069458842277527,
          0.1390480101108551,
          0.13797414302825928,
          0.13804103434085846,
          0.13514237105846405,
          0.13162918388843536,
          0.13204438984394073,
          0.12853392958641052,
          0.12745563685894012,
          0.12576930224895477,
          0.12459252029657364,
          0.12683087587356567,
          0.12178167700767517,
          0.12540970742702484,
          0.12353397905826569,
          0.11858893930912018,
          0.12102321535348892,
          0.11869867146015167,
          0.12042045593261719,
          0.1146770790219307,
          0.11637534946203232,
          0.11666281521320343,
          0.11215946078300476,
          0.11320314556360245,
          0.10858357697725296,
          0.11358007043600082,
          0.11072903871536255,
          0.1085880771279335,
          0.10669361054897308,
          0.10424016416072845,
          0.10665292292833328,
          0.10516991466283798,
          0.10629108548164368,
          0.1014065369963646,
          0.10145174711942673,
          0.10131636261940002,
          0.09932592511177063,
          0.09984296560287476,
          0.09761559963226318,
          0.09869257360696793,
          0.09952669590711594,
          0.09224638342857361,
          0.09631914645433426,
          0.0959780365228653,
          0.09113813191652298,
          0.09345610439777374,
          0.09460928291082382
         ]
        },
        {
         "line": {
          "color": "pink"
         },
         "mode": "lines+markers",
         "name": "Validation Loss - Standard Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          0.34139484167099,
          0.33808138966560364,
          0.33040928840637207,
          0.331817090511322,
          0.3445572853088379,
          0.3357117474079132,
          0.33911746740341187,
          0.34023770689964294,
          0.34234046936035156,
          0.3536278307437897,
          0.3649872839450836,
          0.36419427394866943,
          0.36603298783302307,
          0.3672890067100525,
          0.37943175435066223,
          0.37264183163642883,
          0.38017937541007996,
          0.3957492411136627,
          0.4043750464916229,
          0.39978915452957153,
          0.39820584654808044,
          0.40468040108680725,
          0.4173763394355774,
          0.41339677572250366,
          0.41985011100769043,
          0.4311561584472656,
          0.4427333176136017,
          0.44878390431404114,
          0.45919501781463623,
          0.46903419494628906,
          0.46380752325057983,
          0.47124841809272766,
          0.4751245379447937,
          0.47615692019462585,
          0.4944750666618347,
          0.492519736289978,
          0.5171215534210205,
          0.513014018535614,
          0.5073089599609375,
          0.5117404460906982,
          0.533534049987793,
          0.5270663499832153,
          0.5415680408477783,
          0.549069881439209,
          0.5570813417434692,
          0.5468703508377075,
          0.569063663482666,
          0.5691246390342712,
          0.5827741026878357,
          0.5772860050201416,
          0.5752834677696228,
          0.5950447916984558,
          0.5939041972160339,
          0.59773188829422,
          0.6038683652877808,
          0.6346507668495178,
          0.6086829304695129,
          0.627648651599884,
          0.6209918260574341,
          0.6320046782493591,
          0.6208198070526123,
          0.646187961101532,
          0.6470239162445068,
          0.6603178977966309,
          0.6578608751296997,
          0.659665048122406,
          0.6594985127449036,
          0.6799684762954712,
          0.688678503036499,
          0.7074725031852722,
          0.6899312138557434,
          0.6897902488708496,
          0.7047523260116577,
          0.7106185555458069,
          0.7020578384399414,
          0.7067074775695801,
          0.710456907749176,
          0.7192407250404358,
          0.7429423928260803,
          0.7607890367507935,
          0.7386175394058228,
          0.7628578543663025,
          0.7621734142303467,
          0.8038408160209656,
          0.826441764831543,
          0.7672826647758484,
          0.7818571925163269,
          0.7997775077819824,
          0.7983894348144531,
          0.7987825870513916,
          0.8158020377159119,
          0.8201950788497925,
          0.8405015468597412,
          0.8226025700569153,
          0.8383749723434448,
          0.8464747071266174,
          0.8513116836547852,
          0.8505795001983643,
          0.8743162751197815,
          0.8675692081451416
         ]
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Metric Type"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Loss"
        },
        "xaxis": {
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_accuracy(histories):\n",
    "    # Create figure for accuracy\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add accuracy traces with specified colors\n",
    "    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n",
    "    for name, history in histories:\n",
    "        train_color, val_color = color_map[name]\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['accuracy']))),\n",
    "                                 y=history.history['accuracy'],\n",
    "                                 name=f'Training Accuracy - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=train_color)))\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_accuracy']))),\n",
    "                                 y=history.history['val_accuracy'],\n",
    "                                 name=f'Validation Accuracy - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=val_color)))\n",
    "\n",
    "    # Update layout for accuracy graph\n",
    "    fig.update_layout(title='Training and Validation Accuracy',\n",
    "                      xaxis_title='Epochs',\n",
    "                      yaxis_title='Accuracy',\n",
    "                      legend_title='Metric Type')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_loss(histories):\n",
    "    # Create figure for loss\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add loss traces with specified colors\n",
    "    color_map = {'Custom Loss': ('blue', 'red'), 'Standard Loss': ('skyblue', 'pink')}\n",
    "    for name, history in histories:\n",
    "        train_color, val_color = color_map[name]\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['loss']))),\n",
    "                                 y=history.history['loss'],\n",
    "                                 name=f'Training Loss - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=train_color)))\n",
    "        fig.add_trace(go.Scatter(x=list(range(len(history.history['val_loss']))),\n",
    "                                 y=history.history['val_loss'],\n",
    "                                 name=f'Validation Loss - {name}',\n",
    "                                 mode='lines+markers',\n",
    "                                 line=dict(color=val_color)))\n",
    "\n",
    "    # Update layout for loss graph\n",
    "    fig.update_layout(title='Training and Validation Loss',\n",
    "                      xaxis_title='Epochs',\n",
    "                      yaxis_title='Loss',\n",
    "                      legend_title='Metric Type')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Assuming you have history1 and history2 as the history objects from your model training\n",
    "plot_accuracy([('Custom Loss', history1_nd), ('Standard Loss', history2)])\n",
    "plot_loss([('Custom Loss', history1_nd), ('Standard Loss', history2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfr_model = LFR(unprivileged_groups=unprivileged_groups, \n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "# Fit the model and transform the dataset\n",
    "lfr_model.fit(dataset_orig_panel19_train)\n",
    "dataset_transf_panel19_train_lfr = lfr_model.transform(dataset_orig_panel19_train, threshold = 0.22)\n",
    "dataset_transf_panel19_train_lfr = dataset_orig_panel19_train.align_datasets(dataset_transf_panel19_train_lfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_distortion_meps(vold, vnew):\n",
    "    # Initialize distortion score\n",
    "    distortion_score = 0.0\n",
    "\n",
    "    # Define weights for different categories of attributes\n",
    "    sensitive_weight = 3.0\n",
    "    health_status_weight = 2.0\n",
    "    socio_economic_weight = 1.0\n",
    "    behavior_weight = 1.5\n",
    "\n",
    "    # Sensitive attributes\n",
    "    for attr in ['AGE', 'RACE', 'SEX=1', 'SEX=2']:\n",
    "        if vold[attr] != vnew[attr]:\n",
    "            distortion_score += sensitive_weight\n",
    "\n",
    "    # Health status indicators\n",
    "    health_attrs = ['PCS42', 'MCS42', 'K6SUM42', 'HIBPDX', 'DIABDX', 'CHDDX', 'ANGIDX', 'MIDX', 'OHRTDX', 'STRKDX', 'EMPHDX', 'CANCERDX', 'JTPAIN', 'ARTHDX', 'ASTHDX', 'ADHDADDX']\n",
    "    for attr in health_attrs:\n",
    "        # Assuming health status attributes are numerical and a difference in value indicates a change in health status\n",
    "        distortion_score += health_status_weight * abs(vold.get(attr, 0) - vnew.get(attr, 0))\n",
    "\n",
    "    # Socioeconomic and environmental factors\n",
    "    socio_attrs = ['REGION=1', 'REGION=2', 'REGION=3', 'REGION=4', 'MARRY', 'FTSTU', 'EMPST', 'POVCAT', 'INSCOV']\n",
    "    for attr in socio_attrs:\n",
    "        if vold[attr] != vnew[attr]:\n",
    "            distortion_score += socio_economic_weight\n",
    "\n",
    "    # Health-related behaviors\n",
    "    behavior_attrs = ['ADSMOK42', 'WLKLIM', 'ACTLIM', 'SOCLIM', 'COGLIM']\n",
    "    for attr in behavior_attrs:\n",
    "        if vold[attr] != vnew[attr]:\n",
    "            distortion_score += behavior_weight\n",
    "\n",
    "    return distortion_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI = DisparateImpactRemover()\n",
    "dataset_transf_panel19_train_di = DI.fit_transform(dataset_orig_panel19_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_transf_panel19_train = BinaryLabelDatasetMetric(\n",
    "dataset_transf_panel19_train_rw,\n",
    "unprivileged_groups=unprivileged_groups,\n",
    "privileged_groups=privileged_groups)\n",
    "explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n",
    "test_results_rw=[explainer_transf_panel19_train.mean_difference()\n",
    "               ,explainer_transf_panel19_train.consistency()\n",
    "               ,explainer_transf_panel19_train.statistical_parity_difference()\n",
    "               ,explainer_transf_panel19_train.disparate_impact()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_transf_panel19_train = BinaryLabelDatasetMetric(\n",
    "dataset_transf_panel19_train_di,\n",
    "unprivileged_groups=unprivileged_groups,\n",
    "privileged_groups=privileged_groups)\n",
    "explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n",
    "test_results_di=[explainer_transf_panel19_train.mean_difference()\n",
    "               ,explainer_transf_panel19_train.consistency()\n",
    "               ,explainer_transf_panel19_train.statistical_parity_difference()\n",
    "               ,explainer_transf_panel19_train.disparate_impact()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/aif360/metrics/dataset_metric.py:82: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "metric_transf_panel19_train = BinaryLabelDatasetMetric(\n",
    "dataset_transf_panel19_train_lfr,\n",
    "unprivileged_groups=unprivileged_groups,\n",
    "privileged_groups=privileged_groups)\n",
    "explainer_transf_panel19_train = MetricTextExplainer(metric_transf_panel19_train)\n",
    "test_results_lfr=[explainer_transf_panel19_train.mean_difference()\n",
    "               ,explainer_transf_panel19_train.consistency()\n",
    "               ,explainer_transf_panel19_train.statistical_parity_difference()\n",
    "               ,explainer_transf_panel19_train.disparate_impact()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n",
       " 'Consistency (Zemel, et al. 2013): [0.83660139]',\n",
       " 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n",
       " 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -2.7755575615628914e-17',\n",
       " 'Consistency (Zemel, et al. 2013): [0.83660139]',\n",
       " 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -2.7755575615628914e-17',\n",
       " 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.9999999999999999']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.0',\n",
       " 'Consistency (Zemel, et al. 2013): [1.]',\n",
       " 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.0',\n",
       " 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): nan']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_lfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.13507447726478142',\n",
       " 'Consistency (Zemel, et al. 2013): [0.83689198]',\n",
       " 'Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): -0.13507447726478142',\n",
       " 'Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49826823461176517']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset_transf_panel19_train_rw.features\n",
    "label = dataset_transf_panel19_train_rw.labels.ravel()  # Flatten the label array if necessary\n",
    "weights = dataset_transf_panel19_train_rw.instance_weights\n",
    "feature_names = dataset_transf_panel19_train_rw.feature_names\n",
    "df_rw = pd.DataFrame(features, columns=feature_names)\n",
    "df_rw['label'] = label\n",
    "df_rw['weights'] = weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PCS42</th>\n",
       "      <th>MCS42</th>\n",
       "      <th>K6SUM42</th>\n",
       "      <th>REGION=1</th>\n",
       "      <th>REGION=2</th>\n",
       "      <th>REGION=3</th>\n",
       "      <th>REGION=4</th>\n",
       "      <th>SEX=1</th>\n",
       "      <th>...</th>\n",
       "      <th>POVCAT=1</th>\n",
       "      <th>POVCAT=2</th>\n",
       "      <th>POVCAT=3</th>\n",
       "      <th>POVCAT=4</th>\n",
       "      <th>POVCAT=5</th>\n",
       "      <th>INSCOV=1</th>\n",
       "      <th>INSCOV=2</th>\n",
       "      <th>INSCOV=3</th>\n",
       "      <th>label</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.93</td>\n",
       "      <td>58.47</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17459.483776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.42</td>\n",
       "      <td>26.57</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14515.313940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.12</td>\n",
       "      <td>50.33</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18465.607681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21762.696983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15825</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.71</td>\n",
       "      <td>62.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3727.042408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.71</td>\n",
       "      <td>62.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4909.081729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15827</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4184.786789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15828</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.97</td>\n",
       "      <td>42.45</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4427.370919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15829</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.68</td>\n",
       "      <td>43.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6010.846084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15830 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AGE  RACE  PCS42  MCS42  K6SUM42  REGION=1  REGION=2  REGION=3  \\\n",
       "0      53.0   1.0  25.93  58.47      3.0       0.0       1.0       0.0   \n",
       "1      56.0   1.0  20.42  26.57     17.0       0.0       1.0       0.0   \n",
       "2      23.0   1.0  53.12  50.33      7.0       0.0       1.0       0.0   \n",
       "3       3.0   1.0  -1.00  -1.00     -1.0       0.0       1.0       0.0   \n",
       "4      27.0   0.0  -1.00  -1.00     -1.0       0.0       0.0       1.0   \n",
       "...     ...   ...    ...    ...      ...       ...       ...       ...   \n",
       "15825  25.0   0.0  56.71  62.39      0.0       0.0       0.0       1.0   \n",
       "15826  25.0   0.0  56.71  62.39      0.0       0.0       0.0       1.0   \n",
       "15827   2.0   1.0  -1.00  -1.00     -1.0       0.0       0.0       1.0   \n",
       "15828  54.0   0.0  43.97  42.45     24.0       1.0       0.0       0.0   \n",
       "15829  73.0   0.0  42.68  43.46      0.0       1.0       0.0       0.0   \n",
       "\n",
       "       REGION=4  SEX=1  ...  POVCAT=1  POVCAT=2  POVCAT=3  POVCAT=4  POVCAT=5  \\\n",
       "0           0.0    1.0  ...       1.0       0.0       0.0       0.0       0.0   \n",
       "1           0.0    0.0  ...       0.0       0.0       1.0       0.0       0.0   \n",
       "2           0.0    0.0  ...       0.0       1.0       0.0       0.0       0.0   \n",
       "3           0.0    1.0  ...       0.0       1.0       0.0       0.0       0.0   \n",
       "4           0.0    1.0  ...       0.0       0.0       1.0       0.0       0.0   \n",
       "...         ...    ...  ...       ...       ...       ...       ...       ...   \n",
       "15825       0.0    1.0  ...       1.0       0.0       0.0       0.0       0.0   \n",
       "15826       0.0    0.0  ...       1.0       0.0       0.0       0.0       0.0   \n",
       "15827       0.0    0.0  ...       1.0       0.0       0.0       0.0       0.0   \n",
       "15828       0.0    0.0  ...       0.0       0.0       1.0       0.0       0.0   \n",
       "15829       0.0    0.0  ...       0.0       0.0       1.0       0.0       0.0   \n",
       "\n",
       "       INSCOV=1  INSCOV=2  INSCOV=3  label       weights  \n",
       "0           0.0       1.0       0.0    1.0  17459.483776  \n",
       "1           0.0       1.0       0.0    1.0  14515.313940  \n",
       "2           0.0       1.0       0.0    0.0  18465.607681  \n",
       "3           0.0       1.0       0.0    0.0  21762.696983  \n",
       "4           1.0       0.0       0.0    0.0      0.000000  \n",
       "...         ...       ...       ...    ...           ...  \n",
       "15825       1.0       0.0       0.0    0.0   3727.042408  \n",
       "15826       1.0       0.0       0.0    0.0   4909.081729  \n",
       "15827       0.0       1.0       0.0    0.0   4184.786789  \n",
       "15828       0.0       1.0       0.0    0.0   4427.370919  \n",
       "15829       0.0       1.0       0.0    0.0   6010.846084  \n",
       "\n",
       "[15830 rows x 140 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PCS42</th>\n",
       "      <th>MCS42</th>\n",
       "      <th>K6SUM42</th>\n",
       "      <th>REGION=1</th>\n",
       "      <th>REGION=2</th>\n",
       "      <th>REGION=3</th>\n",
       "      <th>REGION=4</th>\n",
       "      <th>SEX=1</th>\n",
       "      <th>...</th>\n",
       "      <th>EMPST=4</th>\n",
       "      <th>POVCAT=1</th>\n",
       "      <th>POVCAT=2</th>\n",
       "      <th>POVCAT=3</th>\n",
       "      <th>POVCAT=4</th>\n",
       "      <th>POVCAT=5</th>\n",
       "      <th>INSCOV=1</th>\n",
       "      <th>INSCOV=2</th>\n",
       "      <th>INSCOV=3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.93</td>\n",
       "      <td>58.47</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.42</td>\n",
       "      <td>26.57</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.12</td>\n",
       "      <td>50.33</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15825</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.71</td>\n",
       "      <td>62.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.71</td>\n",
       "      <td>62.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15827</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15828</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.97</td>\n",
       "      <td>42.45</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15829</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.68</td>\n",
       "      <td>43.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15830 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AGE  RACE  PCS42  MCS42  K6SUM42  REGION=1  REGION=2  REGION=3  \\\n",
       "0      53.0   1.0  25.93  58.47      3.0       0.0       1.0       0.0   \n",
       "1      56.0   1.0  20.42  26.57     17.0       0.0       1.0       0.0   \n",
       "2      23.0   1.0  53.12  50.33      7.0       0.0       1.0       0.0   \n",
       "3       3.0   1.0  -1.00  -1.00     -1.0       0.0       1.0       0.0   \n",
       "4      27.0   0.0  -1.00  -1.00     -1.0       0.0       0.0       1.0   \n",
       "...     ...   ...    ...    ...      ...       ...       ...       ...   \n",
       "15825  25.0   0.0  56.71  62.39      0.0       0.0       0.0       1.0   \n",
       "15826  25.0   0.0  56.71  62.39      0.0       0.0       0.0       1.0   \n",
       "15827   2.0   1.0  -1.00  -1.00     -1.0       0.0       0.0       1.0   \n",
       "15828  54.0   0.0  43.97  42.45     24.0       1.0       0.0       0.0   \n",
       "15829  73.0   0.0  42.68  43.46      0.0       1.0       0.0       0.0   \n",
       "\n",
       "       REGION=4  SEX=1  ...  EMPST=4  POVCAT=1  POVCAT=2  POVCAT=3  POVCAT=4  \\\n",
       "0           0.0    1.0  ...      1.0       1.0       0.0       0.0       0.0   \n",
       "1           0.0    0.0  ...      1.0       0.0       0.0       1.0       0.0   \n",
       "2           0.0    0.0  ...      0.0       0.0       1.0       0.0       0.0   \n",
       "3           0.0    1.0  ...      0.0       0.0       1.0       0.0       0.0   \n",
       "4           0.0    1.0  ...      0.0       0.0       0.0       1.0       0.0   \n",
       "...         ...    ...  ...      ...       ...       ...       ...       ...   \n",
       "15825       0.0    1.0  ...      0.0       1.0       0.0       0.0       0.0   \n",
       "15826       0.0    0.0  ...      1.0       1.0       0.0       0.0       0.0   \n",
       "15827       0.0    0.0  ...      0.0       1.0       0.0       0.0       0.0   \n",
       "15828       0.0    0.0  ...      0.0       0.0       0.0       1.0       0.0   \n",
       "15829       0.0    0.0  ...      1.0       0.0       0.0       1.0       0.0   \n",
       "\n",
       "       POVCAT=5  INSCOV=1  INSCOV=2  INSCOV=3  label  \n",
       "0           0.0       0.0       1.0       0.0    1.0  \n",
       "1           0.0       0.0       1.0       0.0    1.0  \n",
       "2           0.0       0.0       1.0       0.0    0.0  \n",
       "3           0.0       0.0       1.0       0.0    0.0  \n",
       "4           0.0       1.0       0.0       0.0    0.0  \n",
       "...         ...       ...       ...       ...    ...  \n",
       "15825       0.0       1.0       0.0       0.0    0.0  \n",
       "15826       0.0       1.0       0.0       0.0    0.0  \n",
       "15827       0.0       0.0       1.0       0.0    0.0  \n",
       "15828       0.0       0.0       1.0       0.0    0.0  \n",
       "15829       0.0       0.0       1.0       0.0    0.0  \n",
       "\n",
       "[15830 rows x 139 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset_orig_panel19_train.features\n",
    "label = dataset_orig_panel19_train.labels\n",
    "feature_names = dataset_orig_panel19_train.feature_names\n",
    "dataset_orig_panel19_train_df = pd.DataFrame(features, columns=feature_names)\n",
    "dataset_orig_panel19_train_df['label'] = label\n",
    "dataset_orig_panel19_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `dataset_transf_panel19_train_lfr` is your LFR transformed dataset\n",
    "# features_lfr = dataset_transf_panel19_train_lfr.features\n",
    "# label_lfr = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary\n",
    "# feature_names_lfr = dataset_transf_panel19_train_lfr.feature_names\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df_lfr = pd.DataFrame(features_lfr, columns=feature_names_lfr)\n",
    "# df_lfr['label'] = label_lfr\n",
    "\n",
    "\n",
    "features = dataset_transf_panel19_train_lfr.features\n",
    "label = dataset_transf_panel19_train_lfr.labels.ravel()  # Flatten the label array if necessary\n",
    "weights = dataset_transf_panel19_train_lfr.instance_weights\n",
    "feature_names = dataset_transf_panel19_train_lfr.feature_names\n",
    "df_lfr = pd.DataFrame(features, columns=feature_names)\n",
    "df_lfr['label'] = label\n",
    "df_lfr['weights'] = weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PCS42</th>\n",
       "      <th>MCS42</th>\n",
       "      <th>K6SUM42</th>\n",
       "      <th>REGION=1</th>\n",
       "      <th>REGION=2</th>\n",
       "      <th>REGION=3</th>\n",
       "      <th>REGION=4</th>\n",
       "      <th>SEX=1</th>\n",
       "      <th>...</th>\n",
       "      <th>POVCAT=1</th>\n",
       "      <th>POVCAT=2</th>\n",
       "      <th>POVCAT=3</th>\n",
       "      <th>POVCAT=4</th>\n",
       "      <th>POVCAT=5</th>\n",
       "      <th>INSCOV=1</th>\n",
       "      <th>INSCOV=2</th>\n",
       "      <th>INSCOV=3</th>\n",
       "      <th>label</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.364698</td>\n",
       "      <td>0.611075</td>\n",
       "      <td>0.319138</td>\n",
       "      <td>0.516439</td>\n",
       "      <td>0.683071</td>\n",
       "      <td>0.513830</td>\n",
       "      <td>0.556535</td>\n",
       "      <td>0.363196</td>\n",
       "      <td>0.402933</td>\n",
       "      <td>0.589060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238986</td>\n",
       "      <td>0.553776</td>\n",
       "      <td>0.493602</td>\n",
       "      <td>0.513294</td>\n",
       "      <td>0.386629</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.498550</td>\n",
       "      <td>0.363095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21854.981705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.379547</td>\n",
       "      <td>0.597793</td>\n",
       "      <td>0.328517</td>\n",
       "      <td>0.502386</td>\n",
       "      <td>0.682940</td>\n",
       "      <td>0.523566</td>\n",
       "      <td>0.577716</td>\n",
       "      <td>0.373460</td>\n",
       "      <td>0.395062</td>\n",
       "      <td>0.600292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237634</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.504359</td>\n",
       "      <td>0.533097</td>\n",
       "      <td>0.374361</td>\n",
       "      <td>0.356593</td>\n",
       "      <td>0.499183</td>\n",
       "      <td>0.369585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18169.604822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.345017</td>\n",
       "      <td>0.593873</td>\n",
       "      <td>0.327056</td>\n",
       "      <td>0.509281</td>\n",
       "      <td>0.682566</td>\n",
       "      <td>0.524971</td>\n",
       "      <td>0.551491</td>\n",
       "      <td>0.349275</td>\n",
       "      <td>0.416220</td>\n",
       "      <td>0.575195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246142</td>\n",
       "      <td>0.570321</td>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.519411</td>\n",
       "      <td>0.420631</td>\n",
       "      <td>0.397081</td>\n",
       "      <td>0.509235</td>\n",
       "      <td>0.382303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17191.832515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370890</td>\n",
       "      <td>0.582912</td>\n",
       "      <td>0.338159</td>\n",
       "      <td>0.494210</td>\n",
       "      <td>0.676912</td>\n",
       "      <td>0.535713</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.369280</td>\n",
       "      <td>0.388361</td>\n",
       "      <td>0.583862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233116</td>\n",
       "      <td>0.577892</td>\n",
       "      <td>0.512044</td>\n",
       "      <td>0.551296</td>\n",
       "      <td>0.382591</td>\n",
       "      <td>0.373165</td>\n",
       "      <td>0.506936</td>\n",
       "      <td>0.374916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20261.485463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.377148</td>\n",
       "      <td>0.587833</td>\n",
       "      <td>0.331081</td>\n",
       "      <td>0.493269</td>\n",
       "      <td>0.679347</td>\n",
       "      <td>0.528798</td>\n",
       "      <td>0.582084</td>\n",
       "      <td>0.376288</td>\n",
       "      <td>0.383771</td>\n",
       "      <td>0.602294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>0.558482</td>\n",
       "      <td>0.514984</td>\n",
       "      <td>0.543083</td>\n",
       "      <td>0.382213</td>\n",
       "      <td>0.364254</td>\n",
       "      <td>0.495283</td>\n",
       "      <td>0.377611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15825</th>\n",
       "      <td>0.343260</td>\n",
       "      <td>0.597189</td>\n",
       "      <td>0.324804</td>\n",
       "      <td>0.512213</td>\n",
       "      <td>0.683004</td>\n",
       "      <td>0.522468</td>\n",
       "      <td>0.548023</td>\n",
       "      <td>0.347933</td>\n",
       "      <td>0.417919</td>\n",
       "      <td>0.574523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246629</td>\n",
       "      <td>0.568838</td>\n",
       "      <td>0.508985</td>\n",
       "      <td>0.514854</td>\n",
       "      <td>0.421901</td>\n",
       "      <td>0.399881</td>\n",
       "      <td>0.508553</td>\n",
       "      <td>0.380749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4111.315754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.596636</td>\n",
       "      <td>0.325113</td>\n",
       "      <td>0.511802</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.522864</td>\n",
       "      <td>0.548279</td>\n",
       "      <td>0.347965</td>\n",
       "      <td>0.417634</td>\n",
       "      <td>0.574357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246577</td>\n",
       "      <td>0.569381</td>\n",
       "      <td>0.509405</td>\n",
       "      <td>0.515495</td>\n",
       "      <td>0.422105</td>\n",
       "      <td>0.400035</td>\n",
       "      <td>0.508671</td>\n",
       "      <td>0.381059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5415.228173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15827</th>\n",
       "      <td>0.364434</td>\n",
       "      <td>0.584266</td>\n",
       "      <td>0.332311</td>\n",
       "      <td>0.494401</td>\n",
       "      <td>0.677981</td>\n",
       "      <td>0.531335</td>\n",
       "      <td>0.573192</td>\n",
       "      <td>0.367677</td>\n",
       "      <td>0.389164</td>\n",
       "      <td>0.590592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240138</td>\n",
       "      <td>0.570064</td>\n",
       "      <td>0.517921</td>\n",
       "      <td>0.541949</td>\n",
       "      <td>0.397951</td>\n",
       "      <td>0.380818</td>\n",
       "      <td>0.500083</td>\n",
       "      <td>0.381961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3896.116219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15828</th>\n",
       "      <td>0.364365</td>\n",
       "      <td>0.591464</td>\n",
       "      <td>0.330698</td>\n",
       "      <td>0.502169</td>\n",
       "      <td>0.683230</td>\n",
       "      <td>0.527371</td>\n",
       "      <td>0.569395</td>\n",
       "      <td>0.362334</td>\n",
       "      <td>0.406439</td>\n",
       "      <td>0.589670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242613</td>\n",
       "      <td>0.560312</td>\n",
       "      <td>0.511955</td>\n",
       "      <td>0.530772</td>\n",
       "      <td>0.397606</td>\n",
       "      <td>0.370904</td>\n",
       "      <td>0.505687</td>\n",
       "      <td>0.379358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4883.851005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15829</th>\n",
       "      <td>0.369746</td>\n",
       "      <td>0.591171</td>\n",
       "      <td>0.330650</td>\n",
       "      <td>0.499689</td>\n",
       "      <td>0.681767</td>\n",
       "      <td>0.527452</td>\n",
       "      <td>0.573792</td>\n",
       "      <td>0.367704</td>\n",
       "      <td>0.397916</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240415</td>\n",
       "      <td>0.559683</td>\n",
       "      <td>0.511546</td>\n",
       "      <td>0.535024</td>\n",
       "      <td>0.389960</td>\n",
       "      <td>0.368115</td>\n",
       "      <td>0.501997</td>\n",
       "      <td>0.377222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6630.588948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15830 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AGE      RACE     PCS42     MCS42   K6SUM42  REGION=1  REGION=2  \\\n",
       "0      0.364698  0.611075  0.319138  0.516439  0.683071  0.513830  0.556535   \n",
       "1      0.379547  0.597793  0.328517  0.502386  0.682940  0.523566  0.577716   \n",
       "2      0.345017  0.593873  0.327056  0.509281  0.682566  0.524971  0.551491   \n",
       "3      0.370890  0.582912  0.338159  0.494210  0.676912  0.535713  0.577500   \n",
       "4      0.377148  0.587833  0.331081  0.493269  0.679347  0.528798  0.582084   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15825  0.343260  0.597189  0.324804  0.512213  0.683004  0.522468  0.548023   \n",
       "15826  0.343186  0.596636  0.325113  0.511802  0.682861  0.522864  0.548279   \n",
       "15827  0.364434  0.584266  0.332311  0.494401  0.677981  0.531335  0.573192   \n",
       "15828  0.364365  0.591464  0.330698  0.502169  0.683230  0.527371  0.569395   \n",
       "15829  0.369746  0.591171  0.330650  0.499689  0.681767  0.527452  0.573792   \n",
       "\n",
       "       REGION=3  REGION=4     SEX=1  ...  POVCAT=1  POVCAT=2  POVCAT=3  \\\n",
       "0      0.363196  0.402933  0.589060  ...  0.238986  0.553776  0.493602   \n",
       "1      0.373460  0.395062  0.600292  ...  0.237634  0.551191  0.504359   \n",
       "2      0.349275  0.416220  0.575195  ...  0.246142  0.570321  0.511401   \n",
       "3      0.369280  0.388361  0.583862  ...  0.233116  0.577892  0.512044   \n",
       "4      0.376288  0.383771  0.602294  ...  0.238654  0.558482  0.514984   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "15825  0.347933  0.417919  0.574523  ...  0.246629  0.568838  0.508985   \n",
       "15826  0.347965  0.417634  0.574357  ...  0.246577  0.569381  0.509405   \n",
       "15827  0.367677  0.389164  0.590592  ...  0.240138  0.570064  0.517921   \n",
       "15828  0.362334  0.406439  0.589670  ...  0.242613  0.560312  0.511955   \n",
       "15829  0.367704  0.397916  0.593893  ...  0.240415  0.559683  0.511546   \n",
       "\n",
       "       POVCAT=4  POVCAT=5  INSCOV=1  INSCOV=2  INSCOV=3  label       weights  \n",
       "0      0.513294  0.386629  0.380919  0.498550  0.363095    0.0  21854.981705  \n",
       "1      0.533097  0.374361  0.356593  0.499183  0.369585    0.0  18169.604822  \n",
       "2      0.519411  0.420631  0.397081  0.509235  0.382303    0.0  17191.832515  \n",
       "3      0.551296  0.382591  0.373165  0.506936  0.374916    0.0  20261.485463  \n",
       "4      0.543083  0.382213  0.364254  0.495283  0.377611    0.0      0.000000  \n",
       "...         ...       ...       ...       ...       ...    ...           ...  \n",
       "15825  0.514854  0.421901  0.399881  0.508553  0.380749    0.0   4111.315754  \n",
       "15826  0.515495  0.422105  0.400035  0.508671  0.381059    0.0   5415.228173  \n",
       "15827  0.541949  0.397951  0.380818  0.500083  0.381961    0.0   3896.116219  \n",
       "15828  0.530772  0.397606  0.370904  0.505687  0.379358    0.0   4883.851005  \n",
       "15829  0.535024  0.389960  0.368115  0.501997  0.377222    0.0   6630.588948  \n",
       "\n",
       "[15830 rows x 140 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PCS42</th>\n",
       "      <th>MCS42</th>\n",
       "      <th>K6SUM42</th>\n",
       "      <th>REGION=1</th>\n",
       "      <th>REGION=2</th>\n",
       "      <th>REGION=3</th>\n",
       "      <th>REGION=4</th>\n",
       "      <th>SEX=1</th>\n",
       "      <th>...</th>\n",
       "      <th>EMPST=4</th>\n",
       "      <th>POVCAT=1</th>\n",
       "      <th>POVCAT=2</th>\n",
       "      <th>POVCAT=3</th>\n",
       "      <th>POVCAT=4</th>\n",
       "      <th>POVCAT=5</th>\n",
       "      <th>INSCOV=1</th>\n",
       "      <th>INSCOV=2</th>\n",
       "      <th>INSCOV=3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.93</td>\n",
       "      <td>58.47</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.42</td>\n",
       "      <td>26.53</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.92</td>\n",
       "      <td>50.28</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15825</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.68</td>\n",
       "      <td>62.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.68</td>\n",
       "      <td>62.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15827</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15828</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.43</td>\n",
       "      <td>42.45</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15829</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.83</td>\n",
       "      <td>43.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15830 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AGE  RACE  PCS42  MCS42  K6SUM42  REGION=1  REGION=2  REGION=3  \\\n",
       "0      53.0   1.0  25.93  58.47      3.0       0.0       1.0       0.0   \n",
       "1      56.0   1.0  20.42  26.53     17.0       0.0       1.0       0.0   \n",
       "2      23.0   1.0  52.92  50.28      7.0       0.0       1.0       0.0   \n",
       "3       3.0   1.0  -1.00  -1.00     -1.0       0.0       1.0       0.0   \n",
       "4      27.0   0.0  -1.00  -1.00     -1.0       0.0       0.0       1.0   \n",
       "...     ...   ...    ...    ...      ...       ...       ...       ...   \n",
       "15825  25.0   0.0  56.68  62.11      0.0       0.0       0.0       1.0   \n",
       "15826  25.0   0.0  56.68  62.11      0.0       0.0       0.0       1.0   \n",
       "15827   2.0   1.0  -1.00  -1.00     -1.0       0.0       0.0       1.0   \n",
       "15828  54.0   0.0  43.43  42.45     24.0       1.0       0.0       0.0   \n",
       "15829  73.0   0.0  41.83  43.46      0.0       1.0       0.0       0.0   \n",
       "\n",
       "       REGION=4  SEX=1  ...  EMPST=4  POVCAT=1  POVCAT=2  POVCAT=3  POVCAT=4  \\\n",
       "0           0.0    1.0  ...      1.0       1.0       0.0       0.0       0.0   \n",
       "1           0.0    0.0  ...      1.0       0.0       0.0       1.0       0.0   \n",
       "2           0.0    0.0  ...      0.0       0.0       1.0       0.0       0.0   \n",
       "3           0.0    1.0  ...      0.0       0.0       1.0       0.0       0.0   \n",
       "4           0.0    1.0  ...      0.0       0.0       0.0       1.0       0.0   \n",
       "...         ...    ...  ...      ...       ...       ...       ...       ...   \n",
       "15825       0.0    1.0  ...      0.0       1.0       0.0       0.0       0.0   \n",
       "15826       0.0    0.0  ...      1.0       1.0       0.0       0.0       0.0   \n",
       "15827       0.0    0.0  ...      0.0       1.0       0.0       0.0       0.0   \n",
       "15828       0.0    0.0  ...      0.0       0.0       0.0       1.0       0.0   \n",
       "15829       0.0    0.0  ...      1.0       0.0       0.0       1.0       0.0   \n",
       "\n",
       "       POVCAT=5  INSCOV=1  INSCOV=2  INSCOV=3  label  \n",
       "0           0.0       0.0       1.0       0.0    1.0  \n",
       "1           0.0       0.0       1.0       0.0    1.0  \n",
       "2           0.0       0.0       1.0       0.0    0.0  \n",
       "3           0.0       0.0       1.0       0.0    0.0  \n",
       "4           0.0       1.0       0.0       0.0    0.0  \n",
       "...         ...       ...       ...       ...    ...  \n",
       "15825       0.0       1.0       0.0       0.0    0.0  \n",
       "15826       0.0       1.0       0.0       0.0    0.0  \n",
       "15827       0.0       0.0       1.0       0.0    0.0  \n",
       "15828       0.0       0.0       1.0       0.0    0.0  \n",
       "15829       0.0       0.0       1.0       0.0    0.0  \n",
       "\n",
       "[15830 rows x 139 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `dataset_transf_panel19_train_di` is your DI transformed dataset\n",
    "features_di = dataset_transf_panel19_train_di.features\n",
    "label_di = dataset_transf_panel19_train_di.labels.ravel()  # Flatten the label array if necessary\n",
    "feature_names_di = dataset_transf_panel19_train_di.feature_names\n",
    "\n",
    "# Create a DataFrame\n",
    "df_di = pd.DataFrame(features_di, columns=feature_names_di)\n",
    "df_di['label'] = label_di\n",
    "df_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8669456727732154, Training Log Loss: 0.3230202782236161\n",
      "Testing Accuracy: 0.8619709412507897, Testing Log Loss: 0.335720614452724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/1117411220.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n",
    "df=dataset_orig_panel19_train_df\n",
    "# Split the DataFrame into features, labels, and weights\n",
    "\n",
    "X = df.drop(['label'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using the sample_weight parameter\n",
    "# We need to extract the corresponding weights for the training samples\n",
    "train_indices = X_train.index\n",
    "train_weights = weights[train_indices]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Probability predictions for log loss calculation\n",
    "y_train_pred_proba = model.predict_proba(X_train)\n",
    "y_test_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate log loss\n",
    "train_loss = log_loss(y_train, y_train_pred_proba)\n",
    "test_loss = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "# Print errors and losses\n",
    "print(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "\n",
    "# Log Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Train vs Test Log Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).to_csv('meps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8651295009475679, Training Log Loss: 0.33949214668723315\n",
      "Testing Accuracy: 0.8562855337965888, Testing Log Loss: 0.3484870792605314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/2225796879.py:63: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n",
    "\n",
    "# Split the DataFrame into features, labels, and weights\n",
    "X = df_rw.drop(['label', 'weights'], axis=1)\n",
    "y = df_rw['label']\n",
    "weights = df_rw['weights']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model_rw = LogisticRegression()\n",
    "\n",
    "# Train the model using the sample_weight parameter\n",
    "# We need to extract the corresponding weights for the training samples\n",
    "train_indices = X_train.index\n",
    "train_weights = weights[train_indices]\n",
    "\n",
    "model_rw.fit(X_train, y_train, sample_weight=train_weights)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model_rw.predict(X_train)\n",
    "y_test_pred = model_rw.predict(X_test)\n",
    "\n",
    "# Probability predictions for log loss calculation\n",
    "y_train_pred_proba = model_rw.predict_proba(X_train)\n",
    "y_test_pred_proba = model_rw.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate log loss\n",
    "train_loss = log_loss(y_train, y_train_pred_proba)\n",
    "test_loss = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "# Print errors and losses\n",
    "print(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "\n",
    "# Log Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Train vs Test Log Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8646557169930512, Training Log Loss: 0.33943553174854924\n",
      "Testing Accuracy: 0.8550221099178774, Testing Log Loss: 0.3481697691277553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/folders/q1/_mtfzg810qx1420xf23g4f7c0000gn/T/ipykernel_4547/789110131.py:62: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Assuming 'df' is your Pandas DataFrame with features, 'label', and 'weights' columns\n",
    "\n",
    "# Split the DataFrame into features, labels, and weights\n",
    "X = df_di.drop(['label'], axis=1)\n",
    "y = df_di['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model_di = LogisticRegression()\n",
    "\n",
    "# Train the model using the sample_weight parameter\n",
    "# We need to extract the corresponding weights for the training samples\n",
    "train_indices = X_train.index\n",
    "train_weights = weights[train_indices]\n",
    "\n",
    "model_di.fit(X_train, y_train, sample_weight=train_weights)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model_di.predict(X_train)\n",
    "y_test_pred = model_di.predict(X_test)\n",
    "\n",
    "# Probability predictions for log loss calculation\n",
    "y_train_pred_proba = model_di.predict_proba(X_train)\n",
    "y_test_pred_proba = model_di.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate log loss\n",
    "train_loss = log_loss(y_train, y_train_pred_proba)\n",
    "test_loss = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "# Print errors and losses\n",
    "print(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "\n",
    "# Log Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Train vs Test Log Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model_lfr \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model (no sample weights are needed as the data is already transformed by LFR)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel_lfr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m model_lfr\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1246\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1244\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1251\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1254\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Assuming 'df_lfr' is your Pandas DataFrame with features and 'label' columns, \n",
    "# resulting from the LFR transformation\n",
    "\n",
    "# Split the DataFrame into features and labels\n",
    "X = df_lfr.drop(['label'], axis=1)\n",
    "y = df_lfr['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the model\n",
    "model_lfr = LogisticRegression()\n",
    "\n",
    "# Train the model (no sample weights are needed as the data is already transformed by LFR)\n",
    "model_lfr.fit(X_train, y_train, sample_weight=train_weights)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model_lfr.predict(X_train)\n",
    "y_test_pred = model_lfr.predict(X_test)\n",
    "\n",
    "# Probability predictions for log loss calculation\n",
    "y_train_pred_proba = model_lfr.predict_proba(X_train)\n",
    "y_test_pred_proba = model_lfr.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate log loss\n",
    "train_loss = log_loss(y_train, y_train_pred_proba)\n",
    "test_loss = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "# Print errors and losses\n",
    "print(f\"Training Accuracy: {train_accuracy}, Training Log Loss: {train_loss}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}, Testing Log Loss: {test_loss}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "\n",
    "# Log Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Train Log Loss', 'Test Log Loss'], [train_loss, test_loss], color=['blue', 'green'])\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Train vs Test Log Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
