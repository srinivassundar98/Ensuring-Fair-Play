<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Discrimination Loss Fn</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d4be639c637f3db3c684c66cefad7e0c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
if (typeof require !== 'undefined') {
require.undef("plotly");
requirejs.config({
    paths: {
        'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']
    }
});
require(['plotly'], function(Plotly) {
    window._Plotly = Plotly;
});
}
</script>



</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Discrimination Loss Fn</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="631adb03" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install aif360</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Defaulting to user installation because normal site-packages is not writeable
Collecting aif360
  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)
Requirement already satisfied: numpy&gt;=1.16 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from aif360) (1.22.3)
Requirement already satisfied: scipy&gt;=1.2.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from aif360) (1.10.1)
Requirement already satisfied: pandas&gt;=0.24.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from aif360) (1.3.5)
Requirement already satisfied: scikit-learn&gt;=1.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from aif360) (1.3.2)
Requirement already satisfied: matplotlib in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from aif360) (3.5.2)
Requirement already satisfied: python-dateutil&gt;=2.7.3 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from pandas&gt;=0.24.0-&gt;aif360) (2.8.2)
Requirement already satisfied: pytz&gt;=2017.3 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from pandas&gt;=0.24.0-&gt;aif360) (2022.1)
Requirement already satisfied: joblib&gt;=1.1.1 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from scikit-learn&gt;=1.0-&gt;aif360) (1.3.2)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from scikit-learn&gt;=1.0-&gt;aif360) (3.1.0)
Requirement already satisfied: cycler&gt;=0.10 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from matplotlib-&gt;aif360) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from matplotlib-&gt;aif360) (4.33.3)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from matplotlib-&gt;aif360) (1.4.2)
Requirement already satisfied: packaging&gt;=20.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from matplotlib-&gt;aif360) (21.3)
Requirement already satisfied: pillow&gt;=6.2.0 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from matplotlib-&gt;aif360) (9.1.0)
Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from matplotlib-&gt;aif360) (3.0.7)
Requirement already satisfied: six&gt;=1.5 in c:\users\srinivas\appdata\roaming\python\python38\site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24.0-&gt;aif360) (1.16.0)
Downloading aif360-0.6.1-py3-none-any.whl (259 kB)
Installing collected packages: aif360
Successfully installed aif360-0.6.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.
Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.
To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.
WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -orch (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -rotobuf (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier
    tinycss2 (&gt;=1.1.0&lt;1.2) ; extra == 'css'
             ~~~~~~~~^
WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -orch (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -rotobuf (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -orch (c:\users\srinivas\appdata\roaming\python\python38\site-packages)
WARNING: Ignoring invalid distribution -rotobuf (c:\users\srinivas\appdata\roaming\python\python38\site-packages)

[notice] A new release of pip is available: 24.3.1 -&gt; 25.0.1
[notice] To update, run: python.exe -m pip install --upgrade pip</code></pre>
</div>
</div>
<div id="48ef4fb3" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime, timedelta</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, <span class="st">'../'</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.datasets <span class="im">import</span> MEPSDataset19</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Fairness metrics</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.metrics <span class="im">import</span> BinaryLabelDatasetMetric, ClassificationMetric</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Explainers</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aif360.explainers <span class="im">import</span> MetricTextExplainer</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalers</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:
pip install 'aif360[inFairness]'</code></pre>
</div>
</div>
<div id="610b78a0" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train <span class="op">=</span> MEPSDataset19()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f0d148fb" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sens_ind <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sens_attr <span class="op">=</span> dataset_orig_panel19_train.protected_attribute_names[sens_ind]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>unprivileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                    dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>privileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                    dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1e7b6b53" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>dataset_orig_panel19_train.feature_names[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>'RACE'</code></pre>
</div>
</div>
<div id="12a8df53" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dataset_orig_panel19_train.features</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dataset_orig_panel19_train.labels.ravel()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale features</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> X_train</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> X_test</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Train logistic regression model</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_scaled, y_train)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities on the same scaled training data</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>train_probabilities <span class="op">=</span> model.predict_proba(X_train_scaled)[:, <span class="dv">1</span>]</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculation of discrimination index without modifying dataset structure</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>sens_attr_index <span class="op">=</span> dataset_orig_panel19_train.feature_names.index(<span class="st">'RACE'</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define unprivileged and privileged values</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>unprivileged_val <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>privileged_val <span class="op">=</span> <span class="fl">1.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\srinivas\AppData\Roaming\Python\Python38\site-packages\sklearn\linear_model\_logistic.py:460: ConvergenceWarning:

lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
</code></pre>
</div>
</div>
<div id="46627d68" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom loss function</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discrimination_loss(output, target, sensitive_features, lambda_val<span class="op">=</span><span class="dv">10</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    prob_unpriv <span class="op">=</span> torch.mean(output[mask_unpriv])</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    prob_priv <span class="op">=</span> torch.mean(output[mask_priv])</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> lambda_val <span class="op">*</span> (prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    loss_val<span class="op">=</span>(<span class="dv">1</span> <span class="op">+</span> lambda_val <span class="op">*</span> discrimination) <span class="op">*</span> standard_loss</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_val,discrimination.item() </span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_accuracy(predictions, targets):</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> (predictions <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (predicted_classes <span class="op">==</span> targets).<span class="bu">float</span>().mean()</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Correctly preparing the sensitive features</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Adjust the threshold according to your specific case</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> torch.tensor((data[:, <span class="dv">1</span>].numpy() <span class="op">&gt;</span> threshold).astype(<span class="bu">float</span>)).<span class="bu">float</span>()</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming similar preparation for test data</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> torch.tensor(X_test_scaled).<span class="bu">float</span>()</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>test_targets <span class="op">=</span> torch.tensor(y_test).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>test_sensitive_features <span class="op">=</span> torch.tensor((test_data[:, <span class="dv">1</span>].numpy() <span class="op">&gt;</span> threshold).astype(<span class="bu">float</span>)).<span class="bu">float</span>()</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> torch.cat((test_data[:, :<span class="dv">1</span>], test_data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model2.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>train_losses, train_accuracies, train_discriminations,train_fairness <span class="op">=</span> [], [], [],[]</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>test_losses, test_accuracies, test_discriminations,test_fairness <span class="op">=</span> [], [], [],[]</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>model2.train()</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model2(features)</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features)</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>    model2.<span class="bu">eval</span>()</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model2(test_features)</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>    train_losses.append(loss.item())</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    train_accuracies.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>    train_discriminations.append(discrimination)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>    train_fairness.append(fairness)</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>    test_fairness.append(fairness)</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss.item())</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>    test_accuracies.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>    test_discriminations.append(test_discrimination)</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>    model2.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 1.2005432844161987, Train Acc: 25.40%, Train Discrimination: 0.014157179743051529 Test Loss: 1.3504408597946167, Test Acc: 68.04%,Test Discrimination: 0.06081617996096611
Epoch 2, Train Loss: 1.1340562105178833, Train Acc: 69.18%, Train Discrimination: 0.037128642201423645 Test Loss: 1.2218055725097656, Test Acc: 82.66%,Test Discrimination: 0.0197057593613863
Epoch 3, Train Loss: 1.1161670684814453, Train Acc: 82.87%, Train Discrimination: 0.011497516185045242 Test Loss: 1.1219680309295654, Test Acc: 82.66%,Test Discrimination: 0.005324292927980423
Epoch 4, Train Loss: 1.074970006942749, Train Acc: 82.87%, Train Discrimination: 0.003039707662537694 Test Loss: 1.0276302099227905, Test Acc: 82.66%,Test Discrimination: 0.0017148805782198906
Epoch 5, Train Loss: 0.9983471632003784, Train Acc: 82.87%, Train Discrimination: 0.000977194868028164 Test Loss: 0.8889058232307434, Test Acc: 82.66%,Test Discrimination: 0.000632383453194052
Epoch 6, Train Loss: 0.8670161962509155, Train Acc: 82.87%, Train Discrimination: 0.00035825211671181023 Test Loss: 0.714589536190033, Test Acc: 82.66%,Test Discrimination: 0.00015712952881585807
Epoch 7, Train Loss: 0.6973254084587097, Train Acc: 82.87%, Train Discrimination: 7.520290091633797e-05 Test Loss: 0.5324214100837708, Test Acc: 82.66%,Test Discrimination: 0.00021039014973212034
Epoch 8, Train Loss: 0.5200832486152649, Train Acc: 82.87%, Train Discrimination: 0.00022246502339839935 Test Loss: 0.4716324508190155, Test Acc: 82.75%,Test Discrimination: 0.010705008171498775
Epoch 9, Train Loss: 0.45311859250068665, Train Acc: 83.01%, Train Discrimination: 0.008226670324802399 Test Loss: 0.7152367234230042, Test Acc: 82.60%,Test Discrimination: 0.05112812668085098
Epoch 10, Train Loss: 0.6443049907684326, Train Acc: 83.39%, Train Discrimination: 0.03706512972712517 Test Loss: 0.6685908436775208, Test Acc: 83.35%,Test Discrimination: 0.046006254851818085
Epoch 11, Train Loss: 0.6075366735458374, Train Acc: 83.77%, Train Discrimination: 0.03349513188004494 Test Loss: 0.4885546863079071, Test Acc: 82.98%,Test Discrimination: 0.018402203917503357
Epoch 12, Train Loss: 0.46437230706214905, Train Acc: 83.35%, Train Discrimination: 0.013792199082672596 Test Loss: 0.43772050738334656, Test Acc: 82.66%,Test Discrimination: 0.004386838525533676
Epoch 13, Train Loss: 0.426496684551239, Train Acc: 82.90%, Train Discrimination: 0.0034285150468349457 Test Loss: 0.45840921998023987, Test Acc: 82.66%,Test Discrimination: 0.0009370779152959585
Epoch 14, Train Loss: 0.4488235116004944, Train Acc: 82.87%, Train Discrimination: 0.0007487634429708123 Test Loss: 0.48420649766921997, Test Acc: 82.66%,Test Discrimination: 0.00020888439030386508
Epoch 15, Train Loss: 0.4745861291885376, Train Acc: 82.87%, Train Discrimination: 0.00017393939197063446 Test Loss: 0.4974011182785034, Test Acc: 82.66%,Test Discrimination: 4.43688259110786e-05
Epoch 16, Train Loss: 0.48769083619117737, Train Acc: 82.87%, Train Discrimination: 4.0095881558954716e-05 Test Loss: 0.49500569701194763, Test Acc: 82.66%,Test Discrimination: 1.2954888006788678e-05
Epoch 17, Train Loss: 0.4854975640773773, Train Acc: 82.87%, Train Discrimination: 1.2169022738817148e-05 Test Loss: 0.47962817549705505, Test Acc: 82.66%,Test Discrimination: 1.7027890862664208e-05
Epoch 18, Train Loss: 0.47075119614601135, Train Acc: 82.87%, Train Discrimination: 1.4123769688012544e-05 Test Loss: 0.4566044211387634, Test Acc: 82.66%,Test Discrimination: 7.318196003325284e-05
Epoch 19, Train Loss: 0.44868138432502747, Train Acc: 82.87%, Train Discrimination: 5.647099533234723e-05 Test Loss: 0.43277832865715027, Test Acc: 82.66%,Test Discrimination: 0.00032303319312632084
Epoch 20, Train Loss: 0.4259135127067566, Train Acc: 82.87%, Train Discrimination: 0.00024351027968805283 Test Loss: 0.4155779778957367, Test Acc: 82.69%,Test Discrimination: 0.0010984605178236961
Epoch 21, Train Loss: 0.40939560532569885, Train Acc: 82.89%, Train Discrimination: 0.0008234671549871564 Test Loss: 0.41082775592803955, Test Acc: 82.72%,Test Discrimination: 0.0028028381057083607
Epoch 22, Train Loss: 0.4043589234352112, Train Acc: 83.07%, Train Discrimination: 0.0021003808360546827 Test Loss: 0.41864466667175293, Test Acc: 83.35%,Test Discrimination: 0.005389069672673941
Epoch 23, Train Loss: 0.41090667247772217, Train Acc: 83.59%, Train Discrimination: 0.004053126089274883 Test Loss: 0.4310532808303833, Test Acc: 84.14%,Test Discrimination: 0.007870962843298912
Epoch 24, Train Loss: 0.4218834638595581, Train Acc: 84.18%, Train Discrimination: 0.005946801044046879 Test Loss: 0.4365692138671875, Test Acc: 84.62%,Test Discrimination: 0.008870854042470455
Epoch 25, Train Loss: 0.42714983224868774, Train Acc: 84.64%, Train Discrimination: 0.006790172308683395 Test Loss: 0.4300886392593384, Test Acc: 84.59%,Test Discrimination: 0.007894868962466717
Epoch 26, Train Loss: 0.42204299569129944, Train Acc: 84.63%, Train Discrimination: 0.006190710701048374 Test Loss: 0.41647931933403015, Test Acc: 84.43%,Test Discrimination: 0.005689942743629217
Epoch 27, Train Loss: 0.4105229377746582, Train Acc: 84.36%, Train Discrimination: 0.004645996727049351 Test Loss: 0.40423160791397095, Test Acc: 83.86%,Test Discrimination: 0.003438601503148675
Epoch 28, Train Loss: 0.39972370862960815, Train Acc: 84.03%, Train Discrimination: 0.002983218291774392 Test Loss: 0.39808389544487, Test Acc: 83.51%,Test Discrimination: 0.001831222209148109
Epoch 29, Train Loss: 0.39392372965812683, Train Acc: 83.64%, Train Discrimination: 0.0017365551320835948 Test Loss: 0.39776673913002014, Test Acc: 83.35%,Test Discrimination: 0.0009413770749233663
Epoch 30, Train Loss: 0.3932155966758728, Train Acc: 83.34%, Train Discrimination: 0.0010058180196210742 Test Loss: 0.4004814922809601, Test Acc: 83.16%,Test Discrimination: 0.0005395567859522998
Epoch 31, Train Loss: 0.39529725909233093, Train Acc: 83.20%, Train Discrimination: 0.0006552800186909735 Test Loss: 0.40331077575683594, Test Acc: 83.04%,Test Discrimination: 0.00040668746805749834
Epoch 32, Train Loss: 0.39764776825904846, Train Acc: 83.13%, Train Discrimination: 0.0005417151842266321 Test Loss: 0.40429383516311646, Test Acc: 83.07%,Test Discrimination: 0.00043160252971574664
Epoch 33, Train Loss: 0.39840617775917053, Train Acc: 83.12%, Train Discrimination: 0.000589441682677716 Test Loss: 0.40266871452331543, Test Acc: 83.16%,Test Discrimination: 0.0006084764027036726
Epoch 34, Train Loss: 0.39677801728248596, Train Acc: 83.19%, Train Discrimination: 0.0007881385972723365 Test Loss: 0.3989834785461426, Test Acc: 83.35%,Test Discrimination: 0.0009820341365411878
Epoch 35, Train Loss: 0.3932212293148041, Train Acc: 83.35%, Train Discrimination: 0.0011707082157954574 Test Loss: 0.39452505111694336, Test Acc: 83.64%,Test Discrimination: 0.0015984605997800827
Epoch 36, Train Loss: 0.3890628218650818, Train Acc: 83.65%, Train Discrimination: 0.0017822475638240576 Test Loss: 0.3908197283744812, Test Acc: 83.77%,Test Discrimination: 0.0024994472041726112
Epoch 37, Train Loss: 0.38574594259262085, Train Acc: 83.96%, Train Discrimination: 0.0026481803506612778 Test Loss: 0.3890173137187958, Test Acc: 84.27%,Test Discrimination: 0.003634978085756302
Epoch 38, Train Loss: 0.3842768371105194, Train Acc: 84.26%, Train Discrimination: 0.0037144499365240335 Test Loss: 0.38918423652648926, Test Acc: 84.55%,Test Discrimination: 0.004832973703742027
Epoch 39, Train Loss: 0.3846794664859772, Train Acc: 84.72%, Train Discrimination: 0.004816039465367794 Test Loss: 0.3902970850467682, Test Acc: 84.62%,Test Discrimination: 0.005801618564873934
Epoch 40, Train Loss: 0.3859330415725708, Train Acc: 84.91%, Train Discrimination: 0.005684469826519489 Test Loss: 0.3907865285873413, Test Acc: 84.78%,Test Discrimination: 0.006219923961907625
Epoch 41, Train Loss: 0.3865598142147064, Train Acc: 85.02%, Train Discrimination: 0.006049678660929203 Test Loss: 0.38967570662498474, Test Acc: 84.74%,Test Discrimination: 0.005942164920270443
Epoch 42, Train Loss: 0.385648638010025, Train Acc: 84.98%, Train Discrimination: 0.005799609236419201 Test Loss: 0.3872186243534088, Test Acc: 84.71%,Test Discrimination: 0.005078119691461325
Epoch 43, Train Loss: 0.38340896368026733, Train Acc: 85.01%, Train Discrimination: 0.005031323991715908 Test Loss: 0.38457104563713074, Test Acc: 84.55%,Test Discrimination: 0.003922297153621912
Epoch 44, Train Loss: 0.3808959424495697, Train Acc: 85.00%, Train Discrimination: 0.003995964303612709 Test Loss: 0.38285356760025024, Test Acc: 84.55%,Test Discrimination: 0.002789432415738702
Epoch 45, Train Loss: 0.3791574537754059, Train Acc: 84.83%, Train Discrimination: 0.0029617254622280598 Test Loss: 0.3824731111526489, Test Acc: 84.49%,Test Discrimination: 0.0018836656818166375
Epoch 46, Train Loss: 0.37859538197517395, Train Acc: 84.80%, Train Discrimination: 0.0021109068766236305 Test Loss: 0.3830282688140869, Test Acc: 84.46%,Test Discrimination: 0.0012709356378763914
Epoch 47, Train Loss: 0.3788890540599823, Train Acc: 84.64%, Train Discrimination: 0.0015142235206440091 Test Loss: 0.3837045729160309, Test Acc: 84.46%,Test Discrimination: 0.0009231196017935872
Epoch 48, Train Loss: 0.3793371915817261, Train Acc: 84.55%, Train Discrimination: 0.001160685089416802 Test Loss: 0.38379067182540894, Test Acc: 84.46%,Test Discrimination: 0.0007817264413461089
Epoch 49, Train Loss: 0.3793098032474518, Train Acc: 84.53%, Train Discrimination: 0.0010072262957692146 Test Loss: 0.38300344347953796, Test Acc: 84.43%,Test Discrimination: 0.0008042200352065265
Epoch 50, Train Loss: 0.37855198979377747, Train Acc: 84.58%, Train Discrimination: 0.0010171417379751801 Test Loss: 0.3815461993217468, Test Acc: 84.52%,Test Discrimination: 0.0009746323339641094
Epoch 51, Train Loss: 0.3772430419921875, Train Acc: 84.70%, Train Discrimination: 0.001170840347185731 Test Loss: 0.3799368441104889, Test Acc: 84.40%,Test Discrimination: 0.0012891972437500954
Epoch 52, Train Loss: 0.3758367896080017, Train Acc: 84.85%, Train Discrimination: 0.0014550797641277313 Test Loss: 0.378708153963089, Test Acc: 84.49%,Test Discrimination: 0.0017290347022935748
Epoch 53, Train Loss: 0.37478238344192505, Train Acc: 84.89%, Train Discrimination: 0.0018409424228593707 Test Loss: 0.3781189024448395, Test Acc: 84.59%,Test Discrimination: 0.0022333788219839334
Epoch 54, Train Loss: 0.3742811977863312, Train Acc: 85.01%, Train Discrimination: 0.0022692368365824223 Test Loss: 0.3780350387096405, Test Acc: 84.65%,Test Discrimination: 0.0027022473514080048
Epoch 55, Train Loss: 0.3741959035396576, Train Acc: 84.97%, Train Discrimination: 0.002650295151397586 Test Loss: 0.37806984782218933, Test Acc: 84.65%,Test Discrimination: 0.003020332893356681
Epoch 56, Train Loss: 0.3741605877876282, Train Acc: 85.00%, Train Discrimination: 0.0028909845277667046 Test Loss: 0.3778536319732666, Test Acc: 84.65%,Test Discrimination: 0.003107474185526371
Epoch 57, Train Loss: 0.3738515079021454, Train Acc: 84.99%, Train Discrimination: 0.0029306598007678986 Test Loss: 0.37728315591812134, Test Acc: 84.65%,Test Discrimination: 0.0029558660462498665
Epoch 58, Train Loss: 0.373189777135849, Train Acc: 85.03%, Train Discrimination: 0.0027682885993272066 Test Loss: 0.37653377652168274, Test Acc: 84.59%,Test Discrimination: 0.0026268502697348595
Epoch 59, Train Loss: 0.3723528981208801, Train Acc: 84.97%, Train Discrimination: 0.002458025934174657 Test Loss: 0.375884473323822, Test Acc: 84.59%,Test Discrimination: 0.002218634355813265
Epoch 60, Train Loss: 0.3716050982475281, Train Acc: 84.97%, Train Discrimination: 0.002081372309476137 Test Loss: 0.3754950165748596, Test Acc: 84.62%,Test Discrimination: 0.001821950078010559
Epoch 61, Train Loss: 0.37110841274261475, Train Acc: 84.92%, Train Discrimination: 0.001714807585813105 Test Loss: 0.37533071637153625, Test Acc: 84.52%,Test Discrimination: 0.0014964420115575194
Epoch 62, Train Loss: 0.3708418607711792, Train Acc: 84.89%, Train Discrimination: 0.0014091026969254017 Test Loss: 0.37521469593048096, Test Acc: 84.40%,Test Discrimination: 0.0012681250227615237
Epoch 63, Train Loss: 0.3706526458263397, Train Acc: 84.82%, Train Discrimination: 0.001187182147987187 Test Loss: 0.37496712803840637, Test Acc: 84.49%,Test Discrimination: 0.0011402207892388105
Epoch 64, Train Loss: 0.3703800141811371, Train Acc: 84.82%, Train Discrimination: 0.0010522634256631136 Test Loss: 0.37451714277267456, Test Acc: 84.55%,Test Discrimination: 0.0011068442836403847
Epoch 65, Train Loss: 0.36995723843574524, Train Acc: 84.82%, Train Discrimination: 0.0009971718536689878 Test Loss: 0.37393832206726074, Test Acc: 84.59%,Test Discrimination: 0.0011567104374989867
Epoch 66, Train Loss: 0.36943379044532776, Train Acc: 84.85%, Train Discrimination: 0.0010106867412105203 Test Loss: 0.3733743131160736, Test Acc: 84.62%,Test Discrimination: 0.0012741395039483905
Epoch 67, Train Loss: 0.368927001953125, Train Acc: 84.93%, Train Discrimination: 0.0010785114718601108 Test Loss: 0.37293753027915955, Test Acc: 84.71%,Test Discrimination: 0.001432808581739664
Epoch 68, Train Loss: 0.36853721737861633, Train Acc: 84.94%, Train Discrimination: 0.0011808876879513264 Test Loss: 0.3726685345172882, Test Acc: 84.78%,Test Discrimination: 0.00160060147754848
Epoch 69, Train Loss: 0.36827659606933594, Train Acc: 84.97%, Train Discrimination: 0.001290497020818293 Test Loss: 0.3724863529205322, Test Acc: 84.78%,Test Discrimination: 0.0017364883096888661
Epoch 70, Train Loss: 0.3680700659751892, Train Acc: 84.97%, Train Discrimination: 0.001377397682517767 Test Loss: 0.37228330969810486, Test Acc: 84.81%,Test Discrimination: 0.0018045335309579968
Epoch 71, Train Loss: 0.36781787872314453, Train Acc: 84.98%, Train Discrimination: 0.0014152934309095144 Test Loss: 0.3719920516014099, Test Acc: 84.81%,Test Discrimination: 0.0017873478354886174
Epoch 72, Train Loss: 0.367470920085907, Train Acc: 85.01%, Train Discrimination: 0.0013934531016275287 Test Loss: 0.3716362416744232, Test Acc: 84.81%,Test Discrimination: 0.0016929487464949489
Epoch 73, Train Loss: 0.3670569658279419, Train Acc: 84.98%, Train Discrimination: 0.0013177713844925165 Test Loss: 0.37129947543144226, Test Acc: 84.74%,Test Discrimination: 0.001551422756165266
Epoch 74, Train Loss: 0.3666563034057617, Train Acc: 84.97%, Train Discrimination: 0.001209201873280108 Test Loss: 0.37103888392448425, Test Acc: 84.78%,Test Discrimination: 0.001398298074491322
Epoch 75, Train Loss: 0.36632972955703735, Train Acc: 84.95%, Train Discrimination: 0.0010942852823063731 Test Loss: 0.3708503842353821, Test Acc: 84.68%,Test Discrimination: 0.0012636117171496153
Epoch 76, Train Loss: 0.3660777509212494, Train Acc: 84.95%, Train Discrimination: 0.0009946973295882344 Test Loss: 0.3706786036491394, Test Acc: 84.65%,Test Discrimination: 0.0011678335722535849
Epoch 77, Train Loss: 0.3658493757247925, Train Acc: 84.90%, Train Discrimination: 0.0009235434117726982 Test Loss: 0.3704555928707123, Test Acc: 84.68%,Test Discrimination: 0.0011214592959731817
Epoch 78, Train Loss: 0.3655897080898285, Train Acc: 84.93%, Train Discrimination: 0.0008892670157365501 Test Loss: 0.3701593577861786, Test Acc: 84.71%,Test Discrimination: 0.001124257338233292
Epoch 79, Train Loss: 0.36527591943740845, Train Acc: 85.01%, Train Discrimination: 0.0008920289692468941 Test Loss: 0.36981409788131714, Test Acc: 84.71%,Test Discrimination: 0.0011673344997689128
Epoch 80, Train Loss: 0.36492496728897095, Train Acc: 85.02%, Train Discrimination: 0.0009252167074009776 Test Loss: 0.3694836497306824, Test Acc: 84.84%,Test Discrimination: 0.0012361345579847693
Epoch 81, Train Loss: 0.3645872175693512, Train Acc: 85.00%, Train Discrimination: 0.0009782722918316722 Test Loss: 0.36920905113220215, Test Acc: 84.78%,Test Discrimination: 0.0013086255639791489
Epoch 82, Train Loss: 0.3642946779727936, Train Acc: 85.04%, Train Discrimination: 0.001033667242154479 Test Loss: 0.36896538734436035, Test Acc: 84.81%,Test Discrimination: 0.001365308417007327
Epoch 83, Train Loss: 0.3640435039997101, Train Acc: 85.08%, Train Discrimination: 0.0010767201893031597 Test Loss: 0.3687349259853363, Test Acc: 84.74%,Test Discrimination: 0.001389343524351716
Epoch 84, Train Loss: 0.3637964129447937, Train Acc: 85.09%, Train Discrimination: 0.0010943880770355463 Test Loss: 0.3684976100921631, Test Acc: 84.74%,Test Discrimination: 0.0013772858073934913
Epoch 85, Train Loss: 0.36352846026420593, Train Acc: 85.07%, Train Discrimination: 0.0010838788002729416 Test Loss: 0.3682655692100525, Test Acc: 84.74%,Test Discrimination: 0.001342118252068758
Epoch 86, Train Loss: 0.36322951316833496, Train Acc: 85.10%, Train Discrimination: 0.0010551849845796824 Test Loss: 0.3680484890937805, Test Acc: 84.78%,Test Discrimination: 0.001289671054109931
Epoch 87, Train Loss: 0.36291611194610596, Train Acc: 85.12%, Train Discrimination: 0.001012626220472157 Test Loss: 0.367868572473526, Test Acc: 84.74%,Test Discrimination: 0.0012446248438209295
Epoch 88, Train Loss: 0.36261463165283203, Train Acc: 85.07%, Train Discrimination: 0.0009748825104907155 Test Loss: 0.3677263557910919, Test Acc: 84.71%,Test Discrimination: 0.0012231699656695127
Epoch 89, Train Loss: 0.36233192682266235, Train Acc: 85.08%, Train Discrimination: 0.0009541662293486297 Test Loss: 0.36760184168815613, Test Acc: 84.71%,Test Discrimination: 0.0012274817563593388
Epoch 90, Train Loss: 0.36206239461898804, Train Acc: 85.08%, Train Discrimination: 0.0009516003774479032 Test Loss: 0.3674684762954712, Test Acc: 84.71%,Test Discrimination: 0.001249900320544839
Epoch 91, Train Loss: 0.3617932200431824, Train Acc: 85.12%, Train Discrimination: 0.0009608706459403038 Test Loss: 0.36729779839515686, Test Acc: 84.71%,Test Discrimination: 0.001275169081054628
Epoch 92, Train Loss: 0.3615138828754425, Train Acc: 85.11%, Train Discrimination: 0.000970585155300796 Test Loss: 0.36708176136016846, Test Acc: 84.65%,Test Discrimination: 0.0012913604732602835
Epoch 93, Train Loss: 0.36122554540634155, Train Acc: 85.11%, Train Discrimination: 0.0009717100183479488 Test Loss: 0.36683353781700134, Test Acc: 84.68%,Test Discrimination: 0.0012964284978806973
Epoch 94, Train Loss: 0.3609369099140167, Train Acc: 85.12%, Train Discrimination: 0.0009649298153817654 Test Loss: 0.3665827810764313, Test Acc: 84.68%,Test Discrimination: 0.0012913164682686329
Epoch 95, Train Loss: 0.3606562912464142, Train Acc: 85.15%, Train Discrimination: 0.0009503709734417498 Test Loss: 0.3663380444049835, Test Acc: 84.68%,Test Discrimination: 0.0012646605027839541
Epoch 96, Train Loss: 0.36038607358932495, Train Acc: 85.15%, Train Discrimination: 0.0009205529349856079 Test Loss: 0.3661346137523651, Test Acc: 84.68%,Test Discrimination: 0.0012472628150135279
Epoch 97, Train Loss: 0.36013227701187134, Train Acc: 85.15%, Train Discrimination: 0.0008976419339887798 Test Loss: 0.36599403619766235, Test Acc: 84.68%,Test Discrimination: 0.0012744254199787974
Epoch 98, Train Loss: 0.35986828804016113, Train Acc: 85.15%, Train Discrimination: 0.0009072819957509637 Test Loss: 0.3659040331840515, Test Acc: 84.65%,Test Discrimination: 0.0013320177095010877
Epoch 99, Train Loss: 0.3595961928367615, Train Acc: 85.17%, Train Discrimination: 0.0009394118096679449 Test Loss: 0.3658422827720642, Test Acc: 84.65%,Test Discrimination: 0.0013930415734648705
Epoch 100, Train Loss: 0.35932740569114685, Train Acc: 85.17%, Train Discrimination: 0.0009756212239153683 Test Loss: 0.36578404903411865, Test Acc: 84.68%,Test Discrimination: 0.0014270032988861203</code></pre>
</div>
</div>
<section id="model-accross-different-values-of-lambda" class="level4">
<h4 class="anchored" data-anchor-id="model-accross-different-values-of-lambda">MODEL ACCROSS DIFFERENT VALUES OF LAMBDA</h4>
</section>
<section id="lambda-1" class="level4">
<h4 class="anchored" data-anchor-id="lambda-1">LAMBDA &lt; 1</h4>
<div id="dfe9bd88" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model3.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>train_losses3, train_accuracies3, train_discriminations3,train_fairness3 <span class="op">=</span> [], [], [],[]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>test_losses3, test_accuracies3, test_discriminations3,test_fairness3 <span class="op">=</span> [], [], [],[]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>model3.train()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model3(features)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.01</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    model3.<span class="bu">eval</span>()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model3(test_features)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features, lambda_val<span class="op">=</span><span class="fl">0.01</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    train_losses3.append(loss.item())</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    train_accuracies3.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    train_discriminations3.append(discrimination)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    train_fairness3.append(fairness)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    test_fairness3.append(fairness)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    test_losses3.append(test_loss.item())</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    test_accuracies3.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    test_discriminations3.append(test_discrimination)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    model3.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.8667126297950745, Train Acc: 51.17%, Train Discrimination: 3.7251006688165944e-06 Test Loss: 0.8151583671569824, Test Acc: 82.66%,Test Discrimination: 8.151709153025877e-06
Epoch 2, Train Loss: 0.8028159737586975, Train Acc: 82.87%, Train Discrimination: 5.417834927357035e-06 Test Loss: 0.7927682399749756, Test Acc: 82.66%,Test Discrimination: 1.979424723685952e-06
Epoch 3, Train Loss: 0.7777947187423706, Train Acc: 82.87%, Train Discrimination: 1.2660869970204658e-06 Test Loss: 0.5724644064903259, Test Acc: 82.66%,Test Discrimination: 6.932599916353865e-08
Epoch 4, Train Loss: 0.5610520243644714, Train Acc: 82.87%, Train Discrimination: 3.190917396977966e-08 Test Loss: 0.380097895860672, Test Acc: 84.43%,Test Discrimination: 2.340957689739298e-05
Epoch 5, Train Loss: 0.3765399754047394, Train Acc: 84.83%, Train Discrimination: 2.0588882762240246e-05 Test Loss: 0.592439591884613, Test Acc: 66.20%,Test Discrimination: 0.00015872564108576626
Epoch 6, Train Loss: 0.5992106795310974, Train Acc: 65.38%, Train Discrimination: 0.00012340088142082095 Test Loss: 0.4511386454105377, Test Acc: 82.09%,Test Discrimination: 0.00012051362864440307
Epoch 7, Train Loss: 0.45450496673583984, Train Acc: 81.35%, Train Discrimination: 0.00010203807323705405 Test Loss: 0.36649468541145325, Test Acc: 84.52%,Test Discrimination: 3.8806603697594255e-05
Epoch 8, Train Loss: 0.362349271774292, Train Acc: 85.65%, Train Discrimination: 3.639939313870855e-05 Test Loss: 0.41390082240104675, Test Acc: 83.67%,Test Discrimination: 1.0739344361354597e-05
Epoch 9, Train Loss: 0.4050651490688324, Train Acc: 84.19%, Train Discrimination: 1.0554621439951006e-05 Test Loss: 0.45325735211372375, Test Acc: 83.16%,Test Discrimination: 5.862522812094539e-06
Epoch 10, Train Loss: 0.44240376353263855, Train Acc: 83.75%, Train Discrimination: 5.7078850659308955e-06 Test Loss: 0.4464418888092041, Test Acc: 83.42%,Test Discrimination: 6.8014569478691556e-06
Epoch 11, Train Loss: 0.43541985750198364, Train Acc: 83.98%, Train Discrimination: 6.523125648527639e-06 Test Loss: 0.4098832607269287, Test Acc: 84.46%,Test Discrimination: 1.3007672350795474e-05
Epoch 12, Train Loss: 0.40017467737197876, Train Acc: 84.54%, Train Discrimination: 1.2425909517332911e-05 Test Loss: 0.375325471162796, Test Acc: 85.00%,Test Discrimination: 2.939316073025111e-05
Epoch 13, Train Loss: 0.36817532777786255, Train Acc: 85.41%, Train Discrimination: 2.790191319945734e-05 Test Loss: 0.36777153611183167, Test Acc: 85.19%,Test Discrimination: 5.928790051257238e-05
Epoch 14, Train Loss: 0.36427536606788635, Train Acc: 85.36%, Train Discrimination: 5.571308429352939e-05 Test Loss: 0.382951945066452, Test Acc: 85.09%,Test Discrimination: 9.152001439360902e-05
Epoch 15, Train Loss: 0.38285794854164124, Train Acc: 84.44%, Train Discrimination: 8.507192978868261e-05 Test Loss: 0.391345739364624, Test Acc: 84.49%,Test Discrimination: 0.00010711271170293912
Epoch 16, Train Loss: 0.3926214277744293, Train Acc: 83.99%, Train Discrimination: 9.935544221661985e-05 Test Loss: 0.3792632520198822, Test Acc: 84.84%,Test Discrimination: 9.975230204872787e-05
Epoch 17, Train Loss: 0.3795488476753235, Train Acc: 84.57%, Train Discrimination: 9.392442734679207e-05 Test Loss: 0.3627530336380005, Test Acc: 85.66%,Test Discrimination: 7.691688369959593e-05
Epoch 18, Train Loss: 0.36050689220428467, Train Acc: 85.30%, Train Discrimination: 7.467842078767717e-05 Test Loss: 0.3587244153022766, Test Acc: 85.25%,Test Discrimination: 5.236986544332467e-05
Epoch 19, Train Loss: 0.3536894619464874, Train Acc: 85.75%, Train Discrimination: 5.286341911414638e-05 Test Loss: 0.36661240458488464, Test Acc: 84.74%,Test Discrimination: 3.4982862416654825e-05
Epoch 20, Train Loss: 0.3593381941318512, Train Acc: 85.65%, Train Discrimination: 3.672096863738261e-05 Test Loss: 0.37579023838043213, Test Acc: 84.74%,Test Discrimination: 2.5721548809087835e-05
Epoch 21, Train Loss: 0.36719581484794617, Train Acc: 85.56%, Train Discrimination: 2.7747670173994265e-05 Test Loss: 0.37788307666778564, Test Acc: 84.81%,Test Discrimination: 2.2407390133594163e-05
Epoch 22, Train Loss: 0.36891403794288635, Train Acc: 85.41%, Train Discrimination: 2.448925624776166e-05 Test Loss: 0.37140586972236633, Test Acc: 84.74%,Test Discrimination: 2.361419319640845e-05
Epoch 23, Train Loss: 0.36281612515449524, Train Acc: 85.47%, Train Discrimination: 2.5747987820068374e-05 Test Loss: 0.3608776926994324, Test Acc: 84.71%,Test Discrimination: 2.9019141948083416e-05
Epoch 24, Train Loss: 0.35332807898521423, Train Acc: 85.72%, Train Discrimination: 3.1071947887539864e-05 Test Loss: 0.35299625992774963, Test Acc: 85.09%,Test Discrimination: 3.848889537039213e-05
Epoch 25, Train Loss: 0.3468959927558899, Train Acc: 85.81%, Train Discrimination: 4.001279376097955e-05 Test Loss: 0.3513491451740265, Test Acc: 85.44%,Test Discrimination: 5.016722570871934e-05
Epoch 26, Train Loss: 0.34677204489707947, Train Acc: 85.97%, Train Discrimination: 5.052131018601358e-05 Test Loss: 0.35344696044921875, Test Acc: 85.85%,Test Discrimination: 5.942699499428272e-05
Epoch 27, Train Loss: 0.34987592697143555, Train Acc: 85.90%, Train Discrimination: 5.839678487973288e-05 Test Loss: 0.35309696197509766, Test Acc: 85.88%,Test Discrimination: 6.211683648871258e-05
Epoch 28, Train Loss: 0.34979936480522156, Train Acc: 85.84%, Train Discrimination: 6.0132362705189735e-05 Test Loss: 0.3492453694343567, Test Acc: 85.79%,Test Discrimination: 5.773717930424027e-05
Epoch 29, Train Loss: 0.345401793718338, Train Acc: 85.88%, Train Discrimination: 5.584961763815954e-05 Test Loss: 0.34627479314804077, Test Acc: 85.44%,Test Discrimination: 4.8900823458097875e-05
Epoch 30, Train Loss: 0.34143155813217163, Train Acc: 85.84%, Train Discrimination: 4.7590368922101334e-05 Test Loss: 0.3476106822490692, Test Acc: 85.12%,Test Discrimination: 4.04833845095709e-05
Epoch 31, Train Loss: 0.34173092246055603, Train Acc: 85.90%, Train Discrimination: 3.9644870412303135e-05 Test Loss: 0.34929725527763367, Test Acc: 85.06%,Test Discrimination: 3.7372396036516875e-05
Epoch 32, Train Loss: 0.34293460845947266, Train Acc: 85.85%, Train Discrimination: 3.6456036468734965e-05 Test Loss: 0.3473863899707794, Test Acc: 85.12%,Test Discrimination: 4.078837810084224e-05
Epoch 33, Train Loss: 0.3412999212741852, Train Acc: 85.90%, Train Discrimination: 3.9263231883523986e-05 Test Loss: 0.3441622257232666, Test Acc: 85.22%,Test Discrimination: 4.9540532927494496e-05
Epoch 34, Train Loss: 0.3387905955314636, Train Acc: 86.07%, Train Discrimination: 4.684799205278978e-05 Test Loss: 0.3429449498653412, Test Acc: 85.44%,Test Discrimination: 6.085947097744793e-05
Epoch 35, Train Loss: 0.33843761682510376, Train Acc: 86.00%, Train Discrimination: 5.659882663167082e-05 Test Loss: 0.34361645579338074, Test Acc: 85.69%,Test Discrimination: 6.981181650189683e-05
Epoch 36, Train Loss: 0.3396836519241333, Train Acc: 86.08%, Train Discrimination: 6.416477845050395e-05 Test Loss: 0.34356948733329773, Test Acc: 85.91%,Test Discrimination: 7.215962978079915e-05
Epoch 37, Train Loss: 0.33966395258903503, Train Acc: 86.09%, Train Discrimination: 6.60472724121064e-05 Test Loss: 0.3422171473503113, Test Acc: 85.47%,Test Discrimination: 6.756046786904335e-05
Epoch 38, Train Loss: 0.33783671259880066, Train Acc: 86.17%, Train Discrimination: 6.201670476002619e-05 Test Loss: 0.3415582776069641, Test Acc: 85.44%,Test Discrimination: 5.929342660238035e-05
Epoch 39, Train Loss: 0.33636274933815, Train Acc: 86.11%, Train Discrimination: 5.480085019371472e-05 Test Loss: 0.3423617482185364, Test Acc: 85.38%,Test Discrimination: 5.202088868827559e-05
Epoch 40, Train Loss: 0.33637312054634094, Train Acc: 85.98%, Train Discrimination: 4.843317947234027e-05 Test Loss: 0.34259530901908875, Test Acc: 85.44%,Test Discrimination: 4.929439091938548e-05
Epoch 41, Train Loss: 0.33625108003616333, Train Acc: 85.98%, Train Discrimination: 4.616288424585946e-05 Test Loss: 0.34129539132118225, Test Acc: 85.53%,Test Discrimination: 5.152654193807393e-05
Epoch 42, Train Loss: 0.3349529802799225, Train Acc: 85.96%, Train Discrimination: 4.829160388908349e-05 Test Loss: 0.33954179286956787, Test Acc: 85.47%,Test Discrimination: 5.7309367548441514e-05
Epoch 43, Train Loss: 0.3335454761981964, Train Acc: 86.11%, Train Discrimination: 5.354979293770157e-05 Test Loss: 0.3387145698070526, Test Acc: 85.72%,Test Discrimination: 6.391905480995774e-05
Epoch 44, Train Loss: 0.33314698934555054, Train Acc: 86.20%, Train Discrimination: 5.9513316955417395e-05 Test Loss: 0.3385342061519623, Test Acc: 85.79%,Test Discrimination: 6.806109013268724e-05
Epoch 45, Train Loss: 0.3331458270549774, Train Acc: 86.28%, Train Discrimination: 6.330511678243056e-05 Test Loss: 0.33802807331085205, Test Acc: 85.91%,Test Discrimination: 6.776352529413998e-05
Epoch 46, Train Loss: 0.3324512541294098, Train Acc: 86.33%, Train Discrimination: 6.325459253275767e-05 Test Loss: 0.3373561203479767, Test Acc: 85.60%,Test Discrimination: 6.373412907123566e-05
Epoch 47, Train Loss: 0.3312854766845703, Train Acc: 86.29%, Train Discrimination: 5.99497725488618e-05 Test Loss: 0.33727580308914185, Test Acc: 85.69%,Test Discrimination: 5.866708670509979e-05
Epoch 48, Train Loss: 0.330600768327713, Train Acc: 86.17%, Train Discrimination: 5.5704385886201635e-05 Test Loss: 0.3375509977340698, Test Acc: 85.69%,Test Discrimination: 5.5558080930495635e-05
Epoch 49, Train Loss: 0.3304065763950348, Train Acc: 86.10%, Train Discrimination: 5.314009467838332e-05 Test Loss: 0.33722394704818726, Test Acc: 85.66%,Test Discrimination: 5.61698543606326e-05
Epoch 50, Train Loss: 0.3299075961112976, Train Acc: 86.14%, Train Discrimination: 5.383648021961562e-05 Test Loss: 0.33620864152908325, Test Acc: 85.69%,Test Discrimination: 6.047372517059557e-05
Epoch 51, Train Loss: 0.3290306627750397, Train Acc: 86.36%, Train Discrimination: 5.776874968432821e-05 Test Loss: 0.3353555202484131, Test Acc: 85.69%,Test Discrimination: 6.670643779216334e-05
Epoch 52, Train Loss: 0.3284599483013153, Train Acc: 86.54%, Train Discrimination: 6.334208592306823e-05 Test Loss: 0.33501118421554565, Test Acc: 85.69%,Test Discrimination: 7.188165182014927e-05
Epoch 53, Train Loss: 0.3282720148563385, Train Acc: 86.54%, Train Discrimination: 6.7972629040014e-05 Test Loss: 0.33472001552581787, Test Acc: 85.66%,Test Discrimination: 7.35237990738824e-05
Epoch 54, Train Loss: 0.3278452157974243, Train Acc: 86.58%, Train Discrimination: 6.950840179342777e-05 Test Loss: 0.33435484766960144, Test Acc: 85.72%,Test Discrimination: 7.143319817259908e-05
Epoch 55, Train Loss: 0.32708266377449036, Train Acc: 86.54%, Train Discrimination: 6.7796056100633e-05 Test Loss: 0.3343304693698883, Test Acc: 85.79%,Test Discrimination: 6.778079841751605e-05
Epoch 56, Train Loss: 0.3264828622341156, Train Acc: 86.54%, Train Discrimination: 6.461376324295998e-05 Test Loss: 0.33449336886405945, Test Acc: 85.79%,Test Discrimination: 6.544098869198933e-05
Epoch 57, Train Loss: 0.3261229991912842, Train Acc: 86.54%, Train Discrimination: 6.250519072636962e-05 Test Loss: 0.33419090509414673, Test Acc: 85.82%,Test Discrimination: 6.629719428019598e-05
Epoch 58, Train Loss: 0.3256376385688782, Train Acc: 86.55%, Train Discrimination: 6.319516251096502e-05 Test Loss: 0.33349138498306274, Test Acc: 85.75%,Test Discrimination: 7.027628453215584e-05
Epoch 59, Train Loss: 0.3249884247779846, Train Acc: 86.64%, Train Discrimination: 6.660151848336682e-05 Test Loss: 0.33291468024253845, Test Acc: 85.75%,Test Discrimination: 7.556574564659968e-05
Epoch 60, Train Loss: 0.3244812488555908, Train Acc: 86.63%, Train Discrimination: 7.119654765119776e-05 Test Loss: 0.3325898051261902, Test Acc: 85.85%,Test Discrimination: 7.893417932791635e-05
Epoch 61, Train Loss: 0.3239729106426239, Train Acc: 86.69%, Train Discrimination: 7.432075653923675e-05 Test Loss: 0.33281195163726807, Test Acc: 85.79%,Test Discrimination: 7.776235725032166e-05
Epoch 62, Train Loss: 0.3235974609851837, Train Acc: 86.68%, Train Discrimination: 7.358122820733115e-05 Test Loss: 0.3325122892856598, Test Acc: 85.72%,Test Discrimination: 7.931552681839094e-05
Epoch 63, Train Loss: 0.32318416237831116, Train Acc: 86.69%, Train Discrimination: 7.496002217521891e-05 Test Loss: 0.33184751868247986, Test Acc: 85.98%,Test Discrimination: 8.349269774043933e-05
Epoch 64, Train Loss: 0.3225880265235901, Train Acc: 86.69%, Train Discrimination: 7.848269160604104e-05 Test Loss: 0.33147355914115906, Test Acc: 85.85%,Test Discrimination: 8.514009095961228e-05
Epoch 65, Train Loss: 0.322157621383667, Train Acc: 86.69%, Train Discrimination: 7.979176007211208e-05 Test Loss: 0.33138102293014526, Test Acc: 85.94%,Test Discrimination: 8.323154906975105e-05
Epoch 66, Train Loss: 0.3216939866542816, Train Acc: 86.68%, Train Discrimination: 7.812700641807169e-05 Test Loss: 0.3315417766571045, Test Acc: 85.91%,Test Discrimination: 8.057511149672791e-05
Epoch 67, Train Loss: 0.3213449716567993, Train Acc: 86.73%, Train Discrimination: 7.591763278469443e-05 Test Loss: 0.33118632435798645, Test Acc: 85.91%,Test Discrimination: 8.267584053101018e-05
Epoch 68, Train Loss: 0.32083484530448914, Train Acc: 86.69%, Train Discrimination: 7.792188262101263e-05 Test Loss: 0.3308059573173523, Test Acc: 85.88%,Test Discrimination: 8.679324673721567e-05
Epoch 69, Train Loss: 0.32039758563041687, Train Acc: 86.76%, Train Discrimination: 8.161070581991225e-05 Test Loss: 0.3308068513870239, Test Acc: 85.91%,Test Discrimination: 8.735182200325653e-05
Epoch 70, Train Loss: 0.3199205994606018, Train Acc: 86.77%, Train Discrimination: 8.232719119405374e-05 Test Loss: 0.3310209810733795, Test Acc: 85.82%,Test Discrimination: 8.546521712560207e-05
Epoch 71, Train Loss: 0.31951481103897095, Train Acc: 86.79%, Train Discrimination: 8.063151472015306e-05 Test Loss: 0.33094578981399536, Test Acc: 85.88%,Test Discrimination: 8.619348227512091e-05
Epoch 72, Train Loss: 0.3191412389278412, Train Acc: 86.84%, Train Discrimination: 8.131023059831932e-05 Test Loss: 0.33060503005981445, Test Acc: 85.85%,Test Discrimination: 8.953562792157754e-05
Epoch 73, Train Loss: 0.318743497133255, Train Acc: 86.88%, Train Discrimination: 8.448320295428857e-05 Test Loss: 0.3303819000720978, Test Acc: 85.94%,Test Discrimination: 9.075300476979464e-05
Epoch 74, Train Loss: 0.31839537620544434, Train Acc: 86.92%, Train Discrimination: 8.580053690820932e-05 Test Loss: 0.3304310441017151, Test Acc: 85.85%,Test Discrimination: 8.827173587633297e-05
Epoch 75, Train Loss: 0.318013072013855, Train Acc: 86.89%, Train Discrimination: 8.370119758183137e-05 Test Loss: 0.3304601013660431, Test Acc: 85.82%,Test Discrimination: 8.705495565664023e-05
Epoch 76, Train Loss: 0.31771308183670044, Train Acc: 86.82%, Train Discrimination: 8.317281026393175e-05 Test Loss: 0.33011361956596375, Test Acc: 85.85%,Test Discrimination: 9.021814912557602e-05
Epoch 77, Train Loss: 0.31732475757598877, Train Acc: 86.88%, Train Discrimination: 8.59168212627992e-05 Test Loss: 0.32998010516166687, Test Acc: 85.88%,Test Discrimination: 9.171348210657015e-05
Epoch 78, Train Loss: 0.31698986887931824, Train Acc: 86.91%, Train Discrimination: 8.720011101104319e-05 Test Loss: 0.3302960693836212, Test Acc: 85.82%,Test Discrimination: 9.02938554645516e-05
Epoch 79, Train Loss: 0.31659555435180664, Train Acc: 86.88%, Train Discrimination: 8.667936344863847e-05 Test Loss: 0.33022141456604004, Test Acc: 85.94%,Test Discrimination: 9.295115887653083e-05
Epoch 80, Train Loss: 0.3162091374397278, Train Acc: 86.91%, Train Discrimination: 8.929642353905365e-05 Test Loss: 0.32989779114723206, Test Acc: 86.20%,Test Discrimination: 9.762447007233277e-05
Epoch 81, Train Loss: 0.3159048855304718, Train Acc: 86.97%, Train Discrimination: 9.322475671069697e-05 Test Loss: 0.33033010363578796, Test Acc: 85.98%,Test Discrimination: 9.538282756693661e-05
Epoch 82, Train Loss: 0.31550678610801697, Train Acc: 87.00%, Train Discrimination: 9.187614341499284e-05 Test Loss: 0.3302987217903137, Test Acc: 86.07%,Test Discrimination: 9.607389074517414e-05
Epoch 83, Train Loss: 0.3151305317878723, Train Acc: 86.99%, Train Discrimination: 9.259096987079829e-05 Test Loss: 0.33009660243988037, Test Acc: 86.07%,Test Discrimination: 9.83712452580221e-05
Epoch 84, Train Loss: 0.3147830069065094, Train Acc: 86.96%, Train Discrimination: 9.466288611292839e-05 Test Loss: 0.3304902911186218, Test Acc: 86.07%,Test Discrimination: 9.552171104587615e-05
Epoch 85, Train Loss: 0.31445392966270447, Train Acc: 86.94%, Train Discrimination: 9.275685442844406e-05 Test Loss: 0.3301943242549896, Test Acc: 85.94%,Test Discrimination: 9.481389861321077e-05
Epoch 86, Train Loss: 0.31421133875846863, Train Acc: 87.01%, Train Discrimination: 9.152546408586204e-05 Test Loss: 0.33064308762550354, Test Acc: 85.94%,Test Discrimination: 9.429848432773724e-05
Epoch 87, Train Loss: 0.3138450086116791, Train Acc: 86.98%, Train Discrimination: 9.210012649418786e-05 Test Loss: 0.3301665186882019, Test Acc: 86.07%,Test Discrimination: 9.688219870440662e-05
Epoch 88, Train Loss: 0.3134913444519043, Train Acc: 87.02%, Train Discrimination: 9.391280036652461e-05 Test Loss: 0.3305714428424835, Test Acc: 86.01%,Test Discrimination: 9.414401574758813e-05
Epoch 89, Train Loss: 0.31314560770988464, Train Acc: 87.03%, Train Discrimination: 9.209509880747646e-05 Test Loss: 0.3304448425769806, Test Acc: 85.98%,Test Discrimination: 9.787642920855433e-05
Epoch 90, Train Loss: 0.3127889037132263, Train Acc: 87.05%, Train Discrimination: 9.552526898914948e-05 Test Loss: 0.33037668466567993, Test Acc: 85.91%,Test Discrimination: 9.883867460303009e-05
Epoch 91, Train Loss: 0.3125114142894745, Train Acc: 87.07%, Train Discrimination: 9.631585271563381e-05 Test Loss: 0.33103108406066895, Test Acc: 86.07%,Test Discrimination: 9.411949577042833e-05
Epoch 92, Train Loss: 0.3122497797012329, Train Acc: 87.07%, Train Discrimination: 9.2629088612739e-05 Test Loss: 0.3306027352809906, Test Acc: 86.04%,Test Discrimination: 9.909753134706989e-05
Epoch 93, Train Loss: 0.3119417726993561, Train Acc: 87.09%, Train Discrimination: 9.677530033513904e-05 Test Loss: 0.33088716864585876, Test Acc: 85.98%,Test Discrimination: 9.407020115759224e-05
Epoch 94, Train Loss: 0.31156593561172485, Train Acc: 87.11%, Train Discrimination: 9.252964082406834e-05 Test Loss: 0.3309270441532135, Test Acc: 85.94%,Test Discrimination: 9.356691589346156e-05
Epoch 95, Train Loss: 0.3112456798553467, Train Acc: 87.12%, Train Discrimination: 9.222632797900587e-05 Test Loss: 0.3304135799407959, Test Acc: 86.07%,Test Discrimination: 9.751143807079643e-05
Epoch 96, Train Loss: 0.31104564666748047, Train Acc: 87.09%, Train Discrimination: 9.545574721414596e-05 Test Loss: 0.3316633701324463, Test Acc: 85.94%,Test Discrimination: 9.117556328419596e-05
Epoch 97, Train Loss: 0.31094881892204285, Train Acc: 87.17%, Train Discrimination: 9.064703044714406e-05 Test Loss: 0.3301100730895996, Test Acc: 86.13%,Test Discrimination: 9.806897287489846e-05
Epoch 98, Train Loss: 0.31075072288513184, Train Acc: 87.14%, Train Discrimination: 9.590760600985959e-05 Test Loss: 0.33059772849082947, Test Acc: 85.94%,Test Discrimination: 9.290817979490384e-05
Epoch 99, Train Loss: 0.31043675541877747, Train Acc: 87.22%, Train Discrimination: 9.17170473258011e-05 Test Loss: 0.3310299515724182, Test Acc: 86.04%,Test Discrimination: 9.277163917431608e-05
Epoch 100, Train Loss: 0.3098422884941101, Train Acc: 87.19%, Train Discrimination: 9.20133461477235e-05 Test Loss: 0.33154693245887756, Test Acc: 85.94%,Test Discrimination: 9.45917417993769e-05</code></pre>
</div>
</div>
</section>
<section id="lambda-1-1" class="level4">
<h4 class="anchored" data-anchor-id="lambda-1-1">LAMBDA = 1</h4>
<div id="d40675e4" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model4 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model4.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>train_losses4, train_accuracies4, train_discriminations4,train_fairness4 <span class="op">=</span> [], [], [],[]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>test_losses4, test_accuracies4, test_discriminations4,test_fairness4 <span class="op">=</span> [], [], [],[]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>model4.train()</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model4(features)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features, lambda_val<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    model4.<span class="bu">eval</span>()</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model4(test_features)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features, lambda_val<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    train_losses4.append(loss.item())</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    train_accuracies4.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    train_discriminations4.append(discrimination)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    train_fairness4.append(fairness)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    test_fairness4.append(fairness)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>    test_losses4.append(test_loss.item())</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    test_accuracies4.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    test_discriminations4.append(test_discrimination)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    model4.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.8948111534118652, Train Acc: 17.14%, Train Discrimination: 0.0005742012872360647 Test Loss: 0.862220048904419, Test Acc: 82.66%,Test Discrimination: 0.002305078087374568
Epoch 2, Train Loss: 0.8459657430648804, Train Acc: 82.87%, Train Discrimination: 0.0013694395311176777 Test Loss: 0.9783310890197754, Test Acc: 82.66%,Test Discrimination: 0.0007584519335068762
Epoch 3, Train Loss: 0.9569812417030334, Train Acc: 82.87%, Train Discrimination: 0.0004502878582570702 Test Loss: 0.8635511994361877, Test Acc: 82.66%,Test Discrimination: 0.0002527807664591819
Epoch 4, Train Loss: 0.8429771065711975, Train Acc: 82.87%, Train Discrimination: 0.00014930379984434694 Test Loss: 0.6453170776367188, Test Acc: 82.66%,Test Discrimination: 4.545060437521897e-05
Epoch 5, Train Loss: 0.6289161443710327, Train Acc: 82.87%, Train Discrimination: 2.056846278719604e-05 Test Loss: 0.4340975880622864, Test Acc: 82.69%,Test Discrimination: 0.00024820634280331433
Epoch 6, Train Loss: 0.4236447811126709, Train Acc: 82.98%, Train Discrimination: 0.00027092182426713407 Test Loss: 0.4630437195301056, Test Acc: 82.60%,Test Discrimination: 0.006717855576425791
Epoch 7, Train Loss: 0.4587309658527374, Train Acc: 82.78%, Train Discrimination: 0.005425104405730963 Test Loss: 0.5609658360481262, Test Acc: 64.94%,Test Discrimination: 0.01213766634464264
Epoch 8, Train Loss: 0.5591057538986206, Train Acc: 65.00%, Train Discrimination: 0.009420579299330711 Test Loss: 0.4718201160430908, Test Acc: 80.57%,Test Discrimination: 0.009785423055291176
Epoch 9, Train Loss: 0.4701448380947113, Train Acc: 80.94%, Train Discrimination: 0.008104786276817322 Test Loss: 0.38430067896842957, Test Acc: 84.71%,Test Discrimination: 0.0046728309243917465
Epoch 10, Train Loss: 0.38014668226242065, Train Acc: 85.08%, Train Discrimination: 0.004301511682569981 Test Loss: 0.3832087218761444, Test Acc: 83.64%,Test Discrimination: 0.0018190320115536451
Epoch 11, Train Loss: 0.3757995367050171, Train Acc: 84.41%, Train Discrimination: 0.0018297890201210976 Test Loss: 0.412378191947937, Test Acc: 83.20%,Test Discrimination: 0.0009401802089996636
Epoch 12, Train Loss: 0.402902752161026, Train Acc: 83.84%, Train Discrimination: 0.0009791299235075712 Test Loss: 0.428307443857193, Test Acc: 83.16%,Test Discrimination: 0.0007239599945023656
Epoch 13, Train Loss: 0.41773852705955505, Train Acc: 83.71%, Train Discrimination: 0.0007540034130215645 Test Loss: 0.4239683747291565, Test Acc: 83.23%,Test Discrimination: 0.0007862607599236071
Epoch 14, Train Loss: 0.41345837712287903, Train Acc: 83.99%, Train Discrimination: 0.0008091149502433836 Test Loss: 0.4055226743221283, Test Acc: 83.83%,Test Discrimination: 0.0011101554846391082
Epoch 15, Train Loss: 0.39595773816108704, Train Acc: 84.22%, Train Discrimination: 0.0011305499356240034 Test Loss: 0.3831002414226532, Test Acc: 84.43%,Test Discrimination: 0.0018461060244590044
Epoch 16, Train Loss: 0.3751753270626068, Train Acc: 84.90%, Train Discrimination: 0.0018527911743149161 Test Loss: 0.3671249747276306, Test Acc: 85.03%,Test Discrimination: 0.003194451564922929
Epoch 17, Train Loss: 0.3613107204437256, Train Acc: 85.27%, Train Discrimination: 0.003146895207464695 Test Loss: 0.363337904214859, Test Acc: 84.93%,Test Discrimination: 0.005130221135914326
Epoch 18, Train Loss: 0.35993218421936035, Train Acc: 85.37%, Train Discrimination: 0.004959811456501484 Test Loss: 0.36858707666397095, Test Acc: 85.60%,Test Discrimination: 0.007148608099669218
Epoch 19, Train Loss: 0.36737972497940063, Train Acc: 85.08%, Train Discrimination: 0.0067993272095918655 Test Loss: 0.3732462227344513, Test Acc: 85.19%,Test Discrimination: 0.008484604768455029
Epoch 20, Train Loss: 0.37334680557250977, Train Acc: 84.78%, Train Discrimination: 0.007999501191079617 Test Loss: 0.370652437210083, Test Acc: 85.19%,Test Discrimination: 0.008673476055264473
Epoch 21, Train Loss: 0.37075650691986084, Train Acc: 84.74%, Train Discrimination: 0.00821955781430006 Test Loss: 0.3628670573234558, Test Acc: 85.47%,Test Discrimination: 0.007824490778148174
Epoch 22, Train Loss: 0.3618774712085724, Train Acc: 85.25%, Train Discrimination: 0.007530253380537033 Test Loss: 0.35592013597488403, Test Acc: 85.60%,Test Discrimination: 0.0062730032950639725
Epoch 23, Train Loss: 0.35307776927948, Train Acc: 85.86%, Train Discrimination: 0.006156868301331997 Test Loss: 0.3570830523967743, Test Acc: 85.03%,Test Discrimination: 0.004269877914339304
Epoch 24, Train Loss: 0.35199764370918274, Train Acc: 85.69%, Train Discrimination: 0.0043389140628278255 Test Loss: 0.3648805618286133, Test Acc: 84.74%,Test Discrimination: 0.0030647350940853357
Epoch 25, Train Loss: 0.358228862285614, Train Acc: 85.53%, Train Discrimination: 0.0031977698672562838 Test Loss: 0.3667066991329193, Test Acc: 84.55%,Test Discrimination: 0.0027728930581361055
Epoch 26, Train Loss: 0.3596973419189453, Train Acc: 85.45%, Train Discrimination: 0.002913468051701784 Test Loss: 0.3621077537536621, Test Acc: 84.71%,Test Discrimination: 0.0029844145756214857
Epoch 27, Train Loss: 0.3555004596710205, Train Acc: 85.60%, Train Discrimination: 0.003113545011729002 Test Loss: 0.3559049069881439, Test Acc: 84.97%,Test Discrimination: 0.0035100411623716354
Epoch 28, Train Loss: 0.35005247592926025, Train Acc: 85.79%, Train Discrimination: 0.0036086475010961294 Test Loss: 0.352242648601532, Test Acc: 85.15%,Test Discrimination: 0.0040298947133123875
Epoch 29, Train Loss: 0.347201406955719, Train Acc: 85.91%, Train Discrimination: 0.0041025253012776375 Test Loss: 0.3509335517883301, Test Acc: 85.41%,Test Discrimination: 0.004461049567908049
Epoch 30, Train Loss: 0.34650900959968567, Train Acc: 85.99%, Train Discrimination: 0.00450153648853302 Test Loss: 0.35114774107933044, Test Acc: 85.63%,Test Discrimination: 0.0048438492231070995
Epoch 31, Train Loss: 0.34720852971076965, Train Acc: 85.83%, Train Discrimination: 0.004843216389417648 Test Loss: 0.3517586290836334, Test Acc: 85.72%,Test Discrimination: 0.0051404209807515144
Epoch 32, Train Loss: 0.34810757637023926, Train Acc: 85.82%, Train Discrimination: 0.005096232518553734 Test Loss: 0.3517192006111145, Test Acc: 85.75%,Test Discrimination: 0.005304928403347731
Epoch 33, Train Loss: 0.34812384843826294, Train Acc: 85.80%, Train Discrimination: 0.005225469823926687 Test Loss: 0.3507087826728821, Test Acc: 85.88%,Test Discrimination: 0.005323120858520269
Epoch 34, Train Loss: 0.34694764018058777, Train Acc: 85.88%, Train Discrimination: 0.0052189030684530735 Test Loss: 0.34918203949928284, Test Acc: 85.57%,Test Discrimination: 0.005215093027800322
Epoch 35, Train Loss: 0.3450755178928375, Train Acc: 85.95%, Train Discrimination: 0.005099745467305183 Test Loss: 0.3479025661945343, Test Acc: 85.57%,Test Discrimination: 0.005029653664678335
Epoch 36, Train Loss: 0.34332409501075745, Train Acc: 86.05%, Train Discrimination: 0.00491506839171052 Test Loss: 0.3474251329898834, Test Acc: 85.66%,Test Discrimination: 0.004832464270293713
Epoch 37, Train Loss: 0.3423124849796295, Train Acc: 85.94%, Train Discrimination: 0.00471882801502943 Test Loss: 0.34779295325279236, Test Acc: 85.47%,Test Discrimination: 0.004666847642511129
Epoch 38, Train Loss: 0.34218746423721313, Train Acc: 86.04%, Train Discrimination: 0.004549836274236441 Test Loss: 0.3481362760066986, Test Acc: 85.31%,Test Discrimination: 0.0045899138785898685
Epoch 39, Train Loss: 0.3423095941543579, Train Acc: 86.04%, Train Discrimination: 0.004468764178454876 Test Loss: 0.3479565382003784, Test Acc: 85.34%,Test Discrimination: 0.004617195576429367
Epoch 40, Train Loss: 0.3421555459499359, Train Acc: 86.06%, Train Discrimination: 0.004489216022193432 Test Loss: 0.34734922647476196, Test Acc: 85.38%,Test Discrimination: 0.004737720359116793
Epoch 41, Train Loss: 0.34172186255455017, Train Acc: 86.09%, Train Discrimination: 0.0045938640832901 Test Loss: 0.3464391827583313, Test Acc: 85.50%,Test Discrimination: 0.0049515534192323685
Epoch 42, Train Loss: 0.3410283327102661, Train Acc: 86.06%, Train Discrimination: 0.004776503890752792 Test Loss: 0.3454667925834656, Test Acc: 85.47%,Test Discrimination: 0.005215461365878582
Epoch 43, Train Loss: 0.34028807282447815, Train Acc: 86.09%, Train Discrimination: 0.005006327759474516 Test Loss: 0.3446483016014099, Test Acc: 85.63%,Test Discrimination: 0.00547810411080718
Epoch 44, Train Loss: 0.3396799862384796, Train Acc: 86.18%, Train Discrimination: 0.005233499687165022 Test Loss: 0.3440929651260376, Test Acc: 85.63%,Test Discrimination: 0.005683063063770533
Epoch 45, Train Loss: 0.3392244279384613, Train Acc: 86.18%, Train Discrimination: 0.005411406978964806 Test Loss: 0.3437230885028839, Test Acc: 85.60%,Test Discrimination: 0.005786689464002848
Epoch 46, Train Loss: 0.3388238549232483, Train Acc: 86.23%, Train Discrimination: 0.005500433966517448 Test Loss: 0.3433622419834137, Test Acc: 85.72%,Test Discrimination: 0.005760561674833298
Epoch 47, Train Loss: 0.33829063177108765, Train Acc: 86.20%, Train Discrimination: 0.005485036876052618 Test Loss: 0.3427587151527405, Test Acc: 85.60%,Test Discrimination: 0.005553071387112141
Epoch 48, Train Loss: 0.3372434079647064, Train Acc: 86.24%, Train Discrimination: 0.00532850157469511 Test Loss: 0.3428235650062561, Test Acc: 85.60%,Test Discrimination: 0.0050995540805161
Epoch 49, Train Loss: 0.33648398518562317, Train Acc: 86.20%, Train Discrimination: 0.004882072564214468 Test Loss: 0.3438594341278076, Test Acc: 85.75%,Test Discrimination: 0.004647944588214159
Epoch 50, Train Loss: 0.33689045906066895, Train Acc: 86.13%, Train Discrimination: 0.004475572146475315 Test Loss: 0.34279054403305054, Test Acc: 85.69%,Test Discrimination: 0.004891620483249426
Epoch 51, Train Loss: 0.33591896295547485, Train Acc: 86.20%, Train Discrimination: 0.00469028577208519 Test Loss: 0.3413585424423218, Test Acc: 85.79%,Test Discrimination: 0.005431998986750841
Epoch 52, Train Loss: 0.33497539162635803, Train Acc: 86.22%, Train Discrimination: 0.005185976158827543 Test Loss: 0.3408142328262329, Test Acc: 85.88%,Test Discrimination: 0.005864784587174654
Epoch 53, Train Loss: 0.33478280901908875, Train Acc: 86.27%, Train Discrimination: 0.005588292144238949 Test Loss: 0.34041446447372437, Test Acc: 85.69%,Test Discrimination: 0.006060647778213024
Epoch 54, Train Loss: 0.3343485891819, Train Acc: 86.33%, Train Discrimination: 0.005762611050158739 Test Loss: 0.33992964029312134, Test Acc: 85.82%,Test Discrimination: 0.005970019847154617
Epoch 55, Train Loss: 0.3332911729812622, Train Acc: 86.32%, Train Discrimination: 0.0056709954515099525 Test Loss: 0.34019899368286133, Test Acc: 85.66%,Test Discrimination: 0.005613189190626144
Epoch 56, Train Loss: 0.3327677249908447, Train Acc: 86.35%, Train Discrimination: 0.005337200593203306 Test Loss: 0.340359091758728, Test Acc: 85.50%,Test Discrimination: 0.005480789113789797
Epoch 57, Train Loss: 0.332645058631897, Train Acc: 86.32%, Train Discrimination: 0.0052249678410589695 Test Loss: 0.3393608033657074, Test Acc: 85.75%,Test Discrimination: 0.005855914205312729
Epoch 58, Train Loss: 0.3316524028778076, Train Acc: 86.41%, Train Discrimination: 0.00555590121075511 Test Loss: 0.3386494219303131, Test Acc: 85.69%,Test Discrimination: 0.006398295052349567
Epoch 59, Train Loss: 0.33122608065605164, Train Acc: 86.43%, Train Discrimination: 0.006040926557034254 Test Loss: 0.33836600184440613, Test Acc: 85.69%,Test Discrimination: 0.006690859328955412
Epoch 60, Train Loss: 0.33094748854637146, Train Acc: 86.48%, Train Discrimination: 0.006295468658208847 Test Loss: 0.3378935754299164, Test Acc: 85.79%,Test Discrimination: 0.006558183114975691
Epoch 61, Train Loss: 0.3299647867679596, Train Acc: 86.45%, Train Discrimination: 0.0061701275408267975 Test Loss: 0.3377578854560852, Test Acc: 85.82%,Test Discrimination: 0.006188309285789728
Epoch 62, Train Loss: 0.32932257652282715, Train Acc: 86.43%, Train Discrimination: 0.005845649167895317 Test Loss: 0.337801069021225, Test Acc: 85.69%,Test Discrimination: 0.006073724012821913
Epoch 63, Train Loss: 0.32895612716674805, Train Acc: 86.36%, Train Discrimination: 0.005740594584494829 Test Loss: 0.33701977133750916, Test Acc: 85.75%,Test Discrimination: 0.006612639874219894
Epoch 64, Train Loss: 0.3281688690185547, Train Acc: 86.52%, Train Discrimination: 0.006195733789354563 Test Loss: 0.3368346691131592, Test Acc: 85.79%,Test Discrimination: 0.007087842561304569
Epoch 65, Train Loss: 0.3279682397842407, Train Acc: 86.56%, Train Discrimination: 0.006596654653549194 Test Loss: 0.3366362750530243, Test Acc: 85.79%,Test Discrimination: 0.006691510323435068
Epoch 66, Train Loss: 0.3271755278110504, Train Acc: 86.55%, Train Discrimination: 0.006254435982555151 Test Loss: 0.3368988037109375, Test Acc: 85.82%,Test Discrimination: 0.006485863588750362
Epoch 67, Train Loss: 0.32695820927619934, Train Acc: 86.51%, Train Discrimination: 0.006070066709071398 Test Loss: 0.3361049294471741, Test Acc: 85.79%,Test Discrimination: 0.0070541612803936005
Epoch 68, Train Loss: 0.3262079954147339, Train Acc: 86.58%, Train Discrimination: 0.006557302549481392 Test Loss: 0.3359152674674988, Test Acc: 85.79%,Test Discrimination: 0.00759136863052845
Epoch 69, Train Loss: 0.3259742856025696, Train Acc: 86.63%, Train Discrimination: 0.007011880166828632 Test Loss: 0.33555248379707336, Test Acc: 85.79%,Test Discrimination: 0.007395173888653517
Epoch 70, Train Loss: 0.32528313994407654, Train Acc: 86.66%, Train Discrimination: 0.006844343151897192 Test Loss: 0.3357302248477936, Test Acc: 85.82%,Test Discrimination: 0.007065238896757364
Epoch 71, Train Loss: 0.3249872922897339, Train Acc: 86.57%, Train Discrimination: 0.006566172000020742 Test Loss: 0.335235059261322, Test Acc: 85.75%,Test Discrimination: 0.007374401204288006
Epoch 72, Train Loss: 0.32438579201698303, Train Acc: 86.57%, Train Discrimination: 0.006828139536082745 Test Loss: 0.3349394202232361, Test Acc: 85.72%,Test Discrimination: 0.007942238822579384
Epoch 73, Train Loss: 0.3240674138069153, Train Acc: 86.67%, Train Discrimination: 0.007298248820006847 Test Loss: 0.3347195088863373, Test Acc: 85.75%,Test Discrimination: 0.007843129336833954
Epoch 74, Train Loss: 0.32354336977005005, Train Acc: 86.62%, Train Discrimination: 0.007216408848762512 Test Loss: 0.334798663854599, Test Acc: 85.79%,Test Discrimination: 0.007438850589096546
Epoch 75, Train Loss: 0.323205828666687, Train Acc: 86.65%, Train Discrimination: 0.006890801712870598 Test Loss: 0.33445727825164795, Test Acc: 85.79%,Test Discrimination: 0.0076538389548659325
Epoch 76, Train Loss: 0.3227193355560303, Train Acc: 86.67%, Train Discrimination: 0.007067491300404072 Test Loss: 0.3341604769229889, Test Acc: 85.75%,Test Discrimination: 0.008161009289324284
Epoch 77, Train Loss: 0.32241666316986084, Train Acc: 86.76%, Train Discrimination: 0.007474199403077364 Test Loss: 0.3339819610118866, Test Acc: 85.66%,Test Discrimination: 0.007992609404027462
Epoch 78, Train Loss: 0.3219328820705414, Train Acc: 86.70%, Train Discrimination: 0.007345939055085182 Test Loss: 0.33404433727264404, Test Acc: 85.63%,Test Discrimination: 0.007817557081580162
Epoch 79, Train Loss: 0.3216778635978699, Train Acc: 86.65%, Train Discrimination: 0.007208542432636023 Test Loss: 0.3336978554725647, Test Acc: 85.72%,Test Discrimination: 0.008280372247099876
Epoch 80, Train Loss: 0.32127708196640015, Train Acc: 86.74%, Train Discrimination: 0.007584818638861179 Test Loss: 0.3335930407047272, Test Acc: 85.69%,Test Discrimination: 0.00846637599170208
Epoch 81, Train Loss: 0.3209996521472931, Train Acc: 86.79%, Train Discrimination: 0.0077421823516488075 Test Loss: 0.333709180355072, Test Acc: 85.75%,Test Discrimination: 0.008068300783634186
Epoch 82, Train Loss: 0.3206944465637207, Train Acc: 86.68%, Train Discrimination: 0.00742760905995965 Test Loss: 0.3334692716598511, Test Acc: 85.75%,Test Discrimination: 0.008324629627168179
Epoch 83, Train Loss: 0.32031241059303284, Train Acc: 86.77%, Train Discrimination: 0.007639349438250065 Test Loss: 0.3334105312824249, Test Acc: 85.72%,Test Discrimination: 0.008620386011898518
Epoch 84, Train Loss: 0.3200802505016327, Train Acc: 86.79%, Train Discrimination: 0.007882635109126568 Test Loss: 0.3334823250770569, Test Acc: 85.69%,Test Discrimination: 0.008309287019073963
Epoch 85, Train Loss: 0.3197619616985321, Train Acc: 86.76%, Train Discrimination: 0.007640537340193987 Test Loss: 0.33345431089401245, Test Acc: 85.63%,Test Discrimination: 0.008483701385557652
Epoch 86, Train Loss: 0.3194602429866791, Train Acc: 86.80%, Train Discrimination: 0.007797683123499155 Test Loss: 0.3334093689918518, Test Acc: 85.75%,Test Discrimination: 0.008867759257555008
Epoch 87, Train Loss: 0.31921136379241943, Train Acc: 86.88%, Train Discrimination: 0.008131732232868671 Test Loss: 0.33344534039497375, Test Acc: 85.69%,Test Discrimination: 0.00871904008090496
Epoch 88, Train Loss: 0.31891241669654846, Train Acc: 86.87%, Train Discrimination: 0.008025644347071648 Test Loss: 0.333479642868042, Test Acc: 85.66%,Test Discrimination: 0.008733541704714298
Epoch 89, Train Loss: 0.3186524212360382, Train Acc: 86.90%, Train Discrimination: 0.008048953488469124 Test Loss: 0.3334639072418213, Test Acc: 85.72%,Test Discrimination: 0.00900481641292572
Epoch 90, Train Loss: 0.3184124827384949, Train Acc: 86.94%, Train Discrimination: 0.008280442096292973 Test Loss: 0.3335970938205719, Test Acc: 85.57%,Test Discrimination: 0.008600234985351562
Epoch 91, Train Loss: 0.3181653916835785, Train Acc: 86.92%, Train Discrimination: 0.00795657467097044 Test Loss: 0.33354657888412476, Test Acc: 85.50%,Test Discrimination: 0.008944403380155563
Epoch 92, Train Loss: 0.31787896156311035, Train Acc: 86.95%, Train Discrimination: 0.008243086747825146 Test Loss: 0.33358699083328247, Test Acc: 85.53%,Test Discrimination: 0.008672085590660572
Epoch 93, Train Loss: 0.31758788228034973, Train Acc: 86.98%, Train Discrimination: 0.008025315590202808 Test Loss: 0.33356019854545593, Test Acc: 85.50%,Test Discrimination: 0.00874628871679306
Epoch 94, Train Loss: 0.3173149824142456, Train Acc: 87.00%, Train Discrimination: 0.00808964017778635 Test Loss: 0.33355697989463806, Test Acc: 85.53%,Test Discrimination: 0.00889852549880743
Epoch 95, Train Loss: 0.31707391142845154, Train Acc: 86.95%, Train Discrimination: 0.00821941252797842 Test Loss: 0.3336179256439209, Test Acc: 85.47%,Test Discrimination: 0.008579921908676624
Epoch 96, Train Loss: 0.3168719708919525, Train Acc: 86.99%, Train Discrimination: 0.00796582829207182 Test Loss: 0.33360356092453003, Test Acc: 85.53%,Test Discrimination: 0.009153065271675587
Epoch 97, Train Loss: 0.316703200340271, Train Acc: 86.98%, Train Discrimination: 0.008434842340648174 Test Loss: 0.3338625133037567, Test Acc: 85.53%,Test Discrimination: 0.008353297598659992
Epoch 98, Train Loss: 0.3165673315525055, Train Acc: 86.96%, Train Discrimination: 0.007799598854035139 Test Loss: 0.3337288796901703, Test Acc: 85.53%,Test Discrimination: 0.009209738112986088
Epoch 99, Train Loss: 0.3163195848464966, Train Acc: 87.04%, Train Discrimination: 0.008499504067003727 Test Loss: 0.33393871784210205, Test Acc: 85.57%,Test Discrimination: 0.008402534760534763
Epoch 100, Train Loss: 0.31598588824272156, Train Acc: 86.89%, Train Discrimination: 0.007862239144742489 Test Loss: 0.3339180648326874, Test Acc: 85.60%,Test Discrimination: 0.008908786810934544</code></pre>
</div>
</div>
</section>
<section id="lambda-1-2" class="level4">
<h4 class="anchored" data-anchor-id="lambda-1-2">LAMBDA &gt; 1</h4>
<div id="75e2b4c8" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>model5 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model5.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>train_losses5, train_accuracies5, train_discriminations5,train_fairness5 <span class="op">=</span> [], [], [],[]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>test_losses5, test_accuracies5, test_discriminations5,test_fairness5 <span class="op">=</span> [], [], [],[]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>model5.train()</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model5(features)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features, lambda_val<span class="op">=</span><span class="dv">10</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    model5.<span class="bu">eval</span>()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model5(test_features)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features, lambda_val<span class="op">=</span><span class="dv">10</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    train_losses5.append(loss.item())</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    train_accuracies5.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    train_discriminations5.append(discrimination)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    train_fairness5.append(fairness)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    test_fairness5.append(fairness)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    test_losses5.append(test_loss.item())</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    test_accuracies5.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    test_discriminations5.append(test_discrimination)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    model5.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.7580418586730957, Train Acc: 74.04%, Train Discrimination: 0.023334426805377007 Test Loss: 1.618756890296936, Test Acc: 50.85%,Test Discrimination: 0.06654535233974457
Epoch 2, Train Loss: 1.4231175184249878, Train Acc: 50.83%, Train Discrimination: 0.046677738428115845 Test Loss: 0.47272711992263794, Test Acc: 82.66%,Test Discrimination: 7.880499651946593e-06
Epoch 3, Train Loss: 0.46807360649108887, Train Acc: 82.87%, Train Discrimination: 0.0003548309614416212 Test Loss: 0.6750913858413696, Test Acc: 82.66%,Test Discrimination: 0.010784532874822617
Epoch 4, Train Loss: 0.6283789873123169, Train Acc: 82.87%, Train Discrimination: 0.0055527593940496445 Test Loss: 0.787500262260437, Test Acc: 82.66%,Test Discrimination: 0.00893236044794321
Epoch 5, Train Loss: 0.7395487427711487, Train Acc: 82.87%, Train Discrimination: 0.0048926835879683495 Test Loss: 0.7755417823791504, Test Acc: 82.66%,Test Discrimination: 0.0054415324702858925
Epoch 6, Train Loss: 0.7377737760543823, Train Acc: 82.87%, Train Discrimination: 0.0029509030282497406 Test Loss: 0.7039012908935547, Test Acc: 82.66%,Test Discrimination: 0.0027794602792710066
Epoch 7, Train Loss: 0.6758265495300293, Train Acc: 82.87%, Train Discrimination: 0.0014072805643081665 Test Loss: 0.6117937564849854, Test Acc: 82.66%,Test Discrimination: 0.0010195492068305612
Epoch 8, Train Loss: 0.5912852883338928, Train Acc: 82.87%, Train Discrimination: 0.000390788190998137 Test Loss: 0.5221425890922546, Test Acc: 82.66%,Test Discrimination: 5.833491377416067e-05
Epoch 9, Train Loss: 0.5076305866241455, Train Acc: 82.87%, Train Discrimination: 3.2122752600116655e-06 Test Loss: 0.4558315575122833, Test Acc: 82.66%,Test Discrimination: 0.0006259588990360498
Epoch 10, Train Loss: 0.44588515162467957, Train Acc: 82.87%, Train Discrimination: 0.0009528392693027854 Test Loss: 0.4323478043079376, Test Acc: 82.69%,Test Discrimination: 0.004553061444312334
Epoch 11, Train Loss: 0.42349863052368164, Train Acc: 82.89%, Train Discrimination: 0.004521580878645182 Test Loss: 0.4549855589866638, Test Acc: 82.79%,Test Discrimination: 0.01247853972017765
Epoch 12, Train Loss: 0.44127681851387024, Train Acc: 83.07%, Train Discrimination: 0.010721142403781414 Test Loss: 0.4939708709716797, Test Acc: 83.04%,Test Discrimination: 0.020251505076885223
Epoch 13, Train Loss: 0.4726760685443878, Train Acc: 83.29%, Train Discrimination: 0.016315976157784462 Test Loss: 0.5066609382629395, Test Acc: 83.20%,Test Discrimination: 0.02249731495976448
Epoch 14, Train Loss: 0.48249155282974243, Train Acc: 83.39%, Train Discrimination: 0.01768244244158268 Test Loss: 0.48407045006752014, Test Acc: 83.10%,Test Discrimination: 0.01864784210920334
Epoch 15, Train Loss: 0.4636102616786957, Train Acc: 83.34%, Train Discrimination: 0.014623057097196579 Test Loss: 0.4499863386154175, Test Acc: 82.88%,Test Discrimination: 0.012290898710489273
Epoch 16, Train Loss: 0.4356330335140228, Train Acc: 83.19%, Train Discrimination: 0.009784474968910217 Test Loss: 0.4251251220703125, Test Acc: 82.75%,Test Discrimination: 0.006767770275473595
Epoch 17, Train Loss: 0.41544309258461, Train Acc: 82.97%, Train Discrimination: 0.005553588271141052 Test Loss: 0.4145296812057495, Test Acc: 82.72%,Test Discrimination: 0.003124645445495844
Epoch 18, Train Loss: 0.40746575593948364, Train Acc: 82.90%, Train Discrimination: 0.0027283979579806328 Test Loss: 0.41516613960266113, Test Acc: 82.69%,Test Discrimination: 0.0011912152403965592
Epoch 19, Train Loss: 0.40890538692474365, Train Acc: 82.89%, Train Discrimination: 0.0011682490585371852 Test Loss: 0.4212435781955719, Test Acc: 82.66%,Test Discrimination: 0.00035031320294365287
Epoch 20, Train Loss: 0.4147799015045166, Train Acc: 82.87%, Train Discrimination: 0.00043333406210877 Test Loss: 0.42755720019340515, Test Acc: 82.66%,Test Discrimination: 7.053612353047356e-05
Epoch 21, Train Loss: 0.42063409090042114, Train Acc: 82.87%, Train Discrimination: 0.00014762479986529797 Test Loss: 0.43078941106796265, Test Acc: 82.66%,Test Discrimination: 1.0727161679824349e-05
Epoch 22, Train Loss: 0.4234977662563324, Train Acc: 82.87%, Train Discrimination: 6.207962724147364e-05 Test Loss: 0.4295097887516022, Test Acc: 82.66%,Test Discrimination: 8.320625056512654e-06
Epoch 23, Train Loss: 0.4222061336040497, Train Acc: 82.87%, Train Discrimination: 5.912829510634765e-05 Test Loss: 0.42412203550338745, Test Acc: 82.69%,Test Discrimination: 4.572206671582535e-05
Epoch 24, Train Loss: 0.41716739535331726, Train Acc: 82.87%, Train Discrimination: 0.0001237674441654235 Test Loss: 0.4162903130054474, Test Acc: 82.66%,Test Discrimination: 0.00020338354806881398
Epoch 25, Train Loss: 0.4098908007144928, Train Acc: 82.89%, Train Discrimination: 0.00031073990976437926 Test Loss: 0.4082392454147339, Test Acc: 82.66%,Test Discrimination: 0.0006066203350201249
Epoch 26, Train Loss: 0.40234678983688354, Train Acc: 82.91%, Train Discrimination: 0.000704120087902993 Test Loss: 0.40199965238571167, Test Acc: 82.63%,Test Discrimination: 0.0013629901222884655
Epoch 27, Train Loss: 0.3963603079319, Train Acc: 82.92%, Train Discrimination: 0.0013708857586607337 Test Loss: 0.39880698919296265, Test Acc: 82.72%,Test Discrimination: 0.0024972050450742245
Epoch 28, Train Loss: 0.39305832982063293, Train Acc: 82.94%, Train Discrimination: 0.0023144527804106474 Test Loss: 0.3987092077732086, Test Acc: 82.79%,Test Discrimination: 0.0038867166731506586
Epoch 29, Train Loss: 0.3925418555736542, Train Acc: 83.14%, Train Discrimination: 0.0034389609936624765 Test Loss: 0.40055179595947266, Test Acc: 83.01%,Test Discrimination: 0.005264663137495518
Epoch 30, Train Loss: 0.39388328790664673, Train Acc: 83.27%, Train Discrimination: 0.004550851881504059 Test Loss: 0.4025264084339142, Test Acc: 83.16%,Test Discrimination: 0.006318294443190098
Epoch 31, Train Loss: 0.39553147554397583, Train Acc: 83.39%, Train Discrimination: 0.005421954672783613 Test Loss: 0.4029039740562439, Test Acc: 83.29%,Test Discrimination: 0.006792198866605759
Epoch 32, Train Loss: 0.3959982991218567, Train Acc: 83.60%, Train Discrimination: 0.005859663709998131 Test Loss: 0.400831937789917, Test Acc: 83.39%,Test Discrimination: 0.006586700212210417
Epoch 33, Train Loss: 0.39445218443870544, Train Acc: 83.69%, Train Discrimination: 0.005770263262093067 Test Loss: 0.39635583758354187, Test Acc: 83.45%,Test Discrimination: 0.005670224782079458
Epoch 34, Train Loss: 0.3909073770046234, Train Acc: 83.71%, Train Discrimination: 0.005120335146784782 Test Loss: 0.39118245244026184, Test Acc: 83.39%,Test Discrimination: 0.00416652113199234
Epoch 35, Train Loss: 0.38660067319869995, Train Acc: 83.68%, Train Discrimination: 0.003961811773478985 Test Loss: 0.3881354033946991, Test Acc: 83.26%,Test Discrimination: 0.0024886741302907467
Epoch 36, Train Loss: 0.38397204875946045, Train Acc: 83.61%, Train Discrimination: 0.0025620372034609318 Test Loss: 0.3889928162097931, Test Acc: 83.13%,Test Discrimination: 0.0012720613740384579
Epoch 37, Train Loss: 0.3847283124923706, Train Acc: 83.47%, Train Discrimination: 0.0014809262938797474 Test Loss: 0.38926035165786743, Test Acc: 83.23%,Test Discrimination: 0.0009010136127471924
Epoch 38, Train Loss: 0.38498324155807495, Train Acc: 83.52%, Train Discrimination: 0.0011449228040874004 Test Loss: 0.3877016007900238, Test Acc: 83.42%,Test Discrimination: 0.0008997524273581803
Epoch 39, Train Loss: 0.3836352229118347, Train Acc: 83.58%, Train Discrimination: 0.0011602132581174374 Test Loss: 0.3856111764907837, Test Acc: 83.77%,Test Discrimination: 0.0010874962899833918
Epoch 40, Train Loss: 0.38181808590888977, Train Acc: 83.78%, Train Discrimination: 0.0013521146029233932 Test Loss: 0.3839602768421173, Test Acc: 83.89%,Test Discrimination: 0.0013619709061458707
Epoch 41, Train Loss: 0.38038164377212524, Train Acc: 83.86%, Train Discrimination: 0.0015955944545567036 Test Loss: 0.3829328119754791, Test Acc: 83.95%,Test Discrimination: 0.001625313307158649
Epoch 42, Train Loss: 0.3793451189994812, Train Acc: 84.08%, Train Discrimination: 0.0018130147363990545 Test Loss: 0.3822029232978821, Test Acc: 84.02%,Test Discrimination: 0.0018827372696250677
Epoch 43, Train Loss: 0.3784838616847992, Train Acc: 84.18%, Train Discrimination: 0.0020217490382492542 Test Loss: 0.38155031204223633, Test Acc: 84.11%,Test Discrimination: 0.0021622744388878345
Epoch 44, Train Loss: 0.3777392506599426, Train Acc: 84.22%, Train Discrimination: 0.0022510820999741554 Test Loss: 0.38100865483283997, Test Acc: 84.33%,Test Discrimination: 0.0024247223045676947
Epoch 45, Train Loss: 0.37707698345184326, Train Acc: 84.28%, Train Discrimination: 0.0024677026085555553 Test Loss: 0.38051527738571167, Test Acc: 84.37%,Test Discrimination: 0.002617271151393652
Epoch 46, Train Loss: 0.37646469473838806, Train Acc: 84.44%, Train Discrimination: 0.0026253482792526484 Test Loss: 0.380009263753891, Test Acc: 84.46%,Test Discrimination: 0.002702017081901431
Epoch 47, Train Loss: 0.37584051489830017, Train Acc: 84.54%, Train Discrimination: 0.002692267531529069 Test Loss: 0.37944847345352173, Test Acc: 84.40%,Test Discrimination: 0.0026684862095862627
Epoch 48, Train Loss: 0.375163733959198, Train Acc: 84.54%, Train Discrimination: 0.002657626522704959 Test Loss: 0.37883034348487854, Test Acc: 84.40%,Test Discrimination: 0.0025280530098825693
Epoch 49, Train Loss: 0.37445786595344543, Train Acc: 84.58%, Train Discrimination: 0.002529242541640997 Test Loss: 0.3781985342502594, Test Acc: 84.43%,Test Discrimination: 0.0023028107825666666
Epoch 50, Train Loss: 0.3737919330596924, Train Acc: 84.59%, Train Discrimination: 0.002325460547581315 Test Loss: 0.377647340297699, Test Acc: 84.49%,Test Discrimination: 0.0020538948010653257
Epoch 51, Train Loss: 0.3731755018234253, Train Acc: 84.59%, Train Discrimination: 0.0020865134429186583 Test Loss: 0.37719810009002686, Test Acc: 84.52%,Test Discrimination: 0.001828867825679481
Epoch 52, Train Loss: 0.37259551882743835, Train Acc: 84.62%, Train Discrimination: 0.0018592079868540168 Test Loss: 0.3767361640930176, Test Acc: 84.59%,Test Discrimination: 0.0016733652446419
Epoch 53, Train Loss: 0.37200677394866943, Train Acc: 84.72%, Train Discrimination: 0.0016883683856576681 Test Loss: 0.3762405216693878, Test Acc: 84.55%,Test Discrimination: 0.0016128825955092907
Epoch 54, Train Loss: 0.37136825919151306, Train Acc: 84.79%, Train Discrimination: 0.001591982552781701 Test Loss: 0.3757925033569336, Test Acc: 84.55%,Test Discrimination: 0.0016333324601873755
Epoch 55, Train Loss: 0.37072232365608215, Train Acc: 84.89%, Train Discrimination: 0.001555597991682589 Test Loss: 0.375399112701416, Test Acc: 84.49%,Test Discrimination: 0.001711636083200574
Epoch 56, Train Loss: 0.37015846371650696, Train Acc: 84.89%, Train Discrimination: 0.0015756861539557576 Test Loss: 0.37502554059028625, Test Acc: 84.52%,Test Discrimination: 0.0018164272187277675
Epoch 57, Train Loss: 0.3696192502975464, Train Acc: 84.95%, Train Discrimination: 0.0016217551892623305 Test Loss: 0.37460270524024963, Test Acc: 84.46%,Test Discrimination: 0.001902737538330257
Epoch 58, Train Loss: 0.3690405488014221, Train Acc: 84.97%, Train Discrimination: 0.0016593349864706397 Test Loss: 0.37401944398880005, Test Acc: 84.43%,Test Discrimination: 0.0018912854138761759
Epoch 59, Train Loss: 0.3683820366859436, Train Acc: 84.98%, Train Discrimination: 0.001636682078242302 Test Loss: 0.37355339527130127, Test Acc: 84.49%,Test Discrimination: 0.0017744142096489668
Epoch 60, Train Loss: 0.36775219440460205, Train Acc: 84.94%, Train Discrimination: 0.0015226994873955846 Test Loss: 0.37313956022262573, Test Acc: 84.46%,Test Discrimination: 0.0015745116397738457
Epoch 61, Train Loss: 0.3672616183757782, Train Acc: 84.97%, Train Discrimination: 0.0013522636145353317 Test Loss: 0.3726447820663452, Test Acc: 84.49%,Test Discrimination: 0.0014969451585784554
Epoch 62, Train Loss: 0.36671143770217896, Train Acc: 84.98%, Train Discrimination: 0.0012679907958954573 Test Loss: 0.37201911211013794, Test Acc: 84.37%,Test Discrimination: 0.0015861069550737739
Epoch 63, Train Loss: 0.36602821946144104, Train Acc: 85.03%, Train Discrimination: 0.001277158735319972 Test Loss: 0.37157320976257324, Test Acc: 84.46%,Test Discrimination: 0.0017376078758388758
Epoch 64, Train Loss: 0.3654622435569763, Train Acc: 84.99%, Train Discrimination: 0.0013167932629585266 Test Loss: 0.3710538148880005, Test Acc: 84.46%,Test Discrimination: 0.0016957068582996726
Epoch 65, Train Loss: 0.36495089530944824, Train Acc: 85.00%, Train Discrimination: 0.0012398052494972944 Test Loss: 0.37020593881607056, Test Acc: 84.49%,Test Discrimination: 0.001378093846142292
Epoch 66, Train Loss: 0.3642999231815338, Train Acc: 85.06%, Train Discrimination: 0.0010001143673434854 Test Loss: 0.3696703612804413, Test Acc: 84.43%,Test Discrimination: 0.0010234001092612743
Epoch 67, Train Loss: 0.36388054490089417, Train Acc: 85.03%, Train Discrimination: 0.0007408916717395186 Test Loss: 0.3691275119781494, Test Acc: 84.43%,Test Discrimination: 0.001053202198818326
Epoch 68, Train Loss: 0.36329013109207153, Train Acc: 85.08%, Train Discrimination: 0.0007270406931638718 Test Loss: 0.36891359090805054, Test Acc: 84.46%,Test Discrimination: 0.0013448471436277032
Epoch 69, Train Loss: 0.3627786636352539, Train Acc: 85.20%, Train Discrimination: 0.0008634442929178476 Test Loss: 0.36851009726524353, Test Acc: 84.43%,Test Discrimination: 0.001318965689279139
Epoch 70, Train Loss: 0.36222442984580994, Train Acc: 85.15%, Train Discrimination: 0.0008231940446421504 Test Loss: 0.3679479658603668, Test Acc: 84.30%,Test Discrimination: 0.0010415359865874052
Epoch 71, Train Loss: 0.3617262542247772, Train Acc: 85.04%, Train Discrimination: 0.0006600291235372424 Test Loss: 0.36763814091682434, Test Acc: 84.37%,Test Discrimination: 0.0011423126561567187
Epoch 72, Train Loss: 0.3611602783203125, Train Acc: 85.09%, Train Discrimination: 0.0007105944096110761 Test Loss: 0.36764296889305115, Test Acc: 84.46%,Test Discrimination: 0.0015278561040759087
Epoch 73, Train Loss: 0.3606736958026886, Train Acc: 85.21%, Train Discrimination: 0.0009190323180519044 Test Loss: 0.36710241436958313, Test Acc: 84.40%,Test Discrimination: 0.0013864119537174702
Epoch 74, Train Loss: 0.36003410816192627, Train Acc: 85.22%, Train Discrimination: 0.0008489530300721526 Test Loss: 0.3667144179344177, Test Acc: 84.37%,Test Discrimination: 0.00114206422585994
Epoch 75, Train Loss: 0.3595317602157593, Train Acc: 85.12%, Train Discrimination: 0.000714876689016819 Test Loss: 0.3664807677268982, Test Acc: 84.49%,Test Discrimination: 0.0014155239332467318
Epoch 76, Train Loss: 0.35886168479919434, Train Acc: 85.26%, Train Discrimination: 0.0008615097030997276 Test Loss: 0.3660942614078522, Test Acc: 84.43%,Test Discrimination: 0.001394286984577775
Epoch 77, Train Loss: 0.35827744007110596, Train Acc: 85.23%, Train Discrimination: 0.0008568963967263699 Test Loss: 0.3655802011489868, Test Acc: 84.37%,Test Discrimination: 0.001058880239725113
Epoch 78, Train Loss: 0.3577783703804016, Train Acc: 85.30%, Train Discrimination: 0.0006842486327514052 Test Loss: 0.3652014136314392, Test Acc: 84.33%,Test Discrimination: 0.001379476161673665
Epoch 79, Train Loss: 0.3571504056453705, Train Acc: 85.39%, Train Discrimination: 0.0008862371905706823 Test Loss: 0.36484137177467346, Test Acc: 84.24%,Test Discrimination: 0.0012912217061966658
Epoch 80, Train Loss: 0.3565741181373596, Train Acc: 85.38%, Train Discrimination: 0.000843220972456038 Test Loss: 0.3645588457584381, Test Acc: 84.33%,Test Discrimination: 0.0009199296473525465
Epoch 81, Train Loss: 0.3561585545539856, Train Acc: 85.42%, Train Discrimination: 0.0006334272911772132 Test Loss: 0.36454150080680847, Test Acc: 84.37%,Test Discrimination: 0.0014129224000498652
Epoch 82, Train Loss: 0.3556634485721588, Train Acc: 85.51%, Train Discrimination: 0.0009396863169968128 Test Loss: 0.36405590176582336, Test Acc: 84.43%,Test Discrimination: 0.0010120744118466973
Epoch 83, Train Loss: 0.3550480902194977, Train Acc: 85.48%, Train Discrimination: 0.0007126408163458109 Test Loss: 0.3638332188129425, Test Acc: 84.49%,Test Discrimination: 0.0012217694893479347
Epoch 84, Train Loss: 0.3544979989528656, Train Acc: 85.53%, Train Discrimination: 0.0008613630780018866 Test Loss: 0.36374470591545105, Test Acc: 84.40%,Test Discrimination: 0.001342870993539691
Epoch 85, Train Loss: 0.3540351688861847, Train Acc: 85.56%, Train Discrimination: 0.0009590573608875275 Test Loss: 0.36370202898979187, Test Acc: 84.49%,Test Discrimination: 0.0010141112143173814
Epoch 86, Train Loss: 0.3537500500679016, Train Acc: 85.47%, Train Discrimination: 0.0007745390175841749 Test Loss: 0.3639340400695801, Test Acc: 84.30%,Test Discrimination: 0.0016717810649424791
Epoch 87, Train Loss: 0.3535130023956299, Train Acc: 85.53%, Train Discrimination: 0.0012026531621813774 Test Loss: 0.3638872504234314, Test Acc: 84.62%,Test Discrimination: 0.0008906418224796653
Epoch 88, Train Loss: 0.3530527353286743, Train Acc: 85.42%, Train Discrimination: 0.0006761008407920599 Test Loss: 0.36356011033058167, Test Acc: 84.49%,Test Discrimination: 0.001394167309626937
Epoch 89, Train Loss: 0.35245180130004883, Train Acc: 85.62%, Train Discrimination: 0.0009962525218725204 Test Loss: 0.363442063331604, Test Acc: 84.65%,Test Discrimination: 0.0013739686692133546
Epoch 90, Train Loss: 0.3521125614643097, Train Acc: 85.62%, Train Discrimination: 0.00099757662974298 Test Loss: 0.36367854475975037, Test Acc: 84.93%,Test Discrimination: 0.0009662124793976545
Epoch 91, Train Loss: 0.3520018756389618, Train Acc: 85.49%, Train Discrimination: 0.0007279837154783309 Test Loss: 0.3636413812637329, Test Acc: 84.62%,Test Discrimination: 0.0016698242397978902
Epoch 92, Train Loss: 0.35175231099128723, Train Acc: 85.70%, Train Discrimination: 0.0012245974503457546 Test Loss: 0.3634784519672394, Test Acc: 84.71%,Test Discrimination: 0.0010756906121969223
Epoch 93, Train Loss: 0.35112443566322327, Train Acc: 85.60%, Train Discrimination: 0.000813674065284431 Test Loss: 0.3633100986480713, Test Acc: 84.71%,Test Discrimination: 0.0011889914749190211
Epoch 94, Train Loss: 0.35072818398475647, Train Acc: 85.61%, Train Discrimination: 0.0009120260365307331 Test Loss: 0.3633374571800232, Test Acc: 84.68%,Test Discrimination: 0.001709948293864727
Epoch 95, Train Loss: 0.3507096469402313, Train Acc: 85.70%, Train Discrimination: 0.0013073609443381429 Test Loss: 0.36365368962287903, Test Acc: 84.84%,Test Discrimination: 0.0010203933343291283
Epoch 96, Train Loss: 0.35045015811920166, Train Acc: 85.62%, Train Discrimination: 0.0008057227241806686 Test Loss: 0.3632471561431885, Test Acc: 84.71%,Test Discrimination: 0.0015084969345480204
Epoch 97, Train Loss: 0.3499599099159241, Train Acc: 85.69%, Train Discrimination: 0.001139883534051478 Test Loss: 0.3632871210575104, Test Acc: 84.90%,Test Discrimination: 0.0012972090626135468
Epoch 98, Train Loss: 0.34961503744125366, Train Acc: 85.68%, Train Discrimination: 0.0009850163478404284 Test Loss: 0.36340224742889404, Test Acc: 84.74%,Test Discrimination: 0.001119046239182353
Epoch 99, Train Loss: 0.34950053691864014, Train Acc: 85.70%, Train Discrimination: 0.0008735205046832561 Test Loss: 0.3632289171218872, Test Acc: 84.81%,Test Discrimination: 0.0017428839346393943
Epoch 100, Train Loss: 0.34939485788345337, Train Acc: 85.77%, Train Discrimination: 0.0013259639963507652 Test Loss: 0.3634076714515686, Test Acc: 84.84%,Test Discrimination: 0.001159182982519269</code></pre>
</div>
</div>
</section>
<section id="lambda-1-3" class="level4">
<h4 class="anchored" data-anchor-id="lambda-1-3">LAMBDA &gt;&gt; 1</h4>
<div id="44cf2ab8" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model6 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model6.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>train_losses6, train_accuracies6, train_discriminations6,train_fairness6 <span class="op">=</span> [], [], [],[]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>test_losses6, test_accuracies6, test_discriminations6,test_fairness6 <span class="op">=</span> [], [], [],[]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>model6.train()</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model6(features)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    model6.<span class="bu">eval</span>()</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model6(test_features)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    train_losses6.append(loss.item())</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    train_accuracies6.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    train_discriminations6.append(discrimination)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    train_fairness6.append(fairness)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    test_fairness6.append(fairness)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    test_losses6.append(test_loss.item())</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    test_accuracies6.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    test_discriminations6.append(test_discrimination)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    model6.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 9.387649536132812, Train Acc: 17.65%, Train Discrimination: 0.08815579116344452 Test Loss: 81.26918029785156, Test Acc: 49.24%,Test Discrimination: 0.9769613146781921
Epoch 2, Train Loss: 53.542686462402344, Train Acc: 49.19%, Train Discrimination: 0.647011399269104 Test Loss: 44.364707946777344, Test Acc: 63.74%,Test Discrimination: 0.5921800136566162
Epoch 3, Train Loss: 28.29759407043457, Train Acc: 64.01%, Train Discrimination: 0.3791804909706116 Test Loss: 17.233318328857422, Test Acc: 82.66%,Test Discrimination: 0.2709856629371643
Epoch 4, Train Loss: 10.756925582885742, Train Acc: 82.87%, Train Discrimination: 0.16784662008285522 Test Loss: 4.707304000854492, Test Acc: 82.66%,Test Discrimination: 0.0816660225391388
Epoch 5, Train Loss: 2.7405943870544434, Train Acc: 82.87%, Train Discrimination: 0.04410111904144287 Test Loss: 0.45184066891670227, Test Acc: 82.66%,Test Discrimination: 1.0698164487621398e-07
Epoch 6, Train Loss: 0.5224968791007996, Train Acc: 82.87%, Train Discrimination: 0.0016956658801063895 Test Loss: 3.002361297607422, Test Acc: 82.69%,Test Discrimination: 0.059652455151081085
Epoch 7, Train Loss: 3.2316396236419678, Train Acc: 82.98%, Train Discrimination: 0.06558888405561447 Test Loss: 2.223464012145996, Test Acc: 82.75%,Test Discrimination: 0.043063800781965256
Epoch 8, Train Loss: 2.472273588180542, Train Acc: 82.88%, Train Discrimination: 0.04965747520327568 Test Loss: 0.54583740234375, Test Acc: 82.66%,Test Discrimination: 0.0024062534794211388
Epoch 9, Train Loss: 0.6853107213973999, Train Acc: 82.87%, Train Discrimination: 0.005832504481077194 Test Loss: 0.65000319480896, Test Acc: 82.66%,Test Discrimination: 0.0030265941750258207
Epoch 10, Train Loss: 0.5100173354148865, Train Acc: 82.87%, Train Discrimination: 0.00042062171269208193 Test Loss: 1.0802360773086548, Test Acc: 82.66%,Test Discrimination: 0.009008234366774559
Epoch 11, Train Loss: 0.7536728382110596, Train Acc: 82.87%, Train Discrimination: 0.003542245365679264 Test Loss: 1.3089654445648193, Test Acc: 82.66%,Test Discrimination: 0.010761010460555553
Epoch 12, Train Loss: 0.9219539165496826, Train Acc: 82.87%, Train Discrimination: 0.00494568794965744 Test Loss: 1.3653030395507812, Test Acc: 82.66%,Test Discrimination: 0.010061012580990791
Epoch 13, Train Loss: 0.9892007112503052, Train Acc: 82.87%, Train Discrimination: 0.004862203262746334 Test Loss: 1.3297209739685059, Test Acc: 82.66%,Test Discrimination: 0.00851255003362894
Epoch 14, Train Loss: 0.996183454990387, Train Acc: 82.87%, Train Discrimination: 0.004184646997600794 Test Loss: 1.2540267705917358, Test Acc: 82.66%,Test Discrimination: 0.006840200629085302
Epoch 15, Train Loss: 0.9731283187866211, Train Acc: 82.87%, Train Discrimination: 0.003367658471688628 Test Loss: 1.1650631427764893, Test Acc: 82.66%,Test Discrimination: 0.005311102606356144
Epoch 16, Train Loss: 0.9368656277656555, Train Acc: 82.87%, Train Discrimination: 0.0025958640035241842 Test Loss: 1.0762407779693604, Test Acc: 82.66%,Test Discrimination: 0.0040077813901007175
Epoch 17, Train Loss: 0.8961784243583679, Train Acc: 82.87%, Train Discrimination: 0.0019332849187776446 Test Loss: 0.9932770133018494, Test Acc: 82.66%,Test Discrimination: 0.0029334572609514
Epoch 18, Train Loss: 0.8547336459159851, Train Acc: 82.87%, Train Discrimination: 0.001387669239193201 Test Loss: 0.9176132082939148, Test Acc: 82.66%,Test Discrimination: 0.002060561440885067
Epoch 19, Train Loss: 0.8138879537582397, Train Acc: 82.87%, Train Discrimination: 0.000947881315369159 Test Loss: 0.8489312529563904, Test Acc: 82.66%,Test Discrimination: 0.0013541262596845627
Epoch 20, Train Loss: 0.7740369439125061, Train Acc: 82.87%, Train Discrimination: 0.0005976518732495606 Test Loss: 0.7867572903633118, Test Acc: 82.66%,Test Discrimination: 0.0007869373657740653
Epoch 21, Train Loss: 0.7353299260139465, Train Acc: 82.87%, Train Discrimination: 0.0003240780206397176 Test Loss: 0.731259822845459, Test Acc: 82.66%,Test Discrimination: 0.0003488937800284475
Epoch 22, Train Loss: 0.6983680129051208, Train Acc: 82.87%, Train Discrimination: 0.00012397671525832266 Test Loss: 0.6848543882369995, Test Acc: 82.66%,Test Discrimination: 6.537682202178985e-05
Epoch 23, Train Loss: 0.6648064851760864, Train Acc: 82.87%, Train Discrimination: 1.1705466022249311e-05 Test Loss: 0.6532831788063049, Test Acc: 82.66%,Test Discrimination: 1.962212445505429e-05
Epoch 24, Train Loss: 0.6381219625473022, Train Acc: 82.87%, Train Discrimination: 3.1346920877695084e-05 Test Loss: 0.6468950510025024, Test Acc: 82.66%,Test Discrimination: 0.00038078753277659416
Epoch 25, Train Loss: 0.6240187883377075, Train Acc: 82.87%, Train Discrimination: 0.000266258924966678 Test Loss: 0.6790441870689392, Test Acc: 82.66%,Test Discrimination: 0.001387298689223826
Epoch 26, Train Loss: 0.6293268799781799, Train Acc: 82.87%, Train Discrimination: 0.00082178640877828 Test Loss: 0.7546381950378418, Test Acc: 82.66%,Test Discrimination: 0.003133774269372225
Epoch 27, Train Loss: 0.6560139060020447, Train Acc: 82.87%, Train Discrimination: 0.0017086869338527322 Test Loss: 0.845920205116272, Test Acc: 82.66%,Test Discrimination: 0.005078879185020924
Epoch 28, Train Loss: 0.689778208732605, Train Acc: 82.87%, Train Discrimination: 0.0026094240602105856 Test Loss: 0.8944037556648254, Test Acc: 82.66%,Test Discrimination: 0.006056603509932756
Epoch 29, Train Loss: 0.7020968794822693, Train Acc: 82.87%, Train Discrimination: 0.0029252872336655855 Test Loss: 0.8713082075119019, Test Acc: 82.66%,Test Discrimination: 0.00550770154222846
Epoch 30, Train Loss: 0.6811984181404114, Train Acc: 82.87%, Train Discrimination: 0.002431034343317151 Test Loss: 0.8101513385772705, Test Acc: 82.66%,Test Discrimination: 0.004138539079576731
Epoch 31, Train Loss: 0.6461146473884583, Train Acc: 82.87%, Train Discrimination: 0.001558899413794279 Test Loss: 0.7585409283638, Test Acc: 82.66%,Test Discrimination: 0.002934914082288742
Epoch 32, Train Loss: 0.6209595203399658, Train Acc: 82.87%, Train Discrimination: 0.000853676931001246 Test Loss: 0.7270353436470032, Test Acc: 82.66%,Test Discrimination: 0.00216152286157012
Epoch 33, Train Loss: 0.6089468002319336, Train Acc: 82.87%, Train Discrimination: 0.0004376417491585016 Test Loss: 0.7099412083625793, Test Acc: 82.66%,Test Discrimination: 0.0017253367695957422
Epoch 34, Train Loss: 0.6027864813804626, Train Acc: 82.87%, Train Discrimination: 0.00019552293815650046 Test Loss: 0.7048642635345459, Test Acc: 82.66%,Test Discrimination: 0.0016044412041082978
Epoch 35, Train Loss: 0.5975457429885864, Train Acc: 82.87%, Train Discrimination: 6.82862737448886e-05 Test Loss: 0.7134603261947632, Test Acc: 82.66%,Test Discrimination: 0.001854076748713851
Epoch 36, Train Loss: 0.5894202589988708, Train Acc: 82.87%, Train Discrimination: 1.5208886907203123e-05 Test Loss: 0.7500569820404053, Test Acc: 82.66%,Test Discrimination: 0.0027368590235710144
Epoch 37, Train Loss: 0.5763527154922485, Train Acc: 82.87%, Train Discrimination: 7.730682796136534e-07 Test Loss: 0.8537835478782654, Test Acc: 82.66%,Test Discrimination: 0.004946850705891848
Epoch 38, Train Loss: 0.5596354603767395, Train Acc: 82.86%, Train Discrimination: 4.496403249731884e-11 Test Loss: 1.12966787815094, Test Acc: 82.66%,Test Discrimination: 0.010359425097703934
Epoch 39, Train Loss: 0.5456379055976868, Train Acc: 82.87%, Train Discrimination: 2.0967720047337934e-05 Test Loss: 1.749912977218628, Test Acc: 81.90%,Test Discrimination: 0.0218657236546278
Epoch 40, Train Loss: 0.5595158338546753, Train Acc: 82.15%, Train Discrimination: 0.0003606612444855273 Test Loss: 1.7165710926055908, Test Acc: 81.21%,Test Discrimination: 0.021080931648612022
Epoch 41, Train Loss: 0.5487715005874634, Train Acc: 81.36%, Train Discrimination: 9.635953028919175e-05 Test Loss: 1.4026128053665161, Test Acc: 81.68%,Test Discrimination: 0.015294515527784824
Epoch 42, Train Loss: 0.548896849155426, Train Acc: 81.93%, Train Discrimination: 5.7177709095412865e-05 Test Loss: 1.2711776494979858, Test Acc: 82.00%,Test Discrimination: 0.012947052717208862
Epoch 43, Train Loss: 0.5564879775047302, Train Acc: 82.26%, Train Discrimination: 0.0002059600519714877 Test Loss: 1.3057690858840942, Test Acc: 82.03%,Test Discrimination: 0.013747233897447586
Epoch 44, Train Loss: 0.5470406413078308, Train Acc: 82.28%, Train Discrimination: 0.0001068676428985782 Test Loss: 1.4524816274642944, Test Acc: 81.96%,Test Discrimination: 0.016710413619875908
Epoch 45, Train Loss: 0.5353537201881409, Train Acc: 82.11%, Train Discrimination: 3.7581040146505984e-07 Test Loss: 1.567578673362732, Test Acc: 82.06%,Test Discrimination: 0.019168350845575333
Epoch 46, Train Loss: 0.535906195640564, Train Acc: 82.22%, Train Discrimination: 0.000129915468278341 Test Loss: 1.4203407764434814, Test Acc: 82.56%,Test Discrimination: 0.016681186854839325
Epoch 47, Train Loss: 0.531633198261261, Train Acc: 82.75%, Train Discrimination: 0.00015158375026658177 Test Loss: 1.1639182567596436, Test Acc: 82.69%,Test Discrimination: 0.011920791119337082
Epoch 48, Train Loss: 0.5247432589530945, Train Acc: 82.89%, Train Discrimination: 5.53380923520308e-05 Test Loss: 0.9793910980224609, Test Acc: 82.66%,Test Discrimination: 0.008385887369513512
Epoch 49, Train Loss: 0.5237360596656799, Train Acc: 82.87%, Train Discrimination: 1.1770685887313448e-05 Test Loss: 0.8819352388381958, Test Acc: 82.66%,Test Discrimination: 0.006505528464913368
Epoch 50, Train Loss: 0.5246250033378601, Train Acc: 82.87%, Train Discrimination: 2.904961547756102e-06 Test Loss: 0.8441849946975708, Test Acc: 82.66%,Test Discrimination: 0.005823005922138691
Epoch 51, Train Loss: 0.5237600207328796, Train Acc: 82.86%, Train Discrimination: 3.3806497867772123e-06 Test Loss: 0.848829984664917, Test Acc: 82.66%,Test Discrimination: 0.0060292137786746025
Epoch 52, Train Loss: 0.5203725695610046, Train Acc: 82.86%, Train Discrimination: 1.2031308870064095e-05 Test Loss: 0.8901640772819519, Test Acc: 82.66%,Test Discrimination: 0.007016628049314022
Epoch 53, Train Loss: 0.5157351493835449, Train Acc: 82.87%, Train Discrimination: 4.073301170137711e-05 Test Loss: 0.9627827405929565, Test Acc: 82.69%,Test Discrimination: 0.008661908097565174
Epoch 54, Train Loss: 0.5120087265968323, Train Acc: 82.88%, Train Discrimination: 0.00010103722161147743 Test Loss: 1.041698932647705, Test Acc: 82.69%,Test Discrimination: 0.010426296852529049
Epoch 55, Train Loss: 0.5097811818122864, Train Acc: 82.89%, Train Discrimination: 0.00016656598018016666 Test Loss: 1.0751997232437134, Test Acc: 82.69%,Test Discrimination: 0.011224757879972458
Epoch 56, Train Loss: 0.5061079263687134, Train Acc: 82.90%, Train Discrimination: 0.00015469640493392944 Test Loss: 1.0423238277435303, Test Acc: 82.69%,Test Discrimination: 0.010627569630742073
Epoch 57, Train Loss: 0.5005254745483398, Train Acc: 82.88%, Train Discrimination: 6.362125714076683e-05 Test Loss: 0.9878251552581787, Test Acc: 82.69%,Test Discrimination: 0.009574047289788723
Epoch 58, Train Loss: 0.49712368845939636, Train Acc: 82.89%, Train Discrimination: 4.359226750239031e-06 Test Loss: 0.9611192941665649, Test Acc: 82.66%,Test Discrimination: 0.009106006473302841
Epoch 59, Train Loss: 0.4957646429538727, Train Acc: 82.87%, Train Discrimination: 4.3250711314613e-06 Test Loss: 0.9859824180603027, Test Acc: 82.60%,Test Discrimination: 0.009730648249387741
Epoch 60, Train Loss: 0.492925763130188, Train Acc: 82.86%, Train Discrimination: 6.577372460014885e-06 Test Loss: 1.0675245523452759, Test Acc: 82.60%,Test Discrimination: 0.011555599980056286
Epoch 61, Train Loss: 0.48864853382110596, Train Acc: 82.83%, Train Discrimination: 1.2139453247073106e-06 Test Loss: 1.165371298789978, Test Acc: 82.66%,Test Discrimination: 0.01373961754143238
Epoch 62, Train Loss: 0.48672032356262207, Train Acc: 82.77%, Train Discrimination: 4.2107232729904354e-05 Test Loss: 1.1608171463012695, Test Acc: 82.69%,Test Discrimination: 0.013816718012094498
Epoch 63, Train Loss: 0.484036386013031, Train Acc: 82.79%, Train Discrimination: 5.554446033784188e-05 Test Loss: 1.0424977540969849, Test Acc: 82.66%,Test Discrimination: 0.011503375135362148
Epoch 64, Train Loss: 0.4791112542152405, Train Acc: 82.82%, Train Discrimination: 8.896889994502999e-06 Test Loss: 0.9329861998558044, Test Acc: 82.69%,Test Discrimination: 0.009333038702607155
Epoch 65, Train Loss: 0.4763670265674591, Train Acc: 82.87%, Train Discrimination: 9.120544177676493e-07 Test Loss: 0.882389485836029, Test Acc: 82.66%,Test Discrimination: 0.008404838852584362
Epoch 66, Train Loss: 0.4731968343257904, Train Acc: 82.90%, Train Discrimination: 1.1155464108014712e-06 Test Loss: 0.8806671500205994, Test Acc: 82.66%,Test Discrimination: 0.008543049916625023
Epoch 67, Train Loss: 0.46901771426200867, Train Acc: 82.90%, Train Discrimination: 5.517190402315464e-06 Test Loss: 0.8956125974655151, Test Acc: 82.66%,Test Discrimination: 0.009054689668118954
Epoch 68, Train Loss: 0.46617546677589417, Train Acc: 82.92%, Train Discrimination: 4.6246022975537926e-05 Test Loss: 0.8815856575965881, Test Acc: 82.69%,Test Discrimination: 0.008912213146686554
Epoch 69, Train Loss: 0.46375617384910583, Train Acc: 82.95%, Train Discrimination: 7.591389294248074e-05 Test Loss: 0.8261144161224365, Test Acc: 82.72%,Test Discrimination: 0.0078079914674162865
Epoch 70, Train Loss: 0.460090696811676, Train Acc: 82.95%, Train Discrimination: 4.5436892833095044e-05 Test Loss: 0.7665850520133972, Test Acc: 82.72%,Test Discrimination: 0.006579641718417406
Epoch 71, Train Loss: 0.4569721519947052, Train Acc: 82.95%, Train Discrimination: 1.1156236723763868e-05 Test Loss: 0.7343977093696594, Test Acc: 82.75%,Test Discrimination: 0.005967148579657078
Epoch 72, Train Loss: 0.454250305891037, Train Acc: 82.97%, Train Discrimination: 1.955104380613193e-06 Test Loss: 0.7368400692939758, Test Acc: 82.72%,Test Discrimination: 0.006162856239825487
Epoch 73, Train Loss: 0.45061275362968445, Train Acc: 83.00%, Train Discrimination: 4.663981599151157e-06 Test Loss: 0.7654218673706055, Test Acc: 82.85%,Test Discrimination: 0.006974315270781517
Epoch 74, Train Loss: 0.4469665288925171, Train Acc: 83.04%, Train Discrimination: 2.4378903617616743e-05 Test Loss: 0.7897643446922302, Test Acc: 82.85%,Test Discrimination: 0.007683797739446163
Epoch 75, Train Loss: 0.44408461451530457, Train Acc: 83.05%, Train Discrimination: 4.749056824948639e-05 Test Loss: 0.7744718790054321, Test Acc: 82.98%,Test Discrimination: 0.00745470030233264
Epoch 76, Train Loss: 0.44053032994270325, Train Acc: 83.07%, Train Discrimination: 2.612941898405552e-05 Test Loss: 0.7363756895065308, Test Acc: 82.94%,Test Discrimination: 0.00667999405413866
Epoch 77, Train Loss: 0.43742185831069946, Train Acc: 83.05%, Train Discrimination: 1.493756258241774e-06 Test Loss: 0.7186870574951172, Test Acc: 83.01%,Test Discrimination: 0.0063874199986457825
Epoch 78, Train Loss: 0.4347156584262848, Train Acc: 83.06%, Train Discrimination: 1.575827468514035e-07 Test Loss: 0.7341176271438599, Test Acc: 83.01%,Test Discrimination: 0.006889901123940945
Epoch 79, Train Loss: 0.43132415413856506, Train Acc: 83.08%, Train Discrimination: 3.7722081742685987e-06 Test Loss: 0.7505673170089722, Test Acc: 83.10%,Test Discrimination: 0.007423443254083395
Epoch 80, Train Loss: 0.4286763668060303, Train Acc: 83.08%, Train Discrimination: 2.468001184752211e-05 Test Loss: 0.7196168899536133, Test Acc: 83.13%,Test Discrimination: 0.006817142944782972
Epoch 81, Train Loss: 0.42548757791519165, Train Acc: 83.13%, Train Discrimination: 1.4702368389407638e-05 Test Loss: 0.6675127744674683, Test Acc: 83.13%,Test Discrimination: 0.005681995768100023
Epoch 82, Train Loss: 0.4226132333278656, Train Acc: 83.17%, Train Discrimination: 3.552713678800501e-07 Test Loss: 0.6420915722846985, Test Acc: 83.20%,Test Discrimination: 0.005184368696063757
Epoch 83, Train Loss: 0.4198686480522156, Train Acc: 83.24%, Train Discrimination: 1.331735006715462e-07 Test Loss: 0.642996609210968, Test Acc: 83.29%,Test Discrimination: 0.005331987049430609
Epoch 84, Train Loss: 0.41701215505599976, Train Acc: 83.29%, Train Discrimination: 1.1427627214288805e-05 Test Loss: 0.6356225609779358, Test Acc: 83.32%,Test Discrimination: 0.00526495510712266
Epoch 85, Train Loss: 0.4147324860095978, Train Acc: 83.35%, Train Discrimination: 2.590899202914443e-05 Test Loss: 0.6030058264732361, Test Acc: 83.39%,Test Discrimination: 0.004546421580016613
Epoch 86, Train Loss: 0.412163108587265, Train Acc: 83.37%, Train Discrimination: 8.555325621273369e-06 Test Loss: 0.5760533213615417, Test Acc: 83.35%,Test Discrimination: 0.003954052925109863
Epoch 87, Train Loss: 0.4101589024066925, Train Acc: 83.42%, Train Discrimination: 4.4904098217557475e-07 Test Loss: 0.5756973028182983, Test Acc: 83.64%,Test Discrimination: 0.004033115226775408
Epoch 88, Train Loss: 0.40790432691574097, Train Acc: 83.45%, Train Discrimination: 3.197442310920451e-06 Test Loss: 0.587220311164856, Test Acc: 83.77%,Test Discrimination: 0.004410644993185997
Epoch 89, Train Loss: 0.4060564935207367, Train Acc: 83.57%, Train Discrimination: 1.7858816136140376e-05 Test Loss: 0.5779192447662354, Test Acc: 83.86%,Test Discrimination: 0.0042433724738657475
Epoch 90, Train Loss: 0.40420669317245483, Train Acc: 83.69%, Train Discrimination: 9.999353096645791e-06 Test Loss: 0.5582870841026306, Test Acc: 83.86%,Test Discrimination: 0.003800278762355447
Epoch 91, Train Loss: 0.40277284383773804, Train Acc: 83.77%, Train Discrimination: 5.928493251872169e-08 Test Loss: 0.5611984729766846, Test Acc: 83.89%,Test Discrimination: 0.003930849488824606
Epoch 92, Train Loss: 0.40136706829071045, Train Acc: 83.92%, Train Discrimination: 7.658791219000705e-07 Test Loss: 0.5746796131134033, Test Acc: 83.92%,Test Discrimination: 0.004326359368860722
Epoch 93, Train Loss: 0.4003506302833557, Train Acc: 84.04%, Train Discrimination: 1.1148275007144548e-05 Test Loss: 0.5590684413909912, Test Acc: 84.02%,Test Discrimination: 0.003965436480939388
Epoch 94, Train Loss: 0.3992685079574585, Train Acc: 84.04%, Train Discrimination: 2.594246552689583e-06 Test Loss: 0.5405011177062988, Test Acc: 84.08%,Test Discrimination: 0.003521748585626483
Epoch 95, Train Loss: 0.3985716700553894, Train Acc: 84.03%, Train Discrimination: 1.156302573690482e-07 Test Loss: 0.5459758043289185, Test Acc: 84.11%,Test Discrimination: 0.003695681458339095
Epoch 96, Train Loss: 0.3977358639240265, Train Acc: 84.08%, Train Discrimination: 4.120234734728001e-06 Test Loss: 0.5403996109962463, Test Acc: 84.21%,Test Discrimination: 0.003579105716198683
Epoch 97, Train Loss: 0.3971681594848633, Train Acc: 84.14%, Train Discrimination: 7.300953257072251e-06 Test Loss: 0.5171228051185608, Test Acc: 84.24%,Test Discrimination: 0.0029970072209835052
Epoch 98, Train Loss: 0.3966599404811859, Train Acc: 84.11%, Train Discrimination: 1.4496492894977564e-08 Test Loss: 0.5155194401741028, Test Acc: 84.40%,Test Discrimination: 0.0029761926271021366
Epoch 99, Train Loss: 0.39611148834228516, Train Acc: 84.16%, Train Discrimination: 1.608049160495284e-06 Test Loss: 0.5186132192611694, Test Acc: 84.33%,Test Discrimination: 0.003076989436522126
Epoch 100, Train Loss: 0.3957403600215912, Train Acc: 84.19%, Train Discrimination: 9.171884812531061e-06 Test Loss: 0.5019014477729797, Test Acc: 84.37%,Test Discrimination: 0.0026586747262626886</code></pre>
</div>
</div>
</section>
<section id="model-accross-different-values-of-k" class="level4">
<h4 class="anchored" data-anchor-id="model-accross-different-values-of-k">MODEL ACCROSS DIFFERENT VALUES OF K</h4>
</section>
<section id="k2" class="level3">
<h3 class="anchored" data-anchor-id="k2">K=2</h3>
<div id="c4baae43" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>model7 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model7.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>train_losses7, train_accuracies7, train_discriminations7,train_fairness7 <span class="op">=</span> [], [], [],[]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>test_losses7, test_accuracies7, test_discriminations7,test_fairness7 <span class="op">=</span> [], [], [],[]</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>model7.train()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model7(features)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    model7.<span class="bu">eval</span>()</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model7(test_features)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    train_losses7.append(loss.item())</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    train_accuracies7.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    train_discriminations7.append(discrimination)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    train_fairness7.append(fairness)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    test_fairness7.append(fairness)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    test_losses7.append(test_loss.item())</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    test_accuracies7.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    test_discriminations7.append(test_discrimination)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    model7.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 4.483503341674805, Train Acc: 17.31%, Train Discrimination: 0.03460327163338661 Test Loss: 74.71599578857422, Test Acc: 57.80%,Test Discrimination: 0.8830800652503967
Epoch 2, Train Loss: 48.427730560302734, Train Acc: 58.08%, Train Discrimination: 0.5753588080406189 Test Loss: 55.81466293334961, Test Acc: 70.21%,Test Discrimination: 0.5916022062301636
Epoch 3, Train Loss: 33.52764129638672, Train Acc: 71.37%, Train Discrimination: 0.35704460740089417 Test Loss: 30.513710021972656, Test Acc: 82.66%,Test Discrimination: 0.33052772283554077
Epoch 4, Train Loss: 18.003868103027344, Train Acc: 82.87%, Train Discrimination: 0.1944621503353119 Test Loss: 15.30838394165039, Test Acc: 82.66%,Test Discrimination: 0.17420940101146698
Epoch 5, Train Loss: 8.995645523071289, Train Acc: 82.87%, Train Discrimination: 0.10034545511007309 Test Loss: 6.9843010902404785, Test Acc: 82.66%,Test Discrimination: 0.08201278746128082
Epoch 6, Train Loss: 4.201853275299072, Train Acc: 82.87%, Train Discrimination: 0.046515293419361115 Test Loss: 2.806886911392212, Test Acc: 82.66%,Test Discrimination: 0.030636807903647423
Epoch 7, Train Loss: 1.8390440940856934, Train Acc: 82.87%, Train Discrimination: 0.017215823754668236 Test Loss: 1.0414294004440308, Test Acc: 82.66%,Test Discrimination: 0.006536983884871006
Epoch 8, Train Loss: 0.8409305214881897, Train Acc: 82.87%, Train Discrimination: 0.0036644272040575743 Test Loss: 0.5802494883537292, Test Acc: 82.66%,Test Discrimination: 4.571756289806217e-05
Epoch 9, Train Loss: 0.5661476254463196, Train Acc: 82.87%, Train Discrimination: 3.598333933041431e-05 Test Loss: 1.0369493961334229, Test Acc: 82.66%,Test Discrimination: 0.009195481427013874
Epoch 10, Train Loss: 0.8271388411521912, Train Acc: 82.87%, Train Discrimination: 0.005683017894625664 Test Loss: 1.9045355319976807, Test Acc: 82.66%,Test Discrimination: 0.026158824563026428
Epoch 11, Train Loss: 1.3373488187789917, Train Acc: 82.87%, Train Discrimination: 0.016002263873815536 Test Loss: 1.93871009349823, Test Acc: 82.66%,Test Discrimination: 0.0258320365101099
Epoch 12, Train Loss: 1.3444193601608276, Train Acc: 82.87%, Train Discrimination: 0.015454724431037903 Test Loss: 1.3899850845336914, Test Acc: 82.66%,Test Discrimination: 0.013921191915869713
Epoch 13, Train Loss: 1.0093894004821777, Train Acc: 82.87%, Train Discrimination: 0.007804943714290857 Test Loss: 1.006516933441162, Test Acc: 82.66%,Test Discrimination: 0.005729077849537134
Epoch 14, Train Loss: 0.8050142526626587, Train Acc: 82.87%, Train Discrimination: 0.002898134058341384 Test Loss: 0.8656861782073975, Test Acc: 82.66%,Test Discrimination: 0.0022705404553562403
Epoch 15, Train Loss: 0.7576221823692322, Train Acc: 82.87%, Train Discrimination: 0.0010072605218738317 Test Loss: 0.8442887663841248, Test Acc: 82.66%,Test Discrimination: 0.0009812356438487768
Epoch 16, Train Loss: 0.7782607674598694, Train Acc: 82.87%, Train Discrimination: 0.000372178852558136 Test Loss: 0.8656256794929504, Test Acc: 82.66%,Test Discrimination: 0.0004896706668660045
Epoch 17, Train Loss: 0.8179818391799927, Train Acc: 82.87%, Train Discrimination: 0.00015440117567777634 Test Loss: 0.897687554359436, Test Acc: 82.66%,Test Discrimination: 0.0002883391862269491
Epoch 18, Train Loss: 0.8581895232200623, Train Acc: 82.87%, Train Discrimination: 7.372199615929276e-05 Test Loss: 0.9283936023712158, Test Acc: 82.66%,Test Discrimination: 0.00020051181490998715
Epoch 19, Train Loss: 0.8924155235290527, Train Acc: 82.87%, Train Discrimination: 4.0773797081783414e-05 Test Loss: 0.953396737575531, Test Acc: 82.66%,Test Discrimination: 0.00016286334721371531
Epoch 20, Train Loss: 0.9186142683029175, Train Acc: 82.87%, Train Discrimination: 2.6037903808173724e-05 Test Loss: 0.9713852405548096, Test Acc: 82.66%,Test Discrimination: 0.0001516860502306372
Epoch 21, Train Loss: 0.9364084601402283, Train Acc: 82.87%, Train Discrimination: 1.8953731341753155e-05 Test Loss: 0.9823570251464844, Test Acc: 82.66%,Test Discrimination: 0.00015858456026762724
Epoch 22, Train Loss: 0.9460846185684204, Train Acc: 82.87%, Train Discrimination: 1.5439518392668106e-05 Test Loss: 0.9869770407676697, Test Acc: 82.66%,Test Discrimination: 0.0001825550280045718
Epoch 23, Train Loss: 0.9481982588768005, Train Acc: 82.87%, Train Discrimination: 1.377472926833434e-05 Test Loss: 0.9863834977149963, Test Acc: 82.66%,Test Discrimination: 0.0002279401960549876
Epoch 24, Train Loss: 0.943425714969635, Train Acc: 82.87%, Train Discrimination: 1.3160861271899194e-05 Test Loss: 0.9821867942810059, Test Acc: 82.66%,Test Discrimination: 0.0003044571785721928
Epoch 25, Train Loss: 0.9324707984924316, Train Acc: 82.87%, Train Discrimination: 1.3143365322321188e-05 Test Loss: 0.9766786098480225, Test Acc: 82.66%,Test Discrimination: 0.0004305345646571368
Epoch 26, Train Loss: 0.9160504341125488, Train Acc: 82.87%, Train Discrimination: 1.3201030014897697e-05 Test Loss: 0.9737045168876648, Test Acc: 82.66%,Test Discrimination: 0.0006447871564887464
Epoch 27, Train Loss: 0.8946930170059204, Train Acc: 82.87%, Train Discrimination: 1.0548998943704646e-05 Test Loss: 0.9804942607879639, Test Acc: 82.66%,Test Discrimination: 0.0010292807128280401
Epoch 28, Train Loss: 0.8690773248672485, Train Acc: 82.87%, Train Discrimination: 4.287732281227363e-06 Test Loss: 1.0079940557479858, Test Acc: 82.66%,Test Discrimination: 0.0017228370998054743
Epoch 29, Train Loss: 0.8402819037437439, Train Acc: 82.87%, Train Discrimination: 7.844814575719283e-08 Test Loss: 1.0845390558242798, Test Acc: 82.66%,Test Discrimination: 0.0031053561251610518
Epoch 30, Train Loss: 0.8096317648887634, Train Acc: 82.87%, Train Discrimination: 1.2584365322254598e-05 Test Loss: 1.237630009651184, Test Acc: 82.66%,Test Discrimination: 0.005602492485195398
Epoch 31, Train Loss: 0.7769319415092468, Train Acc: 82.86%, Train Discrimination: 2.2048816390451975e-05 Test Loss: 1.515739917755127, Test Acc: 82.66%,Test Discrimination: 0.009918371215462685
Epoch 32, Train Loss: 0.7446199655532837, Train Acc: 82.86%, Train Discrimination: 9.512091310170945e-06 Test Loss: 2.0199191570281982, Test Acc: 82.31%,Test Discrimination: 0.017498783767223358
Epoch 33, Train Loss: 0.7187642455101013, Train Acc: 82.60%, Train Discrimination: 5.526294444280211e-06 Test Loss: 2.9799435138702393, Test Acc: 79.91%,Test Discrimination: 0.03148967772722244
Epoch 34, Train Loss: 0.7187192440032959, Train Acc: 80.01%, Train Discrimination: 0.00022425722272600979 Test Loss: 3.8587536811828613, Test Acc: 77.57%,Test Discrimination: 0.044068578630685806
Epoch 35, Train Loss: 0.7554222941398621, Train Acc: 77.31%, Train Discrimination: 0.0008070918847806752 Test Loss: 3.8692870140075684, Test Acc: 77.35%,Test Discrimination: 0.04433117061853409
Epoch 36, Train Loss: 0.749098539352417, Train Acc: 76.96%, Train Discrimination: 0.0007358400616794825 Test Loss: 3.2605228424072266, Test Acc: 78.90%,Test Discrimination: 0.035844262689352036
Epoch 37, Train Loss: 0.7132815718650818, Train Acc: 78.68%, Train Discrimination: 0.00023609802883584052 Test Loss: 2.6782054901123047, Test Acc: 80.13%,Test Discrimination: 0.027583854272961617
Epoch 38, Train Loss: 0.6995784044265747, Train Acc: 80.24%, Train Discrimination: 2.0668569050030783e-05 Test Loss: 2.2937064170837402, Test Acc: 81.05%,Test Discrimination: 0.022086866199970245
Epoch 39, Train Loss: 0.7005956768989563, Train Acc: 81.37%, Train Discrimination: 3.818947789113736e-06 Test Loss: 2.0655272006988525, Test Acc: 81.87%,Test Discrimination: 0.018845921382308006
Epoch 40, Train Loss: 0.7030771970748901, Train Acc: 82.08%, Train Discrimination: 2.248653254355304e-05 Test Loss: 1.9425050020217896, Test Acc: 82.12%,Test Discrimination: 0.017160097137093544
Epoch 41, Train Loss: 0.702216625213623, Train Acc: 82.43%, Train Discrimination: 2.259548091387842e-05 Test Loss: 1.895572543144226, Test Acc: 82.34%,Test Discrimination: 0.016624996438622475
Epoch 42, Train Loss: 0.697911262512207, Train Acc: 82.60%, Train Discrimination: 7.083563104970381e-06 Test Loss: 1.9109855890274048, Test Acc: 82.38%,Test Discrimination: 0.01704932563006878
Epoch 43, Train Loss: 0.692101240158081, Train Acc: 82.66%, Train Discrimination: 9.925970516633242e-07 Test Loss: 1.9802461862564087, Test Acc: 82.38%,Test Discrimination: 0.018308350816369057
Epoch 44, Train Loss: 0.6873932480812073, Train Acc: 82.65%, Train Discrimination: 3.2025283871917054e-05 Test Loss: 2.086864471435547, Test Acc: 82.34%,Test Discrimination: 0.020142365247011185
Epoch 45, Train Loss: 0.6854624152183533, Train Acc: 82.60%, Train Discrimination: 0.00010718829435063526 Test Loss: 2.202829122543335, Test Acc: 82.15%,Test Discrimination: 0.022095991298556328
Epoch 46, Train Loss: 0.6848041415214539, Train Acc: 82.45%, Train Discrimination: 0.00018560884927865118 Test Loss: 2.287667989730835, Test Acc: 81.96%,Test Discrimination: 0.023524954915046692
Epoch 47, Train Loss: 0.6816402673721313, Train Acc: 82.26%, Train Discrimination: 0.00019635311036836356 Test Loss: 2.3183207511901855, Test Acc: 81.78%,Test Discrimination: 0.024064771831035614
Epoch 48, Train Loss: 0.674821138381958, Train Acc: 81.87%, Train Discrimination: 0.00011941119737457484 Test Loss: 2.293004035949707, Test Acc: 81.30%,Test Discrimination: 0.023715976625680923
Epoch 49, Train Loss: 0.668313205242157, Train Acc: 81.48%, Train Discrimination: 2.6974783395417035e-05 Test Loss: 2.2616302967071533, Test Acc: 80.80%,Test Discrimination: 0.023289408534765244
Epoch 50, Train Loss: 0.6660670638084412, Train Acc: 81.08%, Train Discrimination: 9.606428648112342e-07 Test Loss: 2.268005609512329, Test Acc: 80.61%,Test Discrimination: 0.023501696065068245
Epoch 51, Train Loss: 0.6650574207305908, Train Acc: 80.74%, Train Discrimination: 1.8810067558661103e-05 Test Loss: 2.3300399780273438, Test Acc: 80.35%,Test Discrimination: 0.024650676175951958
Epoch 52, Train Loss: 0.6602731943130493, Train Acc: 80.48%, Train Discrimination: 1.2015291758871172e-05 Test Loss: 2.4403512477874756, Test Acc: 80.26%,Test Discrimination: 0.026626646518707275
Epoch 53, Train Loss: 0.6536102294921875, Train Acc: 80.21%, Train Discrimination: 7.927130809548544e-07 Test Loss: 2.562969446182251, Test Acc: 80.10%,Test Discrimination: 0.028858385980129242
Epoch 54, Train Loss: 0.6498368382453918, Train Acc: 80.07%, Train Discrimination: 4.196810914436355e-05 Test Loss: 2.6267974376678467, Test Acc: 80.07%,Test Discrimination: 0.03019084967672825
Epoch 55, Train Loss: 0.6479713916778564, Train Acc: 80.03%, Train Discrimination: 0.00010245611338177696 Test Loss: 2.5756990909576416, Test Acc: 80.29%,Test Discrimination: 0.029682325199246407
Epoch 56, Train Loss: 0.6430484652519226, Train Acc: 80.20%, Train Discrimination: 9.326907456852496e-05 Test Loss: 2.434448003768921, Test Acc: 80.42%,Test Discrimination: 0.027693571522831917
Epoch 57, Train Loss: 0.6362547278404236, Train Acc: 80.43%, Train Discrimination: 3.505972563289106e-05 Test Loss: 2.275125026702881, Test Acc: 80.57%,Test Discrimination: 0.02538483589887619
Epoch 58, Train Loss: 0.6314417123794556, Train Acc: 80.71%, Train Discrimination: 2.613001015561167e-06 Test Loss: 2.1494970321655273, Test Acc: 80.99%,Test Discrimination: 0.023612091317772865
Epoch 59, Train Loss: 0.6280112862586975, Train Acc: 80.99%, Train Discrimination: 1.2286012633921928e-06 Test Loss: 2.074951171875, Test Acc: 81.17%,Test Discrimination: 0.022681573405861855
Epoch 60, Train Loss: 0.6235761046409607, Train Acc: 81.33%, Train Discrimination: 1.7548650248500053e-06 Test Loss: 2.0460093021392822, Test Acc: 81.49%,Test Discrimination: 0.022518357262015343
Epoch 61, Train Loss: 0.6179316639900208, Train Acc: 81.54%, Train Discrimination: 4.802191710950865e-07 Test Loss: 2.0452194213867188, Test Acc: 81.62%,Test Discrimination: 0.02283663861453533
Epoch 62, Train Loss: 0.6126717925071716, Train Acc: 81.70%, Train Discrimination: 1.5335253920056857e-05 Test Loss: 2.0479767322540283, Test Acc: 81.68%,Test Discrimination: 0.023213131353259087
Epoch 63, Train Loss: 0.608626127243042, Train Acc: 81.85%, Train Discrimination: 4.8543766752118245e-05 Test Loss: 2.0303025245666504, Test Acc: 81.78%,Test Discrimination: 0.02322200872004032
Epoch 64, Train Loss: 0.604875385761261, Train Acc: 81.95%, Train Discrimination: 7.490716234315187e-05 Test Loss: 1.9811755418777466, Test Acc: 81.81%,Test Discrimination: 0.02265583910048008
Epoch 65, Train Loss: 0.6003043055534363, Train Acc: 82.05%, Train Discrimination: 7.049005944281816e-05 Test Loss: 1.9088692665100098, Test Acc: 81.87%,Test Discrimination: 0.021650604903697968
Epoch 66, Train Loss: 0.5952484011650085, Train Acc: 82.20%, Train Discrimination: 4.315995829529129e-05 Test Loss: 1.8357270956039429, Test Acc: 81.90%,Test Discrimination: 0.020601244643330574
Epoch 67, Train Loss: 0.59064781665802, Train Acc: 82.25%, Train Discrimination: 1.713519486656878e-05 Test Loss: 1.7824605703353882, Test Acc: 81.90%,Test Discrimination: 0.019886506721377373
Epoch 68, Train Loss: 0.586609423160553, Train Acc: 82.26%, Train Discrimination: 4.632497621059883e-06 Test Loss: 1.7597360610961914, Test Acc: 81.90%,Test Discrimination: 0.01971559412777424
Epoch 69, Train Loss: 0.5824413895606995, Train Acc: 82.23%, Train Discrimination: 1.808967908800696e-06 Test Loss: 1.767928123474121, Test Acc: 81.87%,Test Discrimination: 0.02011249214410782
Epoch 70, Train Loss: 0.5777333974838257, Train Acc: 82.20%, Train Discrimination: 4.0135578274203e-06 Test Loss: 1.800114631652832, Test Acc: 81.90%,Test Discrimination: 0.020960012450814247
Epoch 71, Train Loss: 0.5729309320449829, Train Acc: 82.10%, Train Discrimination: 1.4565560377377551e-05 Test Loss: 1.8395676612854004, Test Acc: 81.81%,Test Discrimination: 0.021952712908387184
Epoch 72, Train Loss: 0.5686407685279846, Train Acc: 82.03%, Train Discrimination: 3.490989547572099e-05 Test Loss: 1.8629604578018188, Test Acc: 81.71%,Test Discrimination: 0.022652631625533104
Epoch 73, Train Loss: 0.5646116733551025, Train Acc: 81.94%, Train Discrimination: 5.087702447781339e-05 Test Loss: 1.8529576063156128, Test Acc: 81.68%,Test Discrimination: 0.022726882249116898
Epoch 74, Train Loss: 0.5601463317871094, Train Acc: 81.87%, Train Discrimination: 4.486816033022478e-05 Test Loss: 1.8121869564056396, Test Acc: 81.68%,Test Discrimination: 0.022214630618691444
Epoch 75, Train Loss: 0.5554467439651489, Train Acc: 81.85%, Train Discrimination: 2.2401818569051102e-05 Test Loss: 1.762442708015442, Test Acc: 81.68%,Test Discrimination: 0.0215261559933424
Epoch 76, Train Loss: 0.5512149333953857, Train Acc: 81.85%, Train Discrimination: 5.672269253409468e-06 Test Loss: 1.728750467300415, Test Acc: 81.65%,Test Discrimination: 0.021148554980754852
Epoch 77, Train Loss: 0.5471388101577759, Train Acc: 81.81%, Train Discrimination: 1.3735284483118448e-06 Test Loss: 1.7215708494186401, Test Acc: 81.71%,Test Discrimination: 0.02129710279405117
Epoch 78, Train Loss: 0.5425512194633484, Train Acc: 81.81%, Train Discrimination: 3.727195007741102e-06 Test Loss: 1.7307288646697998, Test Acc: 81.71%,Test Discrimination: 0.02178090624511242
Epoch 79, Train Loss: 0.5379489660263062, Train Acc: 81.80%, Train Discrimination: 1.6346915799658746e-05 Test Loss: 1.7293145656585693, Test Acc: 81.81%,Test Discrimination: 0.022068334743380547
Epoch 80, Train Loss: 0.5337448120117188, Train Acc: 81.85%, Train Discrimination: 3.376259337528609e-05 Test Loss: 1.6941523551940918, Test Acc: 81.84%,Test Discrimination: 0.021686993539333344
Epoch 81, Train Loss: 0.52928227186203, Train Acc: 81.91%, Train Discrimination: 3.350161568960175e-05 Test Loss: 1.6282286643981934, Test Acc: 81.93%,Test Discrimination: 0.02067955583333969
Epoch 82, Train Loss: 0.5245749354362488, Train Acc: 82.02%, Train Discrimination: 1.619544673303608e-05 Test Loss: 1.5591351985931396, Test Acc: 82.03%,Test Discrimination: 0.019593549892306328
Epoch 83, Train Loss: 0.5202115178108215, Train Acc: 82.10%, Train Discrimination: 4.301860371924704e-06 Test Loss: 1.511826992034912, Test Acc: 82.15%,Test Discrimination: 0.018942764028906822
Epoch 84, Train Loss: 0.515771746635437, Train Acc: 82.19%, Train Discrimination: 2.62167964137916e-06 Test Loss: 1.4891586303710938, Test Acc: 82.15%,Test Discrimination: 0.018797757104039192
Epoch 85, Train Loss: 0.5110124945640564, Train Acc: 82.27%, Train Discrimination: 8.750819688430056e-06 Test Loss: 1.4720326662063599, Test Acc: 82.22%,Test Discrimination: 0.018768634647130966
Epoch 86, Train Loss: 0.5064511299133301, Train Acc: 82.31%, Train Discrimination: 2.274873804708477e-05 Test Loss: 1.4380427598953247, Test Acc: 82.28%,Test Discrimination: 0.018379714339971542
Epoch 87, Train Loss: 0.5019184947013855, Train Acc: 82.35%, Train Discrimination: 2.8566335458890535e-05 Test Loss: 1.3805245161056519, Test Acc: 82.34%,Test Discrimination: 0.01747860573232174
Epoch 88, Train Loss: 0.4971393048763275, Train Acc: 82.41%, Train Discrimination: 1.685201823420357e-05 Test Loss: 1.3178081512451172, Test Acc: 82.38%,Test Discrimination: 0.016448551788926125
Epoch 89, Train Loss: 0.49253949522972107, Train Acc: 82.50%, Train Discrimination: 5.3112435125513e-06 Test Loss: 1.2715510129928589, Test Acc: 82.47%,Test Discrimination: 0.015758657827973366
Epoch 90, Train Loss: 0.48793238401412964, Train Acc: 82.53%, Train Discrimination: 2.837295141944196e-06 Test Loss: 1.2463293075561523, Test Acc: 82.47%,Test Discrimination: 0.015516826882958412
Epoch 91, Train Loss: 0.4830876588821411, Train Acc: 82.64%, Train Discrimination: 7.4060130828002e-06 Test Loss: 1.2260783910751343, Test Acc: 82.53%,Test Discrimination: 0.015378368087112904
Epoch 92, Train Loss: 0.4783867597579956, Train Acc: 82.66%, Train Discrimination: 1.695371429377701e-05 Test Loss: 1.188044786453247, Test Acc: 82.56%,Test Discrimination: 0.01484032440930605
Epoch 93, Train Loss: 0.4736754596233368, Train Acc: 82.72%, Train Discrimination: 1.6853240595082752e-05 Test Loss: 1.12920343875885, Test Acc: 82.60%,Test Discrimination: 0.013827823102474213
Epoch 94, Train Loss: 0.4689403176307678, Train Acc: 82.79%, Train Discrimination: 6.607980594708351e-06 Test Loss: 1.0719878673553467, Test Acc: 82.69%,Test Discrimination: 0.012832283042371273
Epoch 95, Train Loss: 0.4644238352775574, Train Acc: 82.79%, Train Discrimination: 1.460433963984542e-06 Test Loss: 1.0327482223510742, Test Acc: 82.85%,Test Discrimination: 0.012222768738865852
Epoch 96, Train Loss: 0.4598137438297272, Train Acc: 82.88%, Train Discrimination: 2.2293368147074943e-06 Test Loss: 1.0006343126296997, Test Acc: 82.98%,Test Discrimination: 0.011761479079723358
Epoch 97, Train Loss: 0.4553009867668152, Train Acc: 82.89%, Train Discrimination: 7.717414518992882e-06 Test Loss: 0.9524244666099548, Test Acc: 83.04%,Test Discrimination: 0.010917303152382374
Epoch 98, Train Loss: 0.4509793221950531, Train Acc: 82.90%, Train Discrimination: 9.010142093757167e-06 Test Loss: 0.8839199542999268, Test Acc: 83.16%,Test Discrimination: 0.009583394974470139
Epoch 99, Train Loss: 0.446785569190979, Train Acc: 82.98%, Train Discrimination: 2.252535523439292e-06 Test Loss: 0.8201422691345215, Test Acc: 83.20%,Test Discrimination: 0.008328210562467575
Epoch 100, Train Loss: 0.4428839087486267, Train Acc: 83.04%, Train Discrimination: 1.78714131493507e-07 Test Loss: 0.7755098938941956, Test Acc: 83.23%,Test Discrimination: 0.007489658892154694</code></pre>
</div>
</div>
</section>
<section id="k3" class="level3">
<h3 class="anchored" data-anchor-id="k3">K=3</h3>
<div id="70217552" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model8 <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model8.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>train_losses8, train_accuracies8, train_discriminations8,train_fairness8 <span class="op">=</span> [], [], [],[]</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>test_losses8, test_accuracies8, test_discriminations8,test_fairness8 <span class="op">=</span> [], [], [],[]</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>model8.train()</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model8(features)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    model8.<span class="bu">eval</span>()</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model8(test_features)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    train_losses8.append(loss.item())</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    train_accuracies8.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    train_discriminations8.append(discrimination)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    train_fairness8.append(fairness)</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    test_fairness8.append(fairness)</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    test_losses8.append(test_loss.item())</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    test_accuracies8.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>    test_discriminations8.append(test_discrimination)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>    model8.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 1.4284580945968628, Train Acc: 50.78%, Train Discrimination: 0.004369202069938183 Test Loss: -2.619622230529785, Test Acc: 70.81%,Test Discrimination: -0.04505476728081703
Epoch 2, Train Loss: -1.34059739112854, Train Acc: 71.97%, Train Discrimination: -0.028154456987977028 Test Loss: -12.412161827087402, Test Acc: 57.83%,Test Discrimination: -0.12317916005849838
Epoch 3, Train Loss: -6.054940223693848, Train Acc: 58.13%, Train Discrimination: -0.06589246541261673 Test Loss: -32.67656707763672, Test Acc: 57.01%,Test Discrimination: -0.25074848532676697
Epoch 4, Train Loss: -16.936691284179688, Train Acc: 56.90%, Train Discrimination: -0.1358727067708969 Test Loss: -65.37784576416016, Test Acc: 53.19%,Test Discrimination: -0.42094025015830994
Epoch 5, Train Loss: -41.403202056884766, Train Acc: 52.88%, Train Discrimination: -0.27063336968421936 Test Loss: -117.1894760131836, Test Acc: 35.22%,Test Discrimination: -0.5952214002609253
Epoch 6, Train Loss: -94.19654083251953, Train Acc: 34.89%, Train Discrimination: -0.4772120714187622 Test Loss: -122.05814361572266, Test Acc: 21.16%,Test Discrimination: -0.44827666878700256
Epoch 7, Train Loss: -114.27688598632812, Train Acc: 21.39%, Train Discrimination: -0.4170719385147095 Test Loss: -216.1560516357422, Test Acc: 32.50%,Test Discrimination: -0.6791709065437317
Epoch 8, Train Loss: -187.57333374023438, Train Acc: 31.83%, Train Discrimination: -0.5844790935516357 Test Loss: -271.5600891113281, Test Acc: 36.73%,Test Discrimination: -0.699457049369812
Epoch 9, Train Loss: -234.8260498046875, Train Acc: 35.79%, Train Discrimination: -0.5996139049530029 Test Loss: -370.4511413574219, Test Acc: 30.57%,Test Discrimination: -0.7740321755409241
Epoch 10, Train Loss: -334.2851867675781, Train Acc: 30.35%, Train Discrimination: -0.6912295818328857 Test Loss: -424.94451904296875, Test Acc: 24.07%,Test Discrimination: -0.7100571393966675
Epoch 11, Train Loss: -409.30377197265625, Train Acc: 23.99%, Train Discrimination: -0.6765351295471191 Test Loss: -1134.5303955078125, Test Acc: 29.00%,Test Discrimination: -0.8604809641838074
Epoch 12, Train Loss: -1089.350341796875, Train Acc: 28.86%, Train Discrimination: -0.7861008048057556 Test Loss: -2941.006591796875, Test Acc: 32.69%,Test Discrimination: -0.9186602830886841
Epoch 13, Train Loss: -2638.24853515625, Train Acc: 31.81%, Train Discrimination: -0.8207019567489624 Test Loss: -3031.30859375, Test Acc: 22.43%,Test Discrimination: -0.8175597786903381
Epoch 14, Train Loss: -2946.292724609375, Train Acc: 22.60%, Train Discrimination: -0.7919467687606812 Test Loss: -3979.88818359375, Test Acc: 32.15%,Test Discrimination: -1.0650132894515991
Epoch 15, Train Loss: -3527.213623046875, Train Acc: 31.35%, Train Discrimination: -0.9394082427024841 Test Loss: -4105.80322265625, Test Acc: 34.33%,Test Discrimination: -1.0965256690979004
Epoch 16, Train Loss: -3662.0302734375, Train Acc: 33.33%, Train Discrimination: -0.9686622619628906 Test Loss: -4701.6630859375, Test Acc: 26.88%,Test Discrimination: -1.2437340021133423
Epoch 17, Train Loss: -4223.51220703125, Train Acc: 26.67%, Train Discrimination: -1.1112674474716187 Test Loss: -4684.26904296875, Test Acc: 24.54%,Test Discrimination: -1.236858606338501
Epoch 18, Train Loss: -4365.5224609375, Train Acc: 23.70%, Train Discrimination: -1.1458847522735596 Test Loss: -5305.041015625, Test Acc: 33.35%,Test Discrimination: -1.4092029333114624
Epoch 19, Train Loss: -4766.734375, Train Acc: 32.47%, Train Discrimination: -1.2587165832519531 Test Loss: -5978.521484375, Test Acc: 31.49%,Test Discrimination: -1.58829927444458
Epoch 20, Train Loss: -5397.8388671875, Train Acc: 30.72%, Train Discrimination: -1.4252349138259888 Test Loss: -5359.619140625, Test Acc: 24.48%,Test Discrimination: -1.4179649353027344
Epoch 21, Train Loss: -5249.6376953125, Train Acc: 22.65%, Train Discrimination: -1.3792375326156616 Test Loss: -6704.6044921875, Test Acc: 32.34%,Test Discrimination: -1.785778284072876
Epoch 22, Train Loss: -6303.14208984375, Train Acc: 31.33%, Train Discrimination: -1.6676673889160156 Test Loss: -6968.34375, Test Acc: 32.94%,Test Discrimination: -1.858781099319458
Epoch 23, Train Loss: -6689.9228515625, Train Acc: 32.19%, Train Discrimination: -1.7724205255508423 Test Loss: -6439.53515625, Test Acc: 26.37%,Test Discrimination: -1.709326148033142
Epoch 24, Train Loss: -6630.609375, Train Acc: 24.16%, Train Discrimination: -1.747249960899353 Test Loss: -7714.24853515625, Test Acc: 33.04%,Test Discrimination: -2.059643268585205
Epoch 25, Train Loss: -7829.86865234375, Train Acc: 31.69%, Train Discrimination: -2.075723886489868 Test Loss: -8001.4169921875, Test Acc: 33.64%,Test Discrimination: -2.136965751647949
Epoch 26, Train Loss: -8251.001953125, Train Acc: 31.78%, Train Discrimination: -2.1879923343658447 Test Loss: -7145.80517578125, Test Acc: 29.00%,Test Discrimination: -1.8984341621398926
Epoch 27, Train Loss: -7540.8955078125, Train Acc: 26.30%, Train Discrimination: -1.9886080026626587 Test Loss: -8348.55078125, Test Acc: 35.00%,Test Discrimination: -2.2295894622802734
Epoch 28, Train Loss: -8259.0888671875, Train Acc: 33.81%, Train Discrimination: -2.190718173980713 Test Loss: -8861.2333984375, Test Acc: 33.39%,Test Discrimination: -2.3607683181762695
Epoch 29, Train Loss: -8971.36328125, Train Acc: 32.34%, Train Discrimination: -2.373415231704712 Test Loss: -8001.77099609375, Test Acc: 28.68%,Test Discrimination: -2.115750551223755
Epoch 30, Train Loss: -8636.607421875, Train Acc: 26.19%, Train Discrimination: -2.2663609981536865 Test Loss: -9333.9091796875, Test Acc: 31.36%,Test Discrimination: -2.472599506378174
Epoch 31, Train Loss: -9815.439453125, Train Acc: 29.49%, Train Discrimination: -2.5810670852661133 Test Loss: -9560.38671875, Test Acc: 33.80%,Test Discrimination: -2.5347015857696533
Epoch 32, Train Loss: -9370.6474609375, Train Acc: 32.58%, Train Discrimination: -2.4670233726501465 Test Loss: -9809.625, Test Acc: 30.99%,Test Discrimination: -2.589076042175293
Epoch 33, Train Loss: -10240.2158203125, Train Acc: 29.07%, Train Discrimination: -2.682647228240967 Test Loss: -9158.3525390625, Test Acc: 28.21%,Test Discrimination: -2.4054009914398193
Epoch 34, Train Loss: -9861.4755859375, Train Acc: 26.06%, Train Discrimination: -2.570128917694092 Test Loss: -10327.974609375, Test Acc: 30.95%,Test Discrimination: -2.7201669216156006
Epoch 35, Train Loss: -10702.9775390625, Train Acc: 29.45%, Train Discrimination: -2.7978780269622803 Test Loss: -10716.3486328125, Test Acc: 33.10%,Test Discrimination: -2.826763868331909
Epoch 36, Train Loss: -10482.421875, Train Acc: 31.78%, Train Discrimination: -2.7452287673950195 Test Loss: -10720.2099609375, Test Acc: 31.33%,Test Discrimination: -2.818876028060913
Epoch 37, Train Loss: -11197.4736328125, Train Acc: 29.36%, Train Discrimination: -2.922145366668701 Test Loss: -10243.08984375, Test Acc: 29.69%,Test Discrimination: -2.6852872371673584
Epoch 38, Train Loss: -11024.65234375, Train Acc: 27.42%, Train Discrimination: -2.8675594329833984 Test Loss: -11337.8974609375, Test Acc: 32.12%,Test Discrimination: -2.9800829887390137
Epoch 39, Train Loss: -11576.4111328125, Train Acc: 30.35%, Train Discrimination: -3.019883632659912 Test Loss: -11596.0439453125, Test Acc: 32.72%,Test Discrimination: -3.047032356262207
Epoch 40, Train Loss: -11596.845703125, Train Acc: 31.08%, Train Discrimination: -3.0244719982147217 Test Loss: -11014.337890625, Test Acc: 30.16%,Test Discrimination: -2.8812761306762695
Epoch 41, Train Loss: -11838.8203125, Train Acc: 28.21%, Train Discrimination: -3.072261333465576 Test Loss: -11302.287109375, Test Acc: 30.54%,Test Discrimination: -2.955030918121338
Epoch 42, Train Loss: -12089.62109375, Train Acc: 28.53%, Train Discrimination: -3.1357150077819824 Test Loss: -12034.1396484375, Test Acc: 32.63%,Test Discrimination: -3.152071952819824
Epoch 43, Train Loss: -12082.22265625, Train Acc: 30.80%, Train Discrimination: -3.140645742416382 Test Loss: -11878.1708984375, Test Acc: 31.36%,Test Discrimination: -3.102752447128296
Epoch 44, Train Loss: -12490.9462890625, Train Acc: 29.51%, Train Discrimination: -3.237121105194092 Test Loss: -11379.8310546875, Test Acc: 29.91%,Test Discrimination: -2.962574005126953
Epoch 45, Train Loss: -12413.8896484375, Train Acc: 27.77%, Train Discrimination: -3.205350399017334 Test Loss: -12253.5537109375, Test Acc: 32.00%,Test Discrimination: -3.1956419944763184
Epoch 46, Train Loss: -12758.5654296875, Train Acc: 29.92%, Train Discrimination: -3.3013291358947754 Test Loss: -12464.5966796875, Test Acc: 32.15%,Test Discrimination: -3.248563051223755
Epoch 47, Train Loss: -12827.203125, Train Acc: 30.21%, Train Discrimination: -3.3171474933624268 Test Loss: -11932.4052734375, Test Acc: 30.29%,Test Discrimination: -3.0956897735595703
Epoch 48, Train Loss: -12944.169921875, Train Acc: 28.25%, Train Discrimination: -3.333051919937134 Test Loss: -12289.8056640625, Test Acc: 31.17%,Test Discrimination: -3.1877658367156982
Epoch 49, Train Loss: -13165.6982421875, Train Acc: 28.96%, Train Discrimination: -3.389772891998291 Test Loss: -12786.806640625, Test Acc: 32.12%,Test Discrimination: -3.3190224170684814
Epoch 50, Train Loss: -13164.6181640625, Train Acc: 30.17%, Train Discrimination: -3.39286208152771 Test Loss: -12491.1376953125, Test Acc: 31.24%,Test Discrimination: -3.2329001426696777
Epoch 51, Train Loss: -13418.908203125, Train Acc: 29.05%, Train Discrimination: -3.4473087787628174 Test Loss: -12260.076171875, Test Acc: 30.26%,Test Discrimination: -3.165844440460205
Epoch 52, Train Loss: -13460.265625, Train Acc: 28.34%, Train Discrimination: -3.448258399963379 Test Loss: -12844.7744140625, Test Acc: 31.93%,Test Discrimination: -3.319964647293091
Epoch 53, Train Loss: -13610.3798828125, Train Acc: 29.66%, Train Discrimination: -3.491506814956665 Test Loss: -12763.185546875, Test Acc: 31.74%,Test Discrimination: -3.2935197353363037
Epoch 54, Train Loss: -13766.1357421875, Train Acc: 29.32%, Train Discrimination: -3.5246479511260986 Test Loss: -12339.4775390625, Test Acc: 30.23%,Test Discrimination: -3.1728789806365967
Epoch 55, Train Loss: -13802.7568359375, Train Acc: 28.25%, Train Discrimination: -3.5209555625915527 Test Loss: -12776.4453125, Test Acc: 31.65%,Test Discrimination: -3.288273572921753
Epoch 56, Train Loss: -13999.0302734375, Train Acc: 29.39%, Train Discrimination: -3.5749905109405518 Test Loss: -12887.2734375, Test Acc: 31.93%,Test Discrimination: -3.3156542778015137
Epoch 57, Train Loss: -14075.9072265625, Train Acc: 29.75%, Train Discrimination: -3.5922303199768066 Test Loss: -12527.205078125, Test Acc: 30.54%,Test Discrimination: -3.2049922943115234
Epoch 58, Train Loss: -14172.0439453125, Train Acc: 28.58%, Train Discrimination: -3.6002755165100098 Test Loss: -12846.4619140625, Test Acc: 31.55%,Test Discrimination: -3.2872681617736816
Epoch 59, Train Loss: -14346.88671875, Train Acc: 29.33%, Train Discrimination: -3.647033452987671 Test Loss: -12960.4140625, Test Acc: 31.55%,Test Discrimination: -3.313969135284424
Epoch 60, Train Loss: -14429.4482421875, Train Acc: 29.52%, Train Discrimination: -3.66290020942688 Test Loss: -12634.6337890625, Test Acc: 30.39%,Test Discrimination: -3.2209951877593994
Epoch 61, Train Loss: -14531.416015625, Train Acc: 28.62%, Train Discrimination: -3.6724817752838135 Test Loss: -12907.166015625, Test Acc: 31.05%,Test Discrimination: -3.290989637374878
Epoch 62, Train Loss: -14684.666015625, Train Acc: 29.25%, Train Discrimination: -3.7121427059173584 Test Loss: -12973.3720703125, Test Acc: 31.30%,Test Discrimination: -3.305461883544922
Epoch 63, Train Loss: -14781.8515625, Train Acc: 29.34%, Train Discrimination: -3.73148250579834 Test Loss: -12719.3212890625, Test Acc: 30.54%,Test Discrimination: -3.223959445953369
Epoch 64, Train Loss: -14916.712890625, Train Acc: 28.66%, Train Discrimination: -3.740139961242676 Test Loss: -12997.890625, Test Acc: 31.11%,Test Discrimination: -3.298245668411255
Epoch 65, Train Loss: -15028.3779296875, Train Acc: 29.32%, Train Discrimination: -3.7741434574127197 Test Loss: -12899.212890625, Test Acc: 30.89%,Test Discrimination: -3.254356622695923
Epoch 66, Train Loss: -15206.0078125, Train Acc: 28.99%, Train Discrimination: -3.798766851425171 Test Loss: -12865.9072265625, Test Acc: 30.70%,Test Discrimination: -3.231048583984375
Epoch 67, Train Loss: -15332.240234375, Train Acc: 28.81%, Train Discrimination: -3.815639019012451 Test Loss: -13064.2392578125, Test Acc: 31.24%,Test Discrimination: -3.2817490100860596
Epoch 68, Train Loss: -15397.6416015625, Train Acc: 29.52%, Train Discrimination: -3.834493398666382 Test Loss: -12781.72265625, Test Acc: 30.23%,Test Discrimination: -3.1864068508148193
Epoch 69, Train Loss: -15567.77734375, Train Acc: 28.52%, Train Discrimination: -3.838992118835449 Test Loss: -13098.4140625, Test Acc: 31.30%,Test Discrimination: -3.279831886291504
Epoch 70, Train Loss: -15575.626953125, Train Acc: 29.55%, Train Discrimination: -3.8618292808532715 Test Loss: -12816.078125, Test Acc: 29.97%,Test Discrimination: -3.1621084213256836
Epoch 71, Train Loss: -15822.4443359375, Train Acc: 28.28%, Train Discrimination: -3.8644988536834717 Test Loss: -13158.78515625, Test Acc: 31.46%,Test Discrimination: -3.2781641483306885
Epoch 72, Train Loss: -15776.9658203125, Train Acc: 29.67%, Train Discrimination: -3.8840315341949463 Test Loss: -12879.4375, Test Acc: 29.85%,Test Discrimination: -3.1399261951446533
Epoch 73, Train Loss: -16036.7783203125, Train Acc: 27.98%, Train Discrimination: -3.876157760620117 Test Loss: -13263.3095703125, Test Acc: 31.78%,Test Discrimination: -3.2913424968719482
Epoch 74, Train Loss: -15829.59375, Train Acc: 29.87%, Train Discrimination: -3.8817989826202393 Test Loss: -12940.01171875, Test Acc: 29.53%,Test Discrimination: -3.117284059524536
Epoch 75, Train Loss: -16190.85546875, Train Acc: 27.57%, Train Discrimination: -3.8708436489105225 Test Loss: -13384.400390625, Test Acc: 31.87%,Test Discrimination: -3.296456813812256
Epoch 76, Train Loss: -15970.7041015625, Train Acc: 30.06%, Train Discrimination: -3.8920681476593018 Test Loss: -13133.5224609375, Test Acc: 29.63%,Test Discrimination: -3.1392033100128174
Epoch 77, Train Loss: -16535.91796875, Train Acc: 27.74%, Train Discrimination: -3.9280056953430176 Test Loss: -13442.271484375, Test Acc: 31.08%,Test Discrimination: -3.2633793354034424
Epoch 78, Train Loss: -16558.76171875, Train Acc: 29.49%, Train Discrimination: -3.986628293991089 Test Loss: -13364.5615234375, Test Acc: 30.42%,Test Discrimination: -3.1904804706573486
Epoch 79, Train Loss: -16953.591796875, Train Acc: 28.43%, Train Discrimination: -4.016555309295654 Test Loss: -13480.802734375, Test Acc: 30.89%,Test Discrimination: -3.2224721908569336
Epoch 80, Train Loss: -17037.001953125, Train Acc: 28.98%, Train Discrimination: -4.044919967651367 Test Loss: -13542.0361328125, Test Acc: 30.92%,Test Discrimination: -3.2226216793060303
Epoch 81, Train Loss: -17173.115234375, Train Acc: 28.96%, Train Discrimination: -4.059505939483643 Test Loss: -13493.1826171875, Test Acc: 30.39%,Test Discrimination: -3.185468912124634
Epoch 82, Train Loss: -17341.48046875, Train Acc: 28.42%, Train Discrimination: -4.061057090759277 Test Loss: -13688.9560546875, Test Acc: 30.99%,Test Discrimination: -3.253333330154419
Epoch 83, Train Loss: -17182.31640625, Train Acc: 29.48%, Train Discrimination: -4.058352470397949 Test Loss: -13424.2333984375, Test Acc: 29.60%,Test Discrimination: -3.1219265460968018
Epoch 84, Train Loss: -17404.654296875, Train Acc: 27.46%, Train Discrimination: -3.98720645904541 Test Loss: -13840.6005859375, Test Acc: 32.41%,Test Discrimination: -3.321133852005005
Epoch 85, Train Loss: -16247.29296875, Train Acc: 30.75%, Train Discrimination: -3.865043878555298 Test Loss: -13561.0556640625, Test Acc: 29.44%,Test Discrimination: -3.1164374351501465
Epoch 86, Train Loss: -17605.1328125, Train Acc: 27.30%, Train Discrimination: -3.9771475791931152 Test Loss: -13901.4091796875, Test Acc: 31.02%,Test Discrimination: -3.267655611038208
Epoch 87, Train Loss: -17623.875, Train Acc: 29.42%, Train Discrimination: -4.098526954650879 Test Loss: -13826.359375, Test Acc: 30.48%,Test Discrimination: -3.210855007171631
Epoch 88, Train Loss: -18162.150390625, Train Acc: 28.59%, Train Discrimination: -4.150794506072998 Test Loss: -13845.572265625, Test Acc: 30.32%,Test Discrimination: -3.1908133029937744
Epoch 89, Train Loss: -18312.583984375, Train Acc: 28.34%, Train Discrimination: -4.149664402008057 Test Loss: -14022.7607421875, Test Acc: 31.40%,Test Discrimination: -3.281791925430298
Epoch 90, Train Loss: -17732.8203125, Train Acc: 29.85%, Train Discrimination: -4.1013054847717285 Test Loss: -13813.099609375, Test Acc: 29.47%,Test Discrimination: -3.109131336212158
Epoch 91, Train Loss: -18118.501953125, Train Acc: 27.23%, Train Discrimination: -4.002922058105469 Test Loss: -14165.57421875, Test Acc: 32.28%,Test Discrimination: -3.321354866027832
Epoch 92, Train Loss: -17077.693359375, Train Acc: 30.54%, Train Discrimination: -3.9599006175994873 Test Loss: -14059.7939453125, Test Acc: 29.88%,Test Discrimination: -3.1726412773132324
Epoch 93, Train Loss: -18755.994140625, Train Acc: 27.89%, Train Discrimination: -4.161790370941162 Test Loss: -14148.2001953125, Test Acc: 30.10%,Test Discrimination: -3.192291021347046
Epoch 94, Train Loss: -18928.83203125, Train Acc: 28.17%, Train Discrimination: -4.19842004776001 Test Loss: -14278.3974609375, Test Acc: 32.00%,Test Discrimination: -3.3182778358459473
Epoch 95, Train Loss: -17803.8828125, Train Acc: 30.11%, Train Discrimination: -4.078800678253174 Test Loss: -14210.689453125, Test Acc: 29.44%,Test Discrimination: -3.1511380672454834
Epoch 96, Train Loss: -18687.08203125, Train Acc: 27.42%, Train Discrimination: -4.062780380249023 Test Loss: -14349.63671875, Test Acc: 31.46%,Test Discrimination: -3.3015663623809814
Epoch 97, Train Loss: -18475.408203125, Train Acc: 29.78%, Train Discrimination: -4.170883655548096 Test Loss: -14380.609375, Test Acc: 30.45%,Test Discrimination: -3.22025203704834
Epoch 98, Train Loss: -19366.150390625, Train Acc: 28.34%, Train Discrimination: -4.256903171539307 Test Loss: -14381.6142578125, Test Acc: 29.97%,Test Discrimination: -3.1918368339538574
Epoch 99, Train Loss: -19452.361328125, Train Acc: 28.02%, Train Discrimination: -4.236870765686035 Test Loss: -14510.3583984375, Test Acc: 31.43%,Test Discrimination: -3.3034965991973877
Epoch 100, Train Loss: -18683.66015625, Train Acc: 29.75%, Train Discrimination: -4.181940078735352 Test Loss: -14385.3525390625, Test Acc: 29.69%,Test Discrimination: -3.1517956256866455</code></pre>
</div>
</div>
<section id="model-accross-different-values-of-lambda-1" class="level4">
<h4 class="anchored" data-anchor-id="model-accross-different-values-of-lambda-1">MODEL ACCROSS DIFFERENT VALUES OF LAMBDA</h4>
<div id="897524e3" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_comp_metric(title, y_label, val1, val2, val3, val4):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Train Line with Markers</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val1:</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>epochs, y<span class="op">=</span>val1, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'Lambda = 0.1'</span>,</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, width<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val2:</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adding Test Line with Markers</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>epochs, y<span class="op">=</span>val2, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'lambda = 1'</span>,</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, width<span class="op">=</span><span class="dv">2</span>, dash<span class="op">=</span><span class="st">'dot'</span>),</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Test Line with Markers</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val3:</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>epochs, y<span class="op">=</span>val3, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'lambda = 10'</span>,</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Pink'</span>, width<span class="op">=</span><span class="dv">2</span>, dash<span class="op">=</span><span class="st">'dot'</span>),</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Pink'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val4:</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Adding Test Line with Markers</span></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>epochs, y<span class="op">=</span>val4, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'lambda = 100'</span>,</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Purple'</span>, width<span class="op">=</span><span class="dv">2</span>, dash<span class="op">=</span><span class="st">'dot'</span>),</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Purple'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update Layout</span></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span>{<span class="st">'text'</span>: title, <span class="st">'y'</span>:<span class="fl">0.9</span>, <span class="st">'x'</span>:<span class="fl">0.5</span>, <span class="st">'xanchor'</span>: <span class="st">'center'</span>, <span class="st">'yanchor'</span>: <span class="st">'top'</span>},</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>        xaxis_title<span class="op">=</span><span class="st">'Epoch'</span>,</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>        yaxis_title<span class="op">=</span>y_label,</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>        legend<span class="op">=</span><span class="bu">dict</span>(x<span class="op">=</span><span class="fl">0.1</span>, y<span class="op">=</span><span class="fl">1.1</span>, orientation<span class="op">=</span><span class="st">'h'</span>),</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(family<span class="op">=</span><span class="st">"Helvetica, Arial, sans-serif"</span>, size<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"black"</span>),</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>        plot_bgcolor<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(l<span class="op">=</span><span class="dv">40</span>, r<span class="op">=</span><span class="dv">40</span>, t<span class="op">=</span><span class="dv">40</span>, b<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gridlines and Axes styles</span></span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>    fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ca6c93cc" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plot_comp_metric(<span class="st">"TEST_Fairness"</span>, <span class="st">"Fairness"</span>, test_fairness3, test_fairness4, test_fairness5, test_fairness6)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plot_comp_metric(<span class="st">"TEST_Discriminations"</span>, <span class="st">"Discrimination"</span>, test_discriminations3, test_discriminations4, test_discriminations5, test_discriminations6)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plot_comp_metric(<span class="st">"TEST_Losses"</span>, <span class="st">"Losses"</span>, test_losses3,test_losses4,test_losses5,test_losses6)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="435a090e-1c2e-49dc-877e-28804ba47abd" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("435a090e-1c2e-49dc-877e-28804ba47abd")) {                    Plotly.newPlot(                        "435a090e-1c2e-49dc-877e-28804ba47abd",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Lambda = 0.1","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9999962748993312,0.9999945821650726,0.999998733913003,0.999999968090826,0.9999794111172378,0.9998765991185792,0.999897961926763,0.9999636006068613,0.99998944537856,0.9999942921149341,0.9999934768743515,0.9999875740904827,0.9999720980868005,0.9999442869157065,0.9999149280702113,0.9999006445577834,0.9999060755726532,0.9999253215792123,0.9999471365808859,0.9999632790313626,0.999972252329826,0.9999755107437522,0.9999742520121799,0.9999689280521125,0.999959987206239,0.999949478689814,0.9999416032151203,0.9999398676372948,0.9999441503823618,0.9999524096310779,0.9999603551295877,0.9999635439635313,0.9999607367681165,0.9999531520079472,0.9999434011733683,0.9999358352215495,0.9999339527275879,0.99993798329524,0.9999451991498063,0.9999515668205277,0.9999538371157541,0.9999517083961109,0.9999464502070623,0.9999404866830446,0.9999366948832176,0.9999367454074672,0.9999400502274511,0.9999442956141138,0.9999468599053216,0.9999461635197804,0.9999422312503157,0.9999366579140769,0.99993202737096,0.9999304915982066,0.9999322039438994,0.999935386236757,0.9999374948092736,0.999936804837489,0.9999333984815166,0.9999288034523488,0.9999256792434608,0.9999264187717927,0.9999250399778248,0.999921517308394,0.9999202082399279,0.9999218729935819,0.9999240823672153,0.999922078117379,0.9999183892941801,0.999917672808806,0.9999193684852798,0.9999186897694017,0.9999155167970457,0.9999141994630918,0.9999162988024182,0.9999168271897361,0.9999140831787372,0.999912799888989,0.9999133206365514,0.999910703576461,0.9999067752432893,0.999908123856585,0.9999074090301292,0.9999053371138871,0.9999072431455716,0.9999084745359141,0.9999078998735058,0.9999060871996335,0.9999079049011925,0.9999044747310109,0.9999036841472844,0.9999073709113873,0.9999032246996649,0.9999074703591759,0.999907773672021,0.9999045442527859,0.9999093529695529,0.9999040923939901,0.9999082829526742,0.9999079866538523],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 1","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9994257987127639,0.9986305604688823,0.9995497121417429,0.9998506962001557,0.9999794315372128,0.9997290781757329,0.994574895594269,0.9905794207006693,0.9918952137231827,0.99569848831743,0.9981702109798789,0.9990208700764924,0.9992459965869784,0.9991908850497566,0.998869450064376,0.9981472088256851,0.9968531047925353,0.9950401885434985,0.9932006727904081,0.9920004988089204,0.9917804421856999,0.992469746619463,0.993843131698668,0.9956610859371722,0.9968022301327437,0.9970865319482982,0.996886454988271,0.9963913524989039,0.9958974746987224,0.995498463511467,0.9951567836105824,0.9949037674814463,0.9947745301760733,0.9947810969315469,0.9949002545326948,0.9950849316082895,0.9952811719849706,0.9954501637257636,0.9955312358215451,0.9955107839778066,0.9954061359167099,0.9952234961092472,0.9949936722405255,0.994766500312835,0.9945885930210352,0.9944995660334826,0.9945149631239474,0.9946714984253049,0.9951179274357855,0.9955244278535247,0.9953097142279148,0.9948140238411725,0.994411707855761,0.9942373889498413,0.99432900454849,0.9946627994067967,0.994775032158941,0.9944440987892449,0.9939590734429657,0.9937045313417912,0.9938298724591732,0.9941543508321047,0.9942594054155052,0.9938042662106454,0.9934033453464508,0.9937455640174448,0.9939299332909286,0.9934426974505186,0.9929881198331714,0.9931556568481028,0.9934338279999793,0.9931718604639173,0.9927017511799932,0.9927835911512375,0.9931091982871294,0.9929325086995959,0.9925258005969226,0.9926540609449148,0.992791457567364,0.9924151813611388,0.9922578176483512,0.9925723909400403,0.9923606505617499,0.9921173648908734,0.992359462659806,0.9922023168765008,0.9918682677671313,0.9919743556529284,0.9919510465115309,0.991719557903707,0.9920434253290296,0.9917569132521749,0.9919746844097972,0.9919103598222136,0.9917805874720216,0.9920341717079282,0.9915651576593518,0.9922004011459649,0.9915004959329963,0.9921377608552575],"type":"scatter"},{"line":{"color":"Pink","dash":"dot","width":2},"marker":{"color":"Pink","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 10","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.976665573194623,0.9533222615718842,0.9996451690385584,0.9944472406059504,0.9951073164120317,0.9970490969717503,0.9985927194356918,0.9996092118090019,0.99999678772474,0.9990471607306972,0.9954784191213548,0.9892788575962186,0.9836840238422155,0.9823175575584173,0.9853769429028034,0.9902155250310898,0.994446411728859,0.9972716020420194,0.9988317509414628,0.9995666659378912,0.9998523752001347,0.9999379203727585,0.9999408717048937,0.9998762325558346,0.9996892600902356,0.999295879912097,0.9986291142413393,0.9976855472195894,0.9965610390063375,0.9954491481184959,0.9945780453272164,0.9941403362900019,0.9942297367379069,0.9948796648532152,0.996038188226521,0.9974379627965391,0.9985190737061203,0.9988550771959126,0.9988397867418826,0.9986478853970766,0.9984044055454433,0.998186985263601,0.9979782509617507,0.9977489179000258,0.9975322973914444,0.9973746517207474,0.9973077324684709,0.997342373477295,0.997470757458359,0.9976745394524187,0.9979134865570813,0.998140792013146,0.9983116316143423,0.9984080174472183,0.9984444020083174,0.9984243138460442,0.9983782448107377,0.9983406650135294,0.9983633179217577,0.9984773005126044,0.9986477363854647,0.9987320092041045,0.99872284126468,0.9986832067370415,0.9987601947505027,0.9989998856326565,0.9992591083282605,0.9992729593068361,0.9991365557070822,0.9991768059553578,0.9993399708764628,0.9992894055903889,0.9990809676819481,0.9991510469699278,0.9992851233109832,0.9991384902969003,0.9991431036032736,0.9993157513672486,0.9991137628094293,0.999156779027544,0.9993665727088228,0.9990603136830032,0.9992873591836542,0.9991386369219981,0.9990409426391125,0.9992254609824158,0.9987973468378186,0.9993238991592079,0.9990037474781275,0.999002423370257,0.9992720162845217,0.9987754025496542,0.9991863259347156,0.9990879739634693,0.9986926390556619,0.9991942772758193,0.9988601164659485,0.9990149836521596,0.9991264794953167,0.9986740360036492],"type":"scatter"},{"line":{"color":"Purple","dash":"dot","width":2},"marker":{"color":"Purple","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 100","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9118442088365555,0.352988600730896,0.6208195090293884,0.8321533799171448,0.9558988809585571,0.9983043341198936,0.9344111159443855,0.9503425247967243,0.9941674955189228,0.9995793782873079,0.9964577546343207,0.9950543120503426,0.9951377967372537,0.9958153530023992,0.9966323415283114,0.9974041359964758,0.9980667150812224,0.9986123307608068,0.9990521186846308,0.9994023481267504,0.9996759219793603,0.9998760232847417,0.9999882945339778,0.9999686530791223,0.9997337410750333,0.9991782135912217,0.9982913130661473,0.9973905759397894,0.9970747127663344,0.9975689656566828,0.9984411005862057,0.9991463230689988,0.9995623582508415,0.9998044770618435,0.9999317137262551,0.9999847911130928,0.9999992269317204,0.999999999955036,0.9999790322799527,0.9996393387555145,0.9999036404697108,0.9999428222909046,0.9997940399480285,0.9998931323571014,0.9999996241895985,0.9998700845317217,0.9998484162497334,0.999944661907648,0.9999882293141127,0.9999970950384522,0.9999966193502132,0.9999879686911299,0.9999592669882986,0.9998989627783885,0.9998334340198198,0.9998453035950661,0.9999363787428592,0.9999956407732498,0.9999956749288685,0.99999342262754,0.9999987860546753,0.9999578927672701,0.9999444555396622,0.9999911031100055,0.9999990879455822,0.9999988844535892,0.9999944828095977,0.9999537539770245,0.9999240861070575,0.9999545631071669,0.9999888437632762,0.9999980448956194,0.9999953360184008,0.9999756210963824,0.9999525094317505,0.9999738705810159,0.9999985062437418,0.9999998424172531,0.9999962277918257,0.9999753199881525,0.9999852976316106,0.9999996447286321,0.9999998668264993,0.9999885723727857,0.9999740910079709,0.9999914446743787,0.9999995509590178,0.9999968025576891,0.9999821411838639,0.9999900006469034,0.9999999407150675,0.9999992341208781,0.9999888517249929,0.9999974057534473,0.9999998843697426,0.9999958797652653,0.9999926990467429,0.9999999855035071,0.9999983919508395,0.9999908281151875],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"TEST_Fairness","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Fairness"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('435a090e-1c2e-49dc-877e-28804ba47abd');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="c90b0a9a-0fa7-4937-8570-9e767ea68915" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c90b0a9a-0fa7-4937-8570-9e767ea68915")) {                    Plotly.newPlot(                        "c90b0a9a-0fa7-4937-8570-9e767ea68915",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Lambda = 0.1","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[8.151709153025877e-6,1.979424723685952e-6,6.932599916353865e-8,0.00002340957689739298,0.00015872564108576626,0.00012051362864440307,0.000038806603697594255,0.000010739344361354597,5.862522812094539e-6,6.8014569478691556e-6,0.000013007672350795474,0.00002939316073025111,0.00005928790051257238,0.00009152001439360902,0.00010711271170293912,0.00009975230204872787,0.00007691688369959593,0.00005236986544332467,0.000034982862416654825,0.000025721548809087835,0.000022407390133594163,0.00002361419319640845,0.000029019141948083416,0.00003848889537039213,0.00005016722570871934,0.00005942699499428272,0.00006211683648871258,0.00005773717930424027,0.000048900823458097875,0.0000404833845095709,0.000037372396036516875,0.00004078837810084224,0.000049540532927494496,0.00006085947097744793,0.00006981181650189683,0.00007215962978079915,0.00006756046786904335,0.00005929342660238035,0.00005202088868827559,0.00004929439091938548,0.00005152654193807393,0.000057309367548441514,0.00006391905480995774,0.00006806109013268724,0.00006776352529413998,0.00006373412907123566,0.00005866708670509979,0.000055558080930495635,0.0000561698543606326,0.00006047372517059557,0.00006670643779216334,0.00007188165182014927,0.0000735237990738824,0.00007143319817259908,0.00006778079841751605,0.00006544098869198933,0.00006629719428019598,0.00007027628453215584,0.00007556574564659968,0.00007893417932791635,0.00007776235725032166,0.00007931552681839094,0.00008349269774043933,0.00008514009095961228,0.00008323154906975105,0.00008057511149672791,0.00008267584053101018,0.00008679324673721567,0.00008735182200325653,0.00008546521712560207,0.00008619348227512091,0.00008953562792157754,0.00009075300476979464,0.00008827173587633297,0.00008705495565664023,0.00009021814912557602,0.00009171348210657015,0.0000902938554645516,0.00009295115887653083,0.00009762447007233277,0.00009538282756693661,0.00009607389074517414,0.0000983712452580221,0.00009552171104587615,0.00009481389861321077,0.00009429848432773724,0.00009688219870440662,0.00009414401574758813,0.00009787642920855433,0.00009883867460303009,0.00009411949577042833,0.00009909753134706989,0.00009407020115759224,0.00009356691589346156,0.00009751143807079643,0.00009117556328419596,0.00009806897287489846,0.00009290817979490384,0.00009277163917431608,0.0000945917417993769],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 1","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.002305078087374568,0.0007584519335068762,0.0002527807664591819,0.00004545060437521897,0.00024820634280331433,0.006717855576425791,0.01213766634464264,0.009785423055291176,0.0046728309243917465,0.0018190320115536451,0.0009401802089996636,0.0007239599945023656,0.0007862607599236071,0.0011101554846391082,0.0018461060244590044,0.003194451564922929,0.005130221135914326,0.007148608099669218,0.008484604768455029,0.008673476055264473,0.007824490778148174,0.0062730032950639725,0.004269877914339304,0.0030647350940853357,0.0027728930581361055,0.0029844145756214857,0.0035100411623716354,0.0040298947133123875,0.004461049567908049,0.0048438492231070995,0.0051404209807515144,0.005304928403347731,0.005323120858520269,0.005215093027800322,0.005029653664678335,0.004832464270293713,0.004666847642511129,0.0045899138785898685,0.004617195576429367,0.004737720359116793,0.0049515534192323685,0.005215461365878582,0.00547810411080718,0.005683063063770533,0.005786689464002848,0.005760561674833298,0.005553071387112141,0.0050995540805161,0.004647944588214159,0.004891620483249426,0.005431998986750841,0.005864784587174654,0.006060647778213024,0.005970019847154617,0.005613189190626144,0.005480789113789797,0.005855914205312729,0.006398295052349567,0.006690859328955412,0.006558183114975691,0.006188309285789728,0.006073724012821913,0.006612639874219894,0.007087842561304569,0.006691510323435068,0.006485863588750362,0.0070541612803936005,0.00759136863052845,0.007395173888653517,0.007065238896757364,0.007374401204288006,0.007942238822579384,0.007843129336833954,0.007438850589096546,0.0076538389548659325,0.008161009289324284,0.007992609404027462,0.007817557081580162,0.008280372247099876,0.00846637599170208,0.008068300783634186,0.008324629627168179,0.008620386011898518,0.008309287019073963,0.008483701385557652,0.008867759257555008,0.00871904008090496,0.008733541704714298,0.00900481641292572,0.008600234985351562,0.008944403380155563,0.008672085590660572,0.00874628871679306,0.00889852549880743,0.008579921908676624,0.009153065271675587,0.008353297598659992,0.009209738112986088,0.008402534760534763,0.008908786810934544],"type":"scatter"},{"line":{"color":"Pink","dash":"dot","width":2},"marker":{"color":"Pink","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 10","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.06654535233974457,7.880499651946593e-6,0.010784532874822617,0.00893236044794321,0.0054415324702858925,0.0027794602792710066,0.0010195492068305612,0.00005833491377416067,0.0006259588990360498,0.004553061444312334,0.01247853972017765,0.020251505076885223,0.02249731495976448,0.01864784210920334,0.012290898710489273,0.006767770275473595,0.003124645445495844,0.0011912152403965592,0.00035031320294365287,0.00007053612353047356,0.000010727161679824349,8.320625056512654e-6,0.00004572206671582535,0.00020338354806881398,0.0006066203350201249,0.0013629901222884655,0.0024972050450742245,0.0038867166731506586,0.005264663137495518,0.006318294443190098,0.006792198866605759,0.006586700212210417,0.005670224782079458,0.00416652113199234,0.0024886741302907467,0.0012720613740384579,0.0009010136127471924,0.0008997524273581803,0.0010874962899833918,0.0013619709061458707,0.001625313307158649,0.0018827372696250677,0.0021622744388878345,0.0024247223045676947,0.002617271151393652,0.002702017081901431,0.0026684862095862627,0.0025280530098825693,0.0023028107825666666,0.0020538948010653257,0.001828867825679481,0.0016733652446419,0.0016128825955092907,0.0016333324601873755,0.001711636083200574,0.0018164272187277675,0.001902737538330257,0.0018912854138761759,0.0017744142096489668,0.0015745116397738457,0.0014969451585784554,0.0015861069550737739,0.0017376078758388758,0.0016957068582996726,0.001378093846142292,0.0010234001092612743,0.001053202198818326,0.0013448471436277032,0.001318965689279139,0.0010415359865874052,0.0011423126561567187,0.0015278561040759087,0.0013864119537174702,0.00114206422585994,0.0014155239332467318,0.001394286984577775,0.001058880239725113,0.001379476161673665,0.0012912217061966658,0.0009199296473525465,0.0014129224000498652,0.0010120744118466973,0.0012217694893479347,0.001342870993539691,0.0010141112143173814,0.0016717810649424791,0.0008906418224796653,0.001394167309626937,0.0013739686692133546,0.0009662124793976545,0.0016698242397978902,0.0010756906121969223,0.0011889914749190211,0.001709948293864727,0.0010203933343291283,0.0015084969345480204,0.0012972090626135468,0.001119046239182353,0.0017428839346393943,0.001159182982519269],"type":"scatter"},{"line":{"color":"Purple","dash":"dot","width":2},"marker":{"color":"Purple","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 100","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9769613146781921,0.5921800136566162,0.2709856629371643,0.0816660225391388,1.0698164487621398e-7,0.059652455151081085,0.043063800781965256,0.0024062534794211388,0.0030265941750258207,0.009008234366774559,0.010761010460555553,0.010061012580990791,0.00851255003362894,0.006840200629085302,0.005311102606356144,0.0040077813901007175,0.0029334572609514,0.002060561440885067,0.0013541262596845627,0.0007869373657740653,0.0003488937800284475,0.00006537682202178985,0.00001962212445505429,0.00038078753277659416,0.001387298689223826,0.003133774269372225,0.005078879185020924,0.006056603509932756,0.00550770154222846,0.004138539079576731,0.002934914082288742,0.00216152286157012,0.0017253367695957422,0.0016044412041082978,0.001854076748713851,0.0027368590235710144,0.004946850705891848,0.010359425097703934,0.0218657236546278,0.021080931648612022,0.015294515527784824,0.012947052717208862,0.013747233897447586,0.016710413619875908,0.019168350845575333,0.016681186854839325,0.011920791119337082,0.008385887369513512,0.006505528464913368,0.005823005922138691,0.0060292137786746025,0.007016628049314022,0.008661908097565174,0.010426296852529049,0.011224757879972458,0.010627569630742073,0.009574047289788723,0.009106006473302841,0.009730648249387741,0.011555599980056286,0.01373961754143238,0.013816718012094498,0.011503375135362148,0.009333038702607155,0.008404838852584362,0.008543049916625023,0.009054689668118954,0.008912213146686554,0.0078079914674162865,0.006579641718417406,0.005967148579657078,0.006162856239825487,0.006974315270781517,0.007683797739446163,0.00745470030233264,0.00667999405413866,0.0063874199986457825,0.006889901123940945,0.007423443254083395,0.006817142944782972,0.005681995768100023,0.005184368696063757,0.005331987049430609,0.00526495510712266,0.004546421580016613,0.003954052925109863,0.004033115226775408,0.004410644993185997,0.0042433724738657475,0.003800278762355447,0.003930849488824606,0.004326359368860722,0.003965436480939388,0.003521748585626483,0.003695681458339095,0.003579105716198683,0.0029970072209835052,0.0029761926271021366,0.003076989436522126,0.0026586747262626886],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"TEST_Discriminations","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Discrimination"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('c90b0a9a-0fa7-4937-8570-9e767ea68915');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="b1cc06cd-fa3c-4c52-9393-8320f28ba98c" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b1cc06cd-fa3c-4c52-9393-8320f28ba98c")) {                    Plotly.newPlot(                        "b1cc06cd-fa3c-4c52-9393-8320f28ba98c",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Lambda = 0.1","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.8151583671569824,0.7927682399749756,0.5724644064903259,0.380097895860672,0.592439591884613,0.4511386454105377,0.36649468541145325,0.41390082240104675,0.45325735211372375,0.4464418888092041,0.4098832607269287,0.375325471162796,0.36777153611183167,0.382951945066452,0.391345739364624,0.3792632520198822,0.3627530336380005,0.3587244153022766,0.36661240458488464,0.37579023838043213,0.37788307666778564,0.37140586972236633,0.3608776926994324,0.35299625992774963,0.3513491451740265,0.35344696044921875,0.35309696197509766,0.3492453694343567,0.34627479314804077,0.3476106822490692,0.34929725527763367,0.3473863899707794,0.3441622257232666,0.3429449498653412,0.34361645579338074,0.34356948733329773,0.3422171473503113,0.3415582776069641,0.3423617482185364,0.34259530901908875,0.34129539132118225,0.33954179286956787,0.3387145698070526,0.3385342061519623,0.33802807331085205,0.3373561203479767,0.33727580308914185,0.3375509977340698,0.33722394704818726,0.33620864152908325,0.3353555202484131,0.33501118421554565,0.33472001552581787,0.33435484766960144,0.3343304693698883,0.33449336886405945,0.33419090509414673,0.33349138498306274,0.33291468024253845,0.3325898051261902,0.33281195163726807,0.3325122892856598,0.33184751868247986,0.33147355914115906,0.33138102293014526,0.3315417766571045,0.33118632435798645,0.3308059573173523,0.3308068513870239,0.3310209810733795,0.33094578981399536,0.33060503005981445,0.3303819000720978,0.3304310441017151,0.3304601013660431,0.33011361956596375,0.32998010516166687,0.3302960693836212,0.33022141456604004,0.32989779114723206,0.33033010363578796,0.3302987217903137,0.33009660243988037,0.3304902911186218,0.3301943242549896,0.33064308762550354,0.3301665186882019,0.3305714428424835,0.3304448425769806,0.33037668466567993,0.33103108406066895,0.3306027352809906,0.33088716864585876,0.3309270441532135,0.3304135799407959,0.3316633701324463,0.3301100730895996,0.33059772849082947,0.3310299515724182,0.33154693245887756],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 1","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.862220048904419,0.9783310890197754,0.8635511994361877,0.6453170776367188,0.4340975880622864,0.4630437195301056,0.5609658360481262,0.4718201160430908,0.38430067896842957,0.3832087218761444,0.412378191947937,0.428307443857193,0.4239683747291565,0.4055226743221283,0.3831002414226532,0.3671249747276306,0.363337904214859,0.36858707666397095,0.3732462227344513,0.370652437210083,0.3628670573234558,0.35592013597488403,0.3570830523967743,0.3648805618286133,0.3667066991329193,0.3621077537536621,0.3559049069881439,0.352242648601532,0.3509335517883301,0.35114774107933044,0.3517586290836334,0.3517192006111145,0.3507087826728821,0.34918203949928284,0.3479025661945343,0.3474251329898834,0.34779295325279236,0.3481362760066986,0.3479565382003784,0.34734922647476196,0.3464391827583313,0.3454667925834656,0.3446483016014099,0.3440929651260376,0.3437230885028839,0.3433622419834137,0.3427587151527405,0.3428235650062561,0.3438594341278076,0.34279054403305054,0.3413585424423218,0.3408142328262329,0.34041446447372437,0.33992964029312134,0.34019899368286133,0.340359091758728,0.3393608033657074,0.3386494219303131,0.33836600184440613,0.3378935754299164,0.3377578854560852,0.337801069021225,0.33701977133750916,0.3368346691131592,0.3366362750530243,0.3368988037109375,0.3361049294471741,0.3359152674674988,0.33555248379707336,0.3357302248477936,0.335235059261322,0.3349394202232361,0.3347195088863373,0.334798663854599,0.33445727825164795,0.3341604769229889,0.3339819610118866,0.33404433727264404,0.3336978554725647,0.3335930407047272,0.333709180355072,0.3334692716598511,0.3334105312824249,0.3334823250770569,0.33345431089401245,0.3334093689918518,0.33344534039497375,0.333479642868042,0.3334639072418213,0.3335970938205719,0.33354657888412476,0.33358699083328247,0.33356019854545593,0.33355697989463806,0.3336179256439209,0.33360356092453003,0.3338625133037567,0.3337288796901703,0.33393871784210205,0.3339180648326874],"type":"scatter"},{"line":{"color":"Pink","dash":"dot","width":2},"marker":{"color":"Pink","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 10","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[1.618756890296936,0.47272711992263794,0.6750913858413696,0.787500262260437,0.7755417823791504,0.7039012908935547,0.6117937564849854,0.5221425890922546,0.4558315575122833,0.4323478043079376,0.4549855589866638,0.4939708709716797,0.5066609382629395,0.48407045006752014,0.4499863386154175,0.4251251220703125,0.4145296812057495,0.41516613960266113,0.4212435781955719,0.42755720019340515,0.43078941106796265,0.4295097887516022,0.42412203550338745,0.4162903130054474,0.4082392454147339,0.40199965238571167,0.39880698919296265,0.3987092077732086,0.40055179595947266,0.4025264084339142,0.4029039740562439,0.400831937789917,0.39635583758354187,0.39118245244026184,0.3881354033946991,0.3889928162097931,0.38926035165786743,0.3877016007900238,0.3856111764907837,0.3839602768421173,0.3829328119754791,0.3822029232978821,0.38155031204223633,0.38100865483283997,0.38051527738571167,0.380009263753891,0.37944847345352173,0.37883034348487854,0.3781985342502594,0.377647340297699,0.37719810009002686,0.3767361640930176,0.3762405216693878,0.3757925033569336,0.375399112701416,0.37502554059028625,0.37460270524024963,0.37401944398880005,0.37355339527130127,0.37313956022262573,0.3726447820663452,0.37201911211013794,0.37157320976257324,0.3710538148880005,0.37020593881607056,0.3696703612804413,0.3691275119781494,0.36891359090805054,0.36851009726524353,0.3679479658603668,0.36763814091682434,0.36764296889305115,0.36710241436958313,0.3667144179344177,0.3664807677268982,0.3660942614078522,0.3655802011489868,0.3652014136314392,0.36484137177467346,0.3645588457584381,0.36454150080680847,0.36405590176582336,0.3638332188129425,0.36374470591545105,0.36370202898979187,0.3639340400695801,0.3638872504234314,0.36356011033058167,0.363442063331604,0.36367854475975037,0.3636413812637329,0.3634784519672394,0.3633100986480713,0.3633374571800232,0.36365368962287903,0.3632471561431885,0.3632871210575104,0.36340224742889404,0.3632289171218872,0.3634076714515686],"type":"scatter"},{"line":{"color":"Purple","dash":"dot","width":2},"marker":{"color":"Purple","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"lambda = 100","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[81.26918029785156,44.364707946777344,17.233318328857422,4.707304000854492,0.45184066891670227,3.002361297607422,2.223464012145996,0.54583740234375,0.65000319480896,1.0802360773086548,1.3089654445648193,1.3653030395507812,1.3297209739685059,1.2540267705917358,1.1650631427764893,1.0762407779693604,0.9932770133018494,0.9176132082939148,0.8489312529563904,0.7867572903633118,0.731259822845459,0.6848543882369995,0.6532831788063049,0.6468950510025024,0.6790441870689392,0.7546381950378418,0.845920205116272,0.8944037556648254,0.8713082075119019,0.8101513385772705,0.7585409283638,0.7270353436470032,0.7099412083625793,0.7048642635345459,0.7134603261947632,0.7500569820404053,0.8537835478782654,1.12966787815094,1.749912977218628,1.7165710926055908,1.4026128053665161,1.2711776494979858,1.3057690858840942,1.4524816274642944,1.567578673362732,1.4203407764434814,1.1639182567596436,0.9793910980224609,0.8819352388381958,0.8441849946975708,0.848829984664917,0.8901640772819519,0.9627827405929565,1.041698932647705,1.0751997232437134,1.0423238277435303,0.9878251552581787,0.9611192941665649,0.9859824180603027,1.0675245523452759,1.165371298789978,1.1608171463012695,1.0424977540969849,0.9329861998558044,0.882389485836029,0.8806671500205994,0.8956125974655151,0.8815856575965881,0.8261144161224365,0.7665850520133972,0.7343977093696594,0.7368400692939758,0.7654218673706055,0.7897643446922302,0.7744718790054321,0.7363756895065308,0.7186870574951172,0.7341176271438599,0.7505673170089722,0.7196168899536133,0.6675127744674683,0.6420915722846985,0.642996609210968,0.6356225609779358,0.6030058264732361,0.5760533213615417,0.5756973028182983,0.587220311164856,0.5779192447662354,0.5582870841026306,0.5611984729766846,0.5746796131134033,0.5590684413909912,0.5405011177062988,0.5459758043289185,0.5403996109962463,0.5171228051185608,0.5155194401741028,0.5186132192611694,0.5019014477729797],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"TEST_Losses","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Losses"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b1cc06cd-fa3c-4c52-9393-8320f28ba98c');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
<section id="model-accross-different-values-of-k-1" class="level4">
<h4 class="anchored" data-anchor-id="model-accross-different-values-of-k-1">MODEL ACCROSS DIFFERENT VALUES OF K</h4>
<div id="c130e6d2" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_comp_metric_k(title, y_label, val1, val2):</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Train Line with Markers</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val1:</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>epochs, y<span class="op">=</span>val1, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'K = 2'</span>,</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, width<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val2:</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adding Test Line with Markers</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        fig.add_trace(go.Scatter(</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>epochs, y<span class="op">=</span>val2, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">'K = 3'</span>,</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, width<span class="op">=</span><span class="dv">2</span>, dash<span class="op">=</span><span class="st">'dot'</span>),</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update Layout</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span>{<span class="st">'text'</span>: title, <span class="st">'y'</span>:<span class="fl">0.9</span>, <span class="st">'x'</span>:<span class="fl">0.5</span>, <span class="st">'xanchor'</span>: <span class="st">'center'</span>, <span class="st">'yanchor'</span>: <span class="st">'top'</span>},</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>        xaxis_title<span class="op">=</span><span class="st">'Epoch'</span>,</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>        yaxis_title<span class="op">=</span>y_label,</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>        legend<span class="op">=</span><span class="bu">dict</span>(x<span class="op">=</span><span class="fl">0.1</span>, y<span class="op">=</span><span class="fl">1.1</span>, orientation<span class="op">=</span><span class="st">'h'</span>),</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(family<span class="op">=</span><span class="st">"Helvetica, Arial, sans-serif"</span>, size<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"black"</span>),</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>        plot_bgcolor<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(l<span class="op">=</span><span class="dv">40</span>, r<span class="op">=</span><span class="dv">40</span>, t<span class="op">=</span><span class="dv">40</span>, b<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gridlines and Axes styles</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>    fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f08db90c" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>plot_comp_metric_k(<span class="st">"TEST_Fairness"</span>, <span class="st">"Fairness"</span>, test_fairness7, test_fairness8)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>plot_comp_metric_k(<span class="st">"TEST_Discriminations"</span>, <span class="st">"Discrimination"</span>, test_discriminations7, test_discriminations8)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>plot_comp_metric_k(<span class="st">"TEST_Losses"</span>, <span class="st">"Losses"</span>, test_losses7,test_losses8)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="da3316ba-bd52-4a57-b74f-87865597ec34" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("da3316ba-bd52-4a57-b74f-87865597ec34")) {                    Plotly.newPlot(                        "da3316ba-bd52-4a57-b74f-87865597ec34",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"K = 2","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9653967283666134,0.4246411919593811,0.6429553925991058,0.8055378496646881,0.8996545448899269,0.9534847065806389,0.9827841762453318,0.9963355727959424,0.9999640166606696,0.9943169821053743,0.9839977361261845,0.9845452755689621,0.9921950562857091,0.9971018659416586,0.9989927394781262,0.9996278211474419,0.9998455988243222,0.9999262780038407,0.9999592262029182,0.9999739620961918,0.9999810462686582,0.9999845604816073,0.9999862252707317,0.9999868391387281,0.9999868566346777,0.9999867989699851,0.9999894510010563,0.9999957122677188,0.9999999215518542,0.9999874156346777,0.9999779511836095,0.9999904879086898,0.9999944737055557,0.999775742777274,0.9991929081152193,0.9992641599383205,0.9997639019711642,0.99997933143095,0.9999961810522109,0.9999775134674564,0.9999774045190861,0.999992916436895,0.9999990074029483,0.9999679747161281,0.9998928117056494,0.9998143911507213,0.9998036468896316,0.9998805888026254,0.9999730252166046,0.9999990393571352,0.9999811899324413,0.9999879847082411,0.999999207286919,0.9999580318908556,0.9998975438866182,0.9999067309254315,0.9999649402743671,0.9999973869989844,0.9999987713987366,0.9999982451349751,0.9999995197808289,0.9999846647460799,0.9999514562332479,0.9999250928376568,0.9999295099405572,0.9999568400417047,0.9999828648051334,0.9999953675023789,0.9999981910320912,0.9999959864421726,0.9999854344396226,0.9999650901045243,0.9999491229755222,0.9999551318396698,0.999977598181431,0.9999943277307466,0.9999986264715517,0.9999962728049923,0.9999836530842003,0.9999662374066247,0.9999664983843104,0.999983804553267,0.9999956981396281,0.9999973783203586,0.9999912491803116,0.9999772512619529,0.9999714336645411,0.9999831479817658,0.9999946887564874,0.9999971627048581,0.9999925939869172,0.9999830462857062,0.9999831467594049,0.9999933920194053,0.999998539566036,0.9999977706631853,0.999992282585481,0.9999909898579062,0.9999977474644766,0.9999998212858685],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"K = 3","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9956307979300618,1.028154456987977,1.0658924654126167,1.135872706770897,1.2706333696842194,1.4772120714187622,1.4170719385147095,1.5844790935516357,1.599613904953003,1.6912295818328857,1.6765351295471191,1.7861008048057556,1.8207019567489624,1.7919467687606812,1.9394082427024841,1.9686622619628906,2.1112674474716187,2.1458847522735596,2.258716583251953,2.4252349138259888,2.3792375326156616,2.6676673889160156,2.7724205255508423,2.747249960899353,3.075723886489868,3.1879923343658447,2.9886080026626587,3.190718173980713,3.373415231704712,3.2663609981536865,3.5810670852661133,3.4670233726501465,3.682647228240967,3.570128917694092,3.7978780269622803,3.7452287673950195,3.922145366668701,3.8675594329833984,4.019883632659912,4.024471998214722,4.072261333465576,4.135715007781982,4.140645742416382,4.237121105194092,4.205350399017334,4.301329135894775,4.317147493362427,4.333051919937134,4.389772891998291,4.39286208152771,4.447308778762817,4.448258399963379,4.491506814956665,4.524647951126099,4.520955562591553,4.574990510940552,4.592230319976807,4.60027551651001,4.647033452987671,4.66290020942688,4.6724817752838135,4.712142705917358,4.73148250579834,4.740139961242676,4.77414345741272,4.798766851425171,4.815639019012451,4.834493398666382,4.838992118835449,4.8618292808532715,4.864498853683472,4.884031534194946,4.876157760620117,4.881798982620239,4.8708436489105225,4.892068147659302,4.928005695343018,4.986628293991089,5.016555309295654,5.044919967651367,5.059505939483643,5.061057090759277,5.058352470397949,4.98720645904541,4.865043878555298,4.977147579193115,5.098526954650879,5.150794506072998,5.149664402008057,5.1013054847717285,5.002922058105469,4.959900617599487,5.161790370941162,5.19842004776001,5.078800678253174,5.062780380249023,5.170883655548096,5.256903171539307,5.236870765686035,5.181940078735352],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"TEST_Fairness","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Fairness"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('da3316ba-bd52-4a57-b74f-87865597ec34');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="d3495bf7-f273-43ff-80f2-823e0b55c864" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("d3495bf7-f273-43ff-80f2-823e0b55c864")) {                    Plotly.newPlot(                        "d3495bf7-f273-43ff-80f2-823e0b55c864",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"K = 2","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.8830800652503967,0.5916022062301636,0.33052772283554077,0.17420940101146698,0.08201278746128082,0.030636807903647423,0.006536983884871006,0.00004571756289806217,0.009195481427013874,0.026158824563026428,0.0258320365101099,0.013921191915869713,0.005729077849537134,0.0022705404553562403,0.0009812356438487768,0.0004896706668660045,0.0002883391862269491,0.00020051181490998715,0.00016286334721371531,0.0001516860502306372,0.00015858456026762724,0.0001825550280045718,0.0002279401960549876,0.0003044571785721928,0.0004305345646571368,0.0006447871564887464,0.0010292807128280401,0.0017228370998054743,0.0031053561251610518,0.005602492485195398,0.009918371215462685,0.017498783767223358,0.03148967772722244,0.044068578630685806,0.04433117061853409,0.035844262689352036,0.027583854272961617,0.022086866199970245,0.018845921382308006,0.017160097137093544,0.016624996438622475,0.01704932563006878,0.018308350816369057,0.020142365247011185,0.022095991298556328,0.023524954915046692,0.024064771831035614,0.023715976625680923,0.023289408534765244,0.023501696065068245,0.024650676175951958,0.026626646518707275,0.028858385980129242,0.03019084967672825,0.029682325199246407,0.027693571522831917,0.02538483589887619,0.023612091317772865,0.022681573405861855,0.022518357262015343,0.02283663861453533,0.023213131353259087,0.02322200872004032,0.02265583910048008,0.021650604903697968,0.020601244643330574,0.019886506721377373,0.01971559412777424,0.02011249214410782,0.020960012450814247,0.021952712908387184,0.022652631625533104,0.022726882249116898,0.022214630618691444,0.0215261559933424,0.021148554980754852,0.02129710279405117,0.02178090624511242,0.022068334743380547,0.021686993539333344,0.02067955583333969,0.019593549892306328,0.018942764028906822,0.018797757104039192,0.018768634647130966,0.018379714339971542,0.01747860573232174,0.016448551788926125,0.015758657827973366,0.015516826882958412,0.015378368087112904,0.01484032440930605,0.013827823102474213,0.012832283042371273,0.012222768738865852,0.011761479079723358,0.010917303152382374,0.009583394974470139,0.008328210562467575,0.007489658892154694],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"K = 3","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[-0.04505476728081703,-0.12317916005849838,-0.25074848532676697,-0.42094025015830994,-0.5952214002609253,-0.44827666878700256,-0.6791709065437317,-0.699457049369812,-0.7740321755409241,-0.7100571393966675,-0.8604809641838074,-0.9186602830886841,-0.8175597786903381,-1.0650132894515991,-1.0965256690979004,-1.2437340021133423,-1.236858606338501,-1.4092029333114624,-1.58829927444458,-1.4179649353027344,-1.785778284072876,-1.858781099319458,-1.709326148033142,-2.059643268585205,-2.136965751647949,-1.8984341621398926,-2.2295894622802734,-2.3607683181762695,-2.115750551223755,-2.472599506378174,-2.5347015857696533,-2.589076042175293,-2.4054009914398193,-2.7201669216156006,-2.826763868331909,-2.818876028060913,-2.6852872371673584,-2.9800829887390137,-3.047032356262207,-2.8812761306762695,-2.955030918121338,-3.152071952819824,-3.102752447128296,-2.962574005126953,-3.1956419944763184,-3.248563051223755,-3.0956897735595703,-3.1877658367156982,-3.3190224170684814,-3.2329001426696777,-3.165844440460205,-3.319964647293091,-3.2935197353363037,-3.1728789806365967,-3.288273572921753,-3.3156542778015137,-3.2049922943115234,-3.2872681617736816,-3.313969135284424,-3.2209951877593994,-3.290989637374878,-3.305461883544922,-3.223959445953369,-3.298245668411255,-3.254356622695923,-3.231048583984375,-3.2817490100860596,-3.1864068508148193,-3.279831886291504,-3.1621084213256836,-3.2781641483306885,-3.1399261951446533,-3.2913424968719482,-3.117284059524536,-3.296456813812256,-3.1392033100128174,-3.2633793354034424,-3.1904804706573486,-3.2224721908569336,-3.2226216793060303,-3.185468912124634,-3.253333330154419,-3.1219265460968018,-3.321133852005005,-3.1164374351501465,-3.267655611038208,-3.210855007171631,-3.1908133029937744,-3.281791925430298,-3.109131336212158,-3.321354866027832,-3.1726412773132324,-3.192291021347046,-3.3182778358459473,-3.1511380672454834,-3.3015663623809814,-3.22025203704834,-3.1918368339538574,-3.3034965991973877,-3.1517956256866455],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"TEST_Discriminations","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Discrimination"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('d3495bf7-f273-43ff-80f2-823e0b55c864');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="254eaae7-0fd9-4032-a447-cca6d436db69" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("254eaae7-0fd9-4032-a447-cca6d436db69")) {                    Plotly.newPlot(                        "254eaae7-0fd9-4032-a447-cca6d436db69",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"K = 2","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[74.71599578857422,55.81466293334961,30.513710021972656,15.30838394165039,6.9843010902404785,2.806886911392212,1.0414294004440308,0.5802494883537292,1.0369493961334229,1.9045355319976807,1.93871009349823,1.3899850845336914,1.006516933441162,0.8656861782073975,0.8442887663841248,0.8656256794929504,0.897687554359436,0.9283936023712158,0.953396737575531,0.9713852405548096,0.9823570251464844,0.9869770407676697,0.9863834977149963,0.9821867942810059,0.9766786098480225,0.9737045168876648,0.9804942607879639,1.0079940557479858,1.0845390558242798,1.237630009651184,1.515739917755127,2.0199191570281982,2.9799435138702393,3.8587536811828613,3.8692870140075684,3.2605228424072266,2.6782054901123047,2.2937064170837402,2.0655272006988525,1.9425050020217896,1.895572543144226,1.9109855890274048,1.9802461862564087,2.086864471435547,2.202829122543335,2.287667989730835,2.3183207511901855,2.293004035949707,2.2616302967071533,2.268005609512329,2.3300399780273438,2.4403512477874756,2.562969446182251,2.6267974376678467,2.5756990909576416,2.434448003768921,2.275125026702881,2.1494970321655273,2.074951171875,2.0460093021392822,2.0452194213867188,2.0479767322540283,2.0303025245666504,1.9811755418777466,1.9088692665100098,1.8357270956039429,1.7824605703353882,1.7597360610961914,1.767928123474121,1.800114631652832,1.8395676612854004,1.8629604578018188,1.8529576063156128,1.8121869564056396,1.762442708015442,1.728750467300415,1.7215708494186401,1.7307288646697998,1.7293145656585693,1.6941523551940918,1.6282286643981934,1.5591351985931396,1.511826992034912,1.4891586303710938,1.4720326662063599,1.4380427598953247,1.3805245161056519,1.3178081512451172,1.2715510129928589,1.2463293075561523,1.2260783910751343,1.188044786453247,1.12920343875885,1.0719878673553467,1.0327482223510742,1.0006343126296997,0.9524244666099548,0.8839199542999268,0.8201422691345215,0.7755098938941956],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"K = 3","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[-2.619622230529785,-12.412161827087402,-32.67656707763672,-65.37784576416016,-117.1894760131836,-122.05814361572266,-216.1560516357422,-271.5600891113281,-370.4511413574219,-424.94451904296875,-1134.5303955078125,-2941.006591796875,-3031.30859375,-3979.88818359375,-4105.80322265625,-4701.6630859375,-4684.26904296875,-5305.041015625,-5978.521484375,-5359.619140625,-6704.6044921875,-6968.34375,-6439.53515625,-7714.24853515625,-8001.4169921875,-7145.80517578125,-8348.55078125,-8861.2333984375,-8001.77099609375,-9333.9091796875,-9560.38671875,-9809.625,-9158.3525390625,-10327.974609375,-10716.3486328125,-10720.2099609375,-10243.08984375,-11337.8974609375,-11596.0439453125,-11014.337890625,-11302.287109375,-12034.1396484375,-11878.1708984375,-11379.8310546875,-12253.5537109375,-12464.5966796875,-11932.4052734375,-12289.8056640625,-12786.806640625,-12491.1376953125,-12260.076171875,-12844.7744140625,-12763.185546875,-12339.4775390625,-12776.4453125,-12887.2734375,-12527.205078125,-12846.4619140625,-12960.4140625,-12634.6337890625,-12907.166015625,-12973.3720703125,-12719.3212890625,-12997.890625,-12899.212890625,-12865.9072265625,-13064.2392578125,-12781.72265625,-13098.4140625,-12816.078125,-13158.78515625,-12879.4375,-13263.3095703125,-12940.01171875,-13384.400390625,-13133.5224609375,-13442.271484375,-13364.5615234375,-13480.802734375,-13542.0361328125,-13493.1826171875,-13688.9560546875,-13424.2333984375,-13840.6005859375,-13561.0556640625,-13901.4091796875,-13826.359375,-13845.572265625,-14022.7607421875,-13813.099609375,-14165.57421875,-14059.7939453125,-14148.2001953125,-14278.3974609375,-14210.689453125,-14349.63671875,-14380.609375,-14381.6142578125,-14510.3583984375,-14385.3525390625],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"TEST_Losses","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Losses"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('254eaae7-0fd9-4032-a447-cca6d436db69');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
<section id="loss-accuracy-and-discrimination-for-model-with-discrimination-function" class="level4">
<h4 class="anchored" data-anchor-id="loss-accuracy-and-discrimination-for-model-with-discrimination-function">LOSS ACCURACY AND DISCRIMINATION FOR MODEL WITH DISCRIMINATION FUNCTION</h4>
<div id="1c2ba048" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_metric(title, y_label, train_data, test_data):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Train Line with Markers</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(go.Scatter(</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>epochs, y<span class="op">=</span>train_data, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">'Train'</span>,</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, width<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Test Line with Markers</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(go.Scatter(</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>epochs, y<span class="op">=</span>test_data, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">'Test'</span>,</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, width<span class="op">=</span><span class="dv">2</span>, dash<span class="op">=</span><span class="st">'dot'</span>),</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update Layout</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span>{<span class="st">'text'</span>: title, <span class="st">'y'</span>:<span class="fl">0.9</span>, <span class="st">'x'</span>:<span class="fl">0.5</span>, <span class="st">'xanchor'</span>: <span class="st">'center'</span>, <span class="st">'yanchor'</span>: <span class="st">'top'</span>},</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        xaxis_title<span class="op">=</span><span class="st">'Epoch'</span>,</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        yaxis_title<span class="op">=</span>y_label,</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        legend<span class="op">=</span><span class="bu">dict</span>(x<span class="op">=</span><span class="fl">0.1</span>, y<span class="op">=</span><span class="fl">1.1</span>, orientation<span class="op">=</span><span class="st">'h'</span>),</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(family<span class="op">=</span><span class="st">"Helvetica, Arial, sans-serif"</span>, size<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"black"</span>),</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        plot_bgcolor<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(l<span class="op">=</span><span class="dv">40</span>, r<span class="op">=</span><span class="dv">40</span>, t<span class="op">=</span><span class="dv">40</span>, b<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gridlines and Axes styles</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>    fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="32eb0b3c" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Loss"</span>, <span class="st">"Loss"</span>, train_losses, test_losses)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Accuracy"</span>, <span class="st">"Accuracy (%)"</span>, train_accuracies, test_accuracies)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Discrimination"</span>, <span class="st">"Discrimination"</span>, train_discriminations, test_discriminations)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Fairness"</span>, <span class="st">"Fairness"</span>, train_fairness, test_fairness)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="98e44e74-e855-4a51-8d8d-9bd83c996d4a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("98e44e74-e855-4a51-8d8d-9bd83c996d4a")) {                    Plotly.newPlot(                        "98e44e74-e855-4a51-8d8d-9bd83c996d4a",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[1.2005432844161987,1.1340562105178833,1.1161670684814453,1.074970006942749,0.9983471632003784,0.8670161962509155,0.6973254084587097,0.5200832486152649,0.45311859250068665,0.6443049907684326,0.6075366735458374,0.46437230706214905,0.426496684551239,0.4488235116004944,0.4745861291885376,0.48769083619117737,0.4854975640773773,0.47075119614601135,0.44868138432502747,0.4259135127067566,0.40939560532569885,0.4043589234352112,0.41090667247772217,0.4218834638595581,0.42714983224868774,0.42204299569129944,0.4105229377746582,0.39972370862960815,0.39392372965812683,0.3932155966758728,0.39529725909233093,0.39764776825904846,0.39840617775917053,0.39677801728248596,0.3932212293148041,0.3890628218650818,0.38574594259262085,0.3842768371105194,0.3846794664859772,0.3859330415725708,0.3865598142147064,0.385648638010025,0.38340896368026733,0.3808959424495697,0.3791574537754059,0.37859538197517395,0.3788890540599823,0.3793371915817261,0.3793098032474518,0.37855198979377747,0.3772430419921875,0.3758367896080017,0.37478238344192505,0.3742811977863312,0.3741959035396576,0.3741605877876282,0.3738515079021454,0.373189777135849,0.3723528981208801,0.3716050982475281,0.37110841274261475,0.3708418607711792,0.3706526458263397,0.3703800141811371,0.36995723843574524,0.36943379044532776,0.368927001953125,0.36853721737861633,0.36827659606933594,0.3680700659751892,0.36781787872314453,0.367470920085907,0.3670569658279419,0.3666563034057617,0.36632972955703735,0.3660777509212494,0.3658493757247925,0.3655897080898285,0.36527591943740845,0.36492496728897095,0.3645872175693512,0.3642946779727936,0.3640435039997101,0.3637964129447937,0.36352846026420593,0.36322951316833496,0.36291611194610596,0.36261463165283203,0.36233192682266235,0.36206239461898804,0.3617932200431824,0.3615138828754425,0.36122554540634155,0.3609369099140167,0.3606562912464142,0.36038607358932495,0.36013227701187134,0.35986828804016113,0.3595961928367615,0.35932740569114685],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[1.3504408597946167,1.2218055725097656,1.1219680309295654,1.0276302099227905,0.8889058232307434,0.714589536190033,0.5324214100837708,0.4716324508190155,0.7152367234230042,0.6685908436775208,0.4885546863079071,0.43772050738334656,0.45840921998023987,0.48420649766921997,0.4974011182785034,0.49500569701194763,0.47962817549705505,0.4566044211387634,0.43277832865715027,0.4155779778957367,0.41082775592803955,0.41864466667175293,0.4310532808303833,0.4365692138671875,0.4300886392593384,0.41647931933403015,0.40423160791397095,0.39808389544487,0.39776673913002014,0.4004814922809601,0.40331077575683594,0.40429383516311646,0.40266871452331543,0.3989834785461426,0.39452505111694336,0.3908197283744812,0.3890173137187958,0.38918423652648926,0.3902970850467682,0.3907865285873413,0.38967570662498474,0.3872186243534088,0.38457104563713074,0.38285356760025024,0.3824731111526489,0.3830282688140869,0.3837045729160309,0.38379067182540894,0.38300344347953796,0.3815461993217468,0.3799368441104889,0.378708153963089,0.3781189024448395,0.3780350387096405,0.37806984782218933,0.3778536319732666,0.37728315591812134,0.37653377652168274,0.375884473323822,0.3754950165748596,0.37533071637153625,0.37521469593048096,0.37496712803840637,0.37451714277267456,0.37393832206726074,0.3733743131160736,0.37293753027915955,0.3726685345172882,0.3724863529205322,0.37228330969810486,0.3719920516014099,0.3716362416744232,0.37129947543144226,0.37103888392448425,0.3708503842353821,0.3706786036491394,0.3704555928707123,0.3701593577861786,0.36981409788131714,0.3694836497306824,0.36920905113220215,0.36896538734436035,0.3687349259853363,0.3684976100921631,0.3682655692100525,0.3680484890937805,0.367868572473526,0.3677263557910919,0.36760184168815613,0.3674684762954712,0.36729779839515686,0.36708176136016846,0.36683353781700134,0.3665827810764313,0.3663380444049835,0.3661346137523651,0.36599403619766235,0.3659040331840515,0.3658422827720642,0.36578404903411865],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Loss","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Loss"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('98e44e74-e855-4a51-8d8d-9bd83c996d4a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="b44abc8c-a280-4b01-b5a3-89cdb172feb9" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b44abc8c-a280-4b01-b5a3-89cdb172feb9")) {                    Plotly.newPlot(                        "b44abc8c-a280-4b01-b5a3-89cdb172feb9",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[25.402715802192688,69.1803514957428,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,83.0148458480835,83.3938717842102,83.77289772033691,83.35438966751099,82.90429711341858,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,82.8727126121521,82.8885018825531,83.07011723518372,83.5912823677063,84.18351411819458,84.64150428771973,84.63360667228699,84.35723185539246,84.0255856513977,83.63866209983826,83.33860039710999,83.20435881614685,83.13329219818115,83.11749696731567,83.18856358528137,83.34649205207825,83.646559715271,83.96241068840027,84.26247835159302,84.72046852111816,84.90998148918152,85.02053022384644,84.98104810714722,85.0126326084137,84.9968433380127,84.83101725578308,84.7994327545166,84.64150428771973,84.54674482345581,84.53095555305481,84.57832932472229,84.70467329025269,84.84681248664856,84.88629460334778,85.0126326084137,84.97315049171448,84.9968433380127,84.98894572257996,85.02842783927917,84.97315049171448,84.96525287628174,84.91787910461426,84.88629460334778,84.82311964035034,84.8152220249176,84.82311964035034,84.84681248664856,84.93366837501526,84.941565990448,84.97315049171448,84.97315049171448,84.98104810714722,85.0126326084137,84.98104810714722,84.97315049171448,84.94946360588074,84.94946360588074,84.90208387374878,84.925776720047,85.0126326084137,85.02053022384644,84.9968433380127,85.03632545471191,85.07580757141113,85.09159684181213,85.0679099559784,85.09949445724487,85.12318134307861,85.0679099559784,85.07580757141113,85.0836992263794,85.12318134307861,85.10739207267761,85.10739207267761,85.12318134307861,85.14687418937683,85.14687418937683,85.15477180480957,85.14687418937683,85.17056107521057,85.17056107521057],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[68.03537607192993,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.75426626205444,82.59633779525757,83.35438966751099,82.97536373138428,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.65950679779053,82.691091299057,82.72267580032349,83.35438966751099,84.14403200149536,84.61781144142151,84.58622694015503,84.42829847335815,83.85975956916809,83.51231813430786,83.35438966751099,83.16487669944763,83.03853273391724,83.07011723518372,83.16487669944763,83.35438966751099,83.63866209983826,83.76500606536865,84.27037000656128,84.55464243888855,84.61781144142151,84.77573990821838,84.7441554069519,84.71257090568542,84.55464243888855,84.55464243888855,84.49147343635559,84.45988893508911,84.45988893508911,84.45988893508911,84.42829847335815,84.52305793762207,84.39671397209167,84.49147343635559,84.58622694015503,84.64940190315247,84.64940190315247,84.64940190315247,84.64940190315247,84.58622694015503,84.58622694015503,84.61781144142151,84.52305793762207,84.39671397209167,84.49147343635559,84.55464243888855,84.58622694015503,84.61781144142151,84.71257090568542,84.77573990821838,84.77573990821838,84.80733036994934,84.80733036994934,84.80733036994934,84.7441554069519,84.77573990821838,84.68098640441895,84.64940190315247,84.68098640441895,84.71257090568542,84.71257090568542,84.83891487121582,84.77573990821838,84.80733036994934,84.7441554069519,84.7441554069519,84.7441554069519,84.77573990821838,84.7441554069519,84.71257090568542,84.71257090568542,84.71257090568542,84.71257090568542,84.64940190315247,84.68098640441895,84.68098640441895,84.68098640441895,84.68098640441895,84.68098640441895,84.64940190315247,84.64940190315247,84.68098640441895],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Accuracy","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Accuracy (%)"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b44abc8c-a280-4b01-b5a3-89cdb172feb9');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="4303e094-be1a-4a0a-9e0a-bcf754d636b9" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("4303e094-be1a-4a0a-9e0a-bcf754d636b9")) {                    Plotly.newPlot(                        "4303e094-be1a-4a0a-9e0a-bcf754d636b9",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.014157179743051529,0.037128642201423645,0.011497516185045242,0.003039707662537694,0.000977194868028164,0.00035825211671181023,0.00007520290091633797,0.00022246502339839935,0.008226670324802399,0.03706512972712517,0.03349513188004494,0.013792199082672596,0.0034285150468349457,0.0007487634429708123,0.00017393939197063446,0.000040095881558954716,0.000012169022738817148,0.000014123769688012544,0.00005647099533234723,0.00024351027968805283,0.0008234671549871564,0.0021003808360546827,0.004053126089274883,0.005946801044046879,0.006790172308683395,0.006190710701048374,0.004645996727049351,0.002983218291774392,0.0017365551320835948,0.0010058180196210742,0.0006552800186909735,0.0005417151842266321,0.000589441682677716,0.0007881385972723365,0.0011707082157954574,0.0017822475638240576,0.0026481803506612778,0.0037144499365240335,0.004816039465367794,0.005684469826519489,0.006049678660929203,0.005799609236419201,0.005031323991715908,0.003995964303612709,0.0029617254622280598,0.0021109068766236305,0.0015142235206440091,0.001160685089416802,0.0010072262957692146,0.0010171417379751801,0.001170840347185731,0.0014550797641277313,0.0018409424228593707,0.0022692368365824223,0.002650295151397586,0.0028909845277667046,0.0029306598007678986,0.0027682885993272066,0.002458025934174657,0.002081372309476137,0.001714807585813105,0.0014091026969254017,0.001187182147987187,0.0010522634256631136,0.0009971718536689878,0.0010106867412105203,0.0010785114718601108,0.0011808876879513264,0.001290497020818293,0.001377397682517767,0.0014152934309095144,0.0013934531016275287,0.0013177713844925165,0.001209201873280108,0.0010942852823063731,0.0009946973295882344,0.0009235434117726982,0.0008892670157365501,0.0008920289692468941,0.0009252167074009776,0.0009782722918316722,0.001033667242154479,0.0010767201893031597,0.0010943880770355463,0.0010838788002729416,0.0010551849845796824,0.001012626220472157,0.0009748825104907155,0.0009541662293486297,0.0009516003774479032,0.0009608706459403038,0.000970585155300796,0.0009717100183479488,0.0009649298153817654,0.0009503709734417498,0.0009205529349856079,0.0008976419339887798,0.0009072819957509637,0.0009394118096679449,0.0009756212239153683],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.06081617996096611,0.0197057593613863,0.005324292927980423,0.0017148805782198906,0.000632383453194052,0.00015712952881585807,0.00021039014973212034,0.010705008171498775,0.05112812668085098,0.046006254851818085,0.018402203917503357,0.004386838525533676,0.0009370779152959585,0.00020888439030386508,0.0000443688259110786,0.000012954888006788678,0.000017027890862664208,0.00007318196003325284,0.00032303319312632084,0.0010984605178236961,0.0028028381057083607,0.005389069672673941,0.007870962843298912,0.008870854042470455,0.007894868962466717,0.005689942743629217,0.003438601503148675,0.001831222209148109,0.0009413770749233663,0.0005395567859522998,0.00040668746805749834,0.00043160252971574664,0.0006084764027036726,0.0009820341365411878,0.0015984605997800827,0.0024994472041726112,0.003634978085756302,0.004832973703742027,0.005801618564873934,0.006219923961907625,0.005942164920270443,0.005078119691461325,0.003922297153621912,0.002789432415738702,0.0018836656818166375,0.0012709356378763914,0.0009231196017935872,0.0007817264413461089,0.0008042200352065265,0.0009746323339641094,0.0012891972437500954,0.0017290347022935748,0.0022333788219839334,0.0027022473514080048,0.003020332893356681,0.003107474185526371,0.0029558660462498665,0.0026268502697348595,0.002218634355813265,0.001821950078010559,0.0014964420115575194,0.0012681250227615237,0.0011402207892388105,0.0011068442836403847,0.0011567104374989867,0.0012741395039483905,0.001432808581739664,0.00160060147754848,0.0017364883096888661,0.0018045335309579968,0.0017873478354886174,0.0016929487464949489,0.001551422756165266,0.001398298074491322,0.0012636117171496153,0.0011678335722535849,0.0011214592959731817,0.001124257338233292,0.0011673344997689128,0.0012361345579847693,0.0013086255639791489,0.001365308417007327,0.001389343524351716,0.0013772858073934913,0.001342118252068758,0.001289671054109931,0.0012446248438209295,0.0012231699656695127,0.0012274817563593388,0.001249900320544839,0.001275169081054628,0.0012913604732602835,0.0012964284978806973,0.0012913164682686329,0.0012646605027839541,0.0012472628150135279,0.0012744254199787974,0.0013320177095010877,0.0013930415734648705,0.0014270032988861203],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Discrimination","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Discrimination"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('4303e094-be1a-4a0a-9e0a-bcf754d636b9');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="66ac2653-9e7c-4df5-a01e-97cfa55554cb" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("66ac2653-9e7c-4df5-a01e-97cfa55554cb")) {                    Plotly.newPlot(                        "66ac2653-9e7c-4df5-a01e-97cfa55554cb",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9858428202569485,0.9628713577985764,0.9885024838149548,0.9969602923374623,0.9990228051319718,0.9996417478832882,0.9999247970990837,0.9997775349766016,0.9917733296751976,0.9629348702728748,0.9665048681199551,0.9862078009173274,0.996571484953165,0.9992512365570292,0.9998260606080294,0.999959904118441,0.9999878309772612,0.999985876230312,0.9999435290046677,0.999756489720312,0.9991765328450128,0.9978996191639453,0.9959468739107251,0.9940531989559531,0.9932098276913166,0.9938092892989516,0.9953540032729506,0.9970167817082256,0.9982634448679164,0.9989941819803789,0.999344719981309,0.9994582848157734,0.9994105583173223,0.9992118614027277,0.9988292917842045,0.9982177524361759,0.9973518196493387,0.996285550063476,0.9951839605346322,0.9943155301734805,0.9939503213390708,0.9942003907635808,0.9949686760082841,0.9960040356963873,0.9970382745377719,0.9978890931233764,0.998485776479356,0.9988393149105832,0.9989927737042308,0.9989828582620248,0.9988291596528143,0.9985449202358723,0.9981590575771406,0.9977307631634176,0.9973497048486024,0.9971090154722333,0.9970693401992321,0.9972317114006728,0.9975419740658253,0.9979186276905239,0.9982851924141869,0.9985908973030746,0.9988128178520128,0.9989477365743369,0.999002828146331,0.9989893132587895,0.9989214885281399,0.9988191123120487,0.9987095029791817,0.9986226023174822,0.9985847065690905,0.9986065468983725,0.9986822286155075,0.9987907981267199,0.9989057147176936,0.9990053026704118,0.9990764565882273,0.9991107329842634,0.9991079710307531,0.999074783292599,0.9990217277081683,0.9989663327578455,0.9989232798106968,0.9989056119229645,0.9989161211997271,0.9989448150154203,0.9989873737795278,0.9990251174895093,0.9990458337706514,0.9990483996225521,0.9990391293540597,0.9990294148446992,0.999028289981652,0.9990350701846182,0.9990496290265583,0.9990794470650144,0.9991023580660112,0.999092718004249,0.999060588190332,0.9990243787760846],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9858428202569485,0.9628713577985764,0.9885024838149548,0.9969602923374623,0.9990228051319718,0.9996417478832882,0.9999247970990837,0.9997775349766016,0.9917733296751976,0.9629348702728748,0.9665048681199551,0.9862078009173274,0.996571484953165,0.9992512365570292,0.9998260606080294,0.999959904118441,0.9999878309772612,0.999985876230312,0.9999435290046677,0.999756489720312,0.9991765328450128,0.9978996191639453,0.9959468739107251,0.9940531989559531,0.9932098276913166,0.9938092892989516,0.9953540032729506,0.9970167817082256,0.9982634448679164,0.9989941819803789,0.999344719981309,0.9994582848157734,0.9994105583173223,0.9992118614027277,0.9988292917842045,0.9982177524361759,0.9973518196493387,0.996285550063476,0.9951839605346322,0.9943155301734805,0.9939503213390708,0.9942003907635808,0.9949686760082841,0.9960040356963873,0.9970382745377719,0.9978890931233764,0.998485776479356,0.9988393149105832,0.9989927737042308,0.9989828582620248,0.9988291596528143,0.9985449202358723,0.9981590575771406,0.9977307631634176,0.9973497048486024,0.9971090154722333,0.9970693401992321,0.9972317114006728,0.9975419740658253,0.9979186276905239,0.9982851924141869,0.9985908973030746,0.9988128178520128,0.9989477365743369,0.999002828146331,0.9989893132587895,0.9989214885281399,0.9988191123120487,0.9987095029791817,0.9986226023174822,0.9985847065690905,0.9986065468983725,0.9986822286155075,0.9987907981267199,0.9989057147176936,0.9990053026704118,0.9990764565882273,0.9991107329842634,0.9991079710307531,0.999074783292599,0.9990217277081683,0.9989663327578455,0.9989232798106968,0.9989056119229645,0.9989161211997271,0.9989448150154203,0.9989873737795278,0.9990251174895093,0.9990458337706514,0.9990483996225521,0.9990391293540597,0.9990294148446992,0.999028289981652,0.9990350701846182,0.9990496290265583,0.9990794470650144,0.9991023580660112,0.999092718004249,0.999060588190332,0.9990243787760846],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Fairness","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Fairness"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('66ac2653-9e7c-4df5-a01e-97cfa55554cb');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
</section>
<section id="bce-loss-fn" class="level2">
<h2 class="anchored" data-anchor-id="bce-loss-fn">BCE Loss Fn</h2>
<div id="11061b64" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BinaryClassifier(nn.Module):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(BinaryClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, <span class="dv">64</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.fc2(x))</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.squeeze()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom loss function|</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discrimination_loss(output, target, sensitive_features, lambda_val<span class="op">=</span><span class="dv">100</span>, k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    standard_loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    mask_unpriv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">0</span>)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    mask_priv <span class="op">=</span> (sensitive_features <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#discrimination=torch.abs(prob_priv)</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    prob_unpriv <span class="op">=</span> torch.mean(output[mask_unpriv])</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    prob_priv <span class="op">=</span> torch.mean(output[mask_priv])</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    discrimination <span class="op">=</span> lambda_val<span class="op">*</span>(prob_priv <span class="op">-</span> prob_unpriv) <span class="op">**</span> k</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle cases where one group might be missing</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#discrimination=torch.abs(prob_priv)</span></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>    loss_val<span class="op">=</span>standard_loss</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_val,discrimination.item() </span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_accuracy(predictions, targets):</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> (predictions <span class="op">&gt;=</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (predicted_classes <span class="op">==</span> targets).<span class="bu">float</span>().mean()</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.tensor(X_train_scaled).<span class="bu">float</span>()</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor(y_train).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Correctly preparing the sensitive features</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Adjust the threshold according to your specific case</span></span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>sensitive_features <span class="op">=</span> torch.tensor((data[:, <span class="dv">1</span>].numpy() <span class="op">&gt;</span> threshold).astype(<span class="bu">float</span>)).<span class="bu">float</span>()</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> torch.cat((data[:, :<span class="dv">1</span>], data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming similar preparation for test data</span></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> torch.tensor(X_test_scaled).<span class="bu">float</span>()</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>test_targets <span class="op">=</span> torch.tensor(y_test).<span class="bu">float</span>().unsqueeze(<span class="dv">1</span>)</span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>test_sensitive_features <span class="op">=</span> torch.tensor((test_data[:, <span class="dv">1</span>].numpy() <span class="op">&gt;</span> threshold).astype(<span class="bu">float</span>)).<span class="bu">float</span>()</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> torch.cat((test_data[:, :<span class="dv">1</span>], test_data[:, <span class="dv">2</span>:]), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BinaryClassifier(features.shape[<span class="dv">1</span>])</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>train_losses1, train_accuracies1, train_discriminations1,train_fairness1 <span class="op">=</span> [], [], [],[]</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>test_losses1, test_accuracies1, test_discriminations1,test_fairness1 <span class="op">=</span> [], [], [],[]</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(features)</span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    loss, discrimination <span class="op">=</span> discrimination_loss(outputs, targets.squeeze(), sensitive_features)</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> calculate_accuracy(outputs, targets.squeeze())</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation on test data</span></span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>        test_outputs <span class="op">=</span> model(test_features)</span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a>        test_loss,test_discrimination <span class="op">=</span> discrimination_loss(test_outputs, test_targets.squeeze(), test_sensitive_features)</span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>        test_accuracy <span class="op">=</span> calculate_accuracy(test_outputs, test_targets.squeeze())</span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Train Acc: </span><span class="sc">{</span>train_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%, Train Discrimination: </span><span class="sc">{</span>discrimination<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%,Test Discrimination: </span><span class="sc">{</span>test_discrimination<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a>    train_losses1.append(loss.item())</span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a>    train_accuracies1.append(train_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>    train_discriminations1.append(discrimination)</span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>    fairness<span class="op">=</span><span class="dv">1</span><span class="op">-</span>discrimination</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>    train_fairness1.append(fairness)</span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a>    test_fairness1.append(fairness)</span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a>    test_losses1.append(test_loss.item())</span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>    test_accuracies1.append(test_accuracy.item() <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a>    test_discriminations1.append(test_discrimination)</span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a>    model.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Train Loss: 0.6134274005889893, Train Acc: 65.26%, Train Discrimination: 0.20013360679149628 Test Loss: 0.5357574224472046, Test Acc: 82.66%,Test Discrimination: 0.0808272510766983
Epoch 2, Train Loss: 0.5260337591171265, Train Acc: 82.87%, Train Discrimination: 0.03571147844195366 Test Loss: 0.4763486981391907, Test Acc: 79.06%,Test Discrimination: 0.8350032567977905
Epoch 3, Train Loss: 0.4802321195602417, Train Acc: 78.40%, Train Discrimination: 0.7649175524711609 Test Loss: 0.3763365149497986, Test Acc: 84.81%,Test Discrimination: 0.4385420083999634
Epoch 4, Train Loss: 0.37259721755981445, Train Acc: 85.47%, Train Discrimination: 0.4454280138015747 Test Loss: 0.42487236857414246, Test Acc: 83.51%,Test Discrimination: 0.12675483524799347
Epoch 5, Train Loss: 0.41500017046928406, Train Acc: 84.18%, Train Discrimination: 0.14125816524028778 Test Loss: 0.4300435185432434, Test Acc: 83.83%,Test Discrimination: 0.13620014488697052
Epoch 6, Train Loss: 0.4198794960975647, Train Acc: 84.37%, Train Discrimination: 0.15133412182331085 Test Loss: 0.3860330283641815, Test Acc: 84.59%,Test Discrimination: 0.35307714343070984
Epoch 7, Train Loss: 0.37966153025627136, Train Acc: 85.39%, Train Discrimination: 0.36894407868385315 Test Loss: 0.3782263398170471, Test Acc: 85.44%,Test Discrimination: 0.8565509915351868
Epoch 8, Train Loss: 0.37712201476097107, Train Acc: 84.88%, Train Discrimination: 0.8253031373023987 Test Loss: 0.39177942276000977, Test Acc: 84.71%,Test Discrimination: 1.1371583938598633
Epoch 9, Train Loss: 0.392727792263031, Train Acc: 84.16%, Train Discrimination: 1.0591684579849243 Test Loss: 0.374019593000412, Test Acc: 85.60%,Test Discrimination: 0.901528000831604
Epoch 10, Train Loss: 0.3726418614387512, Train Acc: 84.86%, Train Discrimination: 0.8526864647865295 Test Loss: 0.36674070358276367, Test Acc: 84.93%,Test Discrimination: 0.524904727935791
Epoch 11, Train Loss: 0.36137402057647705, Train Acc: 85.45%, Train Discrimination: 0.5123254060745239 Test Loss: 0.37892723083496094, Test Acc: 84.74%,Test Discrimination: 0.3088102638721466
Epoch 12, Train Loss: 0.3706624507904053, Train Acc: 85.40%, Train Discrimination: 0.3106633424758911 Test Loss: 0.383313924074173, Test Acc: 84.52%,Test Discrimination: 0.2505933940410614
Epoch 13, Train Loss: 0.37412750720977783, Train Acc: 85.19%, Train Discrimination: 0.25809013843536377 Test Loss: 0.37120336294174194, Test Acc: 84.87%,Test Discrimination: 0.3016584813594818
Epoch 14, Train Loss: 0.36291953921318054, Train Acc: 85.52%, Train Discrimination: 0.31302934885025024 Test Loss: 0.3574870824813843, Test Acc: 85.06%,Test Discrimination: 0.4648455083370209
Epoch 15, Train Loss: 0.3513195216655731, Train Acc: 85.57%, Train Discrimination: 0.47291168570518494 Test Loss: 0.35962268710136414, Test Acc: 86.01%,Test Discrimination: 0.7229820489883423
Epoch 16, Train Loss: 0.35604748129844666, Train Acc: 85.58%, Train Discrimination: 0.7065554857254028 Test Loss: 0.3586658239364624, Test Acc: 86.13%,Test Discrimination: 0.7388442754745483
Epoch 17, Train Loss: 0.3551360070705414, Train Acc: 85.60%, Train Discrimination: 0.7258914709091187 Test Loss: 0.35162365436553955, Test Acc: 85.38%,Test Discrimination: 0.5901193618774414
Epoch 18, Train Loss: 0.3467879593372345, Train Acc: 85.92%, Train Discrimination: 0.5994109511375427 Test Loss: 0.35068127512931824, Test Acc: 85.09%,Test Discrimination: 0.42984551191329956
Epoch 19, Train Loss: 0.34421810507774353, Train Acc: 85.97%, Train Discrimination: 0.4518979787826538 Test Loss: 0.3547644019126892, Test Acc: 85.00%,Test Discrimination: 0.3347969651222229
Epoch 20, Train Loss: 0.3472128212451935, Train Acc: 85.79%, Train Discrimination: 0.3611827790737152 Test Loss: 0.35629138350486755, Test Acc: 85.03%,Test Discrimination: 0.3151935338973999
Epoch 21, Train Loss: 0.3484920859336853, Train Acc: 85.72%, Train Discrimination: 0.34112751483917236 Test Loss: 0.3529544770717621, Test Acc: 85.09%,Test Discrimination: 0.3636103570461273
Epoch 22, Train Loss: 0.3457191586494446, Train Acc: 85.81%, Train Discrimination: 0.38558873534202576 Test Loss: 0.3492819666862488, Test Acc: 85.34%,Test Discrimination: 0.46737316250801086
Epoch 23, Train Loss: 0.3430730998516083, Train Acc: 86.09%, Train Discrimination: 0.47895610332489014 Test Loss: 0.3489447832107544, Test Acc: 85.82%,Test Discrimination: 0.5837351083755493
Epoch 24, Train Loss: 0.34372279047966003, Train Acc: 86.27%, Train Discrimination: 0.5795387029647827 Test Loss: 0.34917473793029785, Test Acc: 85.72%,Test Discrimination: 0.6414510607719421
Epoch 25, Train Loss: 0.34429749846458435, Train Acc: 86.21%, Train Discrimination: 0.6264647841453552 Test Loss: 0.34675297141075134, Test Acc: 85.91%,Test Discrimination: 0.6036818027496338
Epoch 26, Train Loss: 0.341439425945282, Train Acc: 86.36%, Train Discrimination: 0.5905063152313232 Test Loss: 0.34481555223464966, Test Acc: 85.47%,Test Discrimination: 0.5130152702331543
Epoch 27, Train Loss: 0.33858758211135864, Train Acc: 86.28%, Train Discrimination: 0.5076196193695068 Test Loss: 0.34553927183151245, Test Acc: 85.41%,Test Discrimination: 0.4365430772304535
Epoch 28, Train Loss: 0.3384379744529724, Train Acc: 86.01%, Train Discrimination: 0.43631723523139954 Test Loss: 0.346224308013916, Test Acc: 85.34%,Test Discrimination: 0.4126248359680176
Epoch 29, Train Loss: 0.3386895954608917, Train Acc: 85.94%, Train Discrimination: 0.41201263666152954 Test Loss: 0.3445855975151062, Test Acc: 85.44%,Test Discrimination: 0.450536847114563
Epoch 30, Train Loss: 0.33712905645370483, Train Acc: 85.98%, Train Discrimination: 0.44369468092918396 Test Loss: 0.34214985370635986, Test Acc: 85.63%,Test Discrimination: 0.543743908405304
Epoch 31, Train Loss: 0.33511975407600403, Train Acc: 86.11%, Train Discrimination: 0.5244483947753906 Test Loss: 0.3415783643722534, Test Acc: 85.85%,Test Discrimination: 0.6625670194625854
Epoch 32, Train Loss: 0.3349342942237854, Train Acc: 86.39%, Train Discrimination: 0.6257960200309753 Test Loss: 0.34182965755462646, Test Acc: 86.04%,Test Discrimination: 0.7388002276420593
Epoch 33, Train Loss: 0.33524447679519653, Train Acc: 86.44%, Train Discrimination: 0.6904962062835693 Test Loss: 0.3407096862792969, Test Acc: 85.82%,Test Discrimination: 0.7204099893569946
Epoch 34, Train Loss: 0.3336443305015564, Train Acc: 86.47%, Train Discrimination: 0.6745143532752991 Test Loss: 0.3401065468788147, Test Acc: 85.75%,Test Discrimination: 0.6455580592155457
Epoch 35, Train Loss: 0.3322693705558777, Train Acc: 86.20%, Train Discrimination: 0.6099650859832764 Test Loss: 0.34097856283187866, Test Acc: 85.72%,Test Discrimination: 0.5888910293579102
Epoch 36, Train Loss: 0.33245590329170227, Train Acc: 86.09%, Train Discrimination: 0.5606443881988525 Test Loss: 0.34085309505462646, Test Acc: 85.79%,Test Discrimination: 0.5950759649276733
Epoch 37, Train Loss: 0.331985205411911, Train Acc: 86.12%, Train Discrimination: 0.5664157271385193 Test Loss: 0.33939042687416077, Test Acc: 85.66%,Test Discrimination: 0.6699245572090149
Epoch 38, Train Loss: 0.3305213749408722, Train Acc: 86.27%, Train Discrimination: 0.6325047016143799 Test Loss: 0.33877697587013245, Test Acc: 85.69%,Test Discrimination: 0.7837302684783936
Epoch 39, Train Loss: 0.3301395773887634, Train Acc: 86.57%, Train Discrimination: 0.732053279876709 Test Loss: 0.33876025676727295, Test Acc: 85.82%,Test Discrimination: 0.8358525037765503
Epoch 40, Train Loss: 0.330069363117218, Train Acc: 86.58%, Train Discrimination: 0.7775329351425171 Test Loss: 0.3378388285636902, Test Acc: 85.60%,Test Discrimination: 0.7715796232223511
Epoch 41, Train Loss: 0.3285832107067108, Train Acc: 86.60%, Train Discrimination: 0.7240158319473267 Test Loss: 0.3378903567790985, Test Acc: 85.72%,Test Discrimination: 0.6841261982917786
Epoch 42, Train Loss: 0.328041672706604, Train Acc: 86.39%, Train Discrimination: 0.6490739583969116 Test Loss: 0.33789023756980896, Test Acc: 85.75%,Test Discrimination: 0.6556568741798401
Epoch 43, Train Loss: 0.3277396559715271, Train Acc: 86.31%, Train Discrimination: 0.625366747379303 Test Loss: 0.336744099855423, Test Acc: 85.85%,Test Discrimination: 0.6993946433067322
Epoch 44, Train Loss: 0.3265928030014038, Train Acc: 86.42%, Train Discrimination: 0.6651010513305664 Test Loss: 0.33588287234306335, Test Acc: 85.82%,Test Discrimination: 0.7834543585777283
Epoch 45, Train Loss: 0.3258735239505768, Train Acc: 86.59%, Train Discrimination: 0.74001544713974 Test Loss: 0.3356870412826538, Test Acc: 85.82%,Test Discrimination: 0.8347284197807312
Epoch 46, Train Loss: 0.3257128894329071, Train Acc: 86.61%, Train Discrimination: 0.7860814929008484 Test Loss: 0.33504873514175415, Test Acc: 85.79%,Test Discrimination: 0.8032159805297852
Epoch 47, Train Loss: 0.3248644471168518, Train Acc: 86.65%, Train Discrimination: 0.760148823261261 Test Loss: 0.3348771929740906, Test Acc: 85.75%,Test Discrimination: 0.739197313785553
Epoch 48, Train Loss: 0.3244439959526062, Train Acc: 86.61%, Train Discrimination: 0.707162618637085 Test Loss: 0.3348325788974762, Test Acc: 85.79%,Test Discrimination: 0.7204441428184509
Epoch 49, Train Loss: 0.3242218792438507, Train Acc: 86.58%, Train Discrimination: 0.6933344006538391 Test Loss: 0.33412808179855347, Test Acc: 85.69%,Test Discrimination: 0.764785885810852
Epoch 50, Train Loss: 0.32340049743652344, Train Acc: 86.68%, Train Discrimination: 0.7319745421409607 Test Loss: 0.33382081985473633, Test Acc: 85.88%,Test Discrimination: 0.8386752605438232
Epoch 51, Train Loss: 0.32294318079948425, Train Acc: 86.66%, Train Discrimination: 0.7953499555587769 Test Loss: 0.33381083607673645, Test Acc: 85.75%,Test Discrimination: 0.8667795062065125
Epoch 52, Train Loss: 0.32262516021728516, Train Acc: 86.66%, Train Discrimination: 0.8197501301765442 Test Loss: 0.3335072696208954, Test Acc: 85.82%,Test Discrimination: 0.8191007375717163
Epoch 53, Train Loss: 0.3218657672405243, Train Acc: 86.71%, Train Discrimination: 0.7800191044807434 Test Loss: 0.33373868465423584, Test Acc: 85.82%,Test Discrimination: 0.7730465531349182
Epoch 54, Train Loss: 0.32167235016822815, Train Acc: 86.62%, Train Discrimination: 0.7415419220924377 Test Loss: 0.3335752487182617, Test Acc: 85.85%,Test Discrimination: 0.7931897640228271
Epoch 55, Train Loss: 0.32116368412971497, Train Acc: 86.65%, Train Discrimination: 0.7588697075843811 Test Loss: 0.33333808183670044, Test Acc: 85.75%,Test Discrimination: 0.8682560324668884
Epoch 56, Train Loss: 0.3206660747528076, Train Acc: 86.68%, Train Discrimination: 0.8230858445167542 Test Loss: 0.3332976698875427, Test Acc: 85.63%,Test Discrimination: 0.9091278314590454
Epoch 57, Train Loss: 0.32037293910980225, Train Acc: 86.80%, Train Discrimination: 0.8586173057556152 Test Loss: 0.3328898251056671, Test Acc: 85.75%,Test Discrimination: 0.875327467918396
Epoch 58, Train Loss: 0.31973350048065186, Train Acc: 86.68%, Train Discrimination: 0.8303385376930237 Test Loss: 0.3328651785850525, Test Acc: 85.75%,Test Discrimination: 0.8477399945259094
Epoch 59, Train Loss: 0.3195272982120514, Train Acc: 86.73%, Train Discrimination: 0.8078646063804626 Test Loss: 0.33263829350471497, Test Acc: 85.69%,Test Discrimination: 0.8837615847587585
Epoch 60, Train Loss: 0.31903254985809326, Train Acc: 86.68%, Train Discrimination: 0.8380828499794006 Test Loss: 0.33269721269607544, Test Acc: 85.60%,Test Discrimination: 0.9491618871688843
Epoch 61, Train Loss: 0.31875187158584595, Train Acc: 86.81%, Train Discrimination: 0.8926598429679871 Test Loss: 0.33265718817710876, Test Acc: 85.63%,Test Discrimination: 0.9491920471191406
Epoch 62, Train Loss: 0.3183782398700714, Train Acc: 86.82%, Train Discrimination: 0.8919209241867065 Test Loss: 0.3325957953929901, Test Acc: 85.72%,Test Discrimination: 0.8965989351272583
Epoch 63, Train Loss: 0.31800737977027893, Train Acc: 86.82%, Train Discrimination: 0.8470218777656555 Test Loss: 0.33266204595565796, Test Acc: 85.72%,Test Discrimination: 0.8899616003036499
Epoch 64, Train Loss: 0.31767937541007996, Train Acc: 86.84%, Train Discrimination: 0.8414261341094971 Test Loss: 0.3326748311519623, Test Acc: 85.66%,Test Discrimination: 0.9413970708847046
Epoch 65, Train Loss: 0.3172518014907837, Train Acc: 86.84%, Train Discrimination: 0.8853862285614014 Test Loss: 0.33270540833473206, Test Acc: 85.63%,Test Discrimination: 0.9599632620811462
Epoch 66, Train Loss: 0.316932737827301, Train Acc: 86.90%, Train Discrimination: 0.903622031211853 Test Loss: 0.33247503638267517, Test Acc: 85.66%,Test Discrimination: 0.9254145622253418
Epoch 67, Train Loss: 0.31656256318092346, Train Acc: 86.84%, Train Discrimination: 0.8776929378509521 Test Loss: 0.3322347104549408, Test Acc: 85.69%,Test Discrimination: 0.9266924858093262
Epoch 68, Train Loss: 0.31618842482566833, Train Acc: 86.86%, Train Discrimination: 0.8789283037185669 Test Loss: 0.3320883810520172, Test Acc: 85.69%,Test Discrimination: 0.964611291885376
Epoch 69, Train Loss: 0.3158738613128662, Train Acc: 86.92%, Train Discrimination: 0.9090332388877869 Test Loss: 0.33181580901145935, Test Acc: 85.69%,Test Discrimination: 0.9556538462638855
Epoch 70, Train Loss: 0.3155333697795868, Train Acc: 86.93%, Train Discrimination: 0.9014745354652405 Test Loss: 0.3316538631916046, Test Acc: 85.66%,Test Discrimination: 0.9307590126991272
Epoch 71, Train Loss: 0.3152202367782593, Train Acc: 86.93%, Train Discrimination: 0.8821228742599487 Test Loss: 0.33174580335617065, Test Acc: 85.69%,Test Discrimination: 0.9565079808235168
Epoch 72, Train Loss: 0.31483298540115356, Train Acc: 86.97%, Train Discrimination: 0.9046382308006287 Test Loss: 0.3319389522075653, Test Acc: 85.72%,Test Discrimination: 0.9841709136962891
Epoch 73, Train Loss: 0.31455299258232117, Train Acc: 87.00%, Train Discrimination: 0.9280744194984436 Test Loss: 0.33193331956863403, Test Acc: 85.63%,Test Discrimination: 0.9458189010620117
Epoch 74, Train Loss: 0.31419095396995544, Train Acc: 87.04%, Train Discrimination: 0.8976611495018005 Test Loss: 0.33195894956588745, Test Acc: 85.60%,Test Discrimination: 0.9513502717018127
Epoch 75, Train Loss: 0.31384193897247314, Train Acc: 87.00%, Train Discrimination: 0.9000087976455688 Test Loss: 0.332019567489624, Test Acc: 85.60%,Test Discrimination: 0.9510520696640015
Epoch 76, Train Loss: 0.31356143951416016, Train Acc: 87.05%, Train Discrimination: 0.897897481918335 Test Loss: 0.33201152086257935, Test Acc: 85.63%,Test Discrimination: 0.9114383459091187
Epoch 77, Train Loss: 0.3133009076118469, Train Acc: 87.03%, Train Discrimination: 0.8665303587913513 Test Loss: 0.3321487009525299, Test Acc: 85.47%,Test Discrimination: 0.9976710081100464
Epoch 78, Train Loss: 0.31315895915031433, Train Acc: 87.09%, Train Discrimination: 0.9332451820373535 Test Loss: 0.3320115804672241, Test Acc: 85.69%,Test Discrimination: 0.9211483597755432
Epoch 79, Train Loss: 0.3128153085708618, Train Acc: 87.10%, Train Discrimination: 0.8740503191947937 Test Loss: 0.3317583203315735, Test Acc: 85.50%,Test Discrimination: 0.9704768657684326
Epoch 80, Train Loss: 0.31230083107948303, Train Acc: 87.07%, Train Discrimination: 0.9113655686378479 Test Loss: 0.33165737986564636, Test Acc: 85.60%,Test Discrimination: 0.9650099873542786
Epoch 81, Train Loss: 0.312034547328949, Train Acc: 87.07%, Train Discrimination: 0.907887876033783 Test Loss: 0.33169323205947876, Test Acc: 85.72%,Test Discrimination: 0.9206826686859131
Epoch 82, Train Loss: 0.31191548705101013, Train Acc: 87.11%, Train Discrimination: 0.8733884692192078 Test Loss: 0.33173856139183044, Test Acc: 85.47%,Test Discrimination: 1.0089668035507202
Epoch 83, Train Loss: 0.3116932511329651, Train Acc: 87.16%, Train Discrimination: 0.9426926374435425 Test Loss: 0.33162587881088257, Test Acc: 85.75%,Test Discrimination: 0.916898787021637
Epoch 84, Train Loss: 0.31130969524383545, Train Acc: 87.08%, Train Discrimination: 0.8692718148231506 Test Loss: 0.3314472734928131, Test Acc: 85.50%,Test Discrimination: 0.9674863219261169
Epoch 85, Train Loss: 0.3109159469604492, Train Acc: 87.12%, Train Discrimination: 0.9078733921051025 Test Loss: 0.33149778842926025, Test Acc: 85.50%,Test Discrimination: 0.9789692163467407
Epoch 86, Train Loss: 0.3106570243835449, Train Acc: 87.11%, Train Discrimination: 0.9184695482254028 Test Loss: 0.33172619342803955, Test Acc: 85.60%,Test Discrimination: 0.9028964042663574
Epoch 87, Train Loss: 0.310575932264328, Train Acc: 87.18%, Train Discrimination: 0.8618366718292236 Test Loss: 0.3316841721534729, Test Acc: 85.60%,Test Discrimination: 1.0036262273788452
Epoch 88, Train Loss: 0.3103819191455841, Train Acc: 87.22%, Train Discrimination: 0.9453766345977783 Test Loss: 0.33155685663223267, Test Acc: 85.50%,Test Discrimination: 0.9100217223167419
Epoch 89, Train Loss: 0.30996719002723694, Train Acc: 87.17%, Train Discrimination: 0.868437647819519 Test Loss: 0.3313591778278351, Test Acc: 85.50%,Test Discrimination: 0.9449762105941772
Epoch 90, Train Loss: 0.30959197878837585, Train Acc: 87.23%, Train Discrimination: 0.8959579467773438 Test Loss: 0.3312327563762665, Test Acc: 85.53%,Test Discrimination: 0.9720578789710999
Epoch 91, Train Loss: 0.30940482020378113, Train Acc: 87.26%, Train Discrimination: 0.9153538346290588 Test Loss: 0.3311755955219269, Test Acc: 85.47%,Test Discrimination: 0.906120777130127
Epoch 92, Train Loss: 0.3092677891254425, Train Acc: 87.17%, Train Discrimination: 0.8625436425209045 Test Loss: 0.33131617307662964, Test Acc: 85.47%,Test Discrimination: 0.9848475456237793
Epoch 93, Train Loss: 0.3090141713619232, Train Acc: 87.31%, Train Discrimination: 0.9256561994552612 Test Loss: 0.3314042389392853, Test Acc: 85.50%,Test Discrimination: 0.8987420201301575
Epoch 94, Train Loss: 0.30876991152763367, Train Acc: 87.20%, Train Discrimination: 0.8606014847755432 Test Loss: 0.33138471841812134, Test Acc: 85.50%,Test Discrimination: 0.9700459241867065
Epoch 95, Train Loss: 0.3084326684474945, Train Acc: 87.32%, Train Discrimination: 0.918921709060669 Test Loss: 0.3313281834125519, Test Acc: 85.53%,Test Discrimination: 0.9282905459403992
Epoch 96, Train Loss: 0.3081008791923523, Train Acc: 87.28%, Train Discrimination: 0.8856826424598694 Test Loss: 0.3313008248806, Test Acc: 85.53%,Test Discrimination: 0.937439501285553
Epoch 97, Train Loss: 0.30782637000083923, Train Acc: 87.30%, Train Discrimination: 0.8936178088188171 Test Loss: 0.3311961889266968, Test Acc: 85.50%,Test Discrimination: 0.9419636130332947
Epoch 98, Train Loss: 0.30757206678390503, Train Acc: 87.33%, Train Discrimination: 0.8969781994819641 Test Loss: 0.3311799168586731, Test Acc: 85.50%,Test Discrimination: 0.918307363986969
Epoch 99, Train Loss: 0.30734243988990784, Train Acc: 87.39%, Train Discrimination: 0.8787064552307129 Test Loss: 0.33124685287475586, Test Acc: 85.60%,Test Discrimination: 0.9741513729095459
Epoch 100, Train Loss: 0.30719390511512756, Train Acc: 87.35%, Train Discrimination: 0.9256513714790344 Test Loss: 0.33203598856925964, Test Acc: 85.60%,Test Discrimination: 0.8613821268081665</code></pre>
</div>
</div>
<div id="2a4745bb" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_metric(title, y_label, train_data, test_data, x_title <span class="op">=</span> <span class="st">"Epoch"</span>, epoch_blue <span class="op">=</span> <span class="st">"train"</span>, epoch_red <span class="op">=</span> <span class="st">"Test"</span>):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">101</span>))</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Train Line with Markers</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(go.Scatter(</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>epochs, y<span class="op">=</span>train_data, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span>epoch_blue,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, width<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'RoyalBlue'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding Test Line with Markers</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(go.Scatter(</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>epochs, y<span class="op">=</span>test_data, mode<span class="op">=</span><span class="st">'lines+markers'</span>,</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span>epoch_red,</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, width<span class="op">=</span><span class="dv">2</span>, dash<span class="op">=</span><span class="st">'dot'</span>),</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        marker<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'Crimson'</span>, size<span class="op">=</span><span class="dv">6</span>, line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'DarkSlateGrey'</span>))</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update Layout</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span>{<span class="st">'text'</span>: title, <span class="st">'y'</span>:<span class="fl">0.9</span>, <span class="st">'x'</span>:<span class="fl">0.5</span>, <span class="st">'xanchor'</span>: <span class="st">'center'</span>, <span class="st">'yanchor'</span>: <span class="st">'top'</span>},</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        xaxis_title<span class="op">=</span>x_title,</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        yaxis_title<span class="op">=</span>y_label,</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        legend<span class="op">=</span><span class="bu">dict</span>(x<span class="op">=</span><span class="fl">0.1</span>, y<span class="op">=</span><span class="fl">1.1</span>, orientation<span class="op">=</span><span class="st">'h'</span>),</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(family<span class="op">=</span><span class="st">"Helvetica, Arial, sans-serif"</span>, size<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"black"</span>),</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        plot_bgcolor<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(l<span class="op">=</span><span class="dv">40</span>, r<span class="op">=</span><span class="dv">40</span>, t<span class="op">=</span><span class="dv">40</span>, b<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gridlines and Axes styles</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(showline<span class="op">=</span><span class="va">True</span>, linewidth<span class="op">=</span><span class="dv">2</span>, linecolor<span class="op">=</span><span class="st">'black'</span>, gridcolor<span class="op">=</span><span class="st">'LightGrey'</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Loss"</span>, <span class="st">"Loss"</span>, train_losses1, test_losses1)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Accuracy"</span>, <span class="st">"Accuracy (%)"</span>, train_accuracies1, test_accuracies1)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Discrimination"</span>, <span class="st">"Discrimination"</span>, train_discriminations1, test_discriminations1)</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"Training and Testing Fairness"</span>, <span class="st">"Fairness"</span>, train_fairness1, test_fairness1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="4076918a-79ee-4ddb-bb41-b143008788c8" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("4076918a-79ee-4ddb-bb41-b143008788c8")) {                    Plotly.newPlot(                        "4076918a-79ee-4ddb-bb41-b143008788c8",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.6134274005889893,0.5260337591171265,0.4802321195602417,0.37259721755981445,0.41500017046928406,0.4198794960975647,0.37966153025627136,0.37712201476097107,0.392727792263031,0.3726418614387512,0.36137402057647705,0.3706624507904053,0.37412750720977783,0.36291953921318054,0.3513195216655731,0.35604748129844666,0.3551360070705414,0.3467879593372345,0.34421810507774353,0.3472128212451935,0.3484920859336853,0.3457191586494446,0.3430730998516083,0.34372279047966003,0.34429749846458435,0.341439425945282,0.33858758211135864,0.3384379744529724,0.3386895954608917,0.33712905645370483,0.33511975407600403,0.3349342942237854,0.33524447679519653,0.3336443305015564,0.3322693705558777,0.33245590329170227,0.331985205411911,0.3305213749408722,0.3301395773887634,0.330069363117218,0.3285832107067108,0.328041672706604,0.3277396559715271,0.3265928030014038,0.3258735239505768,0.3257128894329071,0.3248644471168518,0.3244439959526062,0.3242218792438507,0.32340049743652344,0.32294318079948425,0.32262516021728516,0.3218657672405243,0.32167235016822815,0.32116368412971497,0.3206660747528076,0.32037293910980225,0.31973350048065186,0.3195272982120514,0.31903254985809326,0.31875187158584595,0.3183782398700714,0.31800737977027893,0.31767937541007996,0.3172518014907837,0.316932737827301,0.31656256318092346,0.31618842482566833,0.3158738613128662,0.3155333697795868,0.3152202367782593,0.31483298540115356,0.31455299258232117,0.31419095396995544,0.31384193897247314,0.31356143951416016,0.3133009076118469,0.31315895915031433,0.3128153085708618,0.31230083107948303,0.312034547328949,0.31191548705101013,0.3116932511329651,0.31130969524383545,0.3109159469604492,0.3106570243835449,0.310575932264328,0.3103819191455841,0.30996719002723694,0.30959197878837585,0.30940482020378113,0.3092677891254425,0.3090141713619232,0.30876991152763367,0.3084326684474945,0.3081008791923523,0.30782637000083923,0.30757206678390503,0.30734243988990784,0.30719390511512756],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.5357574224472046,0.4763486981391907,0.3763365149497986,0.42487236857414246,0.4300435185432434,0.3860330283641815,0.3782263398170471,0.39177942276000977,0.374019593000412,0.36674070358276367,0.37892723083496094,0.383313924074173,0.37120336294174194,0.3574870824813843,0.35962268710136414,0.3586658239364624,0.35162365436553955,0.35068127512931824,0.3547644019126892,0.35629138350486755,0.3529544770717621,0.3492819666862488,0.3489447832107544,0.34917473793029785,0.34675297141075134,0.34481555223464966,0.34553927183151245,0.346224308013916,0.3445855975151062,0.34214985370635986,0.3415783643722534,0.34182965755462646,0.3407096862792969,0.3401065468788147,0.34097856283187866,0.34085309505462646,0.33939042687416077,0.33877697587013245,0.33876025676727295,0.3378388285636902,0.3378903567790985,0.33789023756980896,0.336744099855423,0.33588287234306335,0.3356870412826538,0.33504873514175415,0.3348771929740906,0.3348325788974762,0.33412808179855347,0.33382081985473633,0.33381083607673645,0.3335072696208954,0.33373868465423584,0.3335752487182617,0.33333808183670044,0.3332976698875427,0.3328898251056671,0.3328651785850525,0.33263829350471497,0.33269721269607544,0.33265718817710876,0.3325957953929901,0.33266204595565796,0.3326748311519623,0.33270540833473206,0.33247503638267517,0.3322347104549408,0.3320883810520172,0.33181580901145935,0.3316538631916046,0.33174580335617065,0.3319389522075653,0.33193331956863403,0.33195894956588745,0.332019567489624,0.33201152086257935,0.3321487009525299,0.3320115804672241,0.3317583203315735,0.33165737986564636,0.33169323205947876,0.33173856139183044,0.33162587881088257,0.3314472734928131,0.33149778842926025,0.33172619342803955,0.3316841721534729,0.33155685663223267,0.3313591778278351,0.3312327563762665,0.3311755955219269,0.33131617307662964,0.3314042389392853,0.33138471841812134,0.3313281834125519,0.3313008248806,0.3311961889266968,0.3311799168586731,0.33124685287475586,0.33203598856925964],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Loss","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Loss"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('4076918a-79ee-4ddb-bb41-b143008788c8');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="ba0c1dee-902e-4fef-8725-0e1e38563d71" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ba0c1dee-902e-4fef-8725-0e1e38563d71")) {                    Plotly.newPlot(                        "ba0c1dee-902e-4fef-8725-0e1e38563d71",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[65.2558445930481,82.8727126121521,78.39545011520386,85.47062277793884,84.17561650276184,84.3651294708252,85.3916585445404,84.87839698791504,84.15982127189636,84.86260175704956,85.4469358921051,85.39955615997314,85.19425392150879,85.5180025100708,85.56538224220276,85.58117747306824,85.60486435890198,85.92072129249573,85.96810102462769,85.78647971153259,85.7154130935669,85.81017255783081,86.0944390296936,86.26816272735596,86.21288537979126,86.35502457618713,86.28395199775696,86.0075831413269,85.93651056289673,85.97599267959595,86.11023426055908,86.38660907745361,86.44188046455383,86.46557331085205,86.20498776435852,86.0944390296936,86.11813187599182,86.26816272735596,86.56822443008423,86.57612204551697,86.59980893135071,86.38660907745361,86.30764484405518,86.41819357872009,86.59191131591797,86.60770654678345,86.64718866348267,86.60770654678345,86.57612204551697,86.67877316474915,86.6550862789154,86.66298389434814,86.71035766601562,86.61560416221619,86.64718866348267,86.67877316474915,86.7972195148468,86.67877316474915,86.7261528968811,86.67877316474915,86.80511713027954,86.82091236114502,86.82091236114502,86.84459924697876,86.83670163154602,86.89987659454346,86.83670163154602,86.86039447784424,86.9235634803772,86.93146109580994,86.93146109580994,86.97094321250916,87.00252771377563,87.04200983047485,87.00252771377563,87.04990744590759,87.03411221504211,87.08938956260681,87.09728121757507,87.0656967163086,87.0656967163086,87.11307644844055,87.16045618057251,87.08149194717407,87.12097406387329,87.10517883300781,87.18414306640625,87.21572756767273,87.16835379600525,87.23152279853821,87.26310729980469,87.16835379600525,87.31048703193665,87.19993829727173,87.31838464736938,87.27890253067017,87.3025894165039,87.32627630233765,87.38945126533508,87.34996914863586],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[82.65950679779053,79.05874848365784,84.80733036994934,83.51231813430786,83.82817506790161,84.58622694015503,85.43903827667236,84.71257090568542,85.59696674346924,84.93366837501526,84.7441554069519,84.52305793762207,84.8704993724823,85.06001234054565,86.0075831413269,86.13392114639282,85.3758692741394,85.09159684181213,84.9968433380127,85.02842783927917,85.09159684181213,85.34428477287292,85.81806421279907,85.72331070899963,85.91282367706299,85.47062277793884,85.40745377540588,85.34428477287292,85.43903827667236,85.62855124473572,85.84965467453003,86.03916764259338,85.81806421279907,85.75489521026611,85.72331070899963,85.78647971153259,85.66014170646667,85.69172620773315,85.81806421279907,85.59696674346924,85.72331070899963,85.75489521026611,85.84965467453003,85.81806421279907,85.81806421279907,85.78647971153259,85.75489521026611,85.78647971153259,85.69172620773315,85.88123917579651,85.75489521026611,85.81806421279907,85.81806421279907,85.84965467453003,85.75489521026611,85.62855124473572,85.75489521026611,85.75489521026611,85.69172620773315,85.59696674346924,85.62855124473572,85.72331070899963,85.72331070899963,85.66014170646667,85.62855124473572,85.66014170646667,85.69172620773315,85.69172620773315,85.69172620773315,85.66014170646667,85.69172620773315,85.72331070899963,85.62855124473572,85.59696674346924,85.59696674346924,85.62855124473572,85.47062277793884,85.69172620773315,85.5022132396698,85.59696674346924,85.72331070899963,85.47062277793884,85.75489521026611,85.5022132396698,85.5022132396698,85.59696674346924,85.59696674346924,85.5022132396698,85.5022132396698,85.53379774093628,85.47062277793884,85.47062277793884,85.5022132396698,85.5022132396698,85.53379774093628,85.53379774093628,85.5022132396698,85.5022132396698,85.59696674346924,85.59696674346924],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Accuracy","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Accuracy (%)"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ba0c1dee-902e-4fef-8725-0e1e38563d71');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="a2ed4522-a3e0-4f22-a998-185d4a3dd3b1" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("a2ed4522-a3e0-4f22-a998-185d4a3dd3b1")) {                    Plotly.newPlot(                        "a2ed4522-a3e0-4f22-a998-185d4a3dd3b1",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.20013360679149628,0.03571147844195366,0.7649175524711609,0.4454280138015747,0.14125816524028778,0.15133412182331085,0.36894407868385315,0.8253031373023987,1.0591684579849243,0.8526864647865295,0.5123254060745239,0.3106633424758911,0.25809013843536377,0.31302934885025024,0.47291168570518494,0.7065554857254028,0.7258914709091187,0.5994109511375427,0.4518979787826538,0.3611827790737152,0.34112751483917236,0.38558873534202576,0.47895610332489014,0.5795387029647827,0.6264647841453552,0.5905063152313232,0.5076196193695068,0.43631723523139954,0.41201263666152954,0.44369468092918396,0.5244483947753906,0.6257960200309753,0.6904962062835693,0.6745143532752991,0.6099650859832764,0.5606443881988525,0.5664157271385193,0.6325047016143799,0.732053279876709,0.7775329351425171,0.7240158319473267,0.6490739583969116,0.625366747379303,0.6651010513305664,0.74001544713974,0.7860814929008484,0.760148823261261,0.707162618637085,0.6933344006538391,0.7319745421409607,0.7953499555587769,0.8197501301765442,0.7800191044807434,0.7415419220924377,0.7588697075843811,0.8230858445167542,0.8586173057556152,0.8303385376930237,0.8078646063804626,0.8380828499794006,0.8926598429679871,0.8919209241867065,0.8470218777656555,0.8414261341094971,0.8853862285614014,0.903622031211853,0.8776929378509521,0.8789283037185669,0.9090332388877869,0.9014745354652405,0.8821228742599487,0.9046382308006287,0.9280744194984436,0.8976611495018005,0.9000087976455688,0.897897481918335,0.8665303587913513,0.9332451820373535,0.8740503191947937,0.9113655686378479,0.907887876033783,0.8733884692192078,0.9426926374435425,0.8692718148231506,0.9078733921051025,0.9184695482254028,0.8618366718292236,0.9453766345977783,0.868437647819519,0.8959579467773438,0.9153538346290588,0.8625436425209045,0.9256561994552612,0.8606014847755432,0.918921709060669,0.8856826424598694,0.8936178088188171,0.8969781994819641,0.8787064552307129,0.9256513714790344],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.0808272510766983,0.8350032567977905,0.4385420083999634,0.12675483524799347,0.13620014488697052,0.35307714343070984,0.8565509915351868,1.1371583938598633,0.901528000831604,0.524904727935791,0.3088102638721466,0.2505933940410614,0.3016584813594818,0.4648455083370209,0.7229820489883423,0.7388442754745483,0.5901193618774414,0.42984551191329956,0.3347969651222229,0.3151935338973999,0.3636103570461273,0.46737316250801086,0.5837351083755493,0.6414510607719421,0.6036818027496338,0.5130152702331543,0.4365430772304535,0.4126248359680176,0.450536847114563,0.543743908405304,0.6625670194625854,0.7388002276420593,0.7204099893569946,0.6455580592155457,0.5888910293579102,0.5950759649276733,0.6699245572090149,0.7837302684783936,0.8358525037765503,0.7715796232223511,0.6841261982917786,0.6556568741798401,0.6993946433067322,0.7834543585777283,0.8347284197807312,0.8032159805297852,0.739197313785553,0.7204441428184509,0.764785885810852,0.8386752605438232,0.8667795062065125,0.8191007375717163,0.7730465531349182,0.7931897640228271,0.8682560324668884,0.9091278314590454,0.875327467918396,0.8477399945259094,0.8837615847587585,0.9491618871688843,0.9491920471191406,0.8965989351272583,0.8899616003036499,0.9413970708847046,0.9599632620811462,0.9254145622253418,0.9266924858093262,0.964611291885376,0.9556538462638855,0.9307590126991272,0.9565079808235168,0.9841709136962891,0.9458189010620117,0.9513502717018127,0.9510520696640015,0.9114383459091187,0.9976710081100464,0.9211483597755432,0.9704768657684326,0.9650099873542786,0.9206826686859131,1.0089668035507202,0.916898787021637,0.9674863219261169,0.9789692163467407,0.9028964042663574,1.0036262273788452,0.9100217223167419,0.9449762105941772,0.9720578789710999,0.906120777130127,0.9848475456237793,0.8987420201301575,0.9700459241867065,0.9282905459403992,0.937439501285553,0.9419636130332947,0.918307363986969,0.9741513729095459,0.8613821268081665],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Discrimination","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Discrimination"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('a2ed4522-a3e0-4f22-a998-185d4a3dd3b1');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="b1499c27-23fc-4b09-8b40-52f62cb91911" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b1499c27-23fc-4b09-8b40-52f62cb91911")) {                    Plotly.newPlot(                        "b1499c27-23fc-4b09-8b40-52f62cb91911",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"train","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.7998663932085037,0.9642885215580463,0.2350824475288391,0.5545719861984253,0.8587418347597122,0.8486658781766891,0.6310559213161469,0.17469686269760132,-0.059168457984924316,0.14731353521347046,0.4876745939254761,0.6893366575241089,0.7419098615646362,0.6869706511497498,0.5270883142948151,0.29344451427459717,0.27410852909088135,0.4005890488624573,0.5481020212173462,0.6388172209262848,0.6588724851608276,0.6144112646579742,0.5210438966751099,0.4204612970352173,0.3735352158546448,0.40949368476867676,0.49238038063049316,0.5636827647686005,0.5879873633384705,0.556305319070816,0.4755516052246094,0.37420397996902466,0.30950379371643066,0.3254856467247009,0.39003491401672363,0.43935561180114746,0.4335842728614807,0.3674952983856201,0.267946720123291,0.2224670648574829,0.27598416805267334,0.3509260416030884,0.374633252620697,0.3348989486694336,0.25998455286026,0.2139185070991516,0.239851176738739,0.29283738136291504,0.3066655993461609,0.2680254578590393,0.20465004444122314,0.1802498698234558,0.2199808955192566,0.25845807790756226,0.2411302924156189,0.17691415548324585,0.14138269424438477,0.16966146230697632,0.19213539361953735,0.16191715002059937,0.10734015703201294,0.10807907581329346,0.15297812223434448,0.15857386589050293,0.11461377143859863,0.09637796878814697,0.12230706214904785,0.1210716962814331,0.09096676111221313,0.09852546453475952,0.11787712574005127,0.09536176919937134,0.0719255805015564,0.10233885049819946,0.09999120235443115,0.10210251808166504,0.13346964120864868,0.06675481796264648,0.1259496808052063,0.0886344313621521,0.09211212396621704,0.12661153078079224,0.05730736255645752,0.13072818517684937,0.09212660789489746,0.08153045177459717,0.13816332817077637,0.05462336540222168,0.13156235218048096,0.10404205322265625,0.08464616537094116,0.13745635747909546,0.07434380054473877,0.1393985152244568,0.08107829093933105,0.11431735754013062,0.10638219118118286,0.10302180051803589,0.12129354476928711,0.07434862852096558],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"Test","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.7998663932085037,0.9642885215580463,0.2350824475288391,0.5545719861984253,0.8587418347597122,0.8486658781766891,0.6310559213161469,0.17469686269760132,-0.059168457984924316,0.14731353521347046,0.4876745939254761,0.6893366575241089,0.7419098615646362,0.6869706511497498,0.5270883142948151,0.29344451427459717,0.27410852909088135,0.4005890488624573,0.5481020212173462,0.6388172209262848,0.6588724851608276,0.6144112646579742,0.5210438966751099,0.4204612970352173,0.3735352158546448,0.40949368476867676,0.49238038063049316,0.5636827647686005,0.5879873633384705,0.556305319070816,0.4755516052246094,0.37420397996902466,0.30950379371643066,0.3254856467247009,0.39003491401672363,0.43935561180114746,0.4335842728614807,0.3674952983856201,0.267946720123291,0.2224670648574829,0.27598416805267334,0.3509260416030884,0.374633252620697,0.3348989486694336,0.25998455286026,0.2139185070991516,0.239851176738739,0.29283738136291504,0.3066655993461609,0.2680254578590393,0.20465004444122314,0.1802498698234558,0.2199808955192566,0.25845807790756226,0.2411302924156189,0.17691415548324585,0.14138269424438477,0.16966146230697632,0.19213539361953735,0.16191715002059937,0.10734015703201294,0.10807907581329346,0.15297812223434448,0.15857386589050293,0.11461377143859863,0.09637796878814697,0.12230706214904785,0.1210716962814331,0.09096676111221313,0.09852546453475952,0.11787712574005127,0.09536176919937134,0.0719255805015564,0.10233885049819946,0.09999120235443115,0.10210251808166504,0.13346964120864868,0.06675481796264648,0.1259496808052063,0.0886344313621521,0.09211212396621704,0.12661153078079224,0.05730736255645752,0.13072818517684937,0.09212660789489746,0.08153045177459717,0.13816332817077637,0.05462336540222168,0.13156235218048096,0.10404205322265625,0.08464616537094116,0.13745635747909546,0.07434380054473877,0.1393985152244568,0.08107829093933105,0.11431735754013062,0.10638219118118286,0.10302180051803589,0.12129354476928711,0.07434862852096558],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Training and Testing Fairness","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"Epoch"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"Fairness"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b1499c27-23fc-4b09-8b40-52f62cb91911');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<div id="3faf559c" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>plot_metric(<span class="st">"DISPARITY IN FAIRNESS"</span>, <span class="st">"FAIRNESS"</span>, test_fairness, test_fairness1, <span class="st">"EPOCH"</span>, <span class="st">"WITH DISCRIMINATION FUNCTION"</span>, <span class="st">"BCE"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="ff727ae7-18e5-4dcb-882e-057dadc99e7b" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ff727ae7-18e5-4dcb-882e-057dadc99e7b")) {                    Plotly.newPlot(                        "ff727ae7-18e5-4dcb-882e-057dadc99e7b",                        [{"line":{"color":"RoyalBlue","width":2},"marker":{"color":"RoyalBlue","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"WITH DISCRIMINATION FUNCTION","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.9858428202569485,0.9628713577985764,0.9885024838149548,0.9969602923374623,0.9990228051319718,0.9996417478832882,0.9999247970990837,0.9997775349766016,0.9917733296751976,0.9629348702728748,0.9665048681199551,0.9862078009173274,0.996571484953165,0.9992512365570292,0.9998260606080294,0.999959904118441,0.9999878309772612,0.999985876230312,0.9999435290046677,0.999756489720312,0.9991765328450128,0.9978996191639453,0.9959468739107251,0.9940531989559531,0.9932098276913166,0.9938092892989516,0.9953540032729506,0.9970167817082256,0.9982634448679164,0.9989941819803789,0.999344719981309,0.9994582848157734,0.9994105583173223,0.9992118614027277,0.9988292917842045,0.9982177524361759,0.9973518196493387,0.996285550063476,0.9951839605346322,0.9943155301734805,0.9939503213390708,0.9942003907635808,0.9949686760082841,0.9960040356963873,0.9970382745377719,0.9978890931233764,0.998485776479356,0.9988393149105832,0.9989927737042308,0.9989828582620248,0.9988291596528143,0.9985449202358723,0.9981590575771406,0.9977307631634176,0.9973497048486024,0.9971090154722333,0.9970693401992321,0.9972317114006728,0.9975419740658253,0.9979186276905239,0.9982851924141869,0.9985908973030746,0.9988128178520128,0.9989477365743369,0.999002828146331,0.9989893132587895,0.9989214885281399,0.9988191123120487,0.9987095029791817,0.9986226023174822,0.9985847065690905,0.9986065468983725,0.9986822286155075,0.9987907981267199,0.9989057147176936,0.9990053026704118,0.9990764565882273,0.9991107329842634,0.9991079710307531,0.999074783292599,0.9990217277081683,0.9989663327578455,0.9989232798106968,0.9989056119229645,0.9989161211997271,0.9989448150154203,0.9989873737795278,0.9990251174895093,0.9990458337706514,0.9990483996225521,0.9990391293540597,0.9990294148446992,0.999028289981652,0.9990350701846182,0.9990496290265583,0.9990794470650144,0.9991023580660112,0.999092718004249,0.999060588190332,0.9990243787760846],"type":"scatter"},{"line":{"color":"Crimson","dash":"dot","width":2},"marker":{"color":"Crimson","line":{"color":"DarkSlateGrey","width":1},"size":6},"mode":"lines+markers","name":"BCE","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[0.7998663932085037,0.9642885215580463,0.2350824475288391,0.5545719861984253,0.8587418347597122,0.8486658781766891,0.6310559213161469,0.17469686269760132,-0.059168457984924316,0.14731353521347046,0.4876745939254761,0.6893366575241089,0.7419098615646362,0.6869706511497498,0.5270883142948151,0.29344451427459717,0.27410852909088135,0.4005890488624573,0.5481020212173462,0.6388172209262848,0.6588724851608276,0.6144112646579742,0.5210438966751099,0.4204612970352173,0.3735352158546448,0.40949368476867676,0.49238038063049316,0.5636827647686005,0.5879873633384705,0.556305319070816,0.4755516052246094,0.37420397996902466,0.30950379371643066,0.3254856467247009,0.39003491401672363,0.43935561180114746,0.4335842728614807,0.3674952983856201,0.267946720123291,0.2224670648574829,0.27598416805267334,0.3509260416030884,0.374633252620697,0.3348989486694336,0.25998455286026,0.2139185070991516,0.239851176738739,0.29283738136291504,0.3066655993461609,0.2680254578590393,0.20465004444122314,0.1802498698234558,0.2199808955192566,0.25845807790756226,0.2411302924156189,0.17691415548324585,0.14138269424438477,0.16966146230697632,0.19213539361953735,0.16191715002059937,0.10734015703201294,0.10807907581329346,0.15297812223434448,0.15857386589050293,0.11461377143859863,0.09637796878814697,0.12230706214904785,0.1210716962814331,0.09096676111221313,0.09852546453475952,0.11787712574005127,0.09536176919937134,0.0719255805015564,0.10233885049819946,0.09999120235443115,0.10210251808166504,0.13346964120864868,0.06675481796264648,0.1259496808052063,0.0886344313621521,0.09211212396621704,0.12661153078079224,0.05730736255645752,0.13072818517684937,0.09212660789489746,0.08153045177459717,0.13816332817077637,0.05462336540222168,0.13156235218048096,0.10404205322265625,0.08464616537094116,0.13745635747909546,0.07434380054473877,0.1393985152244568,0.08107829093933105,0.11431735754013062,0.10638219118118286,0.10302180051803589,0.12129354476928711,0.07434862852096558],"type":"scatter"}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"DISPARITY IN FAIRNESS","y":0.9,"x":0.5,"xanchor":"center","yanchor":"top"},"legend":{"x":0.1,"y":1.1,"orientation":"h"},"font":{"family":"Helvetica, Arial, sans-serif","size":12,"color":"black"},"margin":{"l":40,"r":40,"t":40,"b":30},"xaxis":{"title":{"text":"EPOCH"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"yaxis":{"title":{"text":"FAIRNESS"},"showline":true,"linewidth":2,"linecolor":"black","gridcolor":"LightGrey"},"plot_bgcolor":"white"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ff727ae7-18e5-4dcb-882e-057dadc99e7b');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/srinivassundar98\.github\.io\/Ensuring-Fair-Play\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>